---
comments: true
description: Compare YOLOv7 and RTDETRv2 for object detection. Explore architecture, performance, and use cases to pick the best model for your project.
keywords: YOLOv7, RTDETRv2, model comparison, object detection, computer vision, machine learning, real-time detection, AI models, Vision Transformers
---

# YOLOv7 vs RTDETRv2: A Detailed Model Comparison

Choosing the right object detection model is crucial for computer vision projects. This page provides a technical comparison between YOLOv7 and RTDETRv2, two state-of-the-art models, to help you make an informed decision. We delve into their architectural differences, performance metrics, and ideal applications.

<script async src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<script defer src="../../javascript/benchmark.js"></script>

<canvas id="modelComparisonChart" width="1024" height="400" active-models='["YOLOv7", "RTDETRv2"]'></canvas>

| Model      | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |
| ---------- | --------------------- | -------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |
| YOLOv7l    | 640                   | 51.4                 | -                              | 6.84                                | 36.9               | 104.7             |
| YOLOv7x    | 640                   | 53.1                 | -                              | 11.57                               | 71.3               | 189.9             |
|            |                       |                      |                                |                                     |                    |                   |
| RTDETRv2-s | 640                   | 48.1                 | -                              | 5.03                                | 20                 | 60                |
| RTDETRv2-m | 640                   | 51.9                 | -                              | 7.51                                | 36                 | 100               |
| RTDETRv2-l | 640                   | 53.4                 | -                              | 9.76                                | 42                 | 136               |
| RTDETRv2-x | 640                   | 54.3                 | -                              | 15.03                               | 76                 | 259               |

## YOLOv7: The Real-time Efficiency Expert

YOLOv7, introduced in July 2022 by authors Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao from the Institute of Information Science, Academia Sinica, Taiwan, is celebrated for its **speed and efficiency** in object detection tasks. It refines the architecture of previous YOLO models, prioritizing rapid inference without significantly sacrificing accuracy.

### Architecture and Key Features

YOLOv7's architecture is built upon Convolutional Neural Networks (CNNs) and incorporates several key features for optimized performance:

- **E-ELAN (Extended Efficient Layer Aggregation Network):** Enhances feature extraction efficiency, allowing the model to learn more effectively.
- **Model Scaling:** Employs compound scaling techniques to adjust model depth and width, enabling flexibility for different computational resources and performance needs.
- **Auxiliary Head Training:** Uses auxiliary loss heads during training to deepen network learning and improve overall accuracy.

These architectural choices enable YOLOv7 to achieve a strong balance between speed and accuracy, making it suitable for real-time applications. For more details, refer to the [YOLOv7 paper on Arxiv](https://arxiv.org/abs/2207.02696) and the [official YOLOv7 GitHub repository](https://github.com/WongKinYiu/yolov7).

### Performance Metrics

YOLOv7 is designed to excel in scenarios where low latency is critical. Its performance is characterized by:

- **mAPval50-95**: Achieves up to 53.1% mAP on the COCO dataset.
- **Inference Speed (T4 TensorRT10)**: As fast as 6.84 ms, enabling real-time processing.
- **Model Size (parameters)**: Starts at 36.9M parameters, offering a compact model size for efficient deployment.

### Use Cases and Strengths

YOLOv7 is particularly well-suited for applications requiring **real-time object detection** on resource-constrained devices, including:

- **Robotics:** Providing fast perception for robotic navigation and interaction.
- **Surveillance:** Enabling real-time monitoring and analysis in security systems. See how YOLOv8 can enhance [security alarm systems](https://www.ultralytics.com/blog/security-alarm-system-projects-with-ultralytics-yolov8).
- **Edge Devices:** Deployment on edge devices with limited computational power, such as [NVIDIA Jetson](https://docs.ultralytics.com/guides/nvidia-jetson/) or [Raspberry Pi](https://docs.ultralytics.com/guides/raspberry-pi/).

Its primary strength is its speed and relatively small model size, making it highly deployable across various hardware platforms. Explore more about YOLOv7's architecture and capabilities in the [YOLOv7 Docs](https://docs.ultralytics.com/models/yolov7/).

[Learn more about YOLOv7](https://docs.ultralytics.com/models/yolov7/){ .md-button }

## RTDETRv2: Accuracy with Transformer Efficiency

RTDETRv2 (Real-Time Detection Transformer version 2), introduced in July 2024 by authors Wenyu Lv, Yian Zhao, Qinyao Chang, Kui Huang, Guanzhong Wang, and Yi Liu from Baidu, takes a different approach by integrating **Vision Transformers (ViT)** for object detection. Unlike YOLO's CNN foundation, RTDETRv2 leverages transformers to capture global image context, potentially leading to higher accuracy, while maintaining real-time performance.

### Architecture and Key Features

RTDETRv2's architecture is defined by:

- **Vision Transformer (ViT) Backbone:** Employs a transformer encoder to process the entire image, capturing long-range dependencies crucial for understanding complex scenes.
- **Hybrid CNN Feature Extraction:** Combines CNNs for initial feature extraction with transformer layers to integrate global context effectively.
- **Anchor-Free Detection:** Simplifies the detection process by removing the need for predefined anchor boxes, enhancing model flexibility and reducing complexity.

This transformer-based design allows RTDETRv2 to potentially achieve superior accuracy, especially in intricate and cluttered environments. Learn more about Vision Transformers from our [Vision Transformer (ViT) glossary](https://www.ultralytics.com/glossary/vision-transformer-vit) page. The [RTDETRv2 paper is available on Arxiv](https://arxiv.org/abs/2407.17140) and the [official GitHub repository](https://github.com/lyuwenyu/RT-DETR/tree/main/rtdetrv2_pytorch) provides implementation details.

### Performance Metrics

RTDETRv2 prioritizes accuracy while maintaining competitive speed, offering the following performance metrics:

- **mAPval50-95**: Achieves up to 54.3% mAPval50-95, demonstrating high accuracy in object detection.
- **Inference Speed (T4 TensorRT10)**: Starts from 5.03 ms, ensuring real-time capability on suitable hardware.
- **Model Size (parameters)**: Begins at 20M parameters, offering a range of model sizes for different deployment needs.

### Use Cases and Strengths

RTDETRv2 is ideally suited for applications where **high accuracy is paramount**, and computational resources are available:

- **Autonomous Vehicles:** Providing reliable and precise environmental perception for safe navigation. Explore [AI in self-driving cars](https://www.ultralytics.com/solutions/ai-in-self-driving) for related applications.
- **Medical Imaging:** Enabling precise anomaly detection in medical images to aid in diagnostics and treatment planning. Discover more about [AI in Healthcare](https://www.ultralytics.com/solutions/ai-in-healthcare) applications.
- **High-Resolution Image Analysis:** Tasks requiring detailed analysis of large images, such as [satellite imagery analysis](https://www.ultralytics.com/blog/using-computer-vision-to-analyse-satellite-imagery) or industrial inspection.

RTDETRv2's strength lies in its transformer architecture, which facilitates robust feature extraction and higher accuracy, making it excellent for complex detection tasks. Further details are available in the [RT-DETR GitHub README](https://github.com/lyuwenyu/RT-DETR/tree/main/rtdetrv2_pytorch#readme).

[Learn more about RTDETRv2](https://docs.ultralytics.com/models/rtdetr/){ .md-button }

## Conclusion

Both YOLOv7 and RTDETRv2 are powerful object detection models, each with unique strengths. YOLOv7 excels in real-time applications requiring speed and efficiency, while RTDETRv2 prioritizes accuracy through its transformer-based architecture. Your choice should align with your project's specific requirementsâ€”speed for time-sensitive tasks or accuracy for detailed analysis.

For other comparisons and models, you might also be interested in:

- [YOLOv8 vs RTDETRv2](https://docs.ultralytics.com/compare/yolov8-vs-rtdetr/)
- [YOLOv5 vs RT-DETR v2](https://docs.ultralytics.com/compare/yolov5-vs-rtdetr/)
- [YOLOv8](https://docs.ultralytics.com/models/yolov8/)
- [YOLOv5](https://docs.ultralytics.com/models/yolov5/)
- [RT-DETR](https://docs.ultralytics.com/models/rtdetr/)
- [YOLO11](https://docs.ultralytics.com/models/yolo11/)
