 <!DOCTYPE html><html class="no-js" lang="en"><head><meta charset="utf-8"/><meta content="width=device-width,initial-scale=1" name="viewport"/><meta content="Discover SAM 3, Meta's next evolution of the Segment Anything Model, introducing Promptable Concept Segmentation with text and image exemplar prompts for detecting all instances of visual concepts across images and videos." name="description"/><meta content="Ultralytics" name="author"/><link href="https://docs.ultralytics.com/models/sam-3/" rel="canonical"/><link href="../sam-2/" rel="prev"/><link href="../mobile-sam/" rel="next"/><link href="/" hreflang="en" rel="alternate"/><link href="/zh/" hreflang="zh" rel="alternate"/><link href="/ko/" hreflang="ko" rel="alternate"/><link href="/ja/" hreflang="ja" rel="alternate"/><link href="/ru/" hreflang="ru" rel="alternate"/><link href="/de/" hreflang="de" rel="alternate"/><link href="/fr/" hreflang="fr" rel="alternate"/><link href="/es/" hreflang="es" rel="alternate"/><link href="/pt/" hreflang="pt" rel="alternate"/><link href="/it/" hreflang="it" rel="alternate"/><link href="/tr/" hreflang="tr" rel="alternate"/><link href="/vi/" hreflang="vi" rel="alternate"/><link href="/ar/" hreflang="ar" rel="alternate"/><link href="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/logo/favicon-yolo.png" rel="icon"/><meta content="zensical-0.0.11" name="generator"/><title>SAM 3: Segment Anything with Concepts - Ultralytics YOLO Docs</title><link href="../../assets/stylesheets/modern/main.bd6182e7.min.css" rel="stylesheet"/><link href="../../assets/stylesheets/modern/palette.dfe2e883.min.css" rel="stylesheet"/><style>:root{}</style><link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/><link href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,500,500i,700,700i%7CJetBrains+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/><style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style><link href="../../stylesheets/style.css" rel="stylesheet"/>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,t)=>(e<<5)-e+t.charCodeAt(0)),0),__md_get=(e,t=localStorage,a=__md_scope)=>JSON.parse(t.getItem(a.pathname+"."+e)),__md_set=(e,t,a=localStorage,_=__md_scope)=>{try{a.setItem(_.pathname+"."+e,JSON.stringify(t))}catch(e){}},document.documentElement.setAttribute("data-platform",navigator.platform)</script>
<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-2M5EHKC0BH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-2M5EHKC0BH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-2M5EHKC0BH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
<style data-doc-kind="true">.doc-kind{display:inline-flex;align-items:center;gap:0.25em;padding:0.21em 0.59em;border-radius:999px;font-weight:700;font-size:0.81em;letter-spacing:0.06em;text-transform:uppercase;line-height:1;color:var(--doc-kind-color,#f8fafc);background:var(--doc-kind-bg,rgba(255,255,255,0.12));}.doc-kind-class{--doc-kind-color:#039dfc;--doc-kind-bg:rgba(3,157,252,0.22);}.doc-kind-function{--doc-kind-color:#fc9803;--doc-kind-bg:rgba(252,152,3,0.22);}.doc-kind-method{--doc-kind-color:#ef5eff;--doc-kind-bg:rgba(239,94,255,0.22);}.doc-kind-property{--doc-kind-color:#02e835;--doc-kind-bg:rgba(2,232,53,0.22);}</style><meta content="SAM 3: Segment Anything with Concepts" name="title"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet"/><meta content="website" property="og:type"/><meta content="https://docs.ultralytics.com/models/sam-3/" property="og:url"/><meta content="SAM 3: Segment Anything with Concepts" property="og:title"/><meta content="" property="og:description"/><meta content="https://github.com/ultralytics/docs/releases/download/0/sam-3-overview.webp" property="og:image"/><meta content="summary_large_image" property="twitter:card"/><meta content="https://docs.ultralytics.com/models/sam-3/" property="twitter:url"/><meta content="SAM 3: Segment Anything with Concepts" property="twitter:title"/><meta content="" property="twitter:description"/><meta content="https://github.com/ultralytics/docs/releases/download/0/sam-3-overview.webp" property="twitter:image"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": ["Article", "FAQPage"], "headline": "SAM 3: Segment Anything with Concepts", "image": ["https://github.com/ultralytics/docs/releases/download/0/sam-3-overview.webp"], "datePublished": "2025-10-13 20:10:17 +0200", "dateModified": "2025-11-24 17:09:37 +0100", "author": [{"@type": "Organization", "name": "Ultralytics", "url": "https://ultralytics.com/"}], "abstract": "", "mainEntity": [{"@type": "Question", "name": "When Will SAM 3 Be Released?", "acceptedAnswer": {"@type": "Answer", "text": "SAM 3 was released by Meta on November 20th, 2025. Ultralytics support is in progress and will ship in an upcoming package update with full docs for predict mode and track mode."}}, {"@type": "Question", "name": "Will SAM 3 Be Integrated Into Ultralytics?", "acceptedAnswer": {"@type": "Answer", "text": "Yes. SAM 3 will be supported in the Ultralytics Python package upon release, including concept segmentation, SAM 2\u2013style visual prompts, and multi-object video tracking. You will be able to export to formats like ONNX and TensorRT for deployment, with streamlined Python and CLI workflows."}}, {"@type": "Question", "name": "What Is Promptable Concept Segmentation (PCS)?", "acceptedAnswer": {"@type": "Answer", "text": "PCS is a new task introduced in SAM 3 that segments all instances of a visual concept in an image or video. Unlike traditional segmentation that targets a specific object instance, PCS finds every occurrence of a category. For example: See related background on object detection and instance segmentation."}}, {"@type": "Question", "name": "How Does SAM 3 Differ From SAM 2?", "acceptedAnswer": {"@type": "Answer", "text": "SAM 3 maintains backward compatibility with SAM 2 visual prompting while adding concept-based capabilities."}}, {"@type": "Question", "name": "What datasets are used to train SAM 3?", "acceptedAnswer": {"@type": "Answer", "text": "SAM 3 is trained on the Segment Anything with Concepts (SA-Co) dataset: Training Data: Benchmark Data: This massive scale and diversity enables SAM 3's superior zero-shot generalization across open-vocabulary concepts."}}, {"@type": "Question", "name": "How does SAM 3 compare to YOLO11 for segmentation?", "acceptedAnswer": {"@type": "Answer", "text": "SAM 3 and YOLO11 serve different use cases: SAM 3 Advantages: YOLO11 Advantages: Recommendation:"}}, {"@type": "Question", "name": "Can SAM 3 handle complex language queries?", "acceptedAnswer": {"@type": "Answer", "text": "SAM 3 is designed for simple noun phrases (e.g., \"red apple\", \"person wearing hat\"). For complex queries requiring reasoning, combine SAM 3 with an MLLM as SAM 3 Agent: Simple queries (native SAM 3): Complex queries (SAM 3 Agent with MLLM): SAM 3 Agent achieves 76.0 gIoU on ReasonSeg validation (vs 65.0 previous best, +16.9% improvement) by combining SAM 3's segmentation with MLLM reasoning capabilities."}}, {"@type": "Question", "name": "How accurate is SAM 3 compared to human performance?", "acceptedAnswer": {"@type": "Answer", "text": "On the SA-Co/Gold benchmark with triple human annotation: SAM 3 achieves strong performance approaching human-level accuracy on open-vocabulary concept segmentation, with the gap primarily on ambiguous or subjective concepts (e.g., \"small window\", \"cozy room\")."}}]}</script></head><body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="default" dir="ltr"><input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/><input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/><label class="md-overlay" for="__drawer"></label><div data-md-component="skip"><a class="md-skip" href="#sam-3-segment-anything-with-concepts"> Skip to content </a></div><div data-md-component="announce"><aside class="md-banner"><div class="md-banner__inner md-grid md-typeset"><a class="banner-wrapper" href="https://www.ultralytics.com/news/ultralytics-raises-30m-series-a" target="_blank"><div class="banner-content-wrapper"><img alt="Ultralytics raises $30M Series A" height="40" src="https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/691dac336b5de2ae8b398bca_writting.svg"/><div class="vc-wrapper"><img alt="Elephant" height="28" src="https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/691dac33c95408144846afc7_image%201.png"/><img alt="SquareOne" height="28" src="https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/691dac3333068d9632cc6df8_image%202.png"/></div></div></a></div></aside></div><header class="md-header md-header--shadow md-header--lifted" data-md-component="header"><nav aria-label="Header" class="md-header__inner md-grid"><a aria-label="Ultralytics YOLO Docs" class="md-header__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs"><img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/></a><label class="md-header__button md-icon" for="__drawer"><svg class="lucide lucide-menu" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 5h16M4 12h16M4 19h16"></path></svg></label><div class="md-header__title" data-md-component="header-title"><div class="md-header__ellipsis"><div class="md-header__topic"><span class="md-ellipsis"> Ultralytics YOLO Docs </span></div><div class="md-header__topic" data-md-component="header-topic"><span class="md-ellipsis"> SAM 3: Segment Anything with Concepts </span></div></div></div><form class="md-header__option" data-md-component="palette"><input aria-label="Switch to light mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_0" name="__palette" type="radio"/><label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg></label><input aria-label="Switch to system preference" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="black" data-md-color-scheme="slate" id="__palette_1" name="__palette" type="radio"/><label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to system preference"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label><input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_2" name="__palette" type="radio"/><label class="md-header__button md-icon" for="__palette_0" hidden="" title="Switch to dark mode"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label></form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<div class="md-header__option"><div class="md-select"><button aria-label="Select language" class="md-header__button md-icon"><svg class="lucide lucide-languages" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m5 8 6 6M4 14l6-6 2-3M2 5h12M7 2h1M22 22l-5-10-5 10M14 18h6"></path></svg></button><div class="md-select__inner"><ul class="md-select__list"><li class="md-select__item"><a class="md-select__link" href="/" hreflang="en"> üá¨üáß English </a></li><li class="md-select__item"><a class="md-select__link" href="/zh/" hreflang="zh"> üá®üá≥ ÁÆÄ‰Ωì‰∏≠Êñá </a></li><li class="md-select__item"><a class="md-select__link" href="/ko/" hreflang="ko"> üá∞üá∑ ÌïúÍµ≠Ïñ¥ </a></li><li class="md-select__item"><a class="md-select__link" href="/ja/" hreflang="ja"> üáØüáµ Êó•Êú¨Ë™û </a></li><li class="md-select__item"><a class="md-select__link" href="/ru/" hreflang="ru"> üá∑üá∫ –†—É—Å—Å–∫–∏–π </a></li><li class="md-select__item"><a class="md-select__link" href="/de/" hreflang="de"> üá©üá™ Deutsch </a></li><li class="md-select__item"><a class="md-select__link" href="/fr/" hreflang="fr"> üá´üá∑ Fran√ßais </a></li><li class="md-select__item"><a class="md-select__link" href="/es/" hreflang="es"> üá™üá∏ Espa√±ol </a></li><li class="md-select__item"><a class="md-select__link" href="/pt/" hreflang="pt"> üáµüáπ Portugu√™s </a></li><li class="md-select__item"><a class="md-select__link" href="/it/" hreflang="it"> üáÆüáπ Italiano </a></li><li class="md-select__item"><a class="md-select__link" href="/tr/" hreflang="tr"> üáπüá∑ T√ºrk√ße </a></li><li class="md-select__item"><a class="md-select__link" href="/vi/" hreflang="vi"> üáªüá≥ Ti·∫øng Vi·ªát </a></li><li class="md-select__item"><a class="md-select__link" href="/ar/" hreflang="ar"> üá∏üá¶ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© </a></li></ul></div></div></div><div class="md-header__source"><a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository"><div class="md-source__icon md-icon"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg></div><div class="md-source__repository"> ultralytics/ultralytics </div></a></div></nav><nav aria-label="Tabs" class="md-tabs" data-md-component="tabs"><div class="md-grid"><ul class="md-tabs__list"><li class="md-tabs__item"><a class="md-tabs__link" href="../.."> Home </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../quickstart/"> Quickstart </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../modes/"> Modes </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../tasks/"> Tasks </a></li><li class="md-tabs__item md-tabs__item--active"><a class="md-tabs__link" href="../"> Models </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../datasets/"> Datasets </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../solutions/"> Solutions </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../guides/"> Guides </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../integrations/"> Integrations </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../hub/"> HUB </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../reference/__init__/"> Reference </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../help/"> Help </a></li></ul></div></nav></header><div class="md-container" data-md-component="container"><main class="md-main" data-md-component="main"><div class="md-main__inner md-grid"><div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0"><label class="md-nav__title" for="__drawer"><a aria-label="Ultralytics YOLO Docs" class="md-nav__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs"><img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/></a> Ultralytics YOLO Docs </label><div class="md-nav__source"><a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository"><div class="md-source__icon md-icon"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg></div><div class="md-source__repository"> ultralytics/ultralytics </div></a></div><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../.."><span class="md-ellipsis"> Home </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../quickstart/"><span class="md-ellipsis"> Quickstart </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../modes/"><span class="md-ellipsis"> Modes </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../tasks/"><span class="md-ellipsis"> Tasks </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"><input checked="" class="md-nav__toggle md-toggle" id="__nav_5" type="checkbox"/><div class="md-nav__link md-nav__container"><a class="md-nav__link" href="../"><span class="md-ellipsis"> Models </span></a><label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex=""><span class="md-nav__icon md-icon"></span></label></div><nav aria-expanded="true" aria-labelledby="__nav_5_label" class="md-nav" data-md-level="1"><label class="md-nav__title" for="__nav_5"><span class="md-nav__icon md-icon"></span> Models </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../yolov3/"><span class="md-ellipsis"> YOLOv3 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../yolov4/"><span class="md-ellipsis"> YOLOv4 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../yolov5/"><span class="md-ellipsis"> YOLOv5 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../yolov6/"><span class="md-ellipsis"> YOLOv6 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../yolov7/"><span class="md-ellipsis"> YOLOv7 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../yolov8/"><span class="md-ellipsis"> YOLOv8 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../yolov9/"><span class="md-ellipsis"> YOLOv9 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../yolov10/"><span class="md-ellipsis"> YOLOv10 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../yolo11/"><span class="md-ellipsis"> YOLO11 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../yolo12/"><span class="md-ellipsis"> YOLO12 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../yolo26/"><span class="md-ellipsis"> YOLO26 üöÄ Coming soon </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../sam/"><span class="md-ellipsis"> SAM (Segment Anything Model) </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../sam-2/"><span class="md-ellipsis"> SAM 2 (Segment Anything Model 2) </span></a></li><li class="md-nav__item md-nav__item--active"><input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/><label class="md-nav__link md-nav__link--active" for="__toc"><span class="md-ellipsis"> SAM 3 (Segment Anything Model 3) üöÄ Coming soon </span><span class="md-nav__icon md-icon"></span></label><a class="md-nav__link md-nav__link--active" href="./"><span class="md-ellipsis"> SAM 3 (Segment Anything Model 3) üöÄ Coming soon </span></a><nav aria-label="On this page" class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc"><span class="md-nav__icon md-icon"></span> On this page </label><ul class="md-nav__list" data-md-component="toc" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="#overview"><span class="md-ellipsis"> Overview </span></a><nav aria-label="Overview" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#what-is-promptable-concept-segmentation-pcs"><span class="md-ellipsis"> What is Promptable Concept Segmentation (PCS)? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#key-performance-metrics"><span class="md-ellipsis"> Key Performance Metrics </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#architecture"><span class="md-ellipsis"> Architecture </span></a><nav aria-label="Architecture" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#core-components"><span class="md-ellipsis"> Core Components </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#key-innovations"><span class="md-ellipsis"> Key Innovations </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#sa-co-dataset"><span class="md-ellipsis"> SA-Co Dataset </span></a><nav aria-label="SA-Co Dataset" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#training-data"><span class="md-ellipsis"> Training Data </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#benchmark-data"><span class="md-ellipsis"> Benchmark Data </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#data-engine-innovations"><span class="md-ellipsis"> Data Engine Innovations </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#installation"><span class="md-ellipsis"> Installation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#how-to-use-sam-3-versatility-in-concept-segmentation"><span class="md-ellipsis"> How to Use SAM 3: Versatility in Concept Segmentation </span></a><nav aria-label="How to Use SAM 3: Versatility in Concept Segmentation" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#supported-tasks-and-models"><span class="md-ellipsis"> Supported Tasks and Models </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#concept-segmentation-examples"><span class="md-ellipsis"> Concept Segmentation Examples </span></a><nav aria-label="Concept Segmentation Examples" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#segment-with-text-prompts"><span class="md-ellipsis"> Segment with Text Prompts </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#segment-with-image-exemplars"><span class="md-ellipsis"> Segment with Image Exemplars </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#interactive-refinement"><span class="md-ellipsis"> Interactive Refinement </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#video-concept-segmentation"><span class="md-ellipsis"> Video Concept Segmentation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#visual-prompts-sam-2-compatibility"><span class="md-ellipsis"> Visual Prompts (SAM 2 Compatibility) </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#performance-benchmarks"><span class="md-ellipsis"> Performance Benchmarks </span></a><nav aria-label="Performance Benchmarks" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#image-segmentation"><span class="md-ellipsis"> Image Segmentation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#video-segmentation-performance"><span class="md-ellipsis"> Video Segmentation Performance </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#few-shot-adaptation"><span class="md-ellipsis"> Few-Shot Adaptation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#interactive-refinement-effectiveness"><span class="md-ellipsis"> Interactive Refinement Effectiveness </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#object-counting-accuracy"><span class="md-ellipsis"> Object Counting Accuracy </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#sam-3-vs-sam-2-vs-yolo-comparison"><span class="md-ellipsis"> SAM 3 vs SAM 2 vs YOLO Comparison </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#evaluation-metrics"><span class="md-ellipsis"> Evaluation Metrics </span></a><nav aria-label="Evaluation Metrics" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#classification-gated-f1-cgf1"><span class="md-ellipsis"> Classification-Gated F1 (CGF1) </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#why-these-metrics"><span class="md-ellipsis"> Why These Metrics? </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#key-ablations-and-insights"><span class="md-ellipsis"> Key Ablations and Insights </span></a><nav aria-label="Key Ablations and Insights" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#impact-of-presence-head"><span class="md-ellipsis"> Impact of Presence Head </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#effect-of-hard-negatives"><span class="md-ellipsis"> Effect of Hard Negatives </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#training-data-scaling"><span class="md-ellipsis"> Training Data Scaling </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#applications"><span class="md-ellipsis"> Applications </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#sam-3-agent-extended-language-reasoning"><span class="md-ellipsis"> SAM 3 Agent: Extended Language Reasoning </span></a><nav aria-label="SAM 3 Agent: Extended Language Reasoning" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#performance-on-reasoning-tasks"><span class="md-ellipsis"> Performance on Reasoning Tasks </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#example-complex-queries"><span class="md-ellipsis"> Example Complex Queries </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#limitations"><span class="md-ellipsis"> Limitations </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#citation"><span class="md-ellipsis"> Citation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#faq"><span class="md-ellipsis"> FAQ </span></a><nav aria-label="FAQ" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#when-will-sam-3-be-released"><span class="md-ellipsis"> When Will SAM 3 Be Released? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#will-sam-3-be-integrated-into-ultralytics"><span class="md-ellipsis"> Will SAM 3 Be Integrated Into Ultralytics? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#what-is-promptable-concept-segmentation-pcs_1"><span class="md-ellipsis"> What Is Promptable Concept Segmentation (PCS)? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#how-does-sam-3-differ-from-sam-2"><span class="md-ellipsis"> How Does SAM 3 Differ From SAM 2? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#what-datasets-are-used-to-train-sam-3"><span class="md-ellipsis"> What datasets are used to train SAM 3? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#how-does-sam-3-compare-to-yolo11-for-segmentation"><span class="md-ellipsis"> How does SAM 3 compare to YOLO11 for segmentation? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#can-sam-3-handle-complex-language-queries"><span class="md-ellipsis"> Can SAM 3 handle complex language queries? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#how-accurate-is-sam-3-compared-to-human-performance"><span class="md-ellipsis"> How accurate is SAM 3 compared to human performance? </span></a></li></ul></nav></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="../mobile-sam/"><span class="md-ellipsis"> MobileSAM (Mobile Segment Anything Model) </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../fast-sam/"><span class="md-ellipsis"> FastSAM (Fast Segment Anything Model) </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../yolo-nas/"><span class="md-ellipsis"> YOLO-NAS (Neural Architecture Search) </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../rtdetr/"><span class="md-ellipsis"> RT-DETR (Realtime Detection Transformer) </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../yolo-world/"><span class="md-ellipsis"> YOLO-World (Real-Time Open-Vocabulary Object Detection) </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../yoloe/"><span class="md-ellipsis"> YOLOE (Real-Time Seeing Anything) </span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../datasets/"><span class="md-ellipsis"> Datasets </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../solutions/"><span class="md-ellipsis"> Solutions </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../guides/"><span class="md-ellipsis"> Guides </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../integrations/"><span class="md-ellipsis"> Integrations </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../hub/"><span class="md-ellipsis"> HUB </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../reference/__init__/"><span class="md-ellipsis"> Reference </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../help/"><span class="md-ellipsis"> Help </span><span class="md-nav__icon md-icon"></span></a></li></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav aria-label="On this page" class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc"><span class="md-nav__icon md-icon"></span> On this page </label><ul class="md-nav__list" data-md-component="toc" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="#overview"><span class="md-ellipsis"> Overview </span></a><nav aria-label="Overview" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#what-is-promptable-concept-segmentation-pcs"><span class="md-ellipsis"> What is Promptable Concept Segmentation (PCS)? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#key-performance-metrics"><span class="md-ellipsis"> Key Performance Metrics </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#architecture"><span class="md-ellipsis"> Architecture </span></a><nav aria-label="Architecture" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#core-components"><span class="md-ellipsis"> Core Components </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#key-innovations"><span class="md-ellipsis"> Key Innovations </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#sa-co-dataset"><span class="md-ellipsis"> SA-Co Dataset </span></a><nav aria-label="SA-Co Dataset" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#training-data"><span class="md-ellipsis"> Training Data </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#benchmark-data"><span class="md-ellipsis"> Benchmark Data </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#data-engine-innovations"><span class="md-ellipsis"> Data Engine Innovations </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#installation"><span class="md-ellipsis"> Installation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#how-to-use-sam-3-versatility-in-concept-segmentation"><span class="md-ellipsis"> How to Use SAM 3: Versatility in Concept Segmentation </span></a><nav aria-label="How to Use SAM 3: Versatility in Concept Segmentation" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#supported-tasks-and-models"><span class="md-ellipsis"> Supported Tasks and Models </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#concept-segmentation-examples"><span class="md-ellipsis"> Concept Segmentation Examples </span></a><nav aria-label="Concept Segmentation Examples" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#segment-with-text-prompts"><span class="md-ellipsis"> Segment with Text Prompts </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#segment-with-image-exemplars"><span class="md-ellipsis"> Segment with Image Exemplars </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#interactive-refinement"><span class="md-ellipsis"> Interactive Refinement </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#video-concept-segmentation"><span class="md-ellipsis"> Video Concept Segmentation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#visual-prompts-sam-2-compatibility"><span class="md-ellipsis"> Visual Prompts (SAM 2 Compatibility) </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#performance-benchmarks"><span class="md-ellipsis"> Performance Benchmarks </span></a><nav aria-label="Performance Benchmarks" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#image-segmentation"><span class="md-ellipsis"> Image Segmentation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#video-segmentation-performance"><span class="md-ellipsis"> Video Segmentation Performance </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#few-shot-adaptation"><span class="md-ellipsis"> Few-Shot Adaptation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#interactive-refinement-effectiveness"><span class="md-ellipsis"> Interactive Refinement Effectiveness </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#object-counting-accuracy"><span class="md-ellipsis"> Object Counting Accuracy </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#sam-3-vs-sam-2-vs-yolo-comparison"><span class="md-ellipsis"> SAM 3 vs SAM 2 vs YOLO Comparison </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#evaluation-metrics"><span class="md-ellipsis"> Evaluation Metrics </span></a><nav aria-label="Evaluation Metrics" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#classification-gated-f1-cgf1"><span class="md-ellipsis"> Classification-Gated F1 (CGF1) </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#why-these-metrics"><span class="md-ellipsis"> Why These Metrics? </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#key-ablations-and-insights"><span class="md-ellipsis"> Key Ablations and Insights </span></a><nav aria-label="Key Ablations and Insights" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#impact-of-presence-head"><span class="md-ellipsis"> Impact of Presence Head </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#effect-of-hard-negatives"><span class="md-ellipsis"> Effect of Hard Negatives </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#training-data-scaling"><span class="md-ellipsis"> Training Data Scaling </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#applications"><span class="md-ellipsis"> Applications </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#sam-3-agent-extended-language-reasoning"><span class="md-ellipsis"> SAM 3 Agent: Extended Language Reasoning </span></a><nav aria-label="SAM 3 Agent: Extended Language Reasoning" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#performance-on-reasoning-tasks"><span class="md-ellipsis"> Performance on Reasoning Tasks </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#example-complex-queries"><span class="md-ellipsis"> Example Complex Queries </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#limitations"><span class="md-ellipsis"> Limitations </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#citation"><span class="md-ellipsis"> Citation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#faq"><span class="md-ellipsis"> FAQ </span></a><nav aria-label="FAQ" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#when-will-sam-3-be-released"><span class="md-ellipsis"> When Will SAM 3 Be Released? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#will-sam-3-be-integrated-into-ultralytics"><span class="md-ellipsis"> Will SAM 3 Be Integrated Into Ultralytics? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#what-is-promptable-concept-segmentation-pcs_1"><span class="md-ellipsis"> What Is Promptable Concept Segmentation (PCS)? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#how-does-sam-3-differ-from-sam-2"><span class="md-ellipsis"> How Does SAM 3 Differ From SAM 2? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#what-datasets-are-used-to-train-sam-3"><span class="md-ellipsis"> What datasets are used to train SAM 3? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#how-does-sam-3-compare-to-yolo11-for-segmentation"><span class="md-ellipsis"> How does SAM 3 compare to YOLO11 for segmentation? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#can-sam-3-handle-complex-language-queries"><span class="md-ellipsis"> Can SAM 3 handle complex language queries? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#how-accurate-is-sam-3-compared-to-human-performance"><span class="md-ellipsis"> How accurate is SAM 3 compared to human performance? </span></a></li></ul></nav></li></ul></nav></div></div></div><div class="md-content" data-md-component="content"><article class="md-content__inner md-typeset"><a class="md-content__button md-icon" href="https://github.com/ultralytics/ultralytics/tree/main/docs/en/models/sam-3.md" rel="edit" title="Edit this page"><svg class="lucide lucide-file-pen" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12.5 22H18a2 2 0 0 0 2-2V7l-5-5H6a2 2 0 0 0-2 2v9.5"></path><path d="M14 2v4a2 2 0 0 0 2 2h4M13.378 15.626a1 1 0 1 0-3.004-3.004l-5.01 5.012a2 2 0 0 0-.506.854l-.837 2.87a.5.5 0 0 0 .62.62l2.87-.837a2 2 0 0 0 .854-.506z"></path></svg></a><a class="md-content__button md-icon" href="javascript:void(0)" onclick="copyMarkdownForLLM(this); return false;" title="Copy page in Markdown format"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg></a><h1 id="sam-3-segment-anything-with-concepts">SAM 3: Segment Anything with Concepts</h1><div class="admonition note"><p class="admonition-title">Released ‚Äî Ultralytics integration in progress</p><p>Meta released SAM-3 on <strong>November 20th, 2025</strong>. Ultralytics is integrating the models now and will ship a package update with native support soon. In the meantime, you can follow the official SAM 3 README steps below to try the upstream release.</p></div><p><img alt="SAM 3 Overview" src="https://github.com/ultralytics/docs/releases/download/0/sam-3-overview.webp"/></p><p><strong>SAM 3</strong> (Segment Anything Model 3) is Meta's released foundation model for <strong>Promptable Concept Segmentation (PCS)</strong>. Building upon <a href="../sam-2/">SAM 2</a>, SAM 3 introduces a fundamentally new capability: detecting, segmenting, and tracking <strong>all instances</strong> of a visual concept specified by text prompts, image exemplars, or both. Unlike previous SAM versions that segment single objects per prompt, SAM 3 can find and segment every occurrence of a concept appearing anywhere in images or videos, aligning with open-vocabulary goals in modern <a href="https://www.ultralytics.com/glossary/instance-segmentation">instance segmentation</a>.</p><p>Ultralytics is actively integrating SAM-3 into the <code>ultralytics</code> package. Until that release lands, you can experiment with the upstream Meta implementation using the official installation and usage steps below.</p><h2 id="overview">Overview</h2><p>SAM 3 achieves a <strong>2√ó performance gain</strong> over existing systems in Promptable Concept Segmentation while maintaining and improving SAM 2's capabilities for interactive <a href="../../tasks/segment/">visual segmentation</a>. The model excels at open-vocabulary segmentation, allowing users to specify concepts using simple noun phrases (e.g., "yellow school bus", "striped cat") or by providing example images of the target object. These capabilities complement production-ready pipelines that rely on streamlined <a href="../../modes/predict/">predict</a> and <a href="../../modes/track/">track</a> workflows.</p><p><img alt="SAM 3 Segmentation" src="https://github.com/ultralytics/docs/releases/download/0/sam-3-segmentation.webp"/></p><h3 id="what-is-promptable-concept-segmentation-pcs">What is Promptable Concept Segmentation (PCS)?</h3><p>The PCS task takes a <strong>concept prompt</strong> as input and returns segmentation masks with unique identities for <strong>all matching object instances</strong>. Concept prompts can be:</p><ul><li><strong>Text</strong>: Simple noun phrases like "red apple" or "person wearing a hat", similar to <a href="https://www.ultralytics.com/glossary/zero-shot-learning">zero-shot learning</a></li><li><strong>Image exemplars</strong>: Bounding boxes around example objects (positive or negative) for fast generalization</li><li><strong>Combined</strong>: Both text and image exemplars together for precise control</li></ul><p>This differs from traditional visual prompts (points, boxes, masks) which segment only a single specific object instance, as popularized by the original <a href="../../models/sam/">SAM family</a>.</p><h3 id="key-performance-metrics">Key Performance Metrics</h3><table><thead><tr><th>Metric</th><th>SAM 3 Achievement</th></tr></thead><tbody><tr><td><strong>LVIS Zero-Shot Mask AP</strong></td><td><strong>47.0</strong> (vs previous best 38.5, +22% improvement)</td></tr><tr><td><strong>SA-Co Benchmark</strong></td><td><strong>2√ó better</strong> than existing systems</td></tr><tr><td><strong>Inference Speed (H200 GPU)</strong></td><td><strong>30 ms</strong> per image with 100+ detected objects</td></tr><tr><td><strong>Video Performance</strong></td><td>Near real-time for ~5 concurrent objects</td></tr><tr><td><strong>MOSEv2 VOS Benchmark</strong></td><td><strong>60.1 J&amp;F</strong> (+25.5% over SAM 2.1, +17% over prior SOTA)</td></tr><tr><td><strong>Interactive Refinement</strong></td><td><strong>+18.6 CGF1</strong> improvement after 3 exemplar prompts</td></tr><tr><td><strong>Human Performance Gap</strong></td><td>Achieves <strong>88%</strong> of estimated lower bound on SA-Co/Gold</td></tr></tbody></table><p>For context on model metrics and trade-offs in production, see <a href="../../guides/model-evaluation-insights/">model evaluation insights</a> and <a href="../../guides/yolo-performance-metrics/">YOLO performance metrics</a>.</p><h2 id="architecture">Architecture</h2><p>SAM 3 consists of a <strong>detector</strong> and <strong>tracker</strong> that share a Perception Encoder (PE) vision backbone. This decoupled design avoids task conflicts while enabling both image-level detection and video-level tracking, with an interface compatible with Ultralytics <a href="../../usage/python/">Python usage</a> and <a href="../../usage/cli/">CLI usage</a>.</p><h3 id="core-components">Core Components</h3><ul><li><p><strong>Detector</strong>: <a href="../rtdetr/">DETR-based architecture</a> for image-level concept detection</p><ul><li>Text encoder for noun phrase prompts</li><li>Exemplar encoder for image-based prompts</li><li>Fusion encoder to condition image features on prompts</li><li>Novel <strong>presence head</strong> that decouples recognition ("what") from localization ("where")</li><li>Mask head for generating instance segmentation masks</li></ul></li><li><p><strong>Tracker</strong>: Memory-based video segmentation inherited from <a href="../sam-2/">SAM 2</a></p><ul><li>Prompt encoder, mask decoder, memory encoder</li><li>Memory bank for storing object appearance across frames</li><li>Temporal disambiguation aided by techniques like a <a href="../../reference/trackers/utils/kalman_filter/">Kalman filter</a> in multi-object settings</li></ul></li><li><p><strong>Presence Token</strong>: A learned global token that predicts whether the target concept is present in the image/frame, improving detection by separating recognition from localization.</p></li></ul><p><img alt="SAM 3 Architecture" src="https://github.com/ultralytics/docs/releases/download/0/sam-3-architecture.webp"/></p><h3 id="key-innovations">Key Innovations</h3><ol><li><strong>Decoupled Recognition and Localization</strong>: The presence head predicts concept presence globally, while proposal queries focus only on localization, avoiding conflicting objectives.</li><li><strong>Unified Concept and Visual Prompts</strong>: Supports both PCS (concept prompts) and PVS (visual prompts like SAM 2's clicks/boxes) in a single model.</li><li><strong>Interactive Exemplar Refinement</strong>: Users can add positive or negative image exemplars to iteratively refine results, with the model generalizing to similar objects rather than just correcting individual instances.</li><li><strong>Temporal Disambiguation</strong>: Uses masklet detection scores and periodic re-prompting to handle occlusions, crowded scenes, and tracking failures in video, aligning with <a href="../../guides/instance-segmentation-and-tracking/">instance segmentation and tracking</a> best practices.</li></ol><h2 id="sa-co-dataset">SA-Co Dataset</h2><p>SAM 3 is trained on <strong>Segment Anything with Concepts (SA-Co)</strong>, Meta's largest and most diverse segmentation dataset to date, expanding beyond common benchmarks like <a href="../../datasets/detect/coco/">COCO</a> and <a href="../../datasets/detect/lvis/">LVIS</a>.</p><h3 id="training-data">Training Data</h3><table><thead><tr><th>Dataset Component</th><th>Description</th><th>Scale</th></tr></thead><tbody><tr><td><strong>SA-Co/HQ</strong></td><td>High-quality human-annotated image data from 4-phase data engine</td><td>5.2M images, 4M unique noun phrases</td></tr><tr><td><strong>SA-Co/SYN</strong></td><td>Synthetic dataset labeled by AI without human involvement</td><td>38M noun phrases, 1.4B masks</td></tr><tr><td><strong>SA-Co/EXT</strong></td><td>15 external datasets enriched with hard negatives</td><td>Varies by source</td></tr><tr><td><strong>SA-Co/VIDEO</strong></td><td>Video annotations with temporal tracking</td><td>52.5K videos, 24.8K unique noun phrases</td></tr></tbody></table><h3 id="benchmark-data">Benchmark Data</h3><p>The <strong>SA-Co evaluation benchmark</strong> contains <strong>214K unique phrases</strong> across <strong>126K images and videos</strong>, providing over <strong>50√ó more concepts</strong> than existing benchmarks. It includes:</p><ul><li><strong>SA-Co/Gold</strong>: 7 domains, triple-annotated for measuring human performance bounds</li><li><strong>SA-Co/Silver</strong>: 10 domains, single human annotation</li><li><strong>SA-Co/Bronze</strong> and <strong>SA-Co/Bio</strong>: 9 existing datasets adapted for concept segmentation</li><li><strong>SA-Co/VEval</strong>: Video benchmark with 3 domains (SA-V, YT-Temporal-1B, SmartGlasses)</li></ul><h3 id="data-engine-innovations">Data Engine Innovations</h3><p>SAM 3's scalable human- and model-in-the-loop data engine achieves <strong>2√ó annotation throughput</strong> through:</p><ol><li><strong>AI Annotators</strong>: <a href="https://arxiv.org/abs/2302.13971">Llama</a> based models propose diverse noun phrases including hard negatives</li><li><strong>AI Verifiers</strong>: Fine-tuned <a href="https://ai.google.dev/gemini-api/docs">multimodal LLMs</a> verify mask quality and exhaustivity at near-human performance</li><li><strong>Active Mining</strong>: Focuses human effort on challenging failure cases where AI struggles</li><li><strong>Ontology-Driven</strong>: Leverages a large ontology grounded in <a href="https://www.wikidata.org/wiki/Wikidata:Main_Page">Wikidata</a> for concept coverage</li></ol><h2 id="installation">Installation</h2><p>SAM 3 will be available directly in the Ultralytics package once integration lands. Installation will remain:</p><div class="highlight"><pre><span></span><code><span></span>pip<span class="w"> </span>install<span class="w"> </span>ultralytics
</code></pre></div><p>Models will download automatically when first used. You can then use standard <a href="../../modes/predict/">predict mode</a> and later <a href="../../modes/export/">export</a> models to formats like <a href="../../integrations/onnx/">ONNX</a> and <a href="../../integrations/tensorrt/">TensorRT</a> for deployment. Watch for a package update with SAM-3 weights and configs soon.</p><h2 id="how-to-use-sam-3-versatility-in-concept-segmentation">How to Use SAM 3: Versatility in Concept Segmentation</h2><div class="admonition warning"><p class="admonition-title">Ultralytics API preview</p><p>The following examples show the intended Ultralytics API once SAM 3 ships in the package. Until integration lands, details may change.</p></div><h3 id="supported-tasks-and-models">Supported Tasks and Models</h3><p>SAM 3 supports both Promptable Concept Segmentation (PCS) and Promptable Visual Segmentation (PVS) tasks:</p><table><thead><tr><th>Task Type</th><th>Prompt Types</th><th>Output</th></tr></thead><tbody><tr><td><strong>Concept Segmentation (PCS)</strong></td><td>Text (noun phrases), image exemplars</td><td>All instances matching the concept</td></tr><tr><td><strong>Visual Segmentation (PVS)</strong></td><td>Points, boxes, masks</td><td>Single object instance (SAM 2 style)</td></tr><tr><td><strong>Interactive Refinement</strong></td><td>Add/remove exemplars or clicks iteratively</td><td>Refined segmentation with improved accuracy</td></tr></tbody></table><h3 id="concept-segmentation-examples">Concept Segmentation Examples</h3><h4 id="segment-with-text-prompts">Segment with Text Prompts</h4><div class="admonition example"><p class="admonition-title">Text-based Concept Segmentation</p><p>Find and segment all instances of a concept using a text description.</p><div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="python" name="__tabbed_1" type="radio"/><input id="cli" name="__tabbed_1" type="radio"/><div class="tabbed-labels"><label for="python">Python</label><label for="cli">CLI</label></div><div class="tabbed-content"><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">SAM</span>
<span></span>
<span></span><span class="c1"># Load SAM 3 model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SAM</span><span class="p">(</span><span class="s2">"sam3.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Segment all instances of a concept</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="s2">"yellow school bus"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Works with descriptive phrases</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="s2">"person wearing a red hat"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Or simple object names</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="s2">"striped cat"</span><span class="p">)</span>
</code></pre></div></div><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="c1"># Segment all matching concepts in an image</span>
<span></span>yolo<span class="w"> </span>segment<span class="w"> </span><span class="nv">model</span><span class="o">=</span>sam3.pt<span class="w"> </span><span class="nv">source</span><span class="o">=</span>path/to/image.jpg<span class="w"> </span><span class="nv">prompt</span><span class="o">=</span><span class="s2">"yellow school bus"</span>
</code></pre></div></div></div></div><div class="admonition warning"><p class="admonition-title">API Preview</p><p>This example shows intended usage. Actual implementation pending Ultralytics integration.</p></div></div><h4 id="segment-with-image-exemplars">Segment with Image Exemplars</h4><div class="admonition example"><p class="admonition-title">Image Exemplar-based Segmentation</p><p>Use one or more example objects to find all similar instances.</p><div class="tabbed-set tabbed-alternate" data-tabs="2:1"><input checked="checked" id="python_1" name="__tabbed_2" type="radio"/><div class="tabbed-labels"><label for="python_1">Python</label></div><div class="tabbed-content"><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">SAM</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SAM</span><span class="p">(</span><span class="s2">"sam3.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Provide a positive example box - finds all similar objects</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span></span>
<span></span><span class="c1"># Add negative examples to exclude certain instances</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
<span></span>    <span class="s2">"path/to/image.jpg"</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="p">[[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">],</span> <span class="p">[</span><span class="mi">500</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">600</span><span class="p">,</span> <span class="mi">350</span><span class="p">]],</span>  <span class="c1"># Two boxes</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>  <span class="c1"># First is positive, second is negative</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Combine text and image exemplars for precision</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="s2">"dog"</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div></div></div><div class="admonition warning"><p class="admonition-title">API Preview</p><p>This example shows intended usage. Actual implementation pending Ultralytics integration.</p></div></div><h4 id="interactive-refinement">Interactive Refinement</h4><div class="admonition example"><p class="admonition-title">Iterative Refinement with Exemplars</p><p>Progressively improve results by adding exemplar prompts based on initial output.</p><div class="tabbed-set tabbed-alternate" data-tabs="3:1"><input checked="checked" id="python_2" name="__tabbed_3" type="radio"/><div class="tabbed-labels"><label for="python_2">Python</label></div><div class="tabbed-content"><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">SAM</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SAM</span><span class="p">(</span><span class="s2">"sam3.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Initial segmentation with text</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="s2">"car"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># If some cars are missed, add a positive exemplar</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
<span></span>    <span class="s2">"path/to/image.jpg"</span><span class="p">,</span>
<span></span>    <span class="n">prompt</span><span class="o">=</span><span class="s2">"car"</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="p">[</span><span class="n">missed_car_box</span><span class="p">],</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># Positive example</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># If false positives appear, add negative exemplars</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
<span></span>    <span class="s2">"path/to/image.jpg"</span><span class="p">,</span>
<span></span>    <span class="n">prompt</span><span class="o">=</span><span class="s2">"car"</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="p">[</span><span class="n">false_positive_box</span><span class="p">],</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>  <span class="c1"># Negative example</span>
<span></span><span class="p">)</span>
</code></pre></div></div></div></div><div class="admonition warning"><p class="admonition-title">API Preview</p><p>This example shows intended usage. Actual implementation pending Ultralytics integration.</p></div></div><h3 id="video-concept-segmentation">Video Concept Segmentation</h3><div class="admonition example"><p class="admonition-title">Track Concepts Across Video</p><p>Detect and track all instances of a concept throughout a video.</p><div class="tabbed-set tabbed-alternate" data-tabs="4:1"><input checked="checked" id="python_3" name="__tabbed_4" type="radio"/><div class="tabbed-labels"><label for="python_3">Python</label></div><div class="tabbed-content"><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics.models.sam</span><span class="w"> </span><span class="kn">import</span> <span class="n">SAM3VideoPredictor</span>
<span></span>
<span></span><span class="c1"># Create video predictor</span>
<span></span><span class="n">predictor</span> <span class="o">=</span> <span class="n">SAM3VideoPredictor</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"sam3.pt"</span><span class="p">,</span> <span class="n">imgsz</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">conf</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Track all instances of a concept</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="s2">"video.mp4"</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="s2">"person wearing blue shirt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Combine text with exemplar for precision</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span>
<span></span>    <span class="n">source</span><span class="o">=</span><span class="s2">"video.mp4"</span><span class="p">,</span>
<span></span>    <span class="n">prompt</span><span class="o">=</span><span class="s2">"kangaroo"</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="p">[</span><span class="n">initial_box</span><span class="p">],</span>  <span class="c1"># Exemplar from first frame</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
<span></span><span class="p">)</span>
</code></pre></div></div></div></div><div class="admonition warning"><p class="admonition-title">API Preview</p><p>This example shows intended usage. Actual implementation pending Ultralytics integration.</p></div></div><p>For broader streaming and production setups, see <a href="../../guides/object-counting/">object tracking</a> and <a href="../../guides/view-results-in-terminal/">view results in terminal</a>.</p><h3 id="visual-prompts-sam-2-compatibility">Visual Prompts (SAM 2 Compatibility)</h3><p>SAM 3 maintains full backward compatibility with SAM 2's visual prompting:</p><div class="admonition example"><p class="admonition-title">SAM 2 Style Visual Prompts</p><div class="tabbed-set tabbed-alternate" data-tabs="5:1"><input checked="checked" id="python_4" name="__tabbed_5" type="radio"/><div class="tabbed-labels"><label for="python_4">Python</label></div><div class="tabbed-content"><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">SAM</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SAM</span><span class="p">(</span><span class="s2">"sam3.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Single point prompt (SAM 2 style)</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">points</span><span class="o">=</span><span class="p">[</span><span class="mi">900</span><span class="p">,</span> <span class="mi">370</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span></span>
<span></span><span class="c1"># Multiple points</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">points</span><span class="o">=</span><span class="p">[[</span><span class="mi">400</span><span class="p">,</span> <span class="mi">370</span><span class="p">],</span> <span class="p">[</span><span class="mi">900</span><span class="p">,</span> <span class="mi">370</span><span class="p">]],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span></span>
<span></span><span class="c1"># Box prompt</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">bboxes</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">])</span>
</code></pre></div></div></div></div><div class="admonition warning"><p class="admonition-title">API Preview</p><p>This example shows intended usage. Actual implementation pending Ultralytics integration.</p></div></div><h2 id="performance-benchmarks">Performance Benchmarks</h2><h3 id="image-segmentation">Image Segmentation</h3><p>SAM 3 achieves state-of-the-art results across multiple benchmarks, including real-world datasets like <a href="../../datasets/detect/lvis/">LVIS</a> and <a href="../../datasets/segment/coco/">COCO for segmentation</a>:</p><table><thead><tr><th>Benchmark</th><th>Metric</th><th>SAM 3</th><th>Previous Best</th><th>Improvement</th></tr></thead><tbody><tr><td><strong>LVIS (zero-shot)</strong></td><td>Mask AP</td><td><strong>47.0</strong></td><td>38.5</td><td>+22.1%</td></tr><tr><td><strong>SA-Co/Gold</strong></td><td>CGF1</td><td><strong>65.0</strong></td><td>34.3 (OWLv2)</td><td>+89.5%</td></tr><tr><td><strong>COCO (zero-shot)</strong></td><td>Box AP</td><td><strong>53.5</strong></td><td>52.2 (T-Rex2)</td><td>+2.5%</td></tr><tr><td><strong>ADE-847 (semantic seg)</strong></td><td>mIoU</td><td><strong>14.7</strong></td><td>9.2 (APE-D)</td><td>+59.8%</td></tr><tr><td><strong>PascalConcept-59</strong></td><td>mIoU</td><td><strong>59.4</strong></td><td>58.5 (APE-D)</td><td>+1.5%</td></tr><tr><td><strong>Cityscapes (semantic seg)</strong></td><td>mIoU</td><td><strong>65.1</strong></td><td>44.2 (APE-D)</td><td>+47.3%</td></tr></tbody></table><p>Explore dataset options for quick experimentation in <a href="../../datasets/">Ultralytics datasets</a>.</p><h3 id="video-segmentation-performance">Video Segmentation Performance</h3><p>SAM 3 shows significant improvements over SAM 2 and prior state-of-the-art across video benchmarks such as <a href="https://davischallenge.org/">DAVIS 2017</a> and <a href="https://youtube-vos.org/">YouTube-VOS</a>:</p><table><thead><tr><th>Benchmark</th><th>Metric</th><th>SAM 3</th><th>SAM 2.1 L</th><th>Improvement</th></tr></thead><tbody><tr><td><strong>MOSEv2</strong></td><td>J&amp;F</td><td><strong>60.1</strong></td><td>47.9</td><td>+25.5%</td></tr><tr><td><strong>DAVIS 2017</strong></td><td>J&amp;F</td><td><strong>92.0</strong></td><td>90.7</td><td>+1.4%</td></tr><tr><td><strong>LVOSv2</strong></td><td>J&amp;F</td><td><strong>88.2</strong></td><td>79.6</td><td>+10.8%</td></tr><tr><td><strong>SA-V</strong></td><td>J&amp;F</td><td><strong>84.6</strong></td><td>78.4</td><td>+7.9%</td></tr><tr><td><strong>YTVOS19</strong></td><td>J&amp;F</td><td><strong>89.6</strong></td><td>89.3</td><td>+0.3%</td></tr></tbody></table><h3 id="few-shot-adaptation">Few-Shot Adaptation</h3><p>SAM 3 excels at adapting to new domains with minimal examples, relevant for <a href="https://www.ultralytics.com/glossary/data-centric-ai">data-centric AI</a> workflows:</p><table><thead><tr><th>Benchmark</th><th>0-shot AP</th><th>10-shot AP</th><th>Previous Best (10-shot)</th></tr></thead><tbody><tr><td><strong>ODinW13</strong></td><td>59.9</td><td><strong>71.6</strong></td><td>67.9 (gDino1.5-Pro)</td></tr><tr><td><strong>RF100-VL</strong></td><td>14.3</td><td><strong>35.7</strong></td><td>33.7 (gDino-T)</td></tr></tbody></table><h3 id="interactive-refinement-effectiveness">Interactive Refinement Effectiveness</h3><p>SAM 3's concept-based prompting with exemplars converges much faster than visual prompting:</p><table><thead><tr><th>Prompts Added</th><th>CGF1 Score</th><th>Gain vs Text-Only</th><th>Gain vs PVS Baseline</th></tr></thead><tbody><tr><td>Text only</td><td>46.4</td><td>baseline</td><td>baseline</td></tr><tr><td>+1 exemplar</td><td>57.6</td><td>+11.2</td><td>+6.7</td></tr><tr><td>+2 exemplars</td><td>62.2</td><td>+15.8</td><td>+9.7</td></tr><tr><td>+3 exemplars</td><td><strong>65.0</strong></td><td><strong>+18.6</strong></td><td><strong>+11.2</strong></td></tr><tr><td>+4 exemplars</td><td>65.7</td><td>+19.3</td><td>+11.5 (plateau)</td></tr></tbody></table><h3 id="object-counting-accuracy">Object Counting Accuracy</h3><p>SAM 3 provides accurate counting by segmenting all instances, a common requirement in <a href="../../guides/object-counting/">object counting</a>:</p><table><thead><tr><th>Benchmark</th><th>Accuracy</th><th>MAE</th><th>vs Best MLLM</th></tr></thead><tbody><tr><td><strong>CountBench</strong></td><td><strong>95.6%</strong></td><td>0.11</td><td>92.4% (Gemini 2.5)</td></tr><tr><td><strong>PixMo-Count</strong></td><td><strong>87.3%</strong></td><td>0.22</td><td>88.8% (Molmo-72B)</td></tr></tbody></table><h2 id="sam-3-vs-sam-2-vs-yolo-comparison">SAM 3 vs SAM 2 vs YOLO Comparison</h2><p>Here we compare SAM 3's capabilities with SAM 2 and <a href="../../models/yolo11/">YOLO11</a> models:</p><table><thead><tr><th>Capability</th><th>SAM 3</th><th>SAM 2</th><th>YOLO11n-seg</th></tr></thead><tbody><tr><td><strong>Concept Segmentation</strong></td><td>‚úÖ All instances from text/exemplars</td><td>‚ùå Not supported</td><td>‚ùå Not supported</td></tr><tr><td><strong>Visual Segmentation</strong></td><td>‚úÖ Single instance (SAM 2 compatible)</td><td>‚úÖ Single instance</td><td>‚úÖ All instances</td></tr><tr><td><strong>Zero-shot Capability</strong></td><td>‚úÖ Open vocabulary</td><td>‚úÖ Geometric prompts</td><td>‚ùå Closed set</td></tr><tr><td><strong>Interactive Refinement</strong></td><td>‚úÖ Exemplars + clicks</td><td>‚úÖ Clicks only</td><td>‚ùå Not supported</td></tr><tr><td><strong>Video Tracking</strong></td><td>‚úÖ Multi-object with identities</td><td>‚úÖ Multi-object</td><td>‚úÖ Multi-object</td></tr><tr><td><strong>LVIS Mask AP (zero-shot)</strong></td><td><strong>47.0</strong></td><td>N/A</td><td>N/A</td></tr><tr><td><strong>MOSEv2 J&amp;F</strong></td><td><strong>60.1</strong></td><td>47.9</td><td>N/A</td></tr><tr><td><strong>Inference Speed (H200)</strong></td><td><strong>30 ms</strong> (100+ objects)</td><td>~23 ms (per object)</td><td><strong>2-3 ms</strong> (image)</td></tr><tr><td><strong>Model Size</strong></td><td>Large (~400+ MB expected)</td><td>162 MB (base)</td><td><strong>5.9 MB</strong></td></tr></tbody></table><p><strong>Key Takeaways</strong>:</p><ul><li><strong>SAM 3</strong>: Best for open-vocabulary concept segmentation, finding all instances of a concept with text or exemplar prompts</li><li><strong>SAM 2</strong>: Best for interactive single-object segmentation in images and videos with geometric prompts</li><li><strong>YOLO11</strong>: Best for real-time, high-speed segmentation in resource-constrained deployments using efficient <a href="../../modes/export/">export pipelines</a> like <a href="../../integrations/onnx/">ONNX</a> and <a href="../../integrations/tensorrt/">TensorRT</a></li></ul><h2 id="evaluation-metrics">Evaluation Metrics</h2><p>SAM 3 introduces new metrics designed for the PCS task, complementing familiar measures like <a href="https://www.ultralytics.com/glossary/f1-score">F1 score</a>, <a href="https://www.ultralytics.com/glossary/precision">precision</a>, and <a href="https://www.ultralytics.com/glossary/recall">recall</a>.</p><h3 id="classification-gated-f1-cgf1">Classification-Gated F1 (CGF1)</h3><p>The primary metric combining localization and classification:</p><p><strong>CGF1 = 100 √ó pmF1 √ó IL_MCC</strong></p><p>Where:</p><ul><li><strong>pmF1</strong> (Positive Macro F1): Measures localization quality on positive examples</li><li><strong>IL_MCC</strong> (Image-Level Matthews Correlation Coefficient): Measures binary classification accuracy ("is the concept present?")</li></ul><h3 id="why-these-metrics">Why These Metrics?</h3><p>Traditional AP metrics don't account for calibration, making models difficult to use in practice. By evaluating only predictions above 0.5 confidence, SAM 3's metrics enforce good calibration and mimic real-world usage patterns in interactive <a href="../../modes/predict/">predict</a> and <a href="../../modes/track/">track</a> loops.</p><h2 id="key-ablations-and-insights">Key Ablations and Insights</h2><h3 id="impact-of-presence-head">Impact of Presence Head</h3><p>The presence head decouples recognition from localization, providing significant improvements:</p><table><thead><tr><th>Configuration</th><th>CGF1</th><th>IL_MCC</th><th>pmF1</th></tr></thead><tbody><tr><td>Without presence</td><td>57.6</td><td>0.77</td><td>74.7</td></tr><tr><td><strong>With presence</strong></td><td><strong>63.3</strong></td><td><strong>0.82</strong></td><td><strong>77.1</strong></td></tr></tbody></table><p>The presence head provides a <strong>+5.7 CGF1 boost</strong> (+9.9%), primarily improving recognition ability (IL_MCC +6.5%).</p><h3 id="effect-of-hard-negatives">Effect of Hard Negatives</h3><table><thead><tr><th>Hard Negatives/Image</th><th>CGF1</th><th>IL_MCC</th><th>pmF1</th></tr></thead><tbody><tr><td>0</td><td>31.8</td><td>0.44</td><td>70.2</td></tr><tr><td>5</td><td>44.8</td><td>0.62</td><td>71.9</td></tr><tr><td><strong>30</strong></td><td><strong>49.2</strong></td><td><strong>0.68</strong></td><td><strong>72.3</strong></td></tr></tbody></table><p>Hard negatives are crucial for open-vocabulary recognition, improving IL_MCC by <strong>54.5%</strong> (0.44 ‚Üí 0.68).</p><h3 id="training-data-scaling">Training Data Scaling</h3><table><thead><tr><th>Data Sources</th><th>CGF1</th><th>IL_MCC</th><th>pmF1</th></tr></thead><tbody><tr><td>External only</td><td>30.9</td><td>0.46</td><td>66.3</td></tr><tr><td>External + Synthetic</td><td>39.7</td><td>0.57</td><td>70.6</td></tr><tr><td>External + HQ</td><td>51.8</td><td>0.71</td><td>73.2</td></tr><tr><td><strong>All three</strong></td><td><strong>54.3</strong></td><td><strong>0.74</strong></td><td><strong>73.5</strong></td></tr></tbody></table><p>High-quality human annotations provide large gains over synthetic or external data alone. For background on data quality practices, see <a href="../../guides/data-collection-and-annotation/">data collection and annotation</a>.</p><h2 id="applications">Applications</h2><p>SAM 3's concept segmentation capability enables new use cases:</p><ul><li><strong>Content Moderation</strong>: Find all instances of specific content types across media libraries</li><li><strong>E-commerce</strong>: Segment all products of a certain type in catalog images, supporting <a href="../../guides/preprocessing_annotated_data/">auto-annotation</a></li><li><strong>Medical Imaging</strong>: Identify all occurrences of specific tissue types or abnormalities</li><li><strong>Autonomous Systems</strong>: Track all instances of traffic signs, pedestrians, or vehicles by category</li><li><strong>Video Analytics</strong>: Count and track all people wearing specific clothing or performing actions</li><li><strong>Dataset Annotation</strong>: Rapidly annotate all instances of rare object categories</li><li><strong>Scientific Research</strong>: Quantify and analyze all specimens matching specific criteria</li></ul><h2 id="sam-3-agent-extended-language-reasoning">SAM 3 Agent: Extended Language Reasoning</h2><p>SAM 3 can be combined with Multimodal Large Language Models (MLLMs) to handle complex queries requiring reasoning, similar in spirit to open-vocabulary systems like <a href="https://arxiv.org/abs/2306.09683">OWLv2</a> and <a href="https://arxiv.org/abs/2401.03533">T-Rex</a>.</p><h3 id="performance-on-reasoning-tasks">Performance on Reasoning Tasks</h3><table><thead><tr><th>Benchmark</th><th>Metric</th><th>SAM 3 Agent (Gemini 2.5 Pro)</th><th>Previous Best</th></tr></thead><tbody><tr><td><strong>ReasonSeg (validation)</strong></td><td>gIoU</td><td><strong>76.0</strong></td><td>65.0 (SoTA)</td></tr><tr><td><strong>ReasonSeg (test)</strong></td><td>gIoU</td><td><strong>73.8</strong></td><td>61.3 (SoTA)</td></tr><tr><td><strong>OmniLabel (validation)</strong></td><td>AP</td><td><strong>46.7</strong></td><td>36.5 (REAL)</td></tr><tr><td><strong>RefCOCO+</strong></td><td>Acc</td><td><strong>91.2</strong></td><td>89.3 (LISA)</td></tr></tbody></table><h3 id="example-complex-queries">Example Complex Queries</h3><p>SAM 3 Agent can handle queries requiring reasoning:</p><ul><li>"People sitting down but not holding a gift box in their hands"</li><li>"The dog closest to the camera that is not wearing a collar"</li><li>"Red objects larger than the person's hand"</li></ul><p>The MLLM proposes simple noun phrase queries to SAM 3, analyzes returned masks, and iterates until satisfied.</p><h2 id="limitations">Limitations</h2><p>While SAM 3 represents a major advancement, it has certain limitations:</p><ul><li><strong>Phrase Complexity</strong>: Best suited for simple noun phrases; long referring expressions or complex reasoning may require MLLM integration</li><li><strong>Ambiguity Handling</strong>: Some concepts remain inherently ambiguous (e.g., "small window", "cozy room")</li><li><strong>Computational Requirements</strong>: Larger and slower than specialized detection models like <a href="../../models/yolo26/">YOLO</a></li><li><strong>Vocabulary Scope</strong>: Focused on atomic visual concepts; compositional reasoning is limited without MLLM assistance</li><li><strong>Rare Concepts</strong>: Performance may degrade on extremely rare or fine-grained concepts not well-represented in training data</li></ul><h2 id="citation">Citation</h2><div class="admonition quote"><div class="tabbed-set tabbed-alternate" data-tabs="6:1"><input checked="checked" id="bibtex" name="__tabbed_6" type="radio"/><div class="tabbed-labels"><label for="bibtex">BibTeX</label></div><div class="tabbed-content"><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sam3_2025</span><span class="p">,</span>
<span></span><span class="w">  </span><span class="na">title</span><span class="w">     </span><span class="p">=</span><span class="w"> </span><span class="s">{SAM 3: Segment Anything with Concepts}</span><span class="p">,</span>
<span></span><span class="w">  </span><span class="na">author</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{Anonymous authors}</span><span class="p">,</span>
<span></span><span class="w">  </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Submitted to ICLR 2026}</span><span class="p">,</span>
<span></span><span class="w">  </span><span class="na">year</span><span class="w">      </span><span class="p">=</span><span class="w"> </span><span class="s">{2025}</span><span class="p">,</span>
<span></span><span class="w">  </span><span class="na">url</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{https://openreview.net/forum?id=r35clVtGzw}</span><span class="p">,</span>
<span></span><span class="w">  </span><span class="na">note</span><span class="w">      </span><span class="p">=</span><span class="w"> </span><span class="s">{Paper ID: 4183, under double-blind review}</span>
<span></span><span class="p">}</span>
</code></pre></div></div></div></div></div><hr/><h2 id="faq">FAQ</h2><h3 id="when-will-sam-3-be-released">When Will SAM 3 Be Released?</h3><p>SAM 3 was released by Meta on <strong>November 20th, 2025</strong>. Ultralytics support is in progress and will ship in an upcoming package update with full docs for <a href="../../modes/predict/">predict mode</a> and <a href="../../modes/track/">track mode</a>.</p><h3 id="will-sam-3-be-integrated-into-ultralytics">Will SAM 3 Be Integrated Into Ultralytics?</h3><p>Yes. SAM 3 will be supported in the Ultralytics Python package upon release, including concept segmentation, SAM 2‚Äìstyle visual prompts, and multi-object video tracking. You will be able to <a href="../../modes/export/">export</a> to formats like <a href="../../integrations/onnx/">ONNX</a> and <a href="../../integrations/tensorrt/">TensorRT</a> for deployment, with streamlined <a href="../../usage/python/">Python</a> and <a href="../../usage/cli/">CLI</a> workflows.</p><div class="admonition warning"><p class="admonition-title">Implementation Timeline</p><p>Code examples in this documentation are preview versions showing intended usage patterns. Actual implementation will be available after Ultralytics completes integration.</p></div><h3 id="what-is-promptable-concept-segmentation-pcs_1">What Is Promptable Concept Segmentation (PCS)?</h3><p>PCS is a new task introduced in SAM 3 that segments <strong>all instances</strong> of a visual concept in an image or video. Unlike traditional segmentation that targets a specific object instance, PCS finds every occurrence of a category. For example:</p><ul><li><strong>Text prompt</strong>: "yellow school bus" ‚Üí segments all yellow school buses in the scene</li><li><strong>Image exemplar</strong>: Box around one dog ‚Üí segments all dogs in the image</li><li><strong>Combined</strong>: "striped cat" + exemplar box ‚Üí segments all striped cats matching the example</li></ul><p>See related background on <a href="https://www.ultralytics.com/glossary/object-detection">object detection</a> and <a href="https://www.ultralytics.com/glossary/instance-segmentation">instance segmentation</a>.</p><h3 id="how-does-sam-3-differ-from-sam-2">How Does SAM 3 Differ From SAM 2?</h3><table><thead><tr><th>Feature</th><th>SAM 2</th><th>SAM 3</th></tr></thead><tbody><tr><td><strong>Task</strong></td><td>Single object per prompt</td><td>All instances of a concept</td></tr><tr><td><strong>Prompt Types</strong></td><td>Points, boxes, masks</td><td>+ Text phrases, image exemplars</td></tr><tr><td><strong>Detection Capability</strong></td><td>Requires external detector</td><td>Built-in open-vocabulary detector</td></tr><tr><td><strong>Recognition</strong></td><td>Geometry-based only</td><td>Text and visual recognition</td></tr><tr><td><strong>Architecture</strong></td><td>Tracker only</td><td>Detector + Tracker with presence head</td></tr><tr><td><strong>Zero-Shot Performance</strong></td><td>N/A (requires visual prompts)</td><td>47.0 AP on LVIS, 2√ó better on SA-Co</td></tr><tr><td><strong>Interactive Refinement</strong></td><td>Clicks only</td><td>Clicks + exemplar generalization</td></tr></tbody></table><p>SAM 3 maintains backward compatibility with <a href="../sam-2/">SAM 2</a> visual prompting while adding concept-based capabilities.</p><h3 id="what-datasets-are-used-to-train-sam-3">What datasets are used to train SAM 3?</h3><p>SAM 3 is trained on the <strong>Segment Anything with Concepts (SA-Co)</strong> dataset:</p><p><strong>Training Data</strong>:</p><ul><li><strong>5.2M images</strong> with <strong>4M unique noun phrases</strong> (SA-Co/HQ) - high-quality human annotations</li><li><strong>52.5K videos</strong> with <strong>24.8K unique noun phrases</strong> (SA-Co/VIDEO)</li><li><strong>1.4B synthetic masks</strong> across <strong>38M noun phrases</strong> (SA-Co/SYN)</li><li><strong>15 external datasets</strong> enriched with hard negatives (SA-Co/EXT)</li></ul><p><strong>Benchmark Data</strong>:</p><ul><li><strong>214K unique concepts</strong> across <strong>126K images/videos</strong></li><li><strong>50√ó more concepts</strong> than existing benchmarks (e.g., LVIS has ~4K concepts)</li><li>Triple annotation on SA-Co/Gold for measuring human performance bounds</li></ul><p>This massive scale and diversity enables SAM 3's superior zero-shot generalization across open-vocabulary concepts.</p><h3 id="how-does-sam-3-compare-to-yolo11-for-segmentation">How does SAM 3 compare to YOLO11 for segmentation?</h3><p>SAM 3 and YOLO11 serve different use cases:</p><p><strong>SAM 3 Advantages</strong>:</p><ul><li><strong>Open-vocabulary</strong>: Segments any concept via text prompts without training</li><li><strong>Zero-shot</strong>: Works on new categories immediately</li><li><strong>Interactive</strong>: Exemplar-based refinement generalizes to similar objects</li><li><strong>Concept-based</strong>: Automatically finds all instances of a category</li><li><strong>Accuracy</strong>: 47.0 AP on LVIS zero-shot instance segmentation</li></ul><p><strong>YOLO11 Advantages</strong>:</p><ul><li><strong>Speed</strong>: 10-15√ó faster inference (2-3ms vs 30ms per image)</li><li><strong>Efficiency</strong>: 70√ó smaller models (5.9MB vs ~400MB expected)</li><li><strong>Resource-friendly</strong>: Runs on edge devices and mobile</li><li><strong>Real-time</strong>: Optimized for production deployments</li></ul><p><strong>Recommendation</strong>:</p><ul><li>Use <strong>SAM 3</strong> for flexible, open-vocabulary segmentation where you need to find all instances of concepts described by text or examples</li><li>Use <strong>YOLO11</strong> for high-speed, production deployments where categories are known in advance</li><li>Use <strong>SAM 2</strong> for interactive single-object segmentation with geometric prompts</li></ul><h3 id="can-sam-3-handle-complex-language-queries">Can SAM 3 handle complex language queries?</h3><p>SAM 3 is designed for simple noun phrases (e.g., "red apple", "person wearing hat"). For complex queries requiring reasoning, combine SAM 3 with an MLLM as <strong>SAM 3 Agent</strong>:</p><p><strong>Simple queries (native SAM 3)</strong>:</p><ul><li>"yellow school bus"</li><li>"striped cat"</li><li>"person wearing red hat"</li></ul><p><strong>Complex queries (SAM 3 Agent with MLLM)</strong>:</p><ul><li>"People sitting down but not holding a gift box"</li><li>"The dog closest to the camera without a collar"</li><li>"Red objects larger than the person's hand"</li></ul><p>SAM 3 Agent achieves <strong>76.0 gIoU</strong> on ReasonSeg validation (vs 65.0 previous best, +16.9% improvement) by combining SAM 3's segmentation with MLLM reasoning capabilities.</p><h3 id="how-accurate-is-sam-3-compared-to-human-performance">How accurate is SAM 3 compared to human performance?</h3><p>On the SA-Co/Gold benchmark with triple human annotation:</p><ul><li><strong>Human lower bound</strong>: 74.2 CGF1 (most conservative annotator)</li><li><strong>SAM 3 performance</strong>: 65.0 CGF1</li><li><strong>Achievement</strong>: <strong>88%</strong> of estimated human lower bound</li><li><strong>Human upper bound</strong>: 81.4 CGF1 (most liberal annotator)</li></ul><p>SAM 3 achieves strong performance approaching human-level accuracy on open-vocabulary concept segmentation, with the gap primarily on ambiguous or subjective concepts (e.g., "small window", "cozy room").</p><br/><br/><div class="git-info"><div class="dates-container"><span class="date-item" title="This page was first created on October 13, 2025"><span class="hover-item">üìÖ</span> Created 1 month ago </span><span class="date-item" title="This page was last updated on November 24, 2025"><span class="hover-item">‚úèÔ∏è</span> Updated 17 days ago </span></div><div class="authors-container"><a class="author-link" href="https://github.com/glenn-jocher" title="glenn-jocher (3 changes)"><img alt="glenn-jocher" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/26833433?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/Y-T-G" title="Y-T-G (1 change)"><img alt="Y-T-G" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/32206511?v=4&amp;s=96"/></a></div></div><div class="share-buttons"><button class="share-button hover-item" onclick="window.open('https://twitter.com/intent/tweet?url=https%3A%2F%2Fdocs.ultralytics.com%2Fmodels%2Fsam-3%2F', 'TwitterShare', 'width=550,height=680,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-x-twitter"></i> Tweet </button><button class="share-button hover-item linkedin" onclick="window.open('https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fdocs.ultralytics.com%2Fmodels%2Fsam-3%2F', 'LinkedinShare', 'width=550,height=730,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-linkedin-in"></i> Share </button></div><br/><h2 id="__comments">Comments</h2><div id="giscus-container"></div></article></div>
<script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div><button class="md-top md-icon" data-md-component="top" hidden="" type="button"><svg class="lucide lucide-circle-arrow-up" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><path d="m16 12-4-4-4 4M12 16V8"></path></svg> Back to top
</button></main><footer class="md-footer"><nav aria-label="Footer" class="md-footer__inner md-grid"><a aria-label="Previous: SAM 2 (Segment Anything Model 2)" class="md-footer__link md-footer__link--prev" href="../sam-2/"><div class="md-footer__button md-icon"><svg class="lucide lucide-arrow-left" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m12 19-7-7 7-7M19 12H5"></path></svg></div><div class="md-footer__title"><span class="md-footer__direction"> Previous </span><div class="md-ellipsis"> SAM 2 (Segment Anything Model 2) </div></div></a><a aria-label="Next: MobileSAM (Mobile Segment Anything Model)" class="md-footer__link md-footer__link--next" href="../mobile-sam/"><div class="md-footer__title"><span class="md-footer__direction"> Next </span><div class="md-ellipsis"> MobileSAM (Mobile Segment Anything Model) </div></div><div class="md-footer__button md-icon"><svg class="lucide lucide-arrow-right" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M5 12h14M12 5l7 7-7 7"></path></svg></div></a></nav><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class="md-copyright"><div class="md-copyright__highlight"><a href="https://www.ultralytics.com/" target="_blank">¬© 2025 Ultralytics Inc.</a> All rights reserved. </div> Made with <a href="https://zensical.org/" rel="noopener" target="_blank"> Zensical </a></div><div class="md-social"><a class="md-social__link" href="https://github.com/ultralytics" rel="noopener" target="_blank" title="github.com"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://www.linkedin.com/company/ultralytics/" rel="noopener" target="_blank" title="www.linkedin.com"><svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://x.com/ultralytics" rel="noopener" target="_blank" title="x.com"><svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://youtube.com/ultralytics?sub_confirmation=1" rel="noopener" target="_blank" title="youtube.com"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M549.7 124.1c-6.2-23.7-24.8-42.3-48.3-48.6C458.9 64 288.1 64 288.1 64S117.3 64 74.7 75.5c-23.5 6.3-42 24.9-48.3 48.6C15 167 15 256.4 15 256.4s0 89.4 11.4 132.3c6.3 23.6 24.8 41.5 48.3 47.8C117.3 448 288.1 448 288.1 448s170.8 0 213.4-11.5c23.5-6.3 42-24.2 48.3-47.8 11.4-42.9 11.4-132.3 11.4-132.3s0-89.4-11.4-132.3zM232.2 337.6V175.2l142.7 81.2z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://hub.docker.com/r/ultralytics/ultralytics/" rel="noopener" target="_blank" title="hub.docker.com"><svg viewbox="0 0 640 512" xmlns="http://www.w3.org/2000/svg"><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://pypi.org/project/ultralytics/" rel="noopener" target="_blank" title="pypi.org"><svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 444.7a20.4 20.4 0 1 1 0-40.7 20.4 20.4 0 1 1 0 40.7M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.6-183.4a20.4 20.4 0 1 1 0 40.8 20.4 20.4 0 1 1 0-40.8" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://discord.com/invite/ultralytics" rel="noopener" target="_blank" title="discord.com"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M492.5 69.8c-.2-.3-.4-.6-.8-.7-38.1-17.5-78.4-30-119.7-37.1-.4-.1-.8 0-1.1.1s-.6.4-.8.8c-5.5 9.9-10.5 20.2-14.9 30.6-44.6-6.8-89.9-6.8-134.4 0-4.5-10.5-9.5-20.7-15.1-30.6-.2-.3-.5-.6-.8-.8s-.7-.2-1.1-.2C162.5 39 122.2 51.5 84.1 69c-.3.1-.6.4-.8.7C7.1 183.5-13.8 294.6-3.6 404.2c0 .3.1.5.2.8s.3.4.5.6c44.4 32.9 94 58 146.8 74.2.4.1.8.1 1.1 0s.7-.4.9-.7c11.3-15.4 21.4-31.8 30-48.8.1-.2.2-.5.2-.8s0-.5-.1-.8-.2-.5-.4-.6-.4-.3-.7-.4c-15.8-6.1-31.2-13.4-45.9-21.9-.3-.2-.5-.4-.7-.6s-.3-.6-.3-.9 0-.6.2-.9.3-.5.6-.7c3.1-2.3 6.2-4.7 9.1-7.1.3-.2.6-.4.9-.4s.7 0 1 .1c96.2 43.9 200.4 43.9 295.5 0 .3-.1.7-.2 1-.2s.7.2.9.4c2.9 2.4 6 4.9 9.1 7.2.2.2.4.4.6.7s.2.6.2.9-.1.6-.3.9-.4.5-.6.6c-14.7 8.6-30 15.9-45.9 21.8-.2.1-.5.2-.7.4s-.3.4-.4.7-.1.5-.1.8.1.5.2.8c8.8 17 18.8 33.3 30 48.8.2.3.6.6.9.7s.8.1 1.1 0c52.9-16.2 102.6-41.3 147.1-74.2.2-.2.4-.4.5-.6s.2-.5.2-.8c12.3-126.8-20.5-236.9-86.9-334.5zm-302 267.7c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.4 59.2-52.8 59.2m195.4 0c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.2 59.2-52.8 59.2" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://reddit.com/r/ultralytics" rel="noopener" target="_blank" title="reddit.com"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M0 256C0 114.6 114.6 0 256 0s256 114.6 256 256-114.6 256-256 256H37.1c-13.7 0-20.5-16.5-10.9-26.2L75 437C28.7 390.7 0 326.7 0 256m349.6-102.4c23.6 0 42.7-19.1 42.7-42.7s-19.1-42.7-42.7-42.7c-20.6 0-37.8 14.6-41.8 34-34.5 3.7-61.4 33-61.4 68.4v.2c-37.5 1.6-71.8 12.3-99 29.1-10.1-7.8-22.8-12.5-36.5-12.5-33 0-59.8 26.8-59.8 59.8 0 24 14.1 44.6 34.4 54.1 2 69.4 77.6 125.2 170.6 125.2s168.7-55.9 170.6-125.3c20.2-9.6 34.1-30.2 34.1-54 0-33-26.8-59.8-59.8-59.8-13.7 0-26.3 4.6-36.4 12.4-27.4-17-62.1-27.7-100-29.1v-.2c0-25.4 18.9-46.5 43.4-49.9 4.4 18.8 21.3 32.8 41.5 32.8zm-172.5 93.3c16.7 0 29.5 17.6 28.5 39.3s-13.5 29.6-30.3 29.6-31.4-8.8-30.4-30.5S160.3 247 177 247zm190.1 38.3c1 21.7-13.7 30.5-30.4 30.5s-29.3-7.9-30.3-29.6 11.8-39.3 28.5-39.3 31.2 16.6 32.1 38.3zm-48.1 56.7c-10.3 24.6-34.6 41.9-63 41.9s-52.7-17.3-63-41.9c-1.2-2.9.8-6.2 3.9-6.5 18.4-1.9 38.3-2.9 59.1-2.9s40.7 1 59.1 2.9c3.1.3 5.1 3.6 3.9 6.5" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://weixin.qq.com/r/mp/LxckPDfEgWr_rXNf90I9" rel="noopener" target="_blank" title="weixin.qq.com"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M385.2 167.6c6.4 0 12.6.3 18.8 1.1C387.4 90.3 303.3 32 207.7 32 100.5 32 13 104.8 13 197.4c0 53.4 29.3 97.5 77.9 131.6l-19.3 58.6 68-34.1c24.4 4.8 43.8 9.7 68.2 9.7 6.2 0 12.1-.3 18.3-.8-4-12.9-6.2-26.6-6.2-40.8-.1-84.9 72.9-154 165.3-154m-104.5-52.9c14.5 0 24.2 9.7 24.2 24.4 0 14.5-9.7 24.2-24.2 24.2-14.8 0-29.3-9.7-29.3-24.2.1-14.7 14.6-24.4 29.3-24.4m-136.4 48.6c-14.5 0-29.3-9.7-29.3-24.2 0-14.8 14.8-24.4 29.3-24.4 14.8 0 24.4 9.7 24.4 24.4 0 14.6-9.6 24.2-24.4 24.2M563 319.4c0-77.9-77.9-141.3-165.4-141.3-92.7 0-165.4 63.4-165.4 141.3S305 460.7 397.6 460.7c19.3 0 38.9-5.1 58.6-9.9l53.4 29.3-14.8-48.6C534 402.1 563 363.2 563 319.4m-219.1-24.5c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.8 0 24.4 9.7 24.4 19.3 0 10-9.7 19.6-24.4 19.6m107.1 0c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.5 0 24.4 9.7 24.4 19.3.1 10-9.9 19.6-24.4 19.6" fill="currentColor"></path></svg></a></div></div></div></footer></div><div class="md-dialog" data-md-component="dialog"><div class="md-dialog__inner md-typeset"></div></div><div class="md-progress" data-md-component="progress" role="progressbar"></div>
<script id="__config" type="application/json">{"annotate":null,"base":"../..","features":["content.action.edit","content.code.annotate","content.code.copy","content.tooltips","toc.follow","navigation.top","navigation.tabs","navigation.tabs.sticky","navigation.prune","navigation.footer","navigation.tracking","navigation.instant","navigation.instant.progress","navigation.indexes","navigation.sections","content.tabs.link"],"search":"../../assets/javascripts/workers/search.5df7522c.min.js","tags":null,"translations":{"clipboard.copied":"Copied to clipboard","clipboard.copy":"Copy to clipboard","search.result.more.one":"1 more on this page","search.result.more.other":"# more on this page","search.result.none":"No matching documents","search.result.one":"1 matching document","search.result.other":"# matching documents","search.result.placeholder":"Type to start searching","search.result.term.missing":"Missing","select.version":"Select version"},"version":null}</script>
<script src="../../assets/javascripts/bundle.21aa498e.min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/ultralytics/llm@v0.1.6/js/chat.min.js"></script>
<script src="https://unpkg.com/tablesort@5.6.0/dist/tablesort.min.js"></script>
<script src="../../javascript/extra.js"></script>
<script src="../../javascript/giscus.js"></script>
<script src="../../javascript/tablesort.js"></script>
<script>
            async function copyMarkdownForLLM(button) {
                const editBtn = document.querySelector('a[title="Edit this page"]');
                if (!editBtn) return;

                const originalHTML = button.innerHTML;
                const checkIcon = '<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 16.17L4.83 12l-1.42 1.41L9 19L21 7l-1.41-1.41L9 16.17z"></path></svg>';

                let rawUrl = editBtn.href.replace('github.com', 'raw.githubusercontent.com');
                rawUrl = rawUrl.replace('/blob/', '/').replace('/tree/', '/');

                try {
                    const response = await fetch(rawUrl);
                    let markdown = await response.text();

                    if (markdown.startsWith('---')) {
                        const frontMatterEnd = markdown.indexOf('\n---\n', 3);
                        if (frontMatterEnd !== -1) {
                            markdown = markdown.substring(frontMatterEnd + 5).trim();
                        }
                    }

                    const title = document.querySelector('h1')?.textContent || document.title;
                    const content = `# ${title}\n\nSource: ${window.location.href}\n\n---\n\n${markdown}`;

                    await navigator.clipboard.writeText(content);
                    button.innerHTML = checkIcon + ' Copied!';
                    setTimeout(() => { button.innerHTML = originalHTML; }, 2000);
                } catch (err) {
                    button.innerHTML = '‚ùå Failed';
                    setTimeout(() => { button.innerHTML = originalHTML; }, 2000);
                }
            }
            </script></body></html>