<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="YOLOE is a real-time open-vocabulary detection and segmentation model that extends YOLO with text, image, or internal vocabulary prompts, enabling detection of any object class with state-of-the-art zero-shot performance." name="description"/>
<meta content="Ultralytics" name="author"/>
<link href="https://docs.ultralytics.com/models/yoloe/" rel="canonical"/>
<link href="../yolo-world/" rel="prev"/>
<link href="../../datasets/" rel="next"/>
<link href="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/logo/favicon-yolo.png" rel="icon"/>
<meta content="mkdocs-1.6.1, mkdocs-material-9.6.14" name="generator"/>
<title>YOLOE: Real-Time Seeing Anything - Ultralytics YOLO Docs</title>
<link href="../../assets/stylesheets/main.342714a4.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<link href="../../assets/_mkdocstrings.css" rel="stylesheet"/>
<link href="../../stylesheets/style.css" rel="stylesheet"/>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-2M5EHKC0BH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-2M5EHKC0BH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-2M5EHKC0BH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
<meta content="YOLOE (Real-Time Seeing Anything)" name="title"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet"/><meta content="YOLOE, open-vocabulary detection, real-time object detection, instance segmentation, YOLO, text prompts, visual prompts, zero-shot detection" name="keywords"/><meta content="website" property="og:type"/><meta content="https://docs.ultralytics.com/models/yoloe" property="og:url"/><meta content="YOLOE (Real-Time Seeing Anything)" property="og:title"/><meta content="YOLOE is a real-time open-vocabulary detection and segmentation model that extends YOLO with text, image, or internal vocabulary prompts, enabling detection of any object class with state-of-the-art zero-shot performance." property="og:description"/><meta content="https://raw.githubusercontent.com/THU-MIG/yoloe/main/figures/visualization.svg" property="og:image"/><meta content="summary_large_image" property="twitter:card"/><meta content="https://docs.ultralytics.com/models/yoloe" property="twitter:url"/><meta content="YOLOE (Real-Time Seeing Anything)" property="twitter:title"/><meta content="YOLOE is a real-time open-vocabulary detection and segmentation model that extends YOLO with text, image, or internal vocabulary prompts, enabling detection of any object class with state-of-the-art zero-shot performance." property="twitter:description"/><meta content="https://raw.githubusercontent.com/THU-MIG/yoloe/main/figures/visualization.svg" property="twitter:image"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": ["Article", "FAQPage"], "headline": "YOLOE (Real-Time Seeing Anything)", "image": ["https://raw.githubusercontent.com/THU-MIG/yoloe/main/figures/visualization.svg"], "datePublished": "2025-03-19 01:07:14 +0100", "dateModified": "2025-04-29 16:32:36 +0800", "author": [{"@type": "Organization", "name": "Ultralytics", "url": "https://ultralytics.com/"}], "abstract": "YOLOE is a real-time open-vocabulary detection and segmentation model that extends YOLO with text, image, or internal vocabulary prompts, enabling detection of any object class with state-of-the-art zero-shot performance.", "mainEntity": [{"@type": "Question", "name": "How does YOLOE differ from YOLO-World?", "acceptedAnswer": {"@type": "Answer", "text": "While both YOLOE and YOLO-World enable open-vocabulary detection, YOLOE offers several advantages. YOLOE achieves +3.5 AP higher accuracy on LVIS while using 3\u00d7 less training resources and running 1.4\u00d7 faster than YOLO-Worldv2. YOLOE also supports three prompting modes (text, visual, and internal vocabulary), whereas YOLO-World primarily focuses on text prompts. Additionally, YOLOE includes built-in instance segmentation capabilities, providing pixel-precise masks for detected objects without additional overhead."}}, {"@type": "Question", "name": "Can I use YOLOE as a regular YOLO model?", "acceptedAnswer": {"@type": "Answer", "text": "Yes, YOLOE can function exactly like a standard YOLO model with no performance penalty. When used in closed-set mode (without prompts), YOLOE's open-vocabulary modules are re-parameterized into the standard detection head, resulting in identical speed and accuracy to equivalent YOLO11 models. This makes YOLOE extremely versatile\u2014you can use it as a traditional detector for maximum speed and then switch to open-vocabulary mode only when needed."}}, {"@type": "Question", "name": "What types of prompts can I use with YOLOE?", "acceptedAnswer": {"@type": "Answer", "text": "YOLOE supports three types of prompts: This flexibility allows you to adapt YOLOE to various scenarios without retraining the model, making it particularly useful for dynamic environments where detection requirements change frequently."}}, {"@type": "Question", "name": "How does YOLOE handle instance segmentation?", "acceptedAnswer": {"@type": "Answer", "text": "YOLOE integrates instance segmentation directly into its architecture by extending the detection head with a mask prediction branch. This approach is similar to YOLOv8-Seg but works for any prompted object class. Segmentation masks are automatically included in inference results and can be accessed via results[0].masks. This unified approach eliminates the need for separate detection and segmentation models, streamlining workflows for applications requiring pixel-precise object boundaries."}}, {"@type": "Question", "name": "How does YOLOE handle inference with custom prompts?", "acceptedAnswer": {"@type": "Answer", "text": "Similar to YOLO-World, YOLOE supports a \"prompt-then-detect\" strategy that utilizes an offline vocabulary to enhance efficiency. Custom prompts like captions or specific object categories are pre-encoded and stored as offline vocabulary embeddings. This approach streamlines the detection process without requiring retraining. You can dynamically set these prompts within the model to tailor it to specific detection tasks:"}}]}</script></head>
<body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="default" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#yoloe-real-time-seeing-anything">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
<aside class="md-banner">
<div class="md-banner__inner md-grid md-typeset">
<div class="banner-wrapper">
<div class="banner-content-wrapper" onclick="window.open('https://docs.ultralytics.com/models/yolo11/')">
<p>Introducing</p>
<img alt="Ultralytics YOLO11" height="40" loading="lazy" src="https://assets-global.website-files.com/646dd1f1a3703e451ba81ecc/67d044caa316aa50fba40a08_Ultralytics_YOLO11_Logotype_Reverse.svg"/>
</div>
</div>
</div>
</aside>
</div>
<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Ultralytics YOLO Docs" class="md-header__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs">
<img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Ultralytics YOLO Docs
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
              YOLOE (Real-Time Seeing Anything)
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_0" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg>
</label>
<input aria-label="Switch to system preference" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="black" data-md-color-scheme="slate" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to system preference">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
</label>
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_2" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_0" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
</label>
</form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<div class="md-header__option">
<div class="md-select">
<button aria-label="Select language" class="md-header__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"></path></svg>
</button>
<div class="md-select__inner">
<ul class="md-select__list">
<li class="md-select__item">
<a class="md-select__link" href="/" hreflang="en">
              üá¨üáß English
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/zh/" hreflang="zh">
              üá®üá≥ ÁÆÄ‰Ωì‰∏≠Êñá
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/ko/" hreflang="ko">
              üá∞üá∑ ÌïúÍµ≠Ïñ¥
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/ja/" hreflang="ja">
              üáØüáµ Êó•Êú¨Ë™û
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/ru/" hreflang="ru">
              üá∑üá∫ –†—É—Å—Å–∫–∏–π
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/de/" hreflang="de">
              üá©üá™ Deutsch
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/fr/" hreflang="fr">
              üá´üá∑ Fran√ßais
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/es/" hreflang="es">
              üá™üá∏ Espa√±ol
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/pt/" hreflang="pt">
              üáµüáπ Portugu√™s
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/it/" hreflang="it">
              üáÆüáπ Italiano
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/tr/" hreflang="tr">
              üáπüá∑ T√ºrk√ße
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/vi/" hreflang="vi">
              üáªüá≥ Ti·∫øng Vi·ªát
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/ar/" hreflang="ar">
              üá∏üá¶ ÿßŸÑÿπÿ±ÿ®Ÿäÿ©
            </a>
</li>
</ul>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
</div>
<div class="md-source__repository">
    ultralytics/ultralytics
  </div>
</a>
</div>
</nav>
<nav aria-label="Tabs" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../..">
  Home
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../quickstart/">
  Quickstart
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../modes/">
  Modes
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../tasks/">
  Tasks
        </a>
</li>
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="../">
  Models
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../datasets/">
  Datasets
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../solutions/">
  Solutions üöÄ
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../guides/">
  Guides
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../integrations/">
  Integrations
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../hub/">
  HUB
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../reference/cfg/__init__/">
  Reference
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../help/">
  Help
        </a>
</li>
</ul>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Ultralytics YOLO Docs" class="md-nav__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs">
<img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/>
</a>
    Ultralytics YOLO Docs
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
</div>
<div class="md-source__repository">
    ultralytics/ultralytics
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../..">
<span class="md-ellipsis">
    Home
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../quickstart/">
<span class="md-ellipsis">
    Quickstart
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../modes/">
<span class="md-ellipsis">
    Modes
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../tasks/">
<span class="md-ellipsis">
    Tasks
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_5" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../">
<span class="md-ellipsis">
    Models
  </span>
</a>
<label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_5_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_5">
<span class="md-nav__icon md-icon"></span>
            Models
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../yolov3/">
<span class="md-ellipsis">
    YOLOv3
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../yolov4/">
<span class="md-ellipsis">
    YOLOv4
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../yolov5/">
<span class="md-ellipsis">
    YOLOv5
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../yolov6/">
<span class="md-ellipsis">
    YOLOv6
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../yolov7/">
<span class="md-ellipsis">
    YOLOv7
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../yolov8/">
<span class="md-ellipsis">
    YOLOv8
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../yolov9/">
<span class="md-ellipsis">
    YOLOv9
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../yolov10/">
<span class="md-ellipsis">
    YOLOv10
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../yolo11/">
<span class="md-ellipsis">
    YOLO11 üöÄ NEW
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../yolo12/">
<span class="md-ellipsis">
    YOLO12
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../sam/">
<span class="md-ellipsis">
    SAM (Segment Anything Model)
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../sam-2/">
<span class="md-ellipsis">
    SAM 2 (Segment Anything Model 2)
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../mobile-sam/">
<span class="md-ellipsis">
    MobileSAM (Mobile Segment Anything Model)
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../fast-sam/">
<span class="md-ellipsis">
    FastSAM (Fast Segment Anything Model)
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../yolo-nas/">
<span class="md-ellipsis">
    YOLO-NAS (Neural Architecture Search)
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../rtdetr/">
<span class="md-ellipsis">
    RT-DETR (Realtime Detection Transformer)
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../yolo-world/">
<span class="md-ellipsis">
    YOLO-World (Real-Time Open-Vocabulary Object Detection)
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    YOLOE (Real-Time Seeing Anything)
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
<span class="md-ellipsis">
    YOLOE (Real-Time Seeing Anything)
  </span>
</a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#introduction">
<span class="md-ellipsis">
      Introduction
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#architecture-overview">
<span class="md-ellipsis">
      Architecture Overview
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#available-models-supported-tasks-and-operating-modes">
<span class="md-ellipsis">
      Available Models, Supported Tasks, and Operating Modes
    </span>
</a>
<nav aria-label="Available Models, Supported Tasks, and Operating Modes" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#textvisual-prompt-models">
<span class="md-ellipsis">
      Text/Visual Prompt models
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#prompt-free-models">
<span class="md-ellipsis">
      Prompt Free models
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#usage-examples">
<span class="md-ellipsis">
      Usage Examples
    </span>
</a>
<nav aria-label="Usage Examples" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#train-usage">
<span class="md-ellipsis">
      Train Usage
    </span>
</a>
<nav aria-label="Train Usage" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#fine-tuning-on-custom-dataset">
<span class="md-ellipsis">
      Fine-Tuning on custom dataset
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#predict-usage">
<span class="md-ellipsis">
      Predict Usage
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#val-usage">
<span class="md-ellipsis">
      Val Usage
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#train-official-models">
<span class="md-ellipsis">
      Train Official Models
    </span>
</a>
<nav aria-label="Train Official Models" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#prepare-datasets">
<span class="md-ellipsis">
      Prepare datasets
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#launching-training-from-scratch">
<span class="md-ellipsis">
      Launching training from scratch
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#yoloe-performance-comparison">
<span class="md-ellipsis">
      YOLOE Performance Comparison
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#comparison-with-previous-models">
<span class="md-ellipsis">
      Comparison with Previous Models
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#use-cases-and-applications">
<span class="md-ellipsis">
      Use Cases and Applications
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#training-and-inference">
<span class="md-ellipsis">
      Training and Inference
    </span>
</a>
<nav aria-label="Training and Inference" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#other-supported-tasks">
<span class="md-ellipsis">
      Other Supported Tasks
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#getting-started">
<span class="md-ellipsis">
      Getting Started
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#citations-and-acknowledgements">
<span class="md-ellipsis">
      Citations and Acknowledgements
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#faq">
<span class="md-ellipsis">
      FAQ
    </span>
</a>
<nav aria-label="FAQ" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#how-does-yoloe-differ-from-yolo-world">
<span class="md-ellipsis">
      How does YOLOE differ from YOLO-World?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#can-i-use-yoloe-as-a-regular-yolo-model">
<span class="md-ellipsis">
      Can I use YOLOE as a regular YOLO model?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#what-types-of-prompts-can-i-use-with-yoloe">
<span class="md-ellipsis">
      What types of prompts can I use with YOLOE?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#how-does-yoloe-handle-instance-segmentation">
<span class="md-ellipsis">
      How does YOLOE handle instance segmentation?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#how-does-yoloe-handle-inference-with-custom-prompts">
<span class="md-ellipsis">
      How does YOLOE handle inference with custom prompts?
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../datasets/">
<span class="md-ellipsis">
    Datasets
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../solutions/">
<span class="md-ellipsis">
    Solutions üöÄ
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../guides/">
<span class="md-ellipsis">
    Guides
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../integrations/">
<span class="md-ellipsis">
    Integrations
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../hub/">
<span class="md-ellipsis">
    HUB
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../reference/cfg/__init__/">
<span class="md-ellipsis">
    Reference
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../help/">
<span class="md-ellipsis">
    Help
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#introduction">
<span class="md-ellipsis">
      Introduction
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#architecture-overview">
<span class="md-ellipsis">
      Architecture Overview
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#available-models-supported-tasks-and-operating-modes">
<span class="md-ellipsis">
      Available Models, Supported Tasks, and Operating Modes
    </span>
</a>
<nav aria-label="Available Models, Supported Tasks, and Operating Modes" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#textvisual-prompt-models">
<span class="md-ellipsis">
      Text/Visual Prompt models
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#prompt-free-models">
<span class="md-ellipsis">
      Prompt Free models
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#usage-examples">
<span class="md-ellipsis">
      Usage Examples
    </span>
</a>
<nav aria-label="Usage Examples" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#train-usage">
<span class="md-ellipsis">
      Train Usage
    </span>
</a>
<nav aria-label="Train Usage" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#fine-tuning-on-custom-dataset">
<span class="md-ellipsis">
      Fine-Tuning on custom dataset
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#predict-usage">
<span class="md-ellipsis">
      Predict Usage
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#val-usage">
<span class="md-ellipsis">
      Val Usage
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#train-official-models">
<span class="md-ellipsis">
      Train Official Models
    </span>
</a>
<nav aria-label="Train Official Models" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#prepare-datasets">
<span class="md-ellipsis">
      Prepare datasets
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#launching-training-from-scratch">
<span class="md-ellipsis">
      Launching training from scratch
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#yoloe-performance-comparison">
<span class="md-ellipsis">
      YOLOE Performance Comparison
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#comparison-with-previous-models">
<span class="md-ellipsis">
      Comparison with Previous Models
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#use-cases-and-applications">
<span class="md-ellipsis">
      Use Cases and Applications
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#training-and-inference">
<span class="md-ellipsis">
      Training and Inference
    </span>
</a>
<nav aria-label="Training and Inference" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#other-supported-tasks">
<span class="md-ellipsis">
      Other Supported Tasks
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#getting-started">
<span class="md-ellipsis">
      Getting Started
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#citations-and-acknowledgements">
<span class="md-ellipsis">
      Citations and Acknowledgements
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#faq">
<span class="md-ellipsis">
      FAQ
    </span>
</a>
<nav aria-label="FAQ" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#how-does-yoloe-differ-from-yolo-world">
<span class="md-ellipsis">
      How does YOLOE differ from YOLO-World?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#can-i-use-yoloe-as-a-regular-yolo-model">
<span class="md-ellipsis">
      Can I use YOLOE as a regular YOLO model?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#what-types-of-prompts-can-i-use-with-yoloe">
<span class="md-ellipsis">
      What types of prompts can I use with YOLOE?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#how-does-yoloe-handle-instance-segmentation">
<span class="md-ellipsis">
      How does YOLOE handle instance segmentation?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#how-does-yoloe-handle-inference-with-custom-prompts">
<span class="md-ellipsis">
      How does YOLOE handle inference with custom prompts?
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<a class="md-content__button md-icon" href="https://github.com/ultralytics/ultralytics/tree/main/docs/en/models/yoloe.md" title="Edit this page">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
</a>
<h1 id="yoloe-real-time-seeing-anything">YOLOE: Real-Time Seeing Anything</h1>
<h2 id="introduction">Introduction</h2>
<p><img alt="YOLOE Prompting Options" src="https://raw.githubusercontent.com/THU-MIG/yoloe/main/figures/visualization.svg"/></p>
<p><a href="https://arxiv.org/html/2503.07465v1">YOLOE (Real-Time Seeing Anything)</a> is a new advancement in zero-shot, promptable YOLO models, designed for <strong>open-vocabulary</strong> detection and segmentation. Unlike previous YOLO models limited to fixed categories, YOLOE uses text, image, or internal vocabulary prompts, enabling real-time detection of any object class. Built upon YOLOv10 and inspired by <a href="../yolo-world/">YOLO-World</a>, YOLOE achieves <strong>state-of-the-art zero-shot performance</strong> with minimal impact on speed and accuracy.</p>
<p align="center">
<br/>
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="405" loading="lazy" src="https://www.youtube.com/embed/HMOoM2NwFIQ" title="YouTube video player" width="720">
</iframe>
<br/>
<strong>Watch:</strong> How to use YOLOE with Ultralytics Python package: Open Vocabulary &amp; Real-Time Seeing Anything üöÄ
</p>
<p>Compared to earlier YOLO models, YOLOE significantly boosts efficiency and accuracy. It improves by <strong>+3.5 AP</strong> over YOLO-Worldv2 on LVIS while using just a third of the training resources and achieving 1.4√ó faster inference speeds. Fine-tuned on COCO, YOLOE-v8-large surpasses YOLOv8-L by <strong>0.1 mAP</strong>, using nearly <strong>4√ó less training time</strong>. This demonstrates YOLOE's exceptional balance of accuracy, efficiency, and versatility. The sections below explore YOLOE's architecture, benchmark comparisons, and integration with the <a href="https://www.ultralytics.com/">Ultralytics</a> framework.</p>
<h2 id="architecture-overview">Architecture Overview</h2>
<p align="center">
<img alt="YOLOE Architecture" src="https://github.com/THU-MIG/yoloe/raw/main/figures/pipeline.svg" width="90%"/>
</p>
<p>YOLOE retains the standard YOLO structure‚Äîa convolutional <strong>backbone</strong> (e.g., CSP-Darknet) for feature extraction, a <strong>neck</strong> (e.g., PAN-FPN) for multi-scale fusion, and an <strong>anchor-free, decoupled</strong> detection <strong>head</strong> (as in YOLOv8/YOLO11) predicting objectness, classes, and boxes independently. YOLOE introduces three novel modules enabling open-vocabulary detection:</p>
<ul>
<li>
<p><strong>Re-parameterizable Region-Text Alignment (RepRTA)</strong>: Supports <strong>text-prompted detection</strong> by refining text <a href="https://www.ultralytics.com/glossary/embeddings">embeddings</a> (e.g., from CLIP) via a small auxiliary network. At inference, this network is folded into the main model, ensuring zero overhead. YOLOE thus detects arbitrary text-labeled objects (e.g., unseen "traffic light") without runtime penalties.</p>
</li>
<li>
<p><strong>Semantic-Activated Visual Prompt Encoder (SAVPE)</strong>: Enables <strong>visual-prompted detection</strong> via a lightweight embedding branch. Given a reference image, SAVPE encodes semantic and activation features, conditioning the model to detect visually similar objects‚Äîa one-shot detection capability useful for logos or specific parts.</p>
</li>
<li>
<p><strong>Lazy Region-Prompt Contrast (LRPC)</strong>: In <strong>prompt-free mode</strong>, YOLOE performs open-set recognition using internal embeddings trained on large vocabularies (1200+ categories from LVIS and Objects365). Without external prompts or encoders, YOLOE identifies objects via embedding similarity lookup, efficiently handling large label spaces at inference.</p>
</li>
</ul>
<p>Additionally, YOLOE integrates real-time <strong>instance segmentation</strong> by extending the detection head with a mask prediction branch (similar to YOLACT or YOLOv8-Seg), adding minimal overhead.</p>
<p>Crucially, YOLOE's open-world modules introduce <strong>no inference cost</strong> when used as a regular closed-set YOLO. Post-training, YOLOE parameters can be re-parameterized into a standard YOLO head, preserving identical FLOPs and speed (e.g., matching <a href="../yolo11/">YOLO11</a> exactly).</p>
<h2 id="available-models-supported-tasks-and-operating-modes">Available Models, Supported Tasks, and Operating Modes</h2>
<p>This section details the models available with their specific pre-trained weights, the tasks they support, and their compatibility with various operating modes such as <a href="../../modes/predict/">Inference</a>, <a href="../../modes/val/">Validation</a>, <a href="../../modes/train/">Training</a>, and <a href="../../modes/export/">Export</a>, denoted by ‚úÖ for supported modes and ‚ùå for unsupported modes.</p>
<h3 id="textvisual-prompt-models">Text/Visual Prompt models</h3>
<table>
<thead>
<tr>
<th>Model Type</th>
<th>Pre-trained Weights</th>
<th>Tasks Supported</th>
<th>Inference</th>
<th>Validation</th>
<th>Training</th>
<th>Export</th>
</tr>
</thead>
<tbody>
<tr>
<td>YOLOE-11S</td>
<td><a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-11s-seg.pt">yoloe-11s-seg.pt</a></td>
<td><a href="../../tasks/segment/">Instance Segmentation</a></td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>YOLOE-11M</td>
<td><a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-11m-seg.pt">yoloe-11m-seg.pt</a></td>
<td><a href="../../tasks/segment/">Instance Segmentation</a></td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>YOLOE-11L</td>
<td><a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-11l-seg.pt">yoloe-11l-seg.pt</a></td>
<td><a href="../../tasks/segment/">Instance Segmentation</a></td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>YOLOE-v8S</td>
<td><a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-v8s-seg.pt">yoloe-v8s-seg.pt</a></td>
<td><a href="../../tasks/segment/">Instance Segmentation</a></td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>YOLOE-v8M</td>
<td><a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-v8m-seg.pt">yoloe-v8m-seg.pt</a></td>
<td><a href="../../tasks/segment/">Instance Segmentation</a></td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>YOLOE-v8L</td>
<td><a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-v8l-seg.pt">yoloe-v8l-seg.pt</a></td>
<td><a href="../../tasks/segment/">Instance Segmentation</a></td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
</tbody>
</table>
<h3 id="prompt-free-models">Prompt Free models</h3>
<table>
<thead>
<tr>
<th>Model Type</th>
<th>Pre-trained Weights</th>
<th>Tasks Supported</th>
<th>Inference</th>
<th>Validation</th>
<th>Training</th>
<th>Export</th>
</tr>
</thead>
<tbody>
<tr>
<td>YOLOE-11S-PF</td>
<td><a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-11s-seg-pf.pt">yoloe-11s-seg-pf.pt</a></td>
<td><a href="../../tasks/segment/">Instance Segmentation</a></td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>YOLOE-11M-PF</td>
<td><a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-11m-seg-pf.pt">yoloe-11m-seg-pf.pt</a></td>
<td><a href="../../tasks/segment/">Instance Segmentation</a></td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>YOLOE-11L-PF</td>
<td><a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-11l-seg-pf.pt">yoloe-11l-seg-pf.pt</a></td>
<td><a href="../../tasks/segment/">Instance Segmentation</a></td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>YOLOE-v8S-PF</td>
<td><a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-v8s-seg-pf.pt">yoloe-v8s-seg-pf.pt</a></td>
<td><a href="../../tasks/segment/">Instance Segmentation</a></td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>YOLOE-v8M-PF</td>
<td><a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-v8m-seg-pf.pt">yoloe-v8m-seg-pf.pt</a></td>
<td><a href="../../tasks/segment/">Instance Segmentation</a></td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>YOLOE-v8L-PF</td>
<td><a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-v8l-seg-pf.pt">yoloe-v8l-seg-pf.pt</a></td>
<td><a href="../../tasks/segment/">Instance Segmentation</a></td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
</tbody>
</table>
<h2 id="usage-examples">Usage Examples</h2>
<p>The YOLOE models are easy to integrate into your Python applications. Ultralytics provides user-friendly <a href="../../usage/python/">Python API</a> and <a href="../../usage/cli/">CLI commands</a> to streamline development.</p>
<h3 id="train-usage">Train Usage</h3>
<h4 id="fine-tuning-on-custom-dataset">Fine-Tuning on custom dataset</h4>
<div class="admonition example">
<p class="admonition-title">Example</p>
<div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio"/><input id="__tabbed_1_2" name="__tabbed_1" type="radio"/><div class="tabbed-labels"><label for="__tabbed_1_1">Fine-Tuning</label><label for="__tabbed_1_2">Linear Probing</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics.models.yolo.yoloe</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOEPESegTrainer</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11s-seg.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
<span></span>    <span class="n">data</span><span class="o">=</span><span class="s2">"coco128-seg.yaml"</span><span class="p">,</span>
<span></span>    <span class="n">epochs</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span>
<span></span>    <span class="n">close_mosaic</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span></span>    <span class="n">batch</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
<span></span>    <span class="n">optimizer</span><span class="o">=</span><span class="s2">"AdamW"</span><span class="p">,</span>
<span></span>    <span class="n">lr0</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
<span></span>    <span class="n">warmup_bias_lr</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span></span>    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span>
<span></span>    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
<span></span>    <span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span></span>    <span class="n">device</span><span class="o">=</span><span class="s2">"0"</span><span class="p">,</span>
<span></span>    <span class="n">trainer</span><span class="o">=</span><span class="n">YOLOEPESegTrainer</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics.models.yolo.yoloe</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOEPESegTrainer</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11s-seg.pt"</span><span class="p">)</span>
<span></span><span class="n">head_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
<span></span><span class="n">freeze</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">head_index</span><span class="p">)]</span>
<span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span></span>    <span class="k">if</span> <span class="s2">"cv3"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
<span></span>        <span class="n">freeze</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">head_index</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span></span>
<span></span><span class="n">freeze</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
<span></span>    <span class="p">[</span>
<span></span>        <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">head_index</span><span class="si">}</span><span class="s2">.cv3.0.0"</span><span class="p">,</span>
<span></span>        <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">head_index</span><span class="si">}</span><span class="s2">.cv3.0.1"</span><span class="p">,</span>
<span></span>        <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">head_index</span><span class="si">}</span><span class="s2">.cv3.1.0"</span><span class="p">,</span>
<span></span>        <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">head_index</span><span class="si">}</span><span class="s2">.cv3.1.1"</span><span class="p">,</span>
<span></span>        <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">head_index</span><span class="si">}</span><span class="s2">.cv3.2.0"</span><span class="p">,</span>
<span></span>        <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">head_index</span><span class="si">}</span><span class="s2">.cv3.2.1"</span><span class="p">,</span>
<span></span>    <span class="p">]</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
<span></span>    <span class="n">data</span><span class="o">=</span><span class="s2">"coco128-seg.yaml"</span><span class="p">,</span>
<span></span>    <span class="n">epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span></span>    <span class="n">close_mosaic</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span></span>    <span class="n">batch</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span></span>    <span class="n">optimizer</span><span class="o">=</span><span class="s2">"AdamW"</span><span class="p">,</span>
<span></span>    <span class="n">lr0</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
<span></span>    <span class="n">warmup_bias_lr</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span></span>    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span>
<span></span>    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
<span></span>    <span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span></span>    <span class="n">device</span><span class="o">=</span><span class="s2">"0"</span><span class="p">,</span>
<span></span>    <span class="n">trainer</span><span class="o">=</span><span class="n">YOLOEPESegTrainer</span><span class="p">,</span>
<span></span>    <span class="n">freeze</span><span class="o">=</span><span class="n">freeze</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div>
</div>
</div>
</div>
</div>
<h3 id="predict-usage">Predict Usage</h3>
<p>YOLOE supports both text-based and visual prompting. Using prompts is straightforward‚Äîjust pass them through the <code>predict</code> method as shown below:</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<div class="tabbed-set tabbed-alternate" data-tabs="2:3"><input checked="checked" id="__tabbed_2_1" name="__tabbed_2" type="radio"/><input id="__tabbed_2_2" name="__tabbed_2" type="radio"/><input id="__tabbed_2_3" name="__tabbed_2" type="radio"/><div class="tabbed-labels"><label for="__tabbed_2_1">Text Prompt</label><label for="__tabbed_2_2">Visual Prompt</label><label for="__tabbed_2_3">Prompt free</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>Text prompts allow you to specify the classes that you wish to detect through textual descriptions. The following code shows how you can use YOLOE to detect people and buses in an image:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span>
<span></span><span class="c1"># Initialize a YOLOE model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.pt"</span><span class="p">)</span>  <span class="c1"># or select yoloe-11s/m-seg.pt for different sizes</span>
<span></span>
<span></span><span class="c1"># Set text prompt to detect person and bus. You only need to do this once after you load the model.</span>
<span></span><span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"person"</span><span class="p">,</span> <span class="s2">"bus"</span><span class="p">]</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">set_classes</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">get_text_pe</span><span class="p">(</span><span class="n">names</span><span class="p">))</span>
<span></span>
<span></span><span class="c1"># Run detection on the given image</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Show results</span>
<span></span><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
</div>
<div class="tabbed-block">
<p>Visual prompts allow you to guide the model by showing it visual examples of the target classes, rather than describing them in text.</p>
<p>The <code>visual_prompts</code> argument takes a dictionary with two keys: <code>bboxes</code> and <code>cls</code>. Each bounding box in <code>bboxes</code> should tightly enclose an example of the object you want the model to detect, and the corresponding entry in <code>cls</code> specifies the class label for that box. This pairing tells the model, "This is what class X looks like‚Äînow find more like it."</p>
<p>Class IDs (<code>cls</code>) in <code>visual_prompts</code> are used to associate each bounding box with a specific category within your prompt. They aren't fixed labels, but temporary identifiers you assign to each example. The only requirement is that class IDs must be sequential, starting from 0. This helps the model correctly associate each box with its respective class.</p>
<p>You can provide visual prompts directly within the same image you want to run inference on. For example:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span></span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics.models.yolo.yoloe</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOEVPSegPredictor</span>
<span></span>
<span></span><span class="c1"># Initialize a YOLOE model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Define visual prompts using bounding boxes and their corresponding class IDs.</span>
<span></span><span class="c1"># Each box highlights an example of the object you want the model to detect.</span>
<span></span><span class="n">visual_prompts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span></span>        <span class="p">[</span>
<span></span>            <span class="p">[</span><span class="mf">221.52</span><span class="p">,</span> <span class="mf">405.8</span><span class="p">,</span> <span class="mf">344.98</span><span class="p">,</span> <span class="mf">857.54</span><span class="p">],</span>  <span class="c1"># Box enclosing person</span>
<span></span>            <span class="p">[</span><span class="mi">120</span><span class="p">,</span> <span class="mi">425</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="mi">445</span><span class="p">],</span>  <span class="c1"># Box enclosing glasses</span>
<span></span>        <span class="p">],</span>
<span></span>    <span class="p">),</span>
<span></span>    <span class="bp">cls</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span></span>        <span class="p">[</span>
<span></span>            <span class="mi">0</span><span class="p">,</span>  <span class="c1"># ID to be assigned for person</span>
<span></span>            <span class="mi">1</span><span class="p">,</span>  <span class="c1"># ID to be assigned for glassses</span>
<span></span>        <span class="p">]</span>
<span></span>    <span class="p">),</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Run inference on an image, using the provided visual prompts as guidance</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
<span></span>    <span class="s2">"ultralytics/assets/bus.jpg"</span><span class="p">,</span>
<span></span>    <span class="n">visual_prompts</span><span class="o">=</span><span class="n">visual_prompts</span><span class="p">,</span>
<span></span>    <span class="n">predictor</span><span class="o">=</span><span class="n">YOLOEVPSegPredictor</span><span class="p">,</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Show results</span>
<span></span><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p>Or you can provide examples from a separate reference image using the <code>refer_image</code> argument. In that case, the <code>bboxes</code> and <code>cls</code> in <code>visual_prompts</code> should describe objects in the reference image, not the target image you're making predictions on:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code>source</code> is a video or stream, the model automatically uses the first frame as the <code>refer_image</code>. This means your <code>visual_prompts</code> are applied to that initial frame to help the model understand what to look for in the rest of the video. Alternatively, you can explicitly pass any specific frame as the <code>refer_image</code> to control which visual examples the model uses as reference.</p>
</div>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span></span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics.models.yolo.yoloe</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOEVPSegPredictor</span>
<span></span>
<span></span><span class="c1"># Initialize a YOLOE model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Define visual prompts based on a separate reference image</span>
<span></span><span class="n">visual_prompts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">221.52</span><span class="p">,</span> <span class="mf">405.8</span><span class="p">,</span> <span class="mf">344.98</span><span class="p">,</span> <span class="mf">857.54</span><span class="p">]]),</span>  <span class="c1"># Box enclosing person</span>
<span></span>    <span class="bp">cls</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]),</span>  <span class="c1"># ID to be assigned for person</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Run prediction on a different image, using reference image to guide what to look for</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
<span></span>    <span class="s2">"ultralytics/assets/zidane.jpg"</span><span class="p">,</span>  <span class="c1"># Target image for detection</span>
<span></span>    <span class="n">refer_image</span><span class="o">=</span><span class="s2">"ultralytics/assets/bus.jpg"</span><span class="p">,</span>  <span class="c1"># Reference image used to get visual prompts</span>
<span></span>    <span class="n">visual_prompts</span><span class="o">=</span><span class="n">visual_prompts</span><span class="p">,</span>
<span></span>    <span class="n">predictor</span><span class="o">=</span><span class="n">YOLOEVPSegPredictor</span><span class="p">,</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Show results</span>
<span></span><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p>You can also pass multiple target images to run prediction on:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span></span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics.models.yolo.yoloe</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOEVPSegPredictor</span>
<span></span>
<span></span><span class="c1"># Initialize a YOLOE model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Define visual prompts using bounding boxes and their corresponding class IDs.</span>
<span></span><span class="c1"># Each box highlights an example of the object you want the model to detect.</span>
<span></span><span class="n">visual_prompts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="p">[</span>
<span></span>        <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span></span>            <span class="p">[</span>
<span></span>                <span class="p">[</span><span class="mf">221.52</span><span class="p">,</span> <span class="mf">405.8</span><span class="p">,</span> <span class="mf">344.98</span><span class="p">,</span> <span class="mf">857.54</span><span class="p">],</span>  <span class="c1"># Box enclosing person</span>
<span></span>                <span class="p">[</span><span class="mi">120</span><span class="p">,</span> <span class="mi">425</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="mi">445</span><span class="p">],</span>  <span class="c1"># Box enclosing glasses</span>
<span></span>            <span class="p">],</span>
<span></span>        <span class="p">),</span>
<span></span>        <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1150</span><span class="p">,</span> <span class="mi">700</span><span class="p">]]),</span>
<span></span>    <span class="p">],</span>
<span></span>    <span class="bp">cls</span><span class="o">=</span><span class="p">[</span>
<span></span>        <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span></span>            <span class="p">[</span>
<span></span>                <span class="mi">0</span><span class="p">,</span>  <span class="c1"># ID to be assigned for person</span>
<span></span>                <span class="mi">1</span><span class="p">,</span>  <span class="c1"># ID to be assigned for glasses</span>
<span></span>            <span class="p">]</span>
<span></span>        <span class="p">),</span>
<span></span>        <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]),</span>
<span></span>    <span class="p">],</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Run inference on multiple image, using the provided visual prompts as guidance</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
<span></span>    <span class="p">[</span><span class="s2">"ultralytics/assets/bus.jpg"</span><span class="p">,</span> <span class="s2">"ultralytics/assets/zidane.jpg"</span><span class="p">],</span>
<span></span>    <span class="n">visual_prompts</span><span class="o">=</span><span class="n">visual_prompts</span><span class="p">,</span>
<span></span>    <span class="n">predictor</span><span class="o">=</span><span class="n">YOLOEVPSegPredictor</span><span class="p">,</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Show results</span>
<span></span><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
</div>
<div class="tabbed-block">
<p>YOLOE also includes prompt-free variants that come with a built-in vocabulary. These models don't require any prompts and work like traditional YOLO models. Instead of relying on user-provided labels or visual examples, they detect objects from a <a href="https://github.com/xinyu1205/recognize-anything/blob/main/ram/data/ram_tag_list.txt">predefined list of 4,585 classes</a> based on the tag set used by the <a href="https://arxiv.org/abs/2310.15200">Recognize Anything Model Plus (RAM++)</a>.</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span>
<span></span><span class="c1"># Initialize a YOLOE model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-seg-pf.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Run prediction. No prompts required.</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Show results</span>
<span></span><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
</div>
</div>
</div>
</div>
<h3 id="val-usage">Val Usage</h3>
<div class="admonition example">
<p class="admonition-title">Example</p>
<div class="tabbed-set tabbed-alternate" data-tabs="3:3"><input checked="checked" id="__tabbed_3_1" name="__tabbed_3" type="radio"/><input id="__tabbed_3_2" name="__tabbed_3" type="radio"/><input id="__tabbed_3_3" name="__tabbed_3" type="radio"><div class="tabbed-labels"><label for="__tabbed_3_1">Text Prompt</label><label for="__tabbed_3_2">Visual Prompt</label><label for="__tabbed_3_3">Prompt Free</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span>
<span></span><span class="c1"># Create a YOLOE model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.pt"</span><span class="p">)</span>  <span class="c1"># or select yoloe-11s/m-seg.pt for different sizes</span>
<span></span>
<span></span><span class="c1"># Conduct model validation on the COCO128-seg example dataset</span>
<span></span><span class="n">metrics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="s2">"coco128-seg.yaml"</span><span class="p">)</span>
</code></pre></div>
</div>
<div class="tabbed-block">
<p>Be default it's using the provided dataset to extract visual embeddings for each category.</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span>
<span></span><span class="c1"># Create a YOLOE model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.pt"</span><span class="p">)</span>  <span class="c1"># or select yoloe-11s/m-seg.pt for different sizes</span>
<span></span>
<span></span><span class="c1"># Conduct model validation on the COCO128-seg example dataset</span>
<span></span><span class="n">metrics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="s2">"coco128-seg.yaml"</span><span class="p">,</span> <span class="n">load_vp</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<p>Alternatively we could use another dataset as a reference dataset to extract visual embeddings for each category.
Note this reference dataset should have exactly the same categories as provided dataset.</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span>
<span></span><span class="c1"># Create a YOLOE model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.pt"</span><span class="p">)</span>  <span class="c1"># or select yoloe-11s/m-seg.pt for different sizes</span>
<span></span>
<span></span><span class="c1"># Conduct model validation on the COCO128-seg example dataset</span>
<span></span><span class="n">metrics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="s2">"coco128-seg.yaml"</span><span class="p">,</span> <span class="n">load_vp</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">refer_data</span><span class="o">=</span><span class="s2">"coco.yaml"</span><span class="p">)</span>
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span>
<span></span><span class="c1"># Create a YOLOE model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.pt"</span><span class="p">)</span>  <span class="c1"># or select yoloe-11s/m-seg.pt for different sizes</span>
<span></span>
<span></span><span class="c1"># Conduct model validation on the COCO128-seg example dataset</span>
<span></span><span class="n">metrics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">val</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="s2">"coco128-seg.yaml"</span><span class="p">)</span>
</code></pre></div>
</div>
</div>
</input></div>
</div>
<p>Model validation on a dataset is streamlined as follows:</p>
<h3 id="train-official-models">Train Official Models</h3>
<h4 id="prepare-datasets">Prepare datasets</h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Training official YOLOE models needs segment annotations for train data, here's <a href="https://github.com/THU-MIG/yoloe/blob/main/tools/generate_sam_masks.py">the script provided by official team</a> that converts datasets to segment annotations, powered by <a href="../sam-2/">SAM2.1 models</a>. Or you can directly download the provided <code>Processed Segment Annotations</code> in following table provided by official team.</p>
</div>
<ul>
<li>Train data</li>
</ul>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Type</th>
<th>Samples</th>
<th>Boxes</th>
<th>Raw Detection Annotations</th>
<th>Processed Segment Annotations</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://opendatalab.com/OpenDataLab/Objects365_v1">Objects365v1</a></td>
<td>Detection</td>
<td>609k</td>
<td>9621k</td>
<td><a href="https://opendatalab.com/OpenDataLab/Objects365_v1">objects365_train.json</a></td>
<td><a href="https://huggingface.co/datasets/jameslahm/yoloe/blob/main/objects365_train_segm.json">objects365_train_segm.json</a></td>
</tr>
<tr>
<td><a href="https://downloads.cs.stanford.edu/nlp/data/gqa/images.zip">GQA</a></td>
<td><a href="https://www.ultralytics.com/glossary/grounding">Grounding</a></td>
<td>621k</td>
<td>3681k</td>
<td><a href="https://huggingface.co/GLIPModel/GLIP/blob/main/mdetr_annotations/final_mixed_train_no_coco.json">final_mixed_train_no_coco.json</a></td>
<td><a href="https://huggingface.co/datasets/jameslahm/yoloe/blob/main/final_mixed_train_no_coco_segm.json">final_mixed_train_no_coco_segm.json</a></td>
</tr>
<tr>
<td><a href="https://shannon.cs.illinois.edu/DenotationGraph/">Flickr30k</a></td>
<td>Grounding</td>
<td>149k</td>
<td>641k</td>
<td><a href="https://huggingface.co/GLIPModel/GLIP/blob/main/mdetr_annotations/final_flickr_separateGT_train.json">final_flickr_separateGT_train.json</a></td>
<td><a href="https://huggingface.co/datasets/jameslahm/yoloe/blob/main/final_flickr_separateGT_train_segm.json">final_flickr_separateGT_train_segm.json</a></td>
</tr>
</tbody>
</table>
<ul>
<li>Val data</li>
</ul>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Type</th>
<th>Annotation Files</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/lvis.yaml">LVIS minival</a></td>
<td>Detection</td>
<td><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/lvis.yaml">minival.txt</a></td>
</tr>
</tbody>
</table>
<h4 id="launching-training-from-scratch">Launching training from scratch</h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code>Visual Prompt</code> models are fine-tuned based on trained-well <code>Text Prompt</code> models.</p>
</div>
<div class="admonition example">
<p class="admonition-title">Example</p>
<div class="tabbed-set tabbed-alternate" data-tabs="4:3"><input checked="checked" id="__tabbed_4_1" name="__tabbed_4" type="radio"/><input id="__tabbed_4_2" name="__tabbed_4" type="radio"/><input id="__tabbed_4_3" name="__tabbed_4" type="radio"/><div class="tabbed-labels"><label for="__tabbed_4_1">Text Prompt</label><label for="__tabbed_4_2">Visual Prompt</label><label for="__tabbed_4_3">Prompt Free</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics.models.yolo.yoloe</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOESegTrainerFromScratch</span>
<span></span>
<span></span><span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
<span></span>    <span class="n">train</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
<span></span>        <span class="n">yolo_data</span><span class="o">=</span><span class="p">[</span><span class="s2">"Objects365.yaml"</span><span class="p">],</span>
<span></span>        <span class="n">grounding_data</span><span class="o">=</span><span class="p">[</span>
<span></span>            <span class="nb">dict</span><span class="p">(</span>
<span></span>                <span class="n">img_path</span><span class="o">=</span><span class="s2">"../datasets/flickr/full_images/"</span><span class="p">,</span>
<span></span>                <span class="n">json_file</span><span class="o">=</span><span class="s2">"../datasets/flickr/annotations/final_flickr_separateGT_train_segm.json"</span><span class="p">,</span>
<span></span>            <span class="p">),</span>
<span></span>            <span class="nb">dict</span><span class="p">(</span>
<span></span>                <span class="n">img_path</span><span class="o">=</span><span class="s2">"../datasets/mixed_grounding/gqa/images"</span><span class="p">,</span>
<span></span>                <span class="n">json_file</span><span class="o">=</span><span class="s2">"../datasets/mixed_grounding/annotations/final_mixed_train_no_coco_segm.json"</span><span class="p">,</span>
<span></span>            <span class="p">),</span>
<span></span>        <span class="p">],</span>
<span></span>    <span class="p">),</span>
<span></span>    <span class="n">val</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">yolo_data</span><span class="o">=</span><span class="p">[</span><span class="s2">"lvis.yaml"</span><span class="p">]),</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.yaml"</span><span class="p">)</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
<span></span>    <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
<span></span>    <span class="n">batch</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
<span></span>    <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
<span></span>    <span class="n">close_mosaic</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span></span>    <span class="n">optimizer</span><span class="o">=</span><span class="s2">"AdamW"</span><span class="p">,</span>
<span></span>    <span class="n">lr0</span><span class="o">=</span><span class="mf">2e-3</span><span class="p">,</span>
<span></span>    <span class="n">warmup_bias_lr</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span></span>    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span>
<span></span>    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
<span></span>    <span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span></span>    <span class="n">trainer</span><span class="o">=</span><span class="n">YOLOESegTrainerFromScratch</span><span class="p">,</span>
<span></span>    <span class="n">device</span><span class="o">=</span><span class="s2">"0,1,2,3,4,5,6,7"</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div>
</div>
<div class="tabbed-block">
<p>Since only the <code>SAVPE</code> module needs to be updating during training.
Converting trained-well Text-prompt model to detection model and adopt detection pipeline with less training cost.
Note this step is optional, you can directly start from segmentation as well.</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span></span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span>
<span></span><span class="n">det_model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l.yaml"</span><span class="p">)</span>
<span></span><span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.pt"</span><span class="p">)</span>
<span></span><span class="n">det_model</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"model"</span><span class="p">])</span>
<span></span><span class="n">det_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"yoloe-11l-seg-det.pt"</span><span class="p">)</span>
</code></pre></div>
<p>Start training:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics.models.yolo.yoloe</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOESegVPTrainer</span>
<span></span>
<span></span><span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
<span></span>    <span class="n">train</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
<span></span>        <span class="n">yolo_data</span><span class="o">=</span><span class="p">[</span><span class="s2">"Objects365.yaml"</span><span class="p">],</span>
<span></span>        <span class="n">grounding_data</span><span class="o">=</span><span class="p">[</span>
<span></span>            <span class="nb">dict</span><span class="p">(</span>
<span></span>                <span class="n">img_path</span><span class="o">=</span><span class="s2">"../datasets/flickr/full_images/"</span><span class="p">,</span>
<span></span>                <span class="n">json_file</span><span class="o">=</span><span class="s2">"../datasets/flickr/annotations/final_flickr_separateGT_train_segm.json"</span><span class="p">,</span>
<span></span>            <span class="p">),</span>
<span></span>            <span class="nb">dict</span><span class="p">(</span>
<span></span>                <span class="n">img_path</span><span class="o">=</span><span class="s2">"../datasets/mixed_grounding/gqa/images"</span><span class="p">,</span>
<span></span>                <span class="n">json_file</span><span class="o">=</span><span class="s2">"../datasets/mixed_grounding/annotations/final_mixed_train_no_coco_segm.json"</span><span class="p">,</span>
<span></span>            <span class="p">),</span>
<span></span>        <span class="p">],</span>
<span></span>    <span class="p">),</span>
<span></span>    <span class="n">val</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">yolo_data</span><span class="o">=</span><span class="p">[</span><span class="s2">"lvis.yaml"</span><span class="p">]),</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.pt"</span><span class="p">)</span>
<span></span><span class="c1"># replace to yoloe-11l-seg-det.pt if converted to detection model</span>
<span></span><span class="c1"># model = YOLOE("yoloe-11l-seg-det.pt")</span>
<span></span>
<span></span><span class="c1"># freeze every layer except of the savpe module.</span>
<span></span><span class="n">head_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
<span></span><span class="n">freeze</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">head_index</span><span class="p">))</span>
<span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span></span>    <span class="k">if</span> <span class="s2">"savpe"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
<span></span>        <span class="n">freeze</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">head_index</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span></span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
<span></span>    <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
<span></span>    <span class="n">batch</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
<span></span>    <span class="n">epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span></span>    <span class="n">close_mosaic</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span></span>    <span class="n">optimizer</span><span class="o">=</span><span class="s2">"AdamW"</span><span class="p">,</span>
<span></span>    <span class="n">lr0</span><span class="o">=</span><span class="mf">16e-3</span><span class="p">,</span>
<span></span>    <span class="n">warmup_bias_lr</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span></span>    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span>
<span></span>    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
<span></span>    <span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span></span>    <span class="n">trainer</span><span class="o">=</span><span class="n">YOLOESegVPTrainer</span><span class="p">,</span>  <span class="c1"># use YOLOEVPTrainer if converted to detection model</span>
<span></span>    <span class="n">device</span><span class="o">=</span><span class="s2">"0,1,2,3,4,5,6,7"</span><span class="p">,</span>
<span></span>    <span class="n">freeze</span><span class="o">=</span><span class="n">freeze</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div>
<p>Convert back to segmentation model after training. Only needed if you converted segmentation model to detection model before training.</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">copy</span><span class="w"> </span><span class="kn">import</span> <span class="n">deepcopy</span>
<span></span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.yaml"</span><span class="p">)</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="n">vp_model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-vp.pt"</span><span class="p">)</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">savpe</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">vp_model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">savpe</span><span class="p">)</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.pt"</span><span class="p">)</span>
</code></pre></div>
</div>
<div class="tabbed-block">
<p>Similar to visual prompt training, for prompt-free model there's only the specialized prompt embedding needs to be updating during training.
Converting trained-well Text-prompt model to detection model and adopt detection pipeline with less training cost.
Note this step is optional, you can directly start from segmentation as well.</p>
<p><div class="highlight"><pre><span></span><code><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span></span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span>
<span></span><span class="n">det_model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l.yaml"</span><span class="p">)</span>
<span></span><span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.pt"</span><span class="p">)</span>
<span></span><span class="n">det_model</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"model"</span><span class="p">])</span>
<span></span><span class="n">det_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"yoloe-11l-seg-det.pt"</span><span class="p">)</span>
</code></pre></div>
Start training:
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span>
<span></span><span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
<span></span>    <span class="n">train</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
<span></span>        <span class="n">yolo_data</span><span class="o">=</span><span class="p">[</span><span class="s2">"Objects365.yaml"</span><span class="p">],</span>
<span></span>        <span class="n">grounding_data</span><span class="o">=</span><span class="p">[</span>
<span></span>            <span class="nb">dict</span><span class="p">(</span>
<span></span>                <span class="n">img_path</span><span class="o">=</span><span class="s2">"../datasets/flickr/full_images/"</span><span class="p">,</span>
<span></span>                <span class="n">json_file</span><span class="o">=</span><span class="s2">"../datasets/flickr/annotations/final_flickr_separateGT_train_segm.json"</span><span class="p">,</span>
<span></span>            <span class="p">),</span>
<span></span>            <span class="nb">dict</span><span class="p">(</span>
<span></span>                <span class="n">img_path</span><span class="o">=</span><span class="s2">"../datasets/mixed_grounding/gqa/images"</span><span class="p">,</span>
<span></span>                <span class="n">json_file</span><span class="o">=</span><span class="s2">"../datasets/mixed_grounding/annotations/final_mixed_train_no_coco_segm.json"</span><span class="p">,</span>
<span></span>            <span class="p">),</span>
<span></span>        <span class="p">],</span>
<span></span>    <span class="p">),</span>
<span></span>    <span class="n">val</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">yolo_data</span><span class="o">=</span><span class="p">[</span><span class="s2">"lvis.yaml"</span><span class="p">]),</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.pt"</span><span class="p">)</span>
<span></span><span class="c1"># replace to yoloe-11l-seg-det.pt if converted to detection model</span>
<span></span><span class="c1"># model = YOLOE("yoloe-11l-seg-det.pt")</span>
<span></span>
<span></span><span class="c1"># freeze layers.</span>
<span></span><span class="n">head_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
<span></span><span class="n">freeze</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">head_index</span><span class="p">)]</span>
<span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span></span>    <span class="k">if</span> <span class="s2">"cv3"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
<span></span>        <span class="n">freeze</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">head_index</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span></span>
<span></span><span class="n">freeze</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
<span></span>    <span class="p">[</span>
<span></span>        <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">head_index</span><span class="si">}</span><span class="s2">.cv3.0.0"</span><span class="p">,</span>
<span></span>        <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">head_index</span><span class="si">}</span><span class="s2">.cv3.0.1"</span><span class="p">,</span>
<span></span>        <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">head_index</span><span class="si">}</span><span class="s2">.cv3.1.0"</span><span class="p">,</span>
<span></span>        <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">head_index</span><span class="si">}</span><span class="s2">.cv3.1.1"</span><span class="p">,</span>
<span></span>        <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">head_index</span><span class="si">}</span><span class="s2">.cv3.2.0"</span><span class="p">,</span>
<span></span>        <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">head_index</span><span class="si">}</span><span class="s2">.cv3.2.1"</span><span class="p">,</span>
<span></span>    <span class="p">]</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
<span></span>    <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
<span></span>    <span class="n">batch</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
<span></span>    <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span></span>    <span class="n">close_mosaic</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span></span>    <span class="n">optimizer</span><span class="o">=</span><span class="s2">"AdamW"</span><span class="p">,</span>
<span></span>    <span class="n">lr0</span><span class="o">=</span><span class="mf">2e-3</span><span class="p">,</span>
<span></span>    <span class="n">warmup_bias_lr</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span></span>    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span>
<span></span>    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
<span></span>    <span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span></span>    <span class="n">trainer</span><span class="o">=</span><span class="n">YOLOEPEFreeTrainer</span><span class="p">,</span>
<span></span>    <span class="n">device</span><span class="o">=</span><span class="s2">"0,1,2,3,4,5,6,7"</span><span class="p">,</span>
<span></span>    <span class="n">freeze</span><span class="o">=</span><span class="n">freeze</span><span class="p">,</span>
<span></span>    <span class="n">single_cls</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># this is needed</span>
<span></span><span class="p">)</span>
</code></pre></div></p>
<p>Convert back to segmentation model after training. Only needed if you converted segmentation model to detection model before training.</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">copy</span><span class="w"> </span><span class="kn">import</span> <span class="n">deepcopy</span>
<span></span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLOE</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-seg.pt"</span><span class="p">)</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span></span>
<span></span><span class="n">pf_model</span> <span class="o">=</span> <span class="n">YOLOE</span><span class="p">(</span><span class="s2">"yoloe-11l-seg-pf.pt"</span><span class="p">)</span>
<span></span><span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"object"</span><span class="p">]</span>
<span></span><span class="n">tpe</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_text_pe</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">set_classes</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">tpe</span><span class="p">)</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fuse</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">pe</span><span class="p">)</span>
<span></span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cv3</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">pf_model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cv3</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cv3</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">pf_model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cv3</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cv3</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">pf_model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cv3</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span></span><span class="k">del</span> <span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">pe</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"yoloe-11l-seg-pf.pt"</span><span class="p">)</span>
</code></pre></div>
</div>
</div>
</div>
</div>
<h2 id="yoloe-performance-comparison">YOLOE Performance Comparison</h2>
<p>YOLOE matches or exceeds the accuracy of closed-set YOLO models on standard benchmarks like COCO, without compromising speed or model size. The table below compares YOLOE-L (built on YOLO11) against corresponding <a href="../yolov8/">YOLOv8</a> and YOLO11 models:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>COCO mAP<sub>50-95</sub></th>
<th>Inference Speed (T4)</th>
<th>Parameters</th>
<th>GFLOPs (640px)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>YOLOv8-L</strong> (closed-set)</td>
<td>52.9%</td>
<td><strong>9.06 ms</strong> (110 FPS)</td>
<td>43.7 M</td>
<td>165.2 B</td>
</tr>
<tr>
<td><strong>YOLO11-L</strong> (closed-set)</td>
<td>53.5%</td>
<td><strong>6.2 ms</strong> (130 FPS)</td>
<td>26.2 M</td>
<td>86.9 B</td>
</tr>
<tr>
<td><strong>YOLOE-L</strong> (open-vocab)</td>
<td>52.6%</td>
<td><strong>6.2 ms</strong> (130 FPS)</td>
<td>26.2 M</td>
<td>86.9 B<sup>‚Ä†</sup></td>
</tr>
</tbody>
</table>
<p><sup>‚Ä†</sup> <em>YOLO11-L and YOLOE-L have identical architectures (prompt modules disabled in YOLO11-L), resulting in identical inference speed and similar GFLOPs estimates.</em></p>
<p>YOLOE-L achieves <strong>52.6% mAP</strong>, surpassing YOLOv8-L (<strong>52.9%</strong>) with roughly <strong>40% fewer parameters</strong> (26M vs. 43.7M). It processes 640√ó640 images in <strong>6.2 ms (161 FPS)</strong> compared to YOLOv8-L's <strong>9.06 ms (110 FPS)</strong>, highlighting YOLO11's efficiency. Crucially, YOLOE's open-vocabulary modules incur <strong>no inference cost</strong>, demonstrating a <strong>"no free lunch trade-off"</strong> design.</p>
<p>For zero-shot and transfer tasks, YOLOE excels: on LVIS, YOLOE-small improves over YOLO-Worldv2 by <strong>+3.5 AP</strong> using <strong>3√ó less training resources</strong>. Fine-tuning YOLOE-L from LVIS to COCO also required <strong>4√ó less training time</strong> than YOLOv8-L, underscoring its efficiency and adaptability. YOLOE further maintains YOLO's hallmark speed, achieving <strong>300+ FPS</strong> on a T4 GPU and <strong>~64 FPS</strong> on iPhone 12 via CoreML, ideal for edge and mobile deployments.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Benchmark conditions:</strong> YOLOE results are from models pre-trained on Objects365, GoldG, and LVIS, then fine-tuned or evaluated on COCO. YOLOE's slight mAP advantage over YOLOv8 comes from extensive pre-training. Without this open-vocab training, YOLOE matches similar-sized YOLO models, affirming its SOTA accuracy and open-world flexibility without performance penalties.</p>
</div>
<h2 id="comparison-with-previous-models">Comparison with Previous Models</h2>
<p>YOLOE introduces notable advancements over prior YOLO models and open-vocabulary detectors:</p>
<ul>
<li>
<p><strong>YOLOE vs YOLOv5:</strong><br/>
<a href="../yolov5/">YOLOv5</a> offered good speed-accuracy balance but required retraining for new classes and used anchor-based heads. In contrast, YOLOE is <strong>anchor-free</strong> and dynamically detects new classes. YOLOE, building on YOLOv8's improvements, achieves higher accuracy (52.6% vs. YOLOv5's ~50% mAP on COCO) and integrates instance segmentation, unlike YOLOv5.</p>
</li>
<li>
<p><strong>YOLOE vs YOLOv8:</strong><br/>
  YOLOE extends <a href="../yolov8/">YOLOv8</a>'s redesigned architecture, achieving similar or superior accuracy (<strong>52.6% mAP with ~26M parameters</strong> vs. YOLOv8-L's <strong>52.9% with ~44M parameters</strong>). It significantly reduces training time due to stronger pre-training. The key advancement is YOLOE's <strong>open-world capability</strong>, detecting unseen objects (e.g., "<strong>bird scooter</strong>" or "<strong>peace symbol</strong>") via prompts, unlike YOLOv8's closed-set design.</p>
</li>
<li>
<p><strong>YOLOE vs YOLO11:</strong><br/>
<a href="../yolo11/">YOLO11</a> improves upon YOLOv8 with enhanced efficiency and fewer parameters (~22% reduction). YOLOE inherits these gains directly, matching YOLO11's inference speed and parameter count (~26M parameters), while adding <strong>open-vocabulary detection and segmentation</strong>. In closed-set scenarios, YOLOE is equivalent to YOLO11, but crucially adds adaptability to detect unseen classes, achieving <strong>YOLO11 + open-world capability</strong> without compromising speed.</p>
</li>
<li>
<p><strong>YOLOE vs previous open-vocabulary detectors:</strong><br/>
  Earlier open-vocab models (GLIP, OWL-ViT, <a href="../yolo-world/">YOLO-World</a>) relied heavily on vision-language <a href="https://www.ultralytics.com/glossary/transformer">transformers</a>, leading to slow inference. YOLOE surpasses these in zero-shot accuracy (e.g., <strong>+3.5 AP vs. YOLO-Worldv2</strong>) while running <strong>1.4√ó faster</strong> with significantly lower training resources. Compared to transformer-based approaches (e.g., GLIP), YOLOE offers orders-of-magnitude faster inference, effectively bridging the accuracy-efficiency gap in open-set detection.</p>
</li>
</ul>
<p>In summary, YOLOE maintains YOLO's renowned speed and efficiency, surpasses predecessors in accuracy, integrates segmentation, and introduces powerful open-world detection, making it uniquely versatile and practical.</p>
<h2 id="use-cases-and-applications">Use Cases and Applications</h2>
<p>YOLOE's open-vocabulary detection and segmentation enable diverse applications beyond traditional fixed-class models:</p>
<ul>
<li>
<p><strong>Open-World Object Detection:</strong><br/>
  Ideal for dynamic scenarios like <a href="https://www.ultralytics.com/blog/understanding-the-integration-of-computer-vision-in-robotics">robotics</a>, where robots recognize previously unseen objects using prompts, or <a href="https://www.ultralytics.com/blog/computer-vision-for-theft-prevention-enhancing-security">security systems</a> quickly adapting to new threats (e.g., hazardous items) without retraining.</p>
</li>
<li>
<p><strong>Few-Shot and One-Shot Detection:</strong><br/>
  Using visual prompts (SAVPE), YOLOE rapidly learns new objects from single reference images‚Äîperfect for <a href="https://www.ultralytics.com/blog/computer-vision-in-manufacturing-improving-production-and-quality">industrial inspection</a> (identifying parts or defects instantly) or <strong>custom surveillance</strong>, enabling visual searches with minimal setup.</p>
</li>
<li>
<p><strong>Large-Vocabulary &amp; Long-Tail Recognition:</strong><br/>
  Equipped with a vocabulary of 1000+ classes, YOLOE excels in tasks like <a href="https://www.ultralytics.com/blog/ai-in-wildlife-conservation">biodiversity monitoring</a> (detecting rare species), <strong>museum collections</strong>, <a href="https://www.ultralytics.com/blog/ai-for-smarter-retail-inventory-management">retail inventory</a>, or <strong>e-commerce</strong>, reliably identifying many classes without extensive per-class training.</p>
</li>
<li>
<p><strong>Interactive Detection and Segmentation:</strong><br/>
  YOLOE supports real-time interactive applications such as <strong>searchable video/image retrieval</strong>, <strong>augmented reality (AR)</strong>, and intuitive <strong>image editing</strong>, driven by natural inputs (text or visual prompts). Users can dynamically isolate, identify, or edit objects precisely using segmentation masks.</p>
</li>
<li>
<p><strong>Automated Data Labeling and Bootstrapping:</strong><br/>
  YOLOE facilitates rapid dataset creation by providing initial bounding box and segmentation annotations, significantly reducing human labeling efforts. Particularly valuable in <strong>analytics of large media collections</strong>, where it can auto-identify objects present, assisting in building specialized models faster.</p>
</li>
<li>
<p><strong>Segmentation for Any Object:</strong><br/>
  Extends segmentation capabilities to arbitrary objects through prompts‚Äîparticularly beneficial for <a href="https://www.ultralytics.com/blog/ai-and-radiology-a-new-era-of-precision-and-efficiency">medical imaging</a>, <strong>microscopy</strong>, or <a href="https://www.ultralytics.com/blog/using-computer-vision-to-analyse-satellite-imagery">satellite imagery analysis</a>, automatically identifying and precisely segmenting structures without specialized pre-trained models. Unlike models like <a href="../sam/">SAM</a>, YOLOE simultaneously recognizes and segments objects automatically, aiding in tasks like <strong>content creation</strong> or <strong>scene understanding</strong>.</p>
</li>
</ul>
<p>Across all these use cases, YOLOE's core advantage is <strong>versatility</strong>, providing a unified model for detection, recognition, and segmentation across dynamic scenarios. Its efficiency ensures real-time performance on resource-constrained devices, ideal for robotics, <a href="https://www.ultralytics.com/blog/ai-in-self-driving-cars">autonomous driving</a>, defense, and beyond.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Choose YOLOE's mode based on your needs:</p>
<ul>
<li><strong>Closed-set mode:</strong> For fixed-class tasks (max speed and accuracy).</li>
<li><strong>Prompted mode:</strong> Add new objects quickly via text or visual prompts.</li>
<li><strong>Prompt-free open-set mode:</strong> General detection across many categories (ideal for cataloging and discovery).</li>
</ul>
<p>Often, combining modes‚Äîsuch as prompt-free discovery followed by targeted prompts‚Äîleverages YOLOE's full potential.</p>
</div>
<h2 id="training-and-inference">Training and Inference</h2>
<p>YOLOE integrates seamlessly with the <a href="../../usage/python/">Ultralytics Python API</a> and <a href="../../usage/cli/">CLI</a>, similar to other YOLO models (YOLOv8, YOLO-World). Here's how to quickly get started:</p>
<div class="admonition example">
<p class="admonition-title">Training and inference with YOLOE</p>
<div class="tabbed-set tabbed-alternate" data-tabs="5:2"><input checked="checked" id="__tabbed_5_1" name="__tabbed_5" type="radio"/><input id="__tabbed_5_2" name="__tabbed_5" type="radio"/><div class="tabbed-labels"><label for="__tabbed_5_1">Python</label><label for="__tabbed_5_2">CLI</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="c1"># Load pre-trained YOLOE model and train on custom data</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yoloe-11s-seg.pt"</span><span class="p">)</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="s2">"path/to/data.yaml"</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">imgsz</span><span class="o">=</span><span class="mi">640</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Run inference using text prompts ("person", "bus")</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">set_classes</span><span class="p">([</span><span class="s2">"person"</span><span class="p">,</span> <span class="s2">"bus"</span><span class="p">])</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="s2">"test_images/street.jpg"</span><span class="p">)</span>
<span></span><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>  <span class="c1"># save annotated output</span>
</code></pre></div>
<p>Here, YOLOE behaves like a standard detector by default but easily switches to prompted detection by specifying classes (<code>set_classes</code>). Results contain bounding boxes, masks, and labels.</p>
</div>
<div class="tabbed-block">
<div class="highlight"><pre><span></span><code><span></span><span class="c1"># Training YOLOE on custom dataset</span>
<span></span>yolo<span class="w"> </span>train<span class="w"> </span><span class="nv">model</span><span class="o">=</span>yoloe-11s-seg.pt<span class="w"> </span><span class="nv">data</span><span class="o">=</span>path/to/data.yaml<span class="w"> </span><span class="nv">epochs</span><span class="o">=</span><span class="m">50</span><span class="w"> </span><span class="nv">imgsz</span><span class="o">=</span><span class="m">640</span>
<span></span>
<span></span><span class="c1"># Inference with text prompts</span>
<span></span>yolo<span class="w"> </span>predict<span class="w"> </span><span class="nv">model</span><span class="o">=</span>yoloe-11s-seg.pt<span class="w"> </span><span class="nv">source</span><span class="o">=</span><span class="s2">"test_images/street.jpg"</span><span class="w"> </span><span class="nv">classes</span><span class="o">=</span><span class="s2">"person,bus"</span>
</code></pre></div>
<p>CLI prompts (<code>classes</code>) guide YOLOE similarly to Python's <code>set_classes</code>. Visual prompting (image-based queries) currently requires the Python API.</p>
</div>
</div>
</div>
</div>
<h3 id="other-supported-tasks">Other Supported Tasks</h3>
<ul>
<li><strong>Validation:</strong> Evaluate accuracy easily with <code>model.val()</code> or <code>yolo val</code>.</li>
<li><strong>Export:</strong> Export YOLOE models (<code>model.export()</code>) to ONNX, TensorRT, etc., facilitating deployment.</li>
<li><strong>Tracking:</strong> YOLOE supports object tracking (<code>yolo track</code>) when integrated, useful for tracking prompted classes in videos.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>YOLOE automatically includes <strong>segmentation masks</strong> in inference results (<code>results[0].masks</code>), simplifying pixel-precise tasks like object extraction or measurement without needing separate models.</p>
</div>
<h2 id="getting-started">Getting Started</h2>
<p>Quickly set up YOLOE with Ultralytics by following these steps:</p>
<ol>
<li>
<p><strong>Installation</strong>:
   Install or update the Ultralytics package:</p>
<div class="highlight"><pre><span></span><code><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>ultralytics
</code></pre></div>
</li>
<li>
<p><strong>Download YOLOE Weights</strong>:
   Pre-trained YOLOE models (e.g., YOLOE-v8-S/L, YOLOE-11 variants) are available from the YOLOE GitHub releases. Simply download your desired <code>.pt</code> file to load into the Ultralytics YOLO class.</p>
</li>
<li>
<p><strong>Hardware Requirements</strong>:</p>
<ul>
<li><strong>Inference</strong>: Recommended GPU (NVIDIA with ‚â•4-8GB VRAM). Small models run efficiently on edge GPUs (e.g., <a href="../../guides/nvidia-jetson/">Jetson</a>) or CPUs at lower resolutions.</li>
<li><strong>Training</strong>: Fine-tuning YOLOE on custom data typically requires just one GPU. Extensive open-vocabulary pre-training (LVIS/Objects365) used by authors required substantial compute (8√ó RTX 4090 GPUs).</li>
</ul>
</li>
<li>
<p><strong>Configuration</strong>:
   YOLOE configurations use standard Ultralytics YAML files. Default configs (e.g., <code>yoloe-11s-seg.yaml</code>) typically suffice, but you can modify backbone, classes, or image size as needed.</p>
</li>
<li>
<p><strong>Running YOLOE</strong>:</p>
<ul>
<li><strong>Quick inference</strong> (prompt-free):
    <div class="highlight"><pre><span></span><code><span></span>yolo<span class="w"> </span>predict<span class="w"> </span><span class="nv">model</span><span class="o">=</span>yoloe-11s-seg-pf.pt<span class="w"> </span><span class="nv">source</span><span class="o">=</span><span class="s2">"image.jpg"</span>
</code></pre></div></li>
<li>
<p><strong>Prompted detection</strong> (text prompt example):</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yoloe-11s-seg.pt"</span><span class="p">)</span>
<span></span><span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"bowl"</span><span class="p">,</span> <span class="s2">"apple"</span><span class="p">]</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">set_classes</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">get_text_pe</span><span class="p">(</span><span class="n">names</span><span class="p">))</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s2">"kitchen.jpg"</span><span class="p">)</span>
<span></span><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
</code></pre></div>
</li>
</ul>
</li>
<li>
<p><strong>Integration Tips</strong>:</p>
<ul>
<li><strong>Class names</strong>: Default YOLOE outputs use LVIS categories; use <code>set_classes()</code> to specify your own labels.</li>
<li><strong>Speed</strong>: YOLOE has no overhead unless using prompts. Text prompts have minimal impact; visual prompts slightly more.</li>
<li><strong>Batch inference</strong>: Supported directly (<code>model.predict([img1, img2])</code>). For image-specific prompts, run images individually.</li>
</ul>
</li>
</ol>
<p>The <a href="https://docs.ultralytics.com/">Ultralytics documentation</a> provides further resources. YOLOE lets you easily explore powerful open-world capabilities within the familiar YOLO ecosystem.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>Pro Tip:</strong>
To maximize YOLOE's zero-shot accuracy, fine-tune from provided checkpoints rather than training from scratch. Use prompt words aligning with common training labels (see LVIS categories) to improve detection accuracy.</p>
</div>
<h2 id="citations-and-acknowledgements">Citations and Acknowledgements</h2>
<p>If YOLOE has contributed to your research or project, please cite the original paper by <strong>Ao Wang, Lihao Liu, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding</strong> from <strong>Tsinghua University</strong>:</p>
<div class="admonition quote">
<div class="tabbed-set tabbed-alternate" data-tabs="6:1"><input checked="checked" id="__tabbed_6_1" name="__tabbed_6" type="radio"/><div class="tabbed-labels"><label for="__tabbed_6_1">BibTeX</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight"><pre><span></span><code><span></span><span class="nc">@misc</span><span class="p">{</span><span class="nl">wang2025yoloerealtimeseeing</span><span class="p">,</span>
<span></span><span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{YOLOE: Real-Time Seeing Anything}</span><span class="p">,</span>
<span></span><span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Ao Wang and Lihao Liu and Hui Chen and Zijia Lin and Jungong Han and Guiguang Ding}</span><span class="p">,</span>
<span></span><span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span></span><span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2503.07465}</span><span class="p">,</span>
<span></span><span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span></span><span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CV}</span><span class="p">,</span>
<span></span><span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2503.07465}</span><span class="p">,</span>
<span></span><span class="p">}</span>
</code></pre></div>
</div>
</div>
</div>
</div>
<p>For further reading, the original YOLOE paper is available on <a href="https://arxiv.org/html/2503.07465v1">arXiv</a>. The project's source code and additional resources can be accessed via their <a href="https://github.com/THU-MIG/yoloe">GitHub repository</a>.</p>
<h2 id="faq">FAQ</h2>
<h3 id="how-does-yoloe-differ-from-yolo-world">How does YOLOE differ from YOLO-World?</h3>
<p>While both YOLOE and <a href="../yolo-world/">YOLO-World</a> enable open-vocabulary detection, YOLOE offers several advantages. YOLOE achieves +3.5 AP higher accuracy on LVIS while using 3√ó less training resources and running 1.4√ó faster than YOLO-Worldv2. YOLOE also supports three prompting modes (text, visual, and internal vocabulary), whereas YOLO-World primarily focuses on text prompts. Additionally, YOLOE includes built-in <a href="https://www.ultralytics.com/blog/what-is-instance-segmentation-a-quick-guide">instance segmentation</a> capabilities, providing pixel-precise masks for detected objects without additional overhead.</p>
<h3 id="can-i-use-yoloe-as-a-regular-yolo-model">Can I use YOLOE as a regular YOLO model?</h3>
<p>Yes, YOLOE can function exactly like a standard YOLO model with no performance penalty. When used in closed-set mode (without prompts), YOLOE's open-vocabulary modules are re-parameterized into the standard detection head, resulting in identical speed and accuracy to equivalent YOLO11 models. This makes YOLOE extremely versatile‚Äîyou can use it as a traditional detector for maximum speed and then switch to open-vocabulary mode only when needed.</p>
<h3 id="what-types-of-prompts-can-i-use-with-yoloe">What types of prompts can I use with YOLOE?</h3>
<p>YOLOE supports three types of prompts:</p>
<ol>
<li><strong>Text prompts</strong>: Specify object classes using natural language (e.g., "person", "traffic light", "bird scooter")</li>
<li><strong>Visual prompts</strong>: Provide reference images of objects you want to detect</li>
<li><strong>Internal vocabulary</strong>: Use YOLOE's built-in vocabulary of 1200+ categories without external prompts</li>
</ol>
<p>This flexibility allows you to adapt YOLOE to various scenarios without retraining the model, making it particularly useful for dynamic environments where detection requirements change frequently.</p>
<h3 id="how-does-yoloe-handle-instance-segmentation">How does YOLOE handle instance segmentation?</h3>
<p>YOLOE integrates instance segmentation directly into its architecture by extending the detection head with a mask prediction branch. This approach is similar to YOLOv8-Seg but works for any prompted object class. Segmentation masks are automatically included in inference results and can be accessed via <code>results[0].masks</code>. This unified approach eliminates the need for separate detection and segmentation models, streamlining workflows for applications requiring pixel-precise object boundaries.</p>
<h3 id="how-does-yoloe-handle-inference-with-custom-prompts">How does YOLOE handle inference with custom prompts?</h3>
<p>Similar to <a href="../yolo-world/">YOLO-World</a>, YOLOE supports a "prompt-then-detect" strategy that utilizes an offline vocabulary to enhance efficiency. Custom prompts like captions or specific object categories are pre-encoded and stored as offline vocabulary embeddings. This approach streamlines the detection process without requiring retraining. You can dynamically set these prompts within the model to tailor it to specific detection tasks:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="c1"># Initialize a YOLOE model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yoloe-11s-seg.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Define custom classes</span>
<span></span><span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"person"</span><span class="p">,</span> <span class="s2">"bus"</span><span class="p">]</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">set_classes</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">get_text_pe</span><span class="p">(</span><span class="n">names</span><span class="p">))</span>
<span></span>
<span></span><span class="c1"># Execute prediction on an image</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Show results</span>
<span></span><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<br/><br/>
<div class="git-info">
<div class="dates-container">
<span class="date-item" title="This page was first created on March 19, 2025">
<span class="hover-item">üìÖ</span> Created 2 months ago
    </span>
<span class="date-item" title="This page was last updated on April 29, 2025">
<span class="hover-item">‚úèÔ∏è</span> Updated 21 days ago
    </span>
</div>
<div class="authors-container">
<a class="author-link" href="https://github.com/Laughing-q" title="Laughing-q (3 changes)">
<img alt="Laughing-q" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/61612323?v=4&amp;s=96"/>
</a>
<a class="author-link" href="https://github.com/RizwanMunawar" title="RizwanMunawar (7 changes)">
<img alt="RizwanMunawar" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/62513924?v=4&amp;s=96"/>
</a>
<a class="author-link" href="https://github.com/glenn-jocher" title="glenn-jocher (3 changes)">
<img alt="glenn-jocher" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/26833433?v=4&amp;s=96"/>
</a>
<a class="author-link" href="https://github.com/Y-T-G" title="Y-T-G (3 changes)">
<img alt="Y-T-G" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/32206511?v=4&amp;s=96"/>
</a>
<a class="author-link" href="https://github.com/JShengP" title="JShengP (1 change)">
<img alt="JShengP" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/36942973?v=4&amp;s=96"/>
</a>
<a class="author-link" href="https://github.com/leonnil" title="leonnil (1 change)">
<img alt="leonnil" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/146309319?v=4&amp;s=96"/>
</a>
</div></div><div class="share-buttons">
<button class="share-button hover-item" onclick="window.open('https://twitter.com/intent/tweet?url=https://docs.ultralytics.com/models/yoloe', 'TwitterShare', 'width=550,height=680,menubar=no,toolbar=no'); return false;">
<i class="fa-brands fa-x-twitter"></i> Tweet
    </button>
<button class="share-button hover-item linkedin" onclick="window.open('https://www.linkedin.com/shareArticle?url=https://docs.ultralytics.com/models/yoloe', 'LinkedinShare', 'width=550,height=730,menubar=no,toolbar=no'); return false;">
<i class="fa-brands fa-linkedin-in"></i> Share
    </button>
</div>
<br/>
<h2 id="__comments">Comments</h2>
<div id="giscus-container"></div>
</article>
</div>
<script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<button class="md-top md-icon" data-md-component="top" hidden="" type="button">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
</main>
<footer class="md-footer">
<nav aria-label="Footer" class="md-footer__inner md-grid">
<a aria-label="Previous: YOLO-World (Real-Time Open-Vocabulary Object Detection)" class="md-footer__link md-footer__link--prev" href="../yolo-world/">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
</div>
<div class="md-footer__title">
<span class="md-footer__direction">
                Previous
              </span>
<div class="md-ellipsis">
                YOLO-World (Real-Time Open-Vocabulary Object Detection)
              </div>
</div>
</a>
<a aria-label="Next: Datasets Overview" class="md-footer__link md-footer__link--next" href="../../datasets/">
<div class="md-footer__title">
<span class="md-footer__direction">
                Next
              </span>
<div class="md-ellipsis">
                Datasets Overview
              </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
<div class="md-copyright__highlight">
<a href="https://www.ultralytics.com/" target="_blank">¬© 2025 Ultralytics Inc.</a> All rights reserved.
    </div>
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
<div class="md-social">
<a class="md-social__link" href="https://github.com/ultralytics" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
</a>
<a class="md-social__link" href="https://www.linkedin.com/company/ultralytics/" rel="noopener" target="_blank" title="www.linkedin.com">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"></path></svg>
</a>
<a class="md-social__link" href="https://x.com/ultralytics" rel="noopener" target="_blank" title="x.com">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"></path></svg>
</a>
<a class="md-social__link" href="https://youtube.com/ultralytics?sub_confirmation=1" rel="noopener" target="_blank" title="youtube.com">
<svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305m-317.51 213.508V175.185l142.739 81.205z"></path></svg>
</a>
<a class="md-social__link" href="https://hub.docker.com/r/ultralytics/ultralytics/" rel="noopener" target="_blank" title="hub.docker.com">
<svg viewbox="0 0 640 512" xmlns="http://www.w3.org/2000/svg"><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"></path></svg>
</a>
<a class="md-social__link" href="https://pypi.org/project/ultralytics/" rel="noopener" target="_blank" title="pypi.org">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3"></path></svg>
</a>
<a class="md-social__link" href="https://discord.com/invite/ultralytics" rel="noopener" target="_blank" title="discord.com">
<svg viewbox="0 0 640 512" xmlns="http://www.w3.org/2000/svg"><path d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485 485 0 0 0 404.081 32.03a1.82 1.82 0 0 0-1.923.91 338 338 0 0 0-14.9 30.6 447.9 447.9 0 0 0-134.426 0 310 310 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.7 483.7 0 0 0-119.688 37.107 1.7 1.7 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.02 2.02 0 0 0 .765 1.375 487.7 487.7 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348 348 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321 321 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251 251 0 0 0 9.109-7.137 1.82 1.82 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.81 1.81 0 0 1 1.924.233 235 235 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.4 301.4 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391 391 0 0 0 30.014 48.815 1.86 1.86 0 0 0 2.063.7A486 486 0 0 0 610.7 405.729a1.88 1.88 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541M222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241m195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241"></path></svg>
</a>
<a class="md-social__link" href="https://reddit.com/r/ultralytics" rel="noopener" target="_blank" title="reddit.com">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M0 256C0 114.6 114.6 0 256 0s256 114.6 256 256-114.6 256-256 256H37.1c-13.7 0-20.5-16.5-10.9-26.2L75 437C28.7 390.7 0 326.7 0 256m349.6-102.4c23.6 0 42.7-19.1 42.7-42.7s-19.1-42.7-42.7-42.7c-20.6 0-37.8 14.6-41.8 34-34.5 3.7-61.4 33-61.4 68.4v.2c-37.5 1.6-71.8 12.3-99 29.1-10.1-7.8-22.8-12.5-36.5-12.5-33 0-59.8 26.8-59.8 59.8 0 24 14.1 44.6 34.4 54.1 2 69.4 77.6 125.2 170.6 125.2s168.7-55.9 170.6-125.3c20.2-9.6 34.1-30.2 34.1-54 0-33-26.8-59.8-59.8-59.8-13.7 0-26.3 4.6-36.4 12.4-27.4-17-62.1-27.7-100-29.1v-.2c0-25.4 18.9-46.5 43.4-49.9 4.4 18.8 21.3 32.8 41.5 32.8zm-172.5 93.3c16.7 0 29.5 17.6 28.5 39.3s-13.5 29.6-30.3 29.6-31.4-8.8-30.4-30.5S160.3 247 177 247zm190.1 38.3c1 21.7-13.7 30.5-30.4 30.5s-29.3-7.9-30.3-29.6 11.8-39.3 28.5-39.3 31.2 16.6 32.1 38.3zm-48.1 56.7c-10.3 24.6-34.6 41.9-63 41.9s-52.7-17.3-63-41.9c-1.2-2.9.8-6.2 3.9-6.5 18.4-1.9 38.3-2.9 59.1-2.9s40.7 1 59.1 2.9c3.1.3 5.1 3.6 3.9 6.5"></path></svg>
</a>
<a class="md-social__link" href="https://weixin.qq.com/r/mp/LxckPDfEgWr_rXNf90I9" rel="noopener" target="_blank" title="weixin.qq.com">
<svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M385.2 167.6c6.4 0 12.6.3 18.8 1.1C387.4 90.3 303.3 32 207.7 32 100.5 32 13 104.8 13 197.4c0 53.4 29.3 97.5 77.9 131.6l-19.3 58.6 68-34.1c24.4 4.8 43.8 9.7 68.2 9.7 6.2 0 12.1-.3 18.3-.8-4-12.9-6.2-26.6-6.2-40.8-.1-84.9 72.9-154 165.3-154m-104.5-52.9c14.5 0 24.2 9.7 24.2 24.4 0 14.5-9.7 24.2-24.2 24.2-14.8 0-29.3-9.7-29.3-24.2.1-14.7 14.6-24.4 29.3-24.4m-136.4 48.6c-14.5 0-29.3-9.7-29.3-24.2 0-14.8 14.8-24.4 29.3-24.4 14.8 0 24.4 9.7 24.4 24.4 0 14.6-9.6 24.2-24.4 24.2M563 319.4c0-77.9-77.9-141.3-165.4-141.3-92.7 0-165.4 63.4-165.4 141.3S305 460.7 397.6 460.7c19.3 0 38.9-5.1 58.6-9.9l53.4 29.3-14.8-48.6C534 402.1 563 363.2 563 319.4m-219.1-24.5c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.8 0 24.4 9.7 24.4 19.3 0 10-9.7 19.6-24.4 19.6m107.1 0c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.5 0 24.4 9.7 24.4 19.3.1 10-9.9 19.6-24.4 19.6"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<div class="md-progress" data-md-component="progress" role="progressbar"></div>
<script id="__config" type="application/json">{"base": "../..", "features": ["content.action.edit", "content.code.annotate", "content.code.copy", "content.tooltips", "toc.follow", "navigation.top", "navigation.tabs", "navigation.tabs.sticky", "navigation.prune", "navigation.footer", "navigation.tracking", "navigation.instant", "navigation.instant.progress", "navigation.indexes", "navigation.sections", "content.tabs.link"], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
<script src="../../assets/javascripts/bundle.13a4f30d.min.js"></script>
<script src="../../javascript/extra.js"></script>
<script src="../../javascript/giscus.js"></script>
<script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
<script src="../../javascript/tablesort.js"></script>
</body>
</html>