<!doctypehtml><html class=no-js lang=en><meta charset=utf-8><meta content=width=device-width,initial-scale=1 name=viewport><meta content="Discover SAM 2, the next generation of Meta's Segment Anything Model, supporting real-time promptable segmentation in both images and videos with state-of-the-art performance. Learn about its key features, datasets, and how to use it."name=description><meta content=Ultralytics name=author><link href=https://docs.ultralytics.com/models/sam-2/ rel=canonical><link href=../sam/ rel=prev><link href=../mobile-sam/ rel=next><link href=../../assets/favicon.ico rel=icon><meta content="mkdocs-1.6.1, mkdocs-material-9.5.42"name=generator><title>SAM 2 (Segment Anything Model 2) - Ultralytics YOLO Docs</title><link href=../../assets/stylesheets/main.0253249f.min.css rel=stylesheet><link href=../../assets/stylesheets/palette.06af60db.min.css rel=stylesheet><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href=https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback rel=stylesheet><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link href=../../assets/_mkdocstrings.css rel=stylesheet><link href=../../stylesheets/style.css rel=stylesheet><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-2M5EHKC0BH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-2M5EHKC0BH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-2M5EHKC0BH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta content="SAM 2 (Segment Anything Model 2)"name=title><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css rel=stylesheet><meta content="SAM 2, SAM 2.1, Segment Anything, video segmentation, image segmentation, promptable segmentation, zero-shot performance, SA-V dataset, Ultralytics, real-time segmentation, AI, machine learning"name=keywords><meta content=website property=og:type><meta content=https://docs.ultralytics.com/models/sam-2 property=og:url><meta content="SAM 2 (Segment Anything Model 2)"property=og:title><meta content="Discover SAM 2, the next generation of Meta's Segment Anything Model, supporting real-time promptable segmentation in both images and videos with state-of-the-art performance. Learn about its key features, datasets, and how to use it."property=og:description><meta content=https://github.com/ultralytics/docs/releases/download/0/sa-v-dataset.avif property=og:image><meta content=summary_large_image property=twitter:card><meta content=https://docs.ultralytics.com/models/sam-2 property=twitter:url><meta content="SAM 2 (Segment Anything Model 2)"property=twitter:title><meta content="Discover SAM 2, the next generation of Meta's Segment Anything Model, supporting real-time promptable segmentation in both images and videos with state-of-the-art performance. Learn about its key features, datasets, and how to use it."property=twitter:description><meta content=https://github.com/ultralytics/docs/releases/download/0/sa-v-dataset.avif property=twitter:image><script type=application/ld+json>{"@context": "https://schema.org", "@type": ["Article", "FAQPage"], "headline": "SAM 2 (Segment Anything Model 2)", "image": ["https://github.com/ultralytics/docs/releases/download/0/sa-v-dataset.avif"], "datePublished": "2024-07-30 11:33:54 +0200", "dateModified": "2024-10-25 08:17:46 +0800", "author": [{"@type": "Organization", "name": "Ultralytics", "url": "https://ultralytics.com/"}], "abstract": "Discover SAM 2, the next generation of Meta's Segment Anything Model, supporting real-time promptable segmentation in both images and videos with state-of-the-art performance. Learn about its key features, datasets, and how to use it.", "mainEntity": [{"@type": "Question", "name": "What is SAM 2 and how does it improve upon the original Segment Anything Model (SAM)?", "acceptedAnswer": {"@type": "Answer", "text": "SAM 2, the successor to Meta's Segment Anything Model (SAM), is a cutting-edge tool designed for comprehensive object segmentation in both images and videos. It excels in handling complex visual data through a unified, promptable model architecture that supports real-time processing and zero-shot generalization. SAM 2 offers several improvements over the original SAM, including: For more details on SAM 2's architecture and capabilities, explore the SAM 2 research paper."}}, {"@type": "Question", "name": "How can I use SAM 2 for real-time video segmentation?", "acceptedAnswer": {"@type": "Answer", "text": "SAM 2 can be utilized for real-time video segmentation by leveraging its promptable interface and real-time inference capabilities. Here's a basic example: For more comprehensive usage, refer to the How to Use SAM 2 section."}}, {"@type": "Question", "name": "What datasets are used to train SAM 2, and how do they enhance its performance?", "acceptedAnswer": {"@type": "Answer", "text": "SAM 2 is trained on the SA-V dataset, one of the largest and most diverse video segmentation datasets available. The SA-V dataset includes: This extensive dataset allows SAM 2 to achieve superior performance across major video segmentation benchmarks and enhances its zero-shot generalization capabilities. For more information, see the SA-V Dataset section."}}, {"@type": "Question", "name": "How does SAM 2 handle occlusions and object reappearances in video segmentation?", "acceptedAnswer": {"@type": "Answer", "text": "SAM 2 includes a sophisticated memory mechanism to manage temporal dependencies and occlusions in video data. The memory mechanism consists of: This mechanism ensures continuity even when objects are temporarily obscured or exit and re-enter the scene. For more details, refer to the Memory Mechanism and Occlusion Handling section."}}, {"@type": "Question", "name": "How does SAM 2 compare to other segmentation models like YOLOv8?", "acceptedAnswer": {"@type": "Answer", "text": "SAM 2 and Ultralytics YOLOv8 serve different purposes and excel in different areas. While SAM 2 is designed for comprehensive object segmentation with advanced features like zero-shot generalization and real-time performance, YOLOv8 is optimized for speed and efficiency in object detection and segmentation tasks. Here's a comparison: For more details, see the SAM 2 comparison vs YOLOv8 section."}}]}</script><body data-md-color-accent=indigo data-md-color-primary=indigo data-md-color-scheme=default dir=ltr><input autocomplete=off class=md-toggle data-md-toggle=drawer id=__drawer type=checkbox><input autocomplete=off class=md-toggle data-md-toggle=search id=__search type=checkbox><label class=md-overlay for=__drawer></label><div data-md-component=skip><a class=md-skip href=#sam-2-segment-anything-model-2> Skip to content </a></div><div data-md-component=announce><aside class=md-banner><div class="md-banner__inner md-grid md-typeset"><div class=banner-wrapper><div class=banner-content-wrapper><p>YOLO Vision 2024 is here!<div class=banner-info-wrapper><img alt="YOLO Vision 24"height=20 loading=lazy src=https://assets-global.website-files.com/646dd1f1a3703e451ba81ecc/66e9a87cfc78245ffa51d6f0_w_yv24.svg width=20><p>September 27, 2024</div><div class=banner-info-wrapper><img alt="YOLO Vision 24"height=20 loading=lazy src=https://assets-global.website-files.com/646dd1f1a3703e451ba81ecc/66e9a87cdfbd25e409560ed8_l_yv24.svg width=20><p>Free hybrid event</div></div><div class=banner-button-wrapper><div class="banner-button-wrapper large"><button onclick="window.open('https://www.ultralytics.com/events/yolovision', '_blank')">Register now</button></div></div></div></div></aside></div><header class="md-header md-header--shadow md-header--lifted"data-md-component=header><nav class="md-header__inner md-grid"aria-label=Header><a aria-label="Ultralytics YOLO Docs"class="md-header__button md-logo"title="Ultralytics YOLO Docs"data-md-component=logo href=../..> <img alt=logo src=https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg> </a><label class="md-header__button md-icon"for=__drawer><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg></label><div class=md-header__title data-md-component=header-title><div class=md-header__ellipsis><div class=md-header__topic><span class=md-ellipsis> Ultralytics YOLO Docs </span></div><div class=md-header__topic data-md-component=header-topic><span class=md-ellipsis> SAM 2 (Segment Anything Model 2) </span></div></div></div><form class=md-header__option data-md-component=palette><input aria-label="Switch to light mode"class=md-option data-md-color-accent=indigo data-md-color-media=(prefers-color-scheme) data-md-color-primary=indigo data-md-color-scheme=default id=__palette_0 name=__palette type=radio><label class="md-header__button md-icon"title="Switch to light mode"for=__palette_1 hidden><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg></label><input aria-label="Switch to system preference"data-md-color-media="(prefers-color-scheme: dark)"class=md-option data-md-color-accent=indigo data-md-color-primary=black data-md-color-scheme=slate id=__palette_1 name=__palette type=radio><label class="md-header__button md-icon"title="Switch to system preference"for=__palette_2 hidden><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label><input aria-label="Switch to dark mode"data-md-color-media="(prefers-color-scheme: light)"class=md-option data-md-color-accent=indigo data-md-color-primary=indigo data-md-color-scheme=default id=__palette_2 name=__palette type=radio><label class="md-header__button md-icon"title="Switch to dark mode"for=__palette_0 hidden><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label></form><script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script><label class="md-header__button md-icon"for=__search><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg></label><div class=md-search data-md-component=search role=dialog><label class=md-search__overlay for=__search></label><div class=md-search__inner role=search><form class=md-search__form name=search><input aria-label=Search autocapitalize=off autocomplete=off autocorrect=off class=md-search__input data-md-component=search-query name=query placeholder=Search required spellcheck=false><label class="md-search__icon md-icon"for=__search><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg></label><nav aria-label=Search class=md-search__options><a class="md-search__icon md-icon"aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share href=javascript:void(0) tabindex=-1 title=Share> <svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a><button class="md-search__icon md-icon"aria-label=Clear tabindex=-1 title=Clear type=reset><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg></button></nav><div class=md-search__suggest data-md-component=search-suggest></div></form><div class=md-search__output><div class=md-search__scrollwrap data-md-scrollfix tabindex=0><div class=md-search-result data-md-component=search-result><div class=md-search-result__meta>Initializing search</div><ol class=md-search-result__list role=presentation></ol></div></div></div></div></div><div class=md-header__source><a title="Go to repository"class=md-source data-md-component=source href=https://github.com/ultralytics/ultralytics> <div class="md-source__icon md-icon"><svg viewbox="0 0 496 512"xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg></div> <div class=md-source__repository>ultralytics/ultralytics</div> </a></div></nav><nav aria-label=Tabs class=md-tabs data-md-component=tabs><div class=md-grid><ul class=md-tabs__list><li class=md-tabs__item><a class=md-tabs__link href=../..> Home </a><li class=md-tabs__item><a class=md-tabs__link href=../../quickstart/> Quickstart </a><li class=md-tabs__item><a class=md-tabs__link href=../../modes/> Modes </a><li class=md-tabs__item><a class=md-tabs__link href=../../tasks/> Tasks </a><li class="md-tabs__item md-tabs__item--active"><a class=md-tabs__link href=../> Models </a><li class=md-tabs__item><a class=md-tabs__link href=../../datasets/> Datasets </a><li class=md-tabs__item><a class=md-tabs__link href=../../solutions/> Solutions üöÄ NEW </a><li class=md-tabs__item><a class=md-tabs__link href=../../guides/> Guides </a><li class=md-tabs__item><a class=md-tabs__link href=../../integrations/> Integrations </a><li class=md-tabs__item><a class=md-tabs__link href=../../hub/> HUB </a><li class=md-tabs__item><a class=md-tabs__link href=../../reference/cfg/__init__/> Reference </a><li class=md-tabs__item><a class=md-tabs__link href=../../help/> Help </a></ul></div></nav></header><div class=md-container data-md-component=container><main class=md-main data-md-component=main><div class="md-main__inner md-grid"><div class="md-sidebar md-sidebar--primary"data-md-component=sidebar data-md-type=navigation><div class=md-sidebar__scrollwrap><div class=md-sidebar__inner><nav class="md-nav md-nav--primary md-nav--lifted"aria-label=Navigation data-md-level=0><label class=md-nav__title for=__drawer><a aria-label="Ultralytics YOLO Docs"class="md-nav__button md-logo"title="Ultralytics YOLO Docs"data-md-component=logo href=../..> <img alt=logo src=https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg> </a> Ultralytics YOLO Docs</label><div class=md-nav__source><a title="Go to repository"class=md-source data-md-component=source href=https://github.com/ultralytics/ultralytics> <div class="md-source__icon md-icon"><svg viewbox="0 0 496 512"xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg></div> <div class=md-source__repository>ultralytics/ultralytics</div> </a></div><ul class=md-nav__list data-md-scrollfix><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../..> <span class=md-ellipsis> Home </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../quickstart/> <span class=md-ellipsis> Quickstart </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../modes/> <span class=md-ellipsis> Modes </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../tasks/> <span class=md-ellipsis> Tasks </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle"checked id=__nav_5 type=checkbox> <div class="md-nav__link md-nav__container"><a class=md-nav__link href=../> <span class=md-ellipsis> Models </span> </a><label class=md-nav__link for=__nav_5 id=__nav_5_label><span class="md-nav__icon md-icon"></span></label></div> <nav aria-expanded=true aria-labelledby=__nav_5_label class=md-nav data-md-level=1><label class=md-nav__title for=__nav_5><span class="md-nav__icon md-icon"></span> Models</label><ul class=md-nav__list data-md-scrollfix><li class=md-nav__item><a class=md-nav__link href=../yolov3/> <span class=md-ellipsis> YOLOv3 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolov4/> <span class=md-ellipsis> YOLOv4 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolov5/> <span class=md-ellipsis> YOLOv5 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolov6/> <span class=md-ellipsis> YOLOv6 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolov7/> <span class=md-ellipsis> YOLOv7 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolov8/> <span class=md-ellipsis> YOLOv8 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolov9/> <span class=md-ellipsis> YOLOv9 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolov10/> <span class=md-ellipsis> YOLOv10 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolo11/> <span class=md-ellipsis> YOLO11 üöÄ NEW </span> </a><li class=md-nav__item><a class=md-nav__link href=../sam/> <span class=md-ellipsis> SAM (Segment Anything Model) </span> </a><li class="md-nav__item md-nav__item--active"><input class="md-nav__toggle md-toggle"id=__toc type=checkbox> <label class="md-nav__link md-nav__link--active"for=__toc><span class=md-ellipsis> SAM 2 (Segment Anything Model 2) </span> <span class="md-nav__icon md-icon"></span></label> <a class="md-nav__link md-nav__link--active"href=./> <span class=md-ellipsis> SAM 2 (Segment Anything Model 2) </span> </a> <nav aria-label="Table of contents"class="md-nav md-nav--secondary"><label class=md-nav__title for=__toc><span class="md-nav__icon md-icon"></span> Table of contents</label><ul class=md-nav__list data-md-component=toc data-md-scrollfix><li class=md-nav__item><a class=md-nav__link href=#key-features> <span class=md-ellipsis> Key Features </span> </a> <nav aria-label="Key Features"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#unified-model-architecture> <span class=md-ellipsis> Unified Model Architecture </span> </a><li class=md-nav__item><a class=md-nav__link href=#real-time-performance> <span class=md-ellipsis> Real-Time Performance </span> </a><li class=md-nav__item><a class=md-nav__link href=#zero-shot-generalization> <span class=md-ellipsis> Zero-Shot Generalization </span> </a><li class=md-nav__item><a class=md-nav__link href=#interactive-refinement> <span class=md-ellipsis> Interactive Refinement </span> </a><li class=md-nav__item><a class=md-nav__link href=#advanced-handling-of-visual-challenges> <span class=md-ellipsis> Advanced Handling of Visual Challenges </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#performance-and-technical-details> <span class=md-ellipsis> Performance and Technical Details </span> </a><li class=md-nav__item><a class=md-nav__link href=#model-architecture> <span class=md-ellipsis> Model Architecture </span> </a> <nav aria-label="Model Architecture"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#core-components> <span class=md-ellipsis> Core Components </span> </a><li class=md-nav__item><a class=md-nav__link href=#memory-mechanism-and-occlusion-handling> <span class=md-ellipsis> Memory Mechanism and Occlusion Handling </span> </a><li class=md-nav__item><a class=md-nav__link href=#multi-mask-ambiguity-resolution> <span class=md-ellipsis> Multi-Mask Ambiguity Resolution </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#sa-v-dataset> <span class=md-ellipsis> SA-V Dataset </span> </a><li class=md-nav__item><a class=md-nav__link href=#benchmarks> <span class=md-ellipsis> Benchmarks </span> </a> <nav aria-label=Benchmarks class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#video-object-segmentation> <span class=md-ellipsis> Video Object Segmentation </span> </a><li class=md-nav__item><a class=md-nav__link href=#interactive-segmentation> <span class=md-ellipsis> Interactive Segmentation </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#installation> <span class=md-ellipsis> Installation </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-to-use-sam-2-versatility-in-image-and-video-segmentation> <span class=md-ellipsis> How to Use SAM 2: Versatility in Image and Video Segmentation </span> </a> <nav aria-label="How to Use SAM 2: Versatility in Image and Video Segmentation"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#sam-2-prediction-examples> <span class=md-ellipsis> SAM 2 Prediction Examples </span> </a> <nav aria-label="SAM 2 Prediction Examples"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#segment-with-prompts> <span class=md-ellipsis> Segment with Prompts </span> </a><li class=md-nav__item><a class=md-nav__link href=#segment-everything> <span class=md-ellipsis> Segment Everything </span> </a></ul></nav></ul></nav><li class=md-nav__item><a class=md-nav__link href=#sam-2-comparison-vs-yolov8> <span class=md-ellipsis> SAM 2 comparison vs YOLOv8 </span> </a><li class=md-nav__item><a class=md-nav__link href=#auto-annotation-efficient-dataset-creation> <span class=md-ellipsis> Auto-Annotation: Efficient Dataset Creation </span> </a> <nav aria-label="Auto-Annotation: Efficient Dataset Creation"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#how-to-auto-annotate-with-sam-2> <span class=md-ellipsis> How to Auto-Annotate with SAM 2 </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#limitations> <span class=md-ellipsis> Limitations </span> </a><li class=md-nav__item><a class=md-nav__link href=#citations-and-acknowledgements> <span class=md-ellipsis> Citations and Acknowledgements </span> </a><li class=md-nav__item><a class=md-nav__link href=#faq> <span class=md-ellipsis> FAQ </span> </a> <nav aria-label=FAQ class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#what-is-sam-2-and-how-does-it-improve-upon-the-original-segment-anything-model-sam> <span class=md-ellipsis> What is SAM 2 and how does it improve upon the original Segment Anything Model (SAM)? </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-can-i-use-sam-2-for-real-time-video-segmentation> <span class=md-ellipsis> How can I use SAM 2 for real-time video segmentation? </span> </a><li class=md-nav__item><a class=md-nav__link href=#what-datasets-are-used-to-train-sam-2-and-how-do-they-enhance-its-performance> <span class=md-ellipsis> What datasets are used to train SAM 2, and how do they enhance its performance? </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-does-sam-2-handle-occlusions-and-object-reappearances-in-video-segmentation> <span class=md-ellipsis> How does SAM 2 handle occlusions and object reappearances in video segmentation? </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-does-sam-2-compare-to-other-segmentation-models-like-yolov8> <span class=md-ellipsis> How does SAM 2 compare to other segmentation models like YOLOv8? </span> </a></ul></nav></ul></nav><li class=md-nav__item><a class=md-nav__link href=../mobile-sam/> <span class=md-ellipsis> MobileSAM (Mobile Segment Anything Model) </span> </a><li class=md-nav__item><a class=md-nav__link href=../fast-sam/> <span class=md-ellipsis> FastSAM (Fast Segment Anything Model) </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolo-nas/> <span class=md-ellipsis> YOLO-NAS (Neural Architecture Search) </span> </a><li class=md-nav__item><a class=md-nav__link href=../rtdetr/> <span class=md-ellipsis> RT-DETR (Realtime Detection Transformer) </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolo-world/> <span class=md-ellipsis> YOLO-World (Real-Time Open-Vocabulary Object Detection) </span> </a></ul></nav><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../datasets/> <span class=md-ellipsis> Datasets </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../solutions/> <span class=md-ellipsis> Solutions üöÄ NEW </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../guides/> <span class=md-ellipsis> Guides </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../integrations/> <span class=md-ellipsis> Integrations </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../hub/> <span class=md-ellipsis> HUB </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../reference/cfg/__init__/> <span class=md-ellipsis> Reference </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../help/> <span class=md-ellipsis> Help </span> <span class="md-nav__icon md-icon"></span> </a></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary"data-md-component=sidebar data-md-type=toc><div class=md-sidebar__scrollwrap><div class=md-sidebar__inner><nav aria-label="Table of contents"class="md-nav md-nav--secondary"><label class=md-nav__title for=__toc><span class="md-nav__icon md-icon"></span> Table of contents</label><ul class=md-nav__list data-md-component=toc data-md-scrollfix><li class=md-nav__item><a class=md-nav__link href=#key-features> <span class=md-ellipsis> Key Features </span> </a> <nav aria-label="Key Features"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#unified-model-architecture> <span class=md-ellipsis> Unified Model Architecture </span> </a><li class=md-nav__item><a class=md-nav__link href=#real-time-performance> <span class=md-ellipsis> Real-Time Performance </span> </a><li class=md-nav__item><a class=md-nav__link href=#zero-shot-generalization> <span class=md-ellipsis> Zero-Shot Generalization </span> </a><li class=md-nav__item><a class=md-nav__link href=#interactive-refinement> <span class=md-ellipsis> Interactive Refinement </span> </a><li class=md-nav__item><a class=md-nav__link href=#advanced-handling-of-visual-challenges> <span class=md-ellipsis> Advanced Handling of Visual Challenges </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#performance-and-technical-details> <span class=md-ellipsis> Performance and Technical Details </span> </a><li class=md-nav__item><a class=md-nav__link href=#model-architecture> <span class=md-ellipsis> Model Architecture </span> </a> <nav aria-label="Model Architecture"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#core-components> <span class=md-ellipsis> Core Components </span> </a><li class=md-nav__item><a class=md-nav__link href=#memory-mechanism-and-occlusion-handling> <span class=md-ellipsis> Memory Mechanism and Occlusion Handling </span> </a><li class=md-nav__item><a class=md-nav__link href=#multi-mask-ambiguity-resolution> <span class=md-ellipsis> Multi-Mask Ambiguity Resolution </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#sa-v-dataset> <span class=md-ellipsis> SA-V Dataset </span> </a><li class=md-nav__item><a class=md-nav__link href=#benchmarks> <span class=md-ellipsis> Benchmarks </span> </a> <nav aria-label=Benchmarks class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#video-object-segmentation> <span class=md-ellipsis> Video Object Segmentation </span> </a><li class=md-nav__item><a class=md-nav__link href=#interactive-segmentation> <span class=md-ellipsis> Interactive Segmentation </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#installation> <span class=md-ellipsis> Installation </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-to-use-sam-2-versatility-in-image-and-video-segmentation> <span class=md-ellipsis> How to Use SAM 2: Versatility in Image and Video Segmentation </span> </a> <nav aria-label="How to Use SAM 2: Versatility in Image and Video Segmentation"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#sam-2-prediction-examples> <span class=md-ellipsis> SAM 2 Prediction Examples </span> </a> <nav aria-label="SAM 2 Prediction Examples"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#segment-with-prompts> <span class=md-ellipsis> Segment with Prompts </span> </a><li class=md-nav__item><a class=md-nav__link href=#segment-everything> <span class=md-ellipsis> Segment Everything </span> </a></ul></nav></ul></nav><li class=md-nav__item><a class=md-nav__link href=#sam-2-comparison-vs-yolov8> <span class=md-ellipsis> SAM 2 comparison vs YOLOv8 </span> </a><li class=md-nav__item><a class=md-nav__link href=#auto-annotation-efficient-dataset-creation> <span class=md-ellipsis> Auto-Annotation: Efficient Dataset Creation </span> </a> <nav aria-label="Auto-Annotation: Efficient Dataset Creation"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#how-to-auto-annotate-with-sam-2> <span class=md-ellipsis> How to Auto-Annotate with SAM 2 </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#limitations> <span class=md-ellipsis> Limitations </span> </a><li class=md-nav__item><a class=md-nav__link href=#citations-and-acknowledgements> <span class=md-ellipsis> Citations and Acknowledgements </span> </a><li class=md-nav__item><a class=md-nav__link href=#faq> <span class=md-ellipsis> FAQ </span> </a> <nav aria-label=FAQ class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#what-is-sam-2-and-how-does-it-improve-upon-the-original-segment-anything-model-sam> <span class=md-ellipsis> What is SAM 2 and how does it improve upon the original Segment Anything Model (SAM)? </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-can-i-use-sam-2-for-real-time-video-segmentation> <span class=md-ellipsis> How can I use SAM 2 for real-time video segmentation? </span> </a><li class=md-nav__item><a class=md-nav__link href=#what-datasets-are-used-to-train-sam-2-and-how-do-they-enhance-its-performance> <span class=md-ellipsis> What datasets are used to train SAM 2, and how do they enhance its performance? </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-does-sam-2-handle-occlusions-and-object-reappearances-in-video-segmentation> <span class=md-ellipsis> How does SAM 2 handle occlusions and object reappearances in video segmentation? </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-does-sam-2-compare-to-other-segmentation-models-like-yolov8> <span class=md-ellipsis> How does SAM 2 compare to other segmentation models like YOLOv8? </span> </a></ul></nav></ul></nav></div></div></div><div class=md-content data-md-component=content><article class="md-content__inner md-typeset"><a class="md-content__button md-icon"title="Edit this page"href=https://github.com/ultralytics/ultralytics/tree/main/docs/en/models/sam-2.md> <svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg> </a><div class="admonition tip"><p class=admonition-title>SAM 2.1<p>We have just supported the more accurate SAM2.1 model. Please give it a try!</div><h1 id=sam-2-segment-anything-model-2>SAM 2: Segment Anything Model 2</h1><p>SAM 2, the successor to Meta's <a href=../sam/>Segment Anything Model (SAM)</a>, is a cutting-edge tool designed for comprehensive object segmentation in both images and videos. It excels in handling complex visual data through a unified, promptable model architecture that supports real-time processing and zero-shot generalization.<p><img alt="SAM 2 Example Results"src=https://github.com/ultralytics/docs/releases/download/0/sa-v-dataset.avif><h2 id=key-features>Key Features</h2><p align=center><br> <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"title="YouTube video player"allowfullscreen frameborder=0 height=405 loading=lazy src=https://www.youtube.com/embed/yXQPLMrNX2s width=720></iframe> <br> <strong>Watch:</strong> How to Run Inference with Meta's SAM2 using Ultralytics | Step-by-Step Guide üéâ<h3 id=unified-model-architecture>Unified Model Architecture</h3><p>SAM 2 combines the capabilities of image and video segmentation in a single model. This unification simplifies deployment and allows for consistent performance across different media types. It leverages a flexible prompt-based interface, enabling users to specify objects of interest through various prompt types, such as points, bounding boxes, or masks.<h3 id=real-time-performance>Real-Time Performance</h3><p>The model achieves real-time inference speeds, processing approximately 44 frames per second. This makes SAM 2 suitable for applications requiring immediate feedback, such as video editing and augmented reality.<h3 id=zero-shot-generalization>Zero-Shot Generalization</h3><p>SAM 2 can segment objects it has never encountered before, demonstrating strong zero-shot generalization. This is particularly useful in diverse or evolving visual domains where pre-defined categories may not cover all possible objects.<h3 id=interactive-refinement>Interactive Refinement</h3><p>Users can iteratively refine the segmentation results by providing additional prompts, allowing for precise control over the output. This interactivity is essential for fine-tuning results in applications like video annotation or medical imaging.<h3 id=advanced-handling-of-visual-challenges>Advanced Handling of Visual Challenges</h3><p>SAM 2 includes mechanisms to manage common video segmentation challenges, such as object occlusion and reappearance. It uses a sophisticated memory mechanism to keep track of objects across frames, ensuring continuity even when objects are temporarily obscured or exit and re-enter the scene.<p>For a deeper understanding of SAM 2's architecture and capabilities, explore the <a href=https://arxiv.org/abs/2401.12741>SAM 2 research paper</a>.<h2 id=performance-and-technical-details>Performance and Technical Details</h2><p>SAM 2 sets a new benchmark in the field, outperforming previous models on various metrics:<table><thead><tr><th>Metric<th>SAM 2<th>Previous SOTA<tbody><tr><td><strong>Interactive Video Segmentation</strong><td><strong>Best</strong><td>-<tr><td><strong>Human Interactions Required</strong><td><strong>3x fewer</strong><td>Baseline<tr><td><strong><a href=https://www.ultralytics.com/glossary/image-segmentation>Image Segmentation</a> Accuracy</strong><td><strong>Improved</strong><td>SAM<tr><td><strong>Inference Speed</strong><td><strong>6x faster</strong><td>SAM</table><h2 id=model-architecture>Model Architecture</h2><h3 id=core-components>Core Components</h3><ul><li><strong>Image and Video Encoder</strong>: Utilizes a <a href=https://www.ultralytics.com/glossary/transformer>transformer</a>-based architecture to extract high-level features from both images and video frames. This component is responsible for understanding the visual content at each timestep.<li><strong>Prompt Encoder</strong>: Processes user-provided prompts (points, boxes, masks) to guide the segmentation task. This allows SAM 2 to adapt to user input and target specific objects within a scene.<li><strong>Memory Mechanism</strong>: Includes a memory encoder, memory bank, and memory attention module. These components collectively store and utilize information from past frames, enabling the model to maintain consistent object tracking over time.<li><strong>Mask Decoder</strong>: Generates the final segmentation masks based on the encoded image features and prompts. In video, it also uses memory context to ensure accurate tracking across frames.</ul><p><img alt="SAM 2 Architecture Diagram"src=https://raw.githubusercontent.com/facebookresearch/sam2/refs/heads/main/assets/model_diagram.png><h3 id=memory-mechanism-and-occlusion-handling>Memory Mechanism and Occlusion Handling</h3><p>The memory mechanism allows SAM 2 to handle temporal dependencies and occlusions in video data. As objects move and interact, SAM 2 records their features in a memory bank. When an object becomes occluded, the model can rely on this memory to predict its position and appearance when it reappears. The occlusion head specifically handles scenarios where objects are not visible, predicting the likelihood of an object being occluded.<h3 id=multi-mask-ambiguity-resolution>Multi-Mask Ambiguity Resolution</h3><p>In situations with ambiguity (e.g., overlapping objects), SAM 2 can generate multiple mask predictions. This feature is crucial for accurately representing complex scenes where a single mask might not sufficiently describe the scene's nuances.<h2 id=sa-v-dataset>SA-V Dataset</h2><p>The SA-V dataset, developed for SAM 2's training, is one of the largest and most diverse video segmentation datasets available. It includes:<ul><li><strong>51,000+ Videos</strong>: Captured across 47 countries, providing a wide range of real-world scenarios.<li><strong>600,000+ Mask Annotations</strong>: Detailed spatio-temporal mask annotations, referred to as "masklets," covering whole objects and parts.<li><strong>Dataset Scale</strong>: It features 4.5 times more videos and 53 times more annotations than previous largest datasets, offering unprecedented diversity and complexity.</ul><h2 id=benchmarks>Benchmarks</h2><h3 id=video-object-segmentation>Video Object Segmentation</h3><p>SAM 2 has demonstrated superior performance across major video segmentation benchmarks:<table><thead><tr><th>Dataset<th>J&F<th>J<th>F<tbody><tr><td><strong>DAVIS 2017</strong><td>82.5<td>79.8<td>85.2<tr><td><strong>YouTube-VOS</strong><td>81.2<td>78.9<td>83.5</table><h3 id=interactive-segmentation>Interactive Segmentation</h3><p>In interactive segmentation tasks, SAM 2 shows significant efficiency and accuracy:<table><thead><tr><th>Dataset<th>NoC@90<th>AUC<tbody><tr><td><strong>DAVIS Interactive</strong><td>1.54<td>0.872</table><h2 id=installation>Installation</h2><p>To install SAM 2, use the following command. All SAM 2 models will automatically download on first use.<div class=highlight><pre><span></span><code><a href=#__codelineno-0-1 id=__codelineno-0-1 name=__codelineno-0-1></a>pip<span class=w> </span>install<span class=w> </span>ultralytics
</code></pre></div><h2 id=how-to-use-sam-2-versatility-in-image-and-video-segmentation>How to Use SAM 2: Versatility in Image and Video Segmentation</h2><p>The following table details the available SAM 2 models, their pre-trained weights, supported tasks, and compatibility with different operating modes like <a href=../../modes/predict/>Inference</a>, <a href=../../modes/val/>Validation</a>, <a href=../../modes/train/>Training</a>, and <a href=../../modes/export/>Export</a>.<table><thead><tr><th>Model Type<th>Pre-trained Weights<th>Tasks Supported<th>Inference<th>Validation<th>Training<th>Export<tbody><tr><td>SAM 2 tiny<td><a href=https://github.com/ultralytics/assets/releases/download/v8.3.0/sam2_t.pt>sam2_t.pt</a><td><a href=../../tasks/segment/>Instance Segmentation</a><td>‚úÖ<td>‚ùå<td>‚ùå<td>‚ùå<tr><td>SAM 2 small<td><a href=https://github.com/ultralytics/assets/releases/download/v8.3.0/sam2_s.pt>sam2_s.pt</a><td><a href=../../tasks/segment/>Instance Segmentation</a><td>‚úÖ<td>‚ùå<td>‚ùå<td>‚ùå<tr><td>SAM 2 base<td><a href=https://github.com/ultralytics/assets/releases/download/v8.3.0/sam2_b.pt>sam2_b.pt</a><td><a href=../../tasks/segment/>Instance Segmentation</a><td>‚úÖ<td>‚ùå<td>‚ùå<td>‚ùå<tr><td>SAM 2 large<td><a href=https://github.com/ultralytics/assets/releases/download/v8.3.0/sam2_l.pt>sam2_l.pt</a><td><a href=../../tasks/segment/>Instance Segmentation</a><td>‚úÖ<td>‚ùå<td>‚ùå<td>‚ùå<tr><td>SAM 2.1 tiny<td><a href=https://github.com/ultralytics/assets/releases/download/v8.3.0/sam2.1_t.pt>sam2.1_t.pt</a><td><a href=../../tasks/segment/>Instance Segmentation</a><td>‚úÖ<td>‚ùå<td>‚ùå<td>‚ùå<tr><td>SAM 2.1 small<td><a href=https://github.com/ultralytics/assets/releases/download/v8.3.0/sam2.1_s.pt>sam2.1_s.pt</a><td><a href=../../tasks/segment/>Instance Segmentation</a><td>‚úÖ<td>‚ùå<td>‚ùå<td>‚ùå<tr><td>SAM 2.1 base<td><a href=https://github.com/ultralytics/assets/releases/download/v8.3.0/sam2.1_b.pt>sam2.1_b.pt</a><td><a href=../../tasks/segment/>Instance Segmentation</a><td>‚úÖ<td>‚ùå<td>‚ùå<td>‚ùå<tr><td>SAM 2.1 large<td><a href=https://github.com/ultralytics/assets/releases/download/v8.3.0/sam2.1_l.pt>sam2.1_l.pt</a><td><a href=../../tasks/segment/>Instance Segmentation</a><td>‚úÖ<td>‚ùå<td>‚ùå<td>‚ùå</table><h3 id=sam-2-prediction-examples>SAM 2 Prediction Examples</h3><p>SAM 2 can be utilized across a broad spectrum of tasks, including real-time video editing, medical imaging, and autonomous systems. Its ability to segment both static and dynamic visual data makes it a versatile tool for researchers and developers.<h4 id=segment-with-prompts>Segment with Prompts</h4><div class="admonition example"><p class=admonition-title>Segment with Prompts<p>Use prompts to segment specific objects in images or videos.<div class="tabbed-set tabbed-alternate"data-tabs=1:1><input checked id=__tabbed_1_1 name=__tabbed_1 type=radio><div class=tabbed-labels><label for=__tabbed_1_1>Python</label></div><div class=tabbed-content><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-1-1 id=__codelineno-1-1 name=__codelineno-1-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>SAM</span>
<a href=#__codelineno-1-2 id=__codelineno-1-2 name=__codelineno-1-2></a>
<a href=#__codelineno-1-3 id=__codelineno-1-3 name=__codelineno-1-3></a><span class=c1># Load a model</span>
<a href=#__codelineno-1-4 id=__codelineno-1-4 name=__codelineno-1-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>SAM</span><span class=p>(</span><span class=s2>"sam2.1_b.pt"</span><span class=p>)</span>
<a href=#__codelineno-1-5 id=__codelineno-1-5 name=__codelineno-1-5></a>
<a href=#__codelineno-1-6 id=__codelineno-1-6 name=__codelineno-1-6></a><span class=c1># Display model information (optional)</span>
<a href=#__codelineno-1-7 id=__codelineno-1-7 name=__codelineno-1-7></a><span class=n>model</span><span class=o>.</span><span class=n>info</span><span class=p>()</span>
<a href=#__codelineno-1-8 id=__codelineno-1-8 name=__codelineno-1-8></a>
<a href=#__codelineno-1-9 id=__codelineno-1-9 name=__codelineno-1-9></a><span class=c1># Run inference with bboxes prompt</span>
<a href=#__codelineno-1-10 id=__codelineno-1-10 name=__codelineno-1-10></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=s2>"path/to/image.jpg"</span><span class=p>,</span> <span class=n>bboxes</span><span class=o>=</span><span class=p>[</span><span class=mi>100</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>])</span>
<a href=#__codelineno-1-11 id=__codelineno-1-11 name=__codelineno-1-11></a>
<a href=#__codelineno-1-12 id=__codelineno-1-12 name=__codelineno-1-12></a><span class=c1># Run inference with single point</span>
<a href=#__codelineno-1-13 id=__codelineno-1-13 name=__codelineno-1-13></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>points</span><span class=o>=</span><span class=p>[</span><span class=mi>900</span><span class=p>,</span> <span class=mi>370</span><span class=p>],</span> <span class=n>labels</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>
<a href=#__codelineno-1-14 id=__codelineno-1-14 name=__codelineno-1-14></a>
<a href=#__codelineno-1-15 id=__codelineno-1-15 name=__codelineno-1-15></a><span class=c1># Run inference with multiple points</span>
<a href=#__codelineno-1-16 id=__codelineno-1-16 name=__codelineno-1-16></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>points</span><span class=o>=</span><span class=p>[[</span><span class=mi>400</span><span class=p>,</span> <span class=mi>370</span><span class=p>],</span> <span class=p>[</span><span class=mi>900</span><span class=p>,</span> <span class=mi>370</span><span class=p>]],</span> <span class=n>labels</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
<a href=#__codelineno-1-17 id=__codelineno-1-17 name=__codelineno-1-17></a>
<a href=#__codelineno-1-18 id=__codelineno-1-18 name=__codelineno-1-18></a><span class=c1># Run inference with multiple points prompt per object</span>
<a href=#__codelineno-1-19 id=__codelineno-1-19 name=__codelineno-1-19></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>points</span><span class=o>=</span><span class=p>[[[</span><span class=mi>400</span><span class=p>,</span> <span class=mi>370</span><span class=p>],</span> <span class=p>[</span><span class=mi>900</span><span class=p>,</span> <span class=mi>370</span><span class=p>]]],</span> <span class=n>labels</span><span class=o>=</span><span class=p>[[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]])</span>
<a href=#__codelineno-1-20 id=__codelineno-1-20 name=__codelineno-1-20></a>
<a href=#__codelineno-1-21 id=__codelineno-1-21 name=__codelineno-1-21></a><span class=c1># Run inference with negative points prompt</span>
<a href=#__codelineno-1-22 id=__codelineno-1-22 name=__codelineno-1-22></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>points</span><span class=o>=</span><span class=p>[[[</span><span class=mi>400</span><span class=p>,</span> <span class=mi>370</span><span class=p>],</span> <span class=p>[</span><span class=mi>900</span><span class=p>,</span> <span class=mi>370</span><span class=p>]]],</span> <span class=n>labels</span><span class=o>=</span><span class=p>[[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>]])</span>
</code></pre></div></div></div></div></div><h4 id=segment-everything>Segment Everything</h4><div class="admonition example"><p class=admonition-title>Segment Everything<p>Segment the entire image or video content without specific prompts.<div class="tabbed-set tabbed-alternate"data-tabs=2:2><input checked id=__tabbed_2_1 name=__tabbed_2 type=radio><input id=__tabbed_2_2 name=__tabbed_2 type=radio><div class=tabbed-labels><label for=__tabbed_2_1>Python</label><label for=__tabbed_2_2>CLI</label></div><div class=tabbed-content><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-2-1 id=__codelineno-2-1 name=__codelineno-2-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>SAM</span>
<a href=#__codelineno-2-2 id=__codelineno-2-2 name=__codelineno-2-2></a>
<a href=#__codelineno-2-3 id=__codelineno-2-3 name=__codelineno-2-3></a><span class=c1># Load a model</span>
<a href=#__codelineno-2-4 id=__codelineno-2-4 name=__codelineno-2-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>SAM</span><span class=p>(</span><span class=s2>"sam2.1_b.pt"</span><span class=p>)</span>
<a href=#__codelineno-2-5 id=__codelineno-2-5 name=__codelineno-2-5></a>
<a href=#__codelineno-2-6 id=__codelineno-2-6 name=__codelineno-2-6></a><span class=c1># Display model information (optional)</span>
<a href=#__codelineno-2-7 id=__codelineno-2-7 name=__codelineno-2-7></a><span class=n>model</span><span class=o>.</span><span class=n>info</span><span class=p>()</span>
<a href=#__codelineno-2-8 id=__codelineno-2-8 name=__codelineno-2-8></a>
<a href=#__codelineno-2-9 id=__codelineno-2-9 name=__codelineno-2-9></a><span class=c1># Run inference</span>
<a href=#__codelineno-2-10 id=__codelineno-2-10 name=__codelineno-2-10></a><span class=n>model</span><span class=p>(</span><span class=s2>"path/to/video.mp4"</span><span class=p>)</span>
</code></pre></div></div><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-3-1 id=__codelineno-3-1 name=__codelineno-3-1></a><span class=c1># Run inference with a SAM 2 model</span>
<a href=#__codelineno-3-2 id=__codelineno-3-2 name=__codelineno-3-2></a>yolo<span class=w> </span>predict<span class=w> </span><span class=nv>model</span><span class=o>=</span>sam2.1_b.pt<span class=w> </span><span class=nv>source</span><span class=o>=</span>path/to/video.mp4
</code></pre></div></div></div></div></div><ul><li>This example demonstrates how SAM 2 can be used to segment the entire content of an image or video if no prompts (bboxes/points/masks) are provided.</ul><h2 id=sam-2-comparison-vs-yolov8>SAM 2 comparison vs YOLOv8</h2><p>Here we compare Meta's smallest SAM 2 model, SAM2-t, with Ultralytics smallest segmentation model, <a href=../../tasks/segment/>YOLOv8n-seg</a>:<table><thead><tr><th>Model<th>Size<br><sup>(MB)</sup><th>Parameters<br><sup>(M)</sup><th>Speed (CPU)<br><sup>(ms/im)</sup><tbody><tr><td><a href=../sam/>Meta SAM-b</a><td>375<td>93.7<td>161440<tr><td>Meta SAM2-b<td>162<td>80.8<td>121923<tr><td>Meta SAM2-t<td>78.1<td>38.9<td>85155<tr><td><a href=../mobile-sam/>MobileSAM</a><td>40.7<td>10.1<td>98543<tr><td><a href=../fast-sam/>FastSAM-s</a> with YOLOv8 backbone<td>23.7<td>11.8<td>140<tr><td>Ultralytics <a href=../../tasks/segment/>YOLOv8n-seg</a><td><strong>6.7</strong> (11.7x smaller)<td><strong>3.4</strong> (11.4x less)<td><strong>79.5</strong> (1071x faster)</table><p>This comparison shows the order-of-magnitude differences in the model sizes and speeds between models. Whereas SAM presents unique capabilities for automatic segmenting, it is not a direct competitor to YOLOv8 segment models, which are smaller, faster and more efficient.<p>Tests run on a 2023 Apple M2 Macbook with 16GB of RAM using <code>torch==2.3.1</code> and <code>ultralytics==8.3.82</code>. To reproduce this test:<div class="admonition example"><p class=admonition-title>Example<div class="tabbed-set tabbed-alternate"data-tabs=3:1><input checked id=__tabbed_3_1 name=__tabbed_3 type=radio><div class=tabbed-labels><label for=__tabbed_3_1>Python</label></div><div class=tabbed-content><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-4-1 id=__codelineno-4-1 name=__codelineno-4-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>ASSETS</span><span class=p>,</span> <span class=n>SAM</span><span class=p>,</span> <span class=n>YOLO</span><span class=p>,</span> <span class=n>FastSAM</span>
<a href=#__codelineno-4-2 id=__codelineno-4-2 name=__codelineno-4-2></a>
<a href=#__codelineno-4-3 id=__codelineno-4-3 name=__codelineno-4-3></a><span class=c1># Profile SAM2-t, SAM2-b, SAM-b, MobileSAM</span>
<a href=#__codelineno-4-4 id=__codelineno-4-4 name=__codelineno-4-4></a><span class=k>for</span> <span class=n>file</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>"sam_b.pt"</span><span class=p>,</span> <span class=s2>"sam2_b.pt"</span><span class=p>,</span> <span class=s2>"sam2_t.pt"</span><span class=p>,</span> <span class=s2>"mobile_sam.pt"</span><span class=p>]:</span>
<a href=#__codelineno-4-5 id=__codelineno-4-5 name=__codelineno-4-5></a>    <span class=n>model</span> <span class=o>=</span> <span class=n>SAM</span><span class=p>(</span><span class=n>file</span><span class=p>)</span>
<a href=#__codelineno-4-6 id=__codelineno-4-6 name=__codelineno-4-6></a>    <span class=n>model</span><span class=o>.</span><span class=n>info</span><span class=p>()</span>
<a href=#__codelineno-4-7 id=__codelineno-4-7 name=__codelineno-4-7></a>    <span class=n>model</span><span class=p>(</span><span class=n>ASSETS</span><span class=p>)</span>
<a href=#__codelineno-4-8 id=__codelineno-4-8 name=__codelineno-4-8></a>
<a href=#__codelineno-4-9 id=__codelineno-4-9 name=__codelineno-4-9></a><span class=c1># Profile FastSAM-s</span>
<a href=#__codelineno-4-10 id=__codelineno-4-10 name=__codelineno-4-10></a><span class=n>model</span> <span class=o>=</span> <span class=n>FastSAM</span><span class=p>(</span><span class=s2>"FastSAM-s.pt"</span><span class=p>)</span>
<a href=#__codelineno-4-11 id=__codelineno-4-11 name=__codelineno-4-11></a><span class=n>model</span><span class=o>.</span><span class=n>info</span><span class=p>()</span>
<a href=#__codelineno-4-12 id=__codelineno-4-12 name=__codelineno-4-12></a><span class=n>model</span><span class=p>(</span><span class=n>ASSETS</span><span class=p>)</span>
<a href=#__codelineno-4-13 id=__codelineno-4-13 name=__codelineno-4-13></a>
<a href=#__codelineno-4-14 id=__codelineno-4-14 name=__codelineno-4-14></a><span class=c1># Profile YOLOv8n-seg</span>
<a href=#__codelineno-4-15 id=__codelineno-4-15 name=__codelineno-4-15></a><span class=n>model</span> <span class=o>=</span> <span class=n>YOLO</span><span class=p>(</span><span class=s2>"yolov8n-seg.pt"</span><span class=p>)</span>
<a href=#__codelineno-4-16 id=__codelineno-4-16 name=__codelineno-4-16></a><span class=n>model</span><span class=o>.</span><span class=n>info</span><span class=p>()</span>
<a href=#__codelineno-4-17 id=__codelineno-4-17 name=__codelineno-4-17></a><span class=n>model</span><span class=p>(</span><span class=n>ASSETS</span><span class=p>)</span>
</code></pre></div></div></div></div></div><h2 id=auto-annotation-efficient-dataset-creation>Auto-Annotation: Efficient Dataset Creation</h2><p>Auto-annotation is a powerful feature of SAM 2, enabling users to generate segmentation datasets quickly and accurately by leveraging pre-trained models. This capability is particularly useful for creating large, high-quality datasets without extensive manual effort.<h3 id=how-to-auto-annotate-with-sam-2>How to Auto-Annotate with SAM 2</h3><p>To auto-annotate your dataset using SAM 2, follow this example:<div class="admonition example"><p class=admonition-title>Auto-Annotation Example<div class=highlight><pre><span></span><code><a href=#__codelineno-5-1 id=__codelineno-5-1 name=__codelineno-5-1></a><span class=kn>from</span> <span class=nn>ultralytics.data.annotator</span> <span class=kn>import</span> <span class=n>auto_annotate</span>
<a href=#__codelineno-5-2 id=__codelineno-5-2 name=__codelineno-5-2></a>
<a href=#__codelineno-5-3 id=__codelineno-5-3 name=__codelineno-5-3></a><span class=n>auto_annotate</span><span class=p>(</span><span class=n>data</span><span class=o>=</span><span class=s2>"path/to/images"</span><span class=p>,</span> <span class=n>det_model</span><span class=o>=</span><span class=s2>"yolov8x.pt"</span><span class=p>,</span> <span class=n>sam_model</span><span class=o>=</span><span class=s2>"sam2_b.pt"</span><span class=p>)</span>
</code></pre></div></div><table><thead><tr><th>Argument<th>Type<th>Description<th>Default<tbody><tr><td><code>data</code><td><code>str</code><td>Path to a folder containing images to be annotated.<td><tr><td><code>det_model</code><td><code>str</code>, optional<td>Pre-trained YOLO detection model. Defaults to 'yolov8x.pt'.<td><code>'yolov8x.pt'</code><tr><td><code>sam_model</code><td><code>str</code>, optional<td>Pre-trained SAM 2 segmentation model. Defaults to 'sam2_b.pt'.<td><code>'sam2_b.pt'</code><tr><td><code>device</code><td><code>str</code>, optional<td>Device to run the models on. Defaults to an empty string (CPU or GPU, if available).<td><tr><td><code>output_dir</code><td><code>str</code>, <code>None</code>, optional<td>Directory to save the annotated results. Defaults to a 'labels' folder in the same directory as 'data'.<td><code>None</code></table><p>This function facilitates the rapid creation of high-quality segmentation datasets, ideal for researchers and developers aiming to accelerate their projects.<h2 id=limitations>Limitations</h2><p>Despite its strengths, SAM 2 has certain limitations:<ul><li><strong>Tracking Stability</strong>: SAM 2 may lose track of objects during extended sequences or significant viewpoint changes.<li><strong>Object Confusion</strong>: The model can sometimes confuse similar-looking objects, particularly in crowded scenes.<li><strong>Efficiency with Multiple Objects</strong>: Segmentation efficiency decreases when processing multiple objects simultaneously due to the lack of inter-object communication.<li><strong>Detail <a href=https://www.ultralytics.com/glossary/accuracy>Accuracy</a></strong>: May miss fine details, especially with fast-moving objects. Additional prompts can partially address this issue, but temporal smoothness is not guaranteed.</ul><h2 id=citations-and-acknowledgements>Citations and Acknowledgements</h2><p>If SAM 2 is a crucial part of your research or development work, please cite it using the following reference:<div class="admonition quote"><div class="tabbed-set tabbed-alternate"data-tabs=4:1><input checked id=__tabbed_4_1 name=__tabbed_4 type=radio><div class=tabbed-labels><label for=__tabbed_4_1>BibTeX</label></div><div class=tabbed-content><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-6-1 id=__codelineno-6-1 name=__codelineno-6-1></a><span class=nc>@article</span><span class=p>{</span><span class=nl>ravi2024sam2</span><span class=p>,</span>
<a href=#__codelineno-6-2 id=__codelineno-6-2 name=__codelineno-6-2></a><span class=w>  </span><span class=na>title</span><span class=p>=</span><span class=s>{SAM 2: Segment Anything in Images and Videos}</span><span class=p>,</span>
<a href=#__codelineno-6-3 id=__codelineno-6-3 name=__codelineno-6-3></a><span class=w>  </span><span class=na>author</span><span class=p>=</span><span class=s>{Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\'a}r, Piotr and Feichtenhofer, Christoph}</span><span class=p>,</span>
<a href=#__codelineno-6-4 id=__codelineno-6-4 name=__codelineno-6-4></a><span class=w>  </span><span class=na>journal</span><span class=p>=</span><span class=s>{arXiv preprint}</span><span class=p>,</span>
<a href=#__codelineno-6-5 id=__codelineno-6-5 name=__codelineno-6-5></a><span class=w>  </span><span class=na>year</span><span class=p>=</span><span class=s>{2024}</span>
<a href=#__codelineno-6-6 id=__codelineno-6-6 name=__codelineno-6-6></a><span class=p>}</span>
</code></pre></div></div></div></div></div><p>We extend our gratitude to Meta AI for their contributions to the AI community with this groundbreaking model and dataset.<h2 id=faq>FAQ</h2><h3 id=what-is-sam-2-and-how-does-it-improve-upon-the-original-segment-anything-model-sam>What is SAM 2 and how does it improve upon the original Segment Anything Model (SAM)?</h3><p>SAM 2, the successor to Meta's <a href=../sam/>Segment Anything Model (SAM)</a>, is a cutting-edge tool designed for comprehensive object segmentation in both images and videos. It excels in handling complex visual data through a unified, promptable model architecture that supports real-time processing and zero-shot generalization. SAM 2 offers several improvements over the original SAM, including:<ul><li><strong>Unified Model Architecture</strong>: Combines image and video segmentation capabilities in a single model.<li><strong>Real-Time Performance</strong>: Processes approximately 44 frames per second, making it suitable for applications requiring immediate feedback.<li><strong>Zero-Shot Generalization</strong>: Segments objects it has never encountered before, useful in diverse visual domains.<li><strong>Interactive Refinement</strong>: Allows users to iteratively refine segmentation results by providing additional prompts.<li><strong>Advanced Handling of Visual Challenges</strong>: Manages common video segmentation challenges like object occlusion and reappearance.</ul><p>For more details on SAM 2's architecture and capabilities, explore the <a href=https://arxiv.org/abs/2401.12741>SAM 2 research paper</a>.<h3 id=how-can-i-use-sam-2-for-real-time-video-segmentation>How can I use SAM 2 for real-time video segmentation?</h3><p>SAM 2 can be utilized for real-time video segmentation by leveraging its promptable interface and real-time inference capabilities. Here's a basic example:<div class="admonition example"><p class=admonition-title>Segment with Prompts<p>Use prompts to segment specific objects in images or videos.<div class="tabbed-set tabbed-alternate"data-tabs=5:1><input checked id=__tabbed_5_1 name=__tabbed_5 type=radio><div class=tabbed-labels><label for=__tabbed_5_1>Python</label></div><div class=tabbed-content><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-7-1 id=__codelineno-7-1 name=__codelineno-7-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>SAM</span>
<a href=#__codelineno-7-2 id=__codelineno-7-2 name=__codelineno-7-2></a>
<a href=#__codelineno-7-3 id=__codelineno-7-3 name=__codelineno-7-3></a><span class=c1># Load a model</span>
<a href=#__codelineno-7-4 id=__codelineno-7-4 name=__codelineno-7-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>SAM</span><span class=p>(</span><span class=s2>"sam2_b.pt"</span><span class=p>)</span>
<a href=#__codelineno-7-5 id=__codelineno-7-5 name=__codelineno-7-5></a>
<a href=#__codelineno-7-6 id=__codelineno-7-6 name=__codelineno-7-6></a><span class=c1># Display model information (optional)</span>
<a href=#__codelineno-7-7 id=__codelineno-7-7 name=__codelineno-7-7></a><span class=n>model</span><span class=o>.</span><span class=n>info</span><span class=p>()</span>
<a href=#__codelineno-7-8 id=__codelineno-7-8 name=__codelineno-7-8></a>
<a href=#__codelineno-7-9 id=__codelineno-7-9 name=__codelineno-7-9></a><span class=c1># Segment with bounding box prompt</span>
<a href=#__codelineno-7-10 id=__codelineno-7-10 name=__codelineno-7-10></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=s2>"path/to/image.jpg"</span><span class=p>,</span> <span class=n>bboxes</span><span class=o>=</span><span class=p>[</span><span class=mi>100</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>])</span>
<a href=#__codelineno-7-11 id=__codelineno-7-11 name=__codelineno-7-11></a>
<a href=#__codelineno-7-12 id=__codelineno-7-12 name=__codelineno-7-12></a><span class=c1># Segment with point prompt</span>
<a href=#__codelineno-7-13 id=__codelineno-7-13 name=__codelineno-7-13></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=s2>"path/to/image.jpg"</span><span class=p>,</span> <span class=n>points</span><span class=o>=</span><span class=p>[</span><span class=mi>150</span><span class=p>,</span> <span class=mi>150</span><span class=p>],</span> <span class=n>labels</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>
</code></pre></div></div></div></div></div><p>For more comprehensive usage, refer to the <a href=#how-to-use-sam-2-versatility-in-image-and-video-segmentation>How to Use SAM 2</a> section.<h3 id=what-datasets-are-used-to-train-sam-2-and-how-do-they-enhance-its-performance>What datasets are used to train SAM 2, and how do they enhance its performance?</h3><p>SAM 2 is trained on the SA-V dataset, one of the largest and most diverse video segmentation datasets available. The SA-V dataset includes:<ul><li><strong>51,000+ Videos</strong>: Captured across 47 countries, providing a wide range of real-world scenarios.<li><strong>600,000+ Mask Annotations</strong>: Detailed spatio-temporal mask annotations, referred to as "masklets," covering whole objects and parts.<li><strong>Dataset Scale</strong>: Features 4.5 times more videos and 53 times more annotations than previous largest datasets, offering unprecedented diversity and complexity.</ul><p>This extensive dataset allows SAM 2 to achieve superior performance across major video segmentation benchmarks and enhances its zero-shot generalization capabilities. For more information, see the <a href=#sa-v-dataset>SA-V Dataset</a> section.<h3 id=how-does-sam-2-handle-occlusions-and-object-reappearances-in-video-segmentation>How does SAM 2 handle occlusions and object reappearances in video segmentation?</h3><p>SAM 2 includes a sophisticated memory mechanism to manage temporal dependencies and occlusions in video data. The memory mechanism consists of:<ul><li><strong>Memory Encoder and Memory Bank</strong>: Stores features from past frames.<li><strong>Memory Attention Module</strong>: Utilizes stored information to maintain consistent object tracking over time.<li><strong>Occlusion Head</strong>: Specifically handles scenarios where objects are not visible, predicting the likelihood of an object being occluded.</ul><p>This mechanism ensures continuity even when objects are temporarily obscured or exit and re-enter the scene. For more details, refer to the <a href=#memory-mechanism-and-occlusion-handling>Memory Mechanism and Occlusion Handling</a> section.<h3 id=how-does-sam-2-compare-to-other-segmentation-models-like-yolov8>How does SAM 2 compare to other segmentation models like YOLOv8?</h3><p>SAM 2 and Ultralytics YOLOv8 serve different purposes and excel in different areas. While SAM 2 is designed for comprehensive object segmentation with advanced features like zero-shot generalization and real-time performance, YOLOv8 is optimized for speed and efficiency in <a href=https://www.ultralytics.com/glossary/object-detection>object detection</a> and segmentation tasks. Here's a comparison:<table><thead><tr><th>Model<th>Size<br><sup>(MB)</sup><th>Parameters<br><sup>(M)</sup><th>Speed (CPU)<br><sup>(ms/im)</sup><tbody><tr><td><a href=../sam/>Meta SAM-b</a><td>375<td>93.7<td>161440<tr><td>Meta SAM2-b<td>162<td>80.8<td>121923<tr><td>Meta SAM2-t<td>78.1<td>38.9<td>85155<tr><td><a href=../mobile-sam/>MobileSAM</a><td>40.7<td>10.1<td>98543<tr><td><a href=../fast-sam/>FastSAM-s</a> with YOLOv8 backbone<td>23.7<td>11.8<td>140<tr><td>Ultralytics <a href=../../tasks/segment/>YOLOv8n-seg</a><td><strong>6.7</strong> (11.7x smaller)<td><strong>3.4</strong> (11.4x less)<td><strong>79.5</strong> (1071x faster)</table><p>For more details, see the <a href=#sam-2-comparison-vs-yolov8>SAM 2 comparison vs YOLOv8</a> section.<div class=git-info><div class=dates-container><span title="This page was first created on July 30, 2024"class=date-item> <span class=hover-item>üìÖ</span> Created 3 months ago </span><span title="This page was last updated on October 25, 2024"class=date-item> <span class=hover-item>‚úèÔ∏è</span> Updated 3 days ago </span></div><div class=authors-container><a title="Laughing-q (4 changes)"class=author-link href=https://github.com/Laughing-q> <img alt=Laughing-q class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/61612323?v=4&s=96> </a><a title="RizwanMunawar (3 changes)"class=author-link href=https://github.com/RizwanMunawar> <img alt=RizwanMunawar class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/62513924?v=4&s=96> </a><a title="glenn-jocher (4 changes)"class=author-link href=https://github.com/glenn-jocher> <img alt=glenn-jocher class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/26833433?v=4&s=96> </a><a title="MatthewNoyce (1 change)"class=author-link href=https://github.com/MatthewNoyce> <img alt=MatthewNoyce class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/131261051?v=4&s=96> </a></div></div><div class=share-buttons><button class="share-button hover-item"onclick="window.open('https://twitter.com/intent/tweet?url=https://docs.ultralytics.com/models/sam-2', 'TwitterShare', 'width=550,height=680,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-x-twitter"></i> Tweet</button><button class="share-button hover-item linkedin"onclick="window.open('https://www.linkedin.com/shareArticle?url=https://docs.ultralytics.com/models/sam-2', 'LinkedinShare', 'width=550,height=730,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-linkedin-in"></i> Share</button></div><h2 id=__comments>Comments</h2><div id=giscus-container></div></article></div><script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script><script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script></div><button class="md-top md-icon"data-md-component=top hidden type=button><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top</button></main><footer class=md-footer><nav class="md-footer__inner md-grid"aria-label=Footer><a aria-label="Previous: SAM (Segment Anything Model)"class="md-footer__link md-footer__link--prev"href=../sam/> <div class="md-footer__button md-icon"><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg></div> <div class=md-footer__title><span class=md-footer__direction> Previous </span><div class=md-ellipsis>SAM (Segment Anything Model)</div></div> </a><a aria-label="Next: MobileSAM (Mobile Segment Anything Model)"class="md-footer__link md-footer__link--next"href=../mobile-sam/> <div class=md-footer__title><span class=md-footer__direction> Next </span><div class=md-ellipsis>MobileSAM (Mobile Segment Anything Model)</div></div> <div class="md-footer__button md-icon"><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg></div> </a></nav><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class=md-copyright><div class=md-copyright__highlight><a href=https://ultralytics.com target=_blank>¬© 2024 Ultralytics Inc.</a> All rights reserved.</div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ rel=noopener target=_blank> Material for MkDocs </a></div><div class=md-social><a class=md-social__link href=https://github.com/ultralytics rel=noopener target=_blank title=github.com> <svg viewbox="0 0 496 512"xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </a><a class=md-social__link href=https://www.linkedin.com/company/ultralytics/ rel=noopener target=_blank title=www.linkedin.com> <svg viewbox="0 0 448 512"xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"></path></svg> </a><a class=md-social__link href=https://twitter.com/ultralytics rel=noopener target=_blank title=twitter.com> <svg viewbox="0 0 512 512"xmlns=http://www.w3.org/2000/svg><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"></path></svg> </a><a class=md-social__link href=https://youtube.com/ultralytics?sub_confirmation=1 rel=noopener target=_blank title=youtube.com> <svg viewbox="0 0 576 512"xmlns=http://www.w3.org/2000/svg><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305m-317.51 213.508V175.185l142.739 81.205z"></path></svg> </a><a class=md-social__link href=https://hub.docker.com/r/ultralytics/ultralytics/ rel=noopener target=_blank title=hub.docker.com> <svg viewbox="0 0 640 512"xmlns=http://www.w3.org/2000/svg><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"></path></svg> </a><a class=md-social__link href=https://pypi.org/project/ultralytics/ rel=noopener target=_blank title=pypi.org> <svg viewbox="0 0 448 512"xmlns=http://www.w3.org/2000/svg><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3"></path></svg> </a><a class=md-social__link href=https://ultralytics.com/discord rel=noopener target=_blank title=ultralytics.com> <svg viewbox="0 0 640 512"xmlns=http://www.w3.org/2000/svg><path d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485 485 0 0 0 404.081 32.03a1.82 1.82 0 0 0-1.923.91 338 338 0 0 0-14.9 30.6 447.9 447.9 0 0 0-134.426 0 310 310 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.7 483.7 0 0 0-119.688 37.107 1.7 1.7 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.02 2.02 0 0 0 .765 1.375 487.7 487.7 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348 348 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321 321 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251 251 0 0 0 9.109-7.137 1.82 1.82 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.81 1.81 0 0 1 1.924.233 235 235 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.4 301.4 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391 391 0 0 0 30.014 48.815 1.86 1.86 0 0 0 2.063.7A486 486 0 0 0 610.7 405.729a1.88 1.88 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541M222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241m195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241"></path></svg> </a><a class=md-social__link href=https://reddit.com/r/ultralytics rel=noopener target=_blank title=reddit.com> <svg viewbox="0 0 512 512"xmlns=http://www.w3.org/2000/svg><path d="M0 256C0 114.6 114.6 0 256 0s256 114.6 256 256-114.6 256-256 256H37.1c-13.7 0-20.5-16.5-10.9-26.2L75 437C28.7 390.7 0 326.7 0 256m349.6-102.4c23.6 0 42.7-19.1 42.7-42.7s-19.1-42.7-42.7-42.7c-20.6 0-37.8 14.6-41.8 34-34.5 3.7-61.4 33-61.4 68.4v.2c-37.5 1.6-71.8 12.3-99 29.1-10.1-7.8-22.8-12.5-36.5-12.5-33 0-59.8 26.8-59.8 59.8 0 24 14.1 44.6 34.4 54.1 2 69.4 77.6 125.2 170.6 125.2s168.7-55.9 170.6-125.3c20.2-9.6 34.1-30.2 34.1-54 0-33-26.8-59.8-59.8-59.8-13.7 0-26.3 4.6-36.4 12.4-27.4-17-62.1-27.7-100-29.1v-.2c0-25.4 18.9-46.5 43.4-49.9 4.4 18.8 21.3 32.8 41.5 32.8zm-172.5 93.3c16.7 0 29.5 17.6 28.5 39.3s-13.5 29.6-30.3 29.6-31.4-8.8-30.4-30.5S160.3 247 177 247zm190.1 38.3c1 21.7-13.7 30.5-30.4 30.5s-29.3-7.9-30.3-29.6 11.8-39.3 28.5-39.3 31.2 16.6 32.1 38.3zm-48.1 56.7c-10.3 24.6-34.6 41.9-63 41.9s-52.7-17.3-63-41.9c-1.2-2.9.8-6.2 3.9-6.5 18.4-1.9 38.3-2.9 59.1-2.9s40.7 1 59.1 2.9c3.1.3 5.1 3.6 3.9 6.5"></path></svg> </a></div></div></div></footer></div><div class=md-dialog data-md-component=dialog><div class="md-dialog__inner md-typeset"></div></div><div class=md-progress data-md-component=progress role=progressbar></div><script id=__config type=application/json>{"base": "../..", "features": ["content.action.edit", "content.code.annotate", "content.code.copy", "content.tooltips", "search.highlight", "search.share", "search.suggest", "toc.follow", "navigation.top", "navigation.tabs", "navigation.tabs.sticky", "navigation.prune", "navigation.footer", "navigation.tracking", "navigation.instant", "navigation.instant.progress", "navigation.indexes", "navigation.sections", "content.tabs.link"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script><script src=../../assets/javascripts/bundle.83f73b43.min.js></script><script src=../../javascript/extra.js></script><script src=../../javascript/giscus.js></script>