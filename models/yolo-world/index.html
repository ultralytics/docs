<!doctypehtml><html class=no-js lang=en><meta charset=utf-8><meta content=width=device-width,initial-scale=1 name=viewport><meta content="Explore the YOLO-World Model for efficient, real-time open-vocabulary object detection using Ultralytics YOLOv8 advancements. Achieve top performance with minimal computation."name=description><meta content=Ultralytics name=author><link href=https://docs.ultralytics.com/models/yolo-world/ rel=canonical><link href=../rtdetr/ rel=prev><link href=../../datasets/ rel=next><link href=../../assets/favicon.ico rel=icon><meta content="mkdocs-1.6.1, mkdocs-material-9.5.42"name=generator><title>YOLO-World (Real-Time Open-Vocabulary Object Detection) - Ultralytics YOLO Docs</title><link href=../../assets/stylesheets/main.0253249f.min.css rel=stylesheet><link href=../../assets/stylesheets/palette.06af60db.min.css rel=stylesheet><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href=https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback rel=stylesheet><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link href=../../assets/_mkdocstrings.css rel=stylesheet><link href=../../stylesheets/style.css rel=stylesheet><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-2M5EHKC0BH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-2M5EHKC0BH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-2M5EHKC0BH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta content="YOLO-World (Real-Time Open-Vocabulary Object Detection)"name=title><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css rel=stylesheet><meta content="YOLO-World, Ultralytics, open-vocabulary detection, YOLOv8, real-time object detection, machine learning, computer vision, AI, deep learning, model training"name=keywords><meta content=website property=og:type><meta content=https://docs.ultralytics.com/models/yolo-world property=og:url><meta content="YOLO-World (Real-Time Open-Vocabulary Object Detection)"property=og:title><meta content="Explore the YOLO-World Model for efficient, real-time open-vocabulary object detection using Ultralytics YOLOv8 advancements. Achieve top performance with minimal computation."property=og:description><meta content=https://github.com/ultralytics/docs/releases/download/0/yolo-world-model-architecture-overview.avif property=og:image><meta content=summary_large_image property=twitter:card><meta content=https://docs.ultralytics.com/models/yolo-world property=twitter:url><meta content="YOLO-World (Real-Time Open-Vocabulary Object Detection)"property=twitter:title><meta content="Explore the YOLO-World Model for efficient, real-time open-vocabulary object detection using Ultralytics YOLOv8 advancements. Achieve top performance with minimal computation."property=twitter:description><meta content=https://github.com/ultralytics/docs/releases/download/0/yolo-world-model-architecture-overview.avif property=twitter:image><script type=application/ld+json>{"@context": "https://schema.org", "@type": ["Article", "FAQPage"], "headline": "YOLO-World (Real-Time Open-Vocabulary Object Detection)", "image": ["https://github.com/ultralytics/docs/releases/download/0/yolo-world-model-architecture-overview.avif"], "datePublished": "2024-02-14 18:46:26 +0800", "dateModified": "2024-10-16 19:14:38 +0200", "author": [{"@type": "Organization", "name": "Ultralytics", "url": "https://ultralytics.com/"}], "abstract": "Explore the YOLO-World Model for efficient, real-time open-vocabulary object detection using Ultralytics YOLOv8 advancements. Achieve top performance with minimal computation.", "mainEntity": [{"@type": "Question", "name": "What is the YOLO-World model and how does it work?", "acceptedAnswer": {"@type": "Answer", "text": "The YOLO-World model is an advanced, real-time object detection approach based on the Ultralytics YOLOv8 framework. It excels in Open-Vocabulary Detection tasks by identifying objects within an image based on descriptive texts. Using vision-language modeling and pre-training on large datasets, YOLO-World achieves high efficiency and performance with significantly reduced computational demands, making it ideal for real-time applications across various industries."}}, {"@type": "Question", "name": "How does YOLO-World handle inference with custom prompts?", "acceptedAnswer": {"@type": "Answer", "text": "YOLO-World supports a \"prompt-then-detect\" strategy, which utilizes an offline vocabulary to enhance efficiency. Custom prompts like captions or specific object categories are pre-encoded and stored as offline vocabulary embeddings. This approach streamlines the detection process without the need for retraining. You can dynamically set these prompts within the model to tailor it to specific detection tasks, as shown below:"}}, {"@type": "Question", "name": "Why should I choose YOLO-World over traditional Open-Vocabulary detection models?", "acceptedAnswer": {"@type": "Answer", "text": "YOLO-World provides several advantages over traditional Open-Vocabulary detection models:"}}, {"@type": "Question", "name": "How do I train a YOLO-World model on my dataset?", "acceptedAnswer": {"@type": "Answer", "text": "Training a YOLO-World model on your dataset is straightforward through the provided Python API or CLI commands. Here's how to start training using Python: Or using CLI:"}}, {"@type": "Question", "name": "What are the available pre-trained YOLO-World models and their supported tasks?", "acceptedAnswer": {"@type": "Answer", "text": "Ultralytics offers multiple pre-trained YOLO-World models supporting various tasks and operating modes:"}}, {"@type": "Question", "name": "How do I reproduce the official results of YOLO-World from scratch?", "acceptedAnswer": {"@type": "Answer", "text": "To reproduce the official results from scratch, you need to prepare the datasets and launch the training using the provided code. The training procedure involves creating a data dictionary and running the train method with a custom trainer:"}}]}</script><body data-md-color-accent=indigo data-md-color-primary=indigo data-md-color-scheme=default dir=ltr><input autocomplete=off class=md-toggle data-md-toggle=drawer id=__drawer type=checkbox><input autocomplete=off class=md-toggle data-md-toggle=search id=__search type=checkbox><label class=md-overlay for=__drawer></label><div data-md-component=skip><a class=md-skip href=#yolo-world-model> Skip to content </a></div><div data-md-component=announce><aside class=md-banner><div class="md-banner__inner md-grid md-typeset"><div class=banner-wrapper><div class=banner-content-wrapper><p>YOLO Vision 2024 is here!<div class=banner-info-wrapper><img alt="YOLO Vision 24"height=20 loading=lazy src=https://assets-global.website-files.com/646dd1f1a3703e451ba81ecc/66e9a87cfc78245ffa51d6f0_w_yv24.svg width=20><p>September 27, 2024</div><div class=banner-info-wrapper><img alt="YOLO Vision 24"height=20 loading=lazy src=https://assets-global.website-files.com/646dd1f1a3703e451ba81ecc/66e9a87cdfbd25e409560ed8_l_yv24.svg width=20><p>Free hybrid event</div></div><div class=banner-button-wrapper><div class="banner-button-wrapper large"><button onclick="window.open('https://www.ultralytics.com/events/yolovision', '_blank')">Register now</button></div></div></div></div></aside></div><header class="md-header md-header--shadow md-header--lifted"data-md-component=header><nav class="md-header__inner md-grid"aria-label=Header><a aria-label="Ultralytics YOLO Docs"class="md-header__button md-logo"title="Ultralytics YOLO Docs"data-md-component=logo href=../..> <img alt=logo src=https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg> </a><label class="md-header__button md-icon"for=__drawer><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg></label><div class=md-header__title data-md-component=header-title><div class=md-header__ellipsis><div class=md-header__topic><span class=md-ellipsis> Ultralytics YOLO Docs </span></div><div class=md-header__topic data-md-component=header-topic><span class=md-ellipsis> YOLO-World (Real-Time Open-Vocabulary Object Detection) </span></div></div></div><form class=md-header__option data-md-component=palette><input aria-label="Switch to light mode"class=md-option data-md-color-accent=indigo data-md-color-media=(prefers-color-scheme) data-md-color-primary=indigo data-md-color-scheme=default id=__palette_0 name=__palette type=radio><label class="md-header__button md-icon"title="Switch to light mode"for=__palette_1 hidden><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg></label><input aria-label="Switch to system preference"data-md-color-media="(prefers-color-scheme: dark)"class=md-option data-md-color-accent=indigo data-md-color-primary=black data-md-color-scheme=slate id=__palette_1 name=__palette type=radio><label class="md-header__button md-icon"title="Switch to system preference"for=__palette_2 hidden><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label><input aria-label="Switch to dark mode"data-md-color-media="(prefers-color-scheme: light)"class=md-option data-md-color-accent=indigo data-md-color-primary=indigo data-md-color-scheme=default id=__palette_2 name=__palette type=radio><label class="md-header__button md-icon"title="Switch to dark mode"for=__palette_0 hidden><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label></form><script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script><label class="md-header__button md-icon"for=__search><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg></label><div class=md-search data-md-component=search role=dialog><label class=md-search__overlay for=__search></label><div class=md-search__inner role=search><form class=md-search__form name=search><input aria-label=Search autocapitalize=off autocomplete=off autocorrect=off class=md-search__input data-md-component=search-query name=query placeholder=Search required spellcheck=false><label class="md-search__icon md-icon"for=__search><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg></label><nav aria-label=Search class=md-search__options><a class="md-search__icon md-icon"aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share href=javascript:void(0) tabindex=-1 title=Share> <svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a><button class="md-search__icon md-icon"aria-label=Clear tabindex=-1 title=Clear type=reset><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg></button></nav><div class=md-search__suggest data-md-component=search-suggest></div></form><div class=md-search__output><div class=md-search__scrollwrap data-md-scrollfix tabindex=0><div class=md-search-result data-md-component=search-result><div class=md-search-result__meta>Initializing search</div><ol class=md-search-result__list role=presentation></ol></div></div></div></div></div><div class=md-header__source><a title="Go to repository"class=md-source data-md-component=source href=https://github.com/ultralytics/ultralytics> <div class="md-source__icon md-icon"><svg viewbox="0 0 496 512"xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg></div> <div class=md-source__repository>ultralytics/ultralytics</div> </a></div></nav><nav aria-label=Tabs class=md-tabs data-md-component=tabs><div class=md-grid><ul class=md-tabs__list><li class=md-tabs__item><a class=md-tabs__link href=../..> Home </a><li class=md-tabs__item><a class=md-tabs__link href=../../quickstart/> Quickstart </a><li class=md-tabs__item><a class=md-tabs__link href=../../modes/> Modes </a><li class=md-tabs__item><a class=md-tabs__link href=../../tasks/> Tasks </a><li class="md-tabs__item md-tabs__item--active"><a class=md-tabs__link href=../> Models </a><li class=md-tabs__item><a class=md-tabs__link href=../../datasets/> Datasets </a><li class=md-tabs__item><a class=md-tabs__link href=../../solutions/> Solutions 🚀 NEW </a><li class=md-tabs__item><a class=md-tabs__link href=../../guides/> Guides </a><li class=md-tabs__item><a class=md-tabs__link href=../../integrations/> Integrations </a><li class=md-tabs__item><a class=md-tabs__link href=../../hub/> HUB </a><li class=md-tabs__item><a class=md-tabs__link href=../../reference/cfg/__init__/> Reference </a><li class=md-tabs__item><a class=md-tabs__link href=../../help/> Help </a></ul></div></nav></header><div class=md-container data-md-component=container><main class=md-main data-md-component=main><div class="md-main__inner md-grid"><div class="md-sidebar md-sidebar--primary"data-md-component=sidebar data-md-type=navigation><div class=md-sidebar__scrollwrap><div class=md-sidebar__inner><nav class="md-nav md-nav--primary md-nav--lifted"aria-label=Navigation data-md-level=0><label class=md-nav__title for=__drawer><a aria-label="Ultralytics YOLO Docs"class="md-nav__button md-logo"title="Ultralytics YOLO Docs"data-md-component=logo href=../..> <img alt=logo src=https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg> </a> Ultralytics YOLO Docs</label><div class=md-nav__source><a title="Go to repository"class=md-source data-md-component=source href=https://github.com/ultralytics/ultralytics> <div class="md-source__icon md-icon"><svg viewbox="0 0 496 512"xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg></div> <div class=md-source__repository>ultralytics/ultralytics</div> </a></div><ul class=md-nav__list data-md-scrollfix><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../..> <span class=md-ellipsis> Home </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../quickstart/> <span class=md-ellipsis> Quickstart </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../modes/> <span class=md-ellipsis> Modes </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../tasks/> <span class=md-ellipsis> Tasks </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle"checked id=__nav_5 type=checkbox> <div class="md-nav__link md-nav__container"><a class=md-nav__link href=../> <span class=md-ellipsis> Models </span> </a><label class=md-nav__link for=__nav_5 id=__nav_5_label><span class="md-nav__icon md-icon"></span></label></div> <nav aria-expanded=true aria-labelledby=__nav_5_label class=md-nav data-md-level=1><label class=md-nav__title for=__nav_5><span class="md-nav__icon md-icon"></span> Models</label><ul class=md-nav__list data-md-scrollfix><li class=md-nav__item><a class=md-nav__link href=../yolov3/> <span class=md-ellipsis> YOLOv3 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolov4/> <span class=md-ellipsis> YOLOv4 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolov5/> <span class=md-ellipsis> YOLOv5 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolov6/> <span class=md-ellipsis> YOLOv6 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolov7/> <span class=md-ellipsis> YOLOv7 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolov8/> <span class=md-ellipsis> YOLOv8 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolov9/> <span class=md-ellipsis> YOLOv9 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolov10/> <span class=md-ellipsis> YOLOv10 </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolo11/> <span class=md-ellipsis> YOLO11 🚀 NEW </span> </a><li class=md-nav__item><a class=md-nav__link href=../sam/> <span class=md-ellipsis> SAM (Segment Anything Model) </span> </a><li class=md-nav__item><a class=md-nav__link href=../sam-2/> <span class=md-ellipsis> SAM 2 (Segment Anything Model 2) </span> </a><li class=md-nav__item><a class=md-nav__link href=../mobile-sam/> <span class=md-ellipsis> MobileSAM (Mobile Segment Anything Model) </span> </a><li class=md-nav__item><a class=md-nav__link href=../fast-sam/> <span class=md-ellipsis> FastSAM (Fast Segment Anything Model) </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolo-nas/> <span class=md-ellipsis> YOLO-NAS (Neural Architecture Search) </span> </a><li class=md-nav__item><a class=md-nav__link href=../rtdetr/> <span class=md-ellipsis> RT-DETR (Realtime Detection Transformer) </span> </a><li class="md-nav__item md-nav__item--active"><input class="md-nav__toggle md-toggle"id=__toc type=checkbox> <label class="md-nav__link md-nav__link--active"for=__toc><span class=md-ellipsis> YOLO-World (Real-Time Open-Vocabulary Object Detection) </span> <span class="md-nav__icon md-icon"></span></label> <a class="md-nav__link md-nav__link--active"href=./> <span class=md-ellipsis> YOLO-World (Real-Time Open-Vocabulary Object Detection) </span> </a> <nav aria-label="Table of contents"class="md-nav md-nav--secondary"><label class=md-nav__title for=__toc><span class="md-nav__icon md-icon"></span> Table of contents</label><ul class=md-nav__list data-md-component=toc data-md-scrollfix><li class=md-nav__item><a class=md-nav__link href=#overview> <span class=md-ellipsis> Overview </span> </a><li class=md-nav__item><a class=md-nav__link href=#key-features> <span class=md-ellipsis> Key Features </span> </a><li class=md-nav__item><a class=md-nav__link href=#available-models-supported-tasks-and-operating-modes> <span class=md-ellipsis> Available Models, Supported Tasks, and Operating Modes </span> </a><li class=md-nav__item><a class=md-nav__link href=#zero-shot-transfer-on-coco-dataset> <span class=md-ellipsis> Zero-shot Transfer on COCO Dataset </span> </a><li class=md-nav__item><a class=md-nav__link href=#usage-examples> <span class=md-ellipsis> Usage Examples </span> </a> <nav aria-label="Usage Examples"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#train-usage> <span class=md-ellipsis> Train Usage </span> </a><li class=md-nav__item><a class=md-nav__link href=#predict-usage> <span class=md-ellipsis> Predict Usage </span> </a><li class=md-nav__item><a class=md-nav__link href=#val-usage> <span class=md-ellipsis> Val Usage </span> </a><li class=md-nav__item><a class=md-nav__link href=#track-usage> <span class=md-ellipsis> Track Usage </span> </a><li class=md-nav__item><a class=md-nav__link href=#set-prompts> <span class=md-ellipsis> Set prompts </span> </a><li class=md-nav__item><a class=md-nav__link href=#benefits-of-saving-with-custom-vocabulary> <span class=md-ellipsis> Benefits of Saving with Custom Vocabulary </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#reproduce-official-results-from-scratchexperimental> <span class=md-ellipsis> Reproduce official results from scratch(Experimental) </span> </a> <nav aria-label="Reproduce official results from scratch(Experimental)"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#prepare-datasets> <span class=md-ellipsis> Prepare datasets </span> </a><li class=md-nav__item><a class=md-nav__link href=#launch-training-from-scratch> <span class=md-ellipsis> Launch training from scratch </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#citations-and-acknowledgements> <span class=md-ellipsis> Citations and Acknowledgements </span> </a><li class=md-nav__item><a class=md-nav__link href=#faq> <span class=md-ellipsis> FAQ </span> </a> <nav aria-label=FAQ class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#what-is-the-yolo-world-model-and-how-does-it-work> <span class=md-ellipsis> What is the YOLO-World model and how does it work? </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-does-yolo-world-handle-inference-with-custom-prompts> <span class=md-ellipsis> How does YOLO-World handle inference with custom prompts? </span> </a><li class=md-nav__item><a class=md-nav__link href=#why-should-i-choose-yolo-world-over-traditional-open-vocabulary-detection-models> <span class=md-ellipsis> Why should I choose YOLO-World over traditional Open-Vocabulary detection models? </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-do-i-train-a-yolo-world-model-on-my-dataset> <span class=md-ellipsis> How do I train a YOLO-World model on my dataset? </span> </a><li class=md-nav__item><a class=md-nav__link href=#what-are-the-available-pre-trained-yolo-world-models-and-their-supported-tasks> <span class=md-ellipsis> What are the available pre-trained YOLO-World models and their supported tasks? </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-do-i-reproduce-the-official-results-of-yolo-world-from-scratch> <span class=md-ellipsis> How do I reproduce the official results of YOLO-World from scratch? </span> </a></ul></nav></ul></nav></ul></nav><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../datasets/> <span class=md-ellipsis> Datasets </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../solutions/> <span class=md-ellipsis> Solutions 🚀 NEW </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../guides/> <span class=md-ellipsis> Guides </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../integrations/> <span class=md-ellipsis> Integrations </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../hub/> <span class=md-ellipsis> HUB </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../reference/cfg/__init__/> <span class=md-ellipsis> Reference </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../help/> <span class=md-ellipsis> Help </span> <span class="md-nav__icon md-icon"></span> </a></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary"data-md-component=sidebar data-md-type=toc><div class=md-sidebar__scrollwrap><div class=md-sidebar__inner><nav aria-label="Table of contents"class="md-nav md-nav--secondary"><label class=md-nav__title for=__toc><span class="md-nav__icon md-icon"></span> Table of contents</label><ul class=md-nav__list data-md-component=toc data-md-scrollfix><li class=md-nav__item><a class=md-nav__link href=#overview> <span class=md-ellipsis> Overview </span> </a><li class=md-nav__item><a class=md-nav__link href=#key-features> <span class=md-ellipsis> Key Features </span> </a><li class=md-nav__item><a class=md-nav__link href=#available-models-supported-tasks-and-operating-modes> <span class=md-ellipsis> Available Models, Supported Tasks, and Operating Modes </span> </a><li class=md-nav__item><a class=md-nav__link href=#zero-shot-transfer-on-coco-dataset> <span class=md-ellipsis> Zero-shot Transfer on COCO Dataset </span> </a><li class=md-nav__item><a class=md-nav__link href=#usage-examples> <span class=md-ellipsis> Usage Examples </span> </a> <nav aria-label="Usage Examples"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#train-usage> <span class=md-ellipsis> Train Usage </span> </a><li class=md-nav__item><a class=md-nav__link href=#predict-usage> <span class=md-ellipsis> Predict Usage </span> </a><li class=md-nav__item><a class=md-nav__link href=#val-usage> <span class=md-ellipsis> Val Usage </span> </a><li class=md-nav__item><a class=md-nav__link href=#track-usage> <span class=md-ellipsis> Track Usage </span> </a><li class=md-nav__item><a class=md-nav__link href=#set-prompts> <span class=md-ellipsis> Set prompts </span> </a><li class=md-nav__item><a class=md-nav__link href=#benefits-of-saving-with-custom-vocabulary> <span class=md-ellipsis> Benefits of Saving with Custom Vocabulary </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#reproduce-official-results-from-scratchexperimental> <span class=md-ellipsis> Reproduce official results from scratch(Experimental) </span> </a> <nav aria-label="Reproduce official results from scratch(Experimental)"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#prepare-datasets> <span class=md-ellipsis> Prepare datasets </span> </a><li class=md-nav__item><a class=md-nav__link href=#launch-training-from-scratch> <span class=md-ellipsis> Launch training from scratch </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#citations-and-acknowledgements> <span class=md-ellipsis> Citations and Acknowledgements </span> </a><li class=md-nav__item><a class=md-nav__link href=#faq> <span class=md-ellipsis> FAQ </span> </a> <nav aria-label=FAQ class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#what-is-the-yolo-world-model-and-how-does-it-work> <span class=md-ellipsis> What is the YOLO-World model and how does it work? </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-does-yolo-world-handle-inference-with-custom-prompts> <span class=md-ellipsis> How does YOLO-World handle inference with custom prompts? </span> </a><li class=md-nav__item><a class=md-nav__link href=#why-should-i-choose-yolo-world-over-traditional-open-vocabulary-detection-models> <span class=md-ellipsis> Why should I choose YOLO-World over traditional Open-Vocabulary detection models? </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-do-i-train-a-yolo-world-model-on-my-dataset> <span class=md-ellipsis> How do I train a YOLO-World model on my dataset? </span> </a><li class=md-nav__item><a class=md-nav__link href=#what-are-the-available-pre-trained-yolo-world-models-and-their-supported-tasks> <span class=md-ellipsis> What are the available pre-trained YOLO-World models and their supported tasks? </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-do-i-reproduce-the-official-results-of-yolo-world-from-scratch> <span class=md-ellipsis> How do I reproduce the official results of YOLO-World from scratch? </span> </a></ul></nav></ul></nav></div></div></div><div class=md-content data-md-component=content><article class="md-content__inner md-typeset"><a class="md-content__button md-icon"title="Edit this page"href=https://github.com/ultralytics/ultralytics/tree/main/docs/en/models/yolo-world.md> <svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg> </a><h1 id=yolo-world-model>YOLO-World Model</h1><p>The YOLO-World Model introduces an advanced, real-time <a href=https://www.ultralytics.com/>Ultralytics</a> <a href=../yolov8/>YOLOv8</a>-based approach for Open-Vocabulary Detection tasks. This innovation enables the detection of any object within an image based on descriptive texts. By significantly lowering computational demands while preserving competitive performance, YOLO-World emerges as a versatile tool for numerous vision-based applications.<p align=center><br> <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"title="YouTube video player"allowfullscreen frameborder=0 height=405 loading=lazy src=https://www.youtube.com/embed/cfTKj96TjSE width=720></iframe> <br> <strong>Watch:</strong> YOLO World training workflow on custom dataset<p><img alt="YOLO-World Model architecture overview"src=https://github.com/ultralytics/docs/releases/download/0/yolo-world-model-architecture-overview.avif><h2 id=overview>Overview</h2><p>YOLO-World tackles the challenges faced by traditional Open-Vocabulary detection models, which often rely on cumbersome <a href=https://www.ultralytics.com/glossary/transformer>Transformer</a> models requiring extensive computational resources. These models' dependence on pre-defined object categories also restricts their utility in dynamic scenarios. YOLO-World revitalizes the YOLOv8 framework with open-vocabulary detection capabilities, employing vision-<a href=https://www.ultralytics.com/glossary/language-modeling>language modeling</a> and pre-training on expansive datasets to excel at identifying a broad array of objects in zero-shot scenarios with unmatched efficiency.<h2 id=key-features>Key Features</h2><ol><li><p><strong>Real-time Solution:</strong> Harnessing the computational speed of CNNs, YOLO-World delivers a swift open-vocabulary detection solution, catering to industries in need of immediate results.</p><li><p><strong>Efficiency and Performance:</strong> YOLO-World slashes computational and resource requirements without sacrificing performance, offering a robust alternative to models like SAM but at a fraction of the computational cost, enabling real-time applications.</p><li><p><strong>Inference with Offline Vocabulary:</strong> YOLO-World introduces a "prompt-then-detect" strategy, employing an offline vocabulary to enhance efficiency further. This approach enables the use of custom prompts computed apriori, including captions or categories, to be encoded and stored as offline vocabulary embeddings, streamlining the detection process.</p><li><p><strong>Powered by YOLOv8:</strong> Built upon <a href=../yolov8/>Ultralytics YOLOv8</a>, YOLO-World leverages the latest advancements in real-time object detection to facilitate open-vocabulary detection with unparalleled accuracy and speed.</p><li><p><strong>Benchmark Excellence:</strong> YOLO-World outperforms existing open-vocabulary detectors, including MDETR and GLIP series, in terms of speed and efficiency on standard benchmarks, showcasing YOLOv8's superior capability on a single NVIDIA V100 GPU.</p><li><p><strong>Versatile Applications:</strong> YOLO-World's innovative approach unlocks new possibilities for a multitude of vision tasks, delivering speed improvements by orders of magnitude over existing methods.</p></ol><h2 id=available-models-supported-tasks-and-operating-modes>Available Models, Supported Tasks, and Operating Modes</h2><p>This section details the models available with their specific pre-trained weights, the tasks they support, and their compatibility with various operating modes such as <a href=../../modes/predict/>Inference</a>, <a href=../../modes/val/>Validation</a>, <a href=../../modes/train/>Training</a>, and <a href=../../modes/export/>Export</a>, denoted by ✅ for supported modes and ❌ for unsupported modes.<div class="admonition note"><p class=admonition-title>Note<p>All the YOLOv8-World weights have been directly migrated from the official <a href=https://github.com/AILab-CVC/YOLO-World>YOLO-World</a> repository, highlighting their excellent contributions.</div><table><thead><tr><th>Model Type<th>Pre-trained Weights<th>Tasks Supported<th>Inference<th>Validation<th>Training<th>Export<tbody><tr><td>YOLOv8s-world<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s-world.pt>yolov8s-world.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>❌<tr><td>YOLOv8s-worldv2<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s-worldv2.pt>yolov8s-worldv2.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>✅<tr><td>YOLOv8m-world<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m-world.pt>yolov8m-world.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>❌<tr><td>YOLOv8m-worldv2<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m-worldv2.pt>yolov8m-worldv2.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>✅<tr><td>YOLOv8l-world<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8l-world.pt>yolov8l-world.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>❌<tr><td>YOLOv8l-worldv2<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8l-worldv2.pt>yolov8l-worldv2.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>✅<tr><td>YOLOv8x-world<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x-world.pt>yolov8x-world.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>❌<tr><td>YOLOv8x-worldv2<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x-worldv2.pt>yolov8x-worldv2.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>✅</table><h2 id=zero-shot-transfer-on-coco-dataset>Zero-shot Transfer on COCO Dataset</h2><table><thead><tr><th>Model Type<th>mAP<th>mAP50<th>mAP75<tbody><tr><td>yolov8s-world<td>37.4<td>52.0<td>40.6<tr><td>yolov8s-worldv2<td>37.7<td>52.2<td>41.0<tr><td>yolov8m-world<td>42.0<td>57.0<td>45.6<tr><td>yolov8m-worldv2<td>43.0<td>58.4<td>46.8<tr><td>yolov8l-world<td>45.7<td>61.3<td>49.8<tr><td>yolov8l-worldv2<td>45.8<td>61.3<td>49.8<tr><td>yolov8x-world<td>47.0<td>63.0<td>51.2<tr><td>yolov8x-worldv2<td>47.1<td>62.8<td>51.4</table><h2 id=usage-examples>Usage Examples</h2><p>The YOLO-World models are easy to integrate into your Python applications. Ultralytics provides user-friendly Python API and CLI commands to streamline development.<h3 id=train-usage>Train Usage</h3><div class="admonition tip"><p class=admonition-title>Tip<p>We strongly recommend to use <code>yolov8-worldv2</code> model for custom training, because it supports deterministic training and also easy to export other formats i.e onnx/tensorrt.</div><p><a href=https://www.ultralytics.com/glossary/object-detection>Object detection</a> is straightforward with the <code>train</code> method, as illustrated below:<div class="admonition example"><p class=admonition-title>Example<div class="tabbed-set tabbed-alternate"data-tabs=1:2><input checked id=__tabbed_1_1 name=__tabbed_1 type=radio><input id=__tabbed_1_2 name=__tabbed_1 type=radio><div class=tabbed-labels><label for=__tabbed_1_1>Python</label><label for=__tabbed_1_2>CLI</label></div><div class=tabbed-content><div class=tabbed-block><p><a href=https://www.ultralytics.com/glossary/pytorch>PyTorch</a> pretrained <code>*.pt</code> models as well as configuration <code>*.yaml</code> files can be passed to the <code>YOLOWorld()</code> class to create a model instance in python:<div class=highlight><pre><span></span><code><a href=#__codelineno-0-1 id=__codelineno-0-1 name=__codelineno-0-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>YOLOWorld</span>
<a href=#__codelineno-0-2 id=__codelineno-0-2 name=__codelineno-0-2></a>
<a href=#__codelineno-0-3 id=__codelineno-0-3 name=__codelineno-0-3></a><span class=c1># Load a pretrained YOLOv8s-worldv2 model</span>
<a href=#__codelineno-0-4 id=__codelineno-0-4 name=__codelineno-0-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>YOLOWorld</span><span class=p>(</span><span class=s2>"yolov8s-worldv2.pt"</span><span class=p>)</span>
<a href=#__codelineno-0-5 id=__codelineno-0-5 name=__codelineno-0-5></a>
<a href=#__codelineno-0-6 id=__codelineno-0-6 name=__codelineno-0-6></a><span class=c1># Train the model on the COCO8 example dataset for 100 epochs</span>
<a href=#__codelineno-0-7 id=__codelineno-0-7 name=__codelineno-0-7></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>data</span><span class=o>=</span><span class=s2>"coco8.yaml"</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>imgsz</span><span class=o>=</span><span class=mi>640</span><span class=p>)</span>
<a href=#__codelineno-0-8 id=__codelineno-0-8 name=__codelineno-0-8></a>
<a href=#__codelineno-0-9 id=__codelineno-0-9 name=__codelineno-0-9></a><span class=c1># Run inference with the YOLOv8n model on the 'bus.jpg' image</span>
<a href=#__codelineno-0-10 id=__codelineno-0-10 name=__codelineno-0-10></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=s2>"path/to/bus.jpg"</span><span class=p>)</span>
</code></pre></div></div><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-1-1 id=__codelineno-1-1 name=__codelineno-1-1></a><span class=c1># Load a pretrained YOLOv8s-worldv2 model and train it on the COCO8 example dataset for 100 epochs</span>
<a href=#__codelineno-1-2 id=__codelineno-1-2 name=__codelineno-1-2></a>yolo<span class=w> </span>train<span class=w> </span><span class=nv>model</span><span class=o>=</span>yolov8s-worldv2.yaml<span class=w> </span><span class=nv>data</span><span class=o>=</span>coco8.yaml<span class=w> </span><span class=nv>epochs</span><span class=o>=</span><span class=m>100</span><span class=w> </span><span class=nv>imgsz</span><span class=o>=</span><span class=m>640</span>
</code></pre></div></div></div></div></div><h3 id=predict-usage>Predict Usage</h3><p>Object detection is straightforward with the <code>predict</code> method, as illustrated below:<div class="admonition example"><p class=admonition-title>Example<div class="tabbed-set tabbed-alternate"data-tabs=2:2><input checked id=__tabbed_2_1 name=__tabbed_2 type=radio><input id=__tabbed_2_2 name=__tabbed_2 type=radio><div class=tabbed-labels><label for=__tabbed_2_1>Python</label><label for=__tabbed_2_2>CLI</label></div><div class=tabbed-content><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-2-1 id=__codelineno-2-1 name=__codelineno-2-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>YOLOWorld</span>
<a href=#__codelineno-2-2 id=__codelineno-2-2 name=__codelineno-2-2></a>
<a href=#__codelineno-2-3 id=__codelineno-2-3 name=__codelineno-2-3></a><span class=c1># Initialize a YOLO-World model</span>
<a href=#__codelineno-2-4 id=__codelineno-2-4 name=__codelineno-2-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>YOLOWorld</span><span class=p>(</span><span class=s2>"yolov8s-world.pt"</span><span class=p>)</span>  <span class=c1># or select yolov8m/l-world.pt for different sizes</span>
<a href=#__codelineno-2-5 id=__codelineno-2-5 name=__codelineno-2-5></a>
<a href=#__codelineno-2-6 id=__codelineno-2-6 name=__codelineno-2-6></a><span class=c1># Execute inference with the YOLOv8s-world model on the specified image</span>
<a href=#__codelineno-2-7 id=__codelineno-2-7 name=__codelineno-2-7></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=s2>"path/to/image.jpg"</span><span class=p>)</span>
<a href=#__codelineno-2-8 id=__codelineno-2-8 name=__codelineno-2-8></a>
<a href=#__codelineno-2-9 id=__codelineno-2-9 name=__codelineno-2-9></a><span class=c1># Show results</span>
<a href=#__codelineno-2-10 id=__codelineno-2-10 name=__codelineno-2-10></a><span class=n>results</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div></div><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-3-1 id=__codelineno-3-1 name=__codelineno-3-1></a><span class=c1># Perform object detection using a YOLO-World model</span>
<a href=#__codelineno-3-2 id=__codelineno-3-2 name=__codelineno-3-2></a>yolo<span class=w> </span>predict<span class=w> </span><span class=nv>model</span><span class=o>=</span>yolov8s-world.pt<span class=w> </span><span class=nv>source</span><span class=o>=</span>path/to/image.jpg<span class=w> </span><span class=nv>imgsz</span><span class=o>=</span><span class=m>640</span>
</code></pre></div></div></div></div></div><p>This snippet demonstrates the simplicity of loading a pre-trained model and running a prediction on an image.<h3 id=val-usage>Val Usage</h3><p>Model validation on a dataset is streamlined as follows:<div class="admonition example"><p class=admonition-title>Example<div class="tabbed-set tabbed-alternate"data-tabs=3:2><input checked id=__tabbed_3_1 name=__tabbed_3 type=radio><input id=__tabbed_3_2 name=__tabbed_3 type=radio><div class=tabbed-labels><label for=__tabbed_3_1>Python</label><label for=__tabbed_3_2>CLI</label></div><div class=tabbed-content><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-4-1 id=__codelineno-4-1 name=__codelineno-4-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>YOLO</span>
<a href=#__codelineno-4-2 id=__codelineno-4-2 name=__codelineno-4-2></a>
<a href=#__codelineno-4-3 id=__codelineno-4-3 name=__codelineno-4-3></a><span class=c1># Create a YOLO-World model</span>
<a href=#__codelineno-4-4 id=__codelineno-4-4 name=__codelineno-4-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>YOLO</span><span class=p>(</span><span class=s2>"yolov8s-world.pt"</span><span class=p>)</span>  <span class=c1># or select yolov8m/l-world.pt for different sizes</span>
<a href=#__codelineno-4-5 id=__codelineno-4-5 name=__codelineno-4-5></a>
<a href=#__codelineno-4-6 id=__codelineno-4-6 name=__codelineno-4-6></a><span class=c1># Conduct model validation on the COCO8 example dataset</span>
<a href=#__codelineno-4-7 id=__codelineno-4-7 name=__codelineno-4-7></a><span class=n>metrics</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>val</span><span class=p>(</span><span class=n>data</span><span class=o>=</span><span class=s2>"coco8.yaml"</span><span class=p>)</span>
</code></pre></div></div><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-5-1 id=__codelineno-5-1 name=__codelineno-5-1></a><span class=c1># Validate a YOLO-World model on the COCO8 dataset with a specified image size</span>
<a href=#__codelineno-5-2 id=__codelineno-5-2 name=__codelineno-5-2></a>yolo<span class=w> </span>val<span class=w> </span><span class=nv>model</span><span class=o>=</span>yolov8s-world.pt<span class=w> </span><span class=nv>data</span><span class=o>=</span>coco8.yaml<span class=w> </span><span class=nv>imgsz</span><span class=o>=</span><span class=m>640</span>
</code></pre></div></div></div></div></div><h3 id=track-usage>Track Usage</h3><p>Object tracking with YOLO-World model on a video/images is streamlined as follows:<div class="admonition example"><p class=admonition-title>Example<div class="tabbed-set tabbed-alternate"data-tabs=4:2><input checked id=__tabbed_4_1 name=__tabbed_4 type=radio><input id=__tabbed_4_2 name=__tabbed_4 type=radio><div class=tabbed-labels><label for=__tabbed_4_1>Python</label><label for=__tabbed_4_2>CLI</label></div><div class=tabbed-content><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-6-1 id=__codelineno-6-1 name=__codelineno-6-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>YOLO</span>
<a href=#__codelineno-6-2 id=__codelineno-6-2 name=__codelineno-6-2></a>
<a href=#__codelineno-6-3 id=__codelineno-6-3 name=__codelineno-6-3></a><span class=c1># Create a YOLO-World model</span>
<a href=#__codelineno-6-4 id=__codelineno-6-4 name=__codelineno-6-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>YOLO</span><span class=p>(</span><span class=s2>"yolov8s-world.pt"</span><span class=p>)</span>  <span class=c1># or select yolov8m/l-world.pt for different sizes</span>
<a href=#__codelineno-6-5 id=__codelineno-6-5 name=__codelineno-6-5></a>
<a href=#__codelineno-6-6 id=__codelineno-6-6 name=__codelineno-6-6></a><span class=c1># Track with a YOLO-World model on a video</span>
<a href=#__codelineno-6-7 id=__codelineno-6-7 name=__codelineno-6-7></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>track</span><span class=p>(</span><span class=n>source</span><span class=o>=</span><span class=s2>"path/to/video.mp4"</span><span class=p>)</span>
</code></pre></div></div><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-7-1 id=__codelineno-7-1 name=__codelineno-7-1></a><span class=c1># Track with a YOLO-World model on the video with a specified image size</span>
<a href=#__codelineno-7-2 id=__codelineno-7-2 name=__codelineno-7-2></a>yolo<span class=w> </span>track<span class=w> </span><span class=nv>model</span><span class=o>=</span>yolov8s-world.pt<span class=w> </span><span class=nv>imgsz</span><span class=o>=</span><span class=m>640</span><span class=w> </span><span class=nv>source</span><span class=o>=</span><span class=s2>"path/to/video/file.mp4"</span>
</code></pre></div></div></div></div></div><div class="admonition note"><p class=admonition-title>Note<p>The YOLO-World models provided by Ultralytics come pre-configured with <a href=../../datasets/detect/coco/>COCO dataset</a> categories as part of their offline vocabulary, enhancing efficiency for immediate application. This integration allows the YOLOv8-World models to directly recognize and predict the 80 standard categories defined in the COCO dataset without requiring additional setup or customization.</div><h3 id=set-prompts>Set prompts</h3><p><img alt="YOLO-World prompt class names overview"src=https://github.com/ultralytics/docs/releases/download/0/yolo-world-prompt-class-names-overview.avif><p>The YOLO-World framework allows for the dynamic specification of classes through custom prompts, empowering users to tailor the model to their specific needs <strong>without retraining</strong>. This feature is particularly useful for adapting the model to new domains or specific tasks that were not originally part of the <a href=https://www.ultralytics.com/glossary/training-data>training data</a>. By setting custom prompts, users can essentially guide the model's focus towards objects of interest, enhancing the relevance and accuracy of the detection results.<p>For instance, if your application only requires detecting 'person' and 'bus' objects, you can specify these classes directly:<div class="admonition example"><p class=admonition-title>Example<div class="tabbed-set tabbed-alternate"data-tabs=5:1><input checked id=__tabbed_5_1 name=__tabbed_5 type=radio><div class=tabbed-labels><label for=__tabbed_5_1>Custom Inference Prompts</label></div><div class=tabbed-content><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-8-1 id=__codelineno-8-1 name=__codelineno-8-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>YOLO</span>
<a href=#__codelineno-8-2 id=__codelineno-8-2 name=__codelineno-8-2></a>
<a href=#__codelineno-8-3 id=__codelineno-8-3 name=__codelineno-8-3></a><span class=c1># Initialize a YOLO-World model</span>
<a href=#__codelineno-8-4 id=__codelineno-8-4 name=__codelineno-8-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>YOLO</span><span class=p>(</span><span class=s2>"yolov8s-world.pt"</span><span class=p>)</span>  <span class=c1># or choose yolov8m/l-world.pt</span>
<a href=#__codelineno-8-5 id=__codelineno-8-5 name=__codelineno-8-5></a>
<a href=#__codelineno-8-6 id=__codelineno-8-6 name=__codelineno-8-6></a><span class=c1># Define custom classes</span>
<a href=#__codelineno-8-7 id=__codelineno-8-7 name=__codelineno-8-7></a><span class=n>model</span><span class=o>.</span><span class=n>set_classes</span><span class=p>([</span><span class=s2>"person"</span><span class=p>,</span> <span class=s2>"bus"</span><span class=p>])</span>
<a href=#__codelineno-8-8 id=__codelineno-8-8 name=__codelineno-8-8></a>
<a href=#__codelineno-8-9 id=__codelineno-8-9 name=__codelineno-8-9></a><span class=c1># Execute prediction for specified categories on an image</span>
<a href=#__codelineno-8-10 id=__codelineno-8-10 name=__codelineno-8-10></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=s2>"path/to/image.jpg"</span><span class=p>)</span>
<a href=#__codelineno-8-11 id=__codelineno-8-11 name=__codelineno-8-11></a>
<a href=#__codelineno-8-12 id=__codelineno-8-12 name=__codelineno-8-12></a><span class=c1># Show results</span>
<a href=#__codelineno-8-13 id=__codelineno-8-13 name=__codelineno-8-13></a><span class=n>results</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div></div></div></div></div><p>You can also save a model after setting custom classes. By doing this you create a version of the YOLO-World model that is specialized for your specific use case. This process embeds your custom class definitions directly into the model file, making the model ready to use with your specified classes without further adjustments. Follow these steps to save and load your custom YOLOv8 model:<div class="admonition example"><p class=admonition-title>Example<div class="tabbed-set tabbed-alternate"data-tabs=6:1><input checked id=__tabbed_6_1 name=__tabbed_6 type=radio><div class=tabbed-labels><label for=__tabbed_6_1>Persisting Models with Custom Vocabulary</label></div><div class=tabbed-content><div class=tabbed-block><p>First load a YOLO-World model, set custom classes for it and save it:<div class=highlight><pre><span></span><code><a href=#__codelineno-9-1 id=__codelineno-9-1 name=__codelineno-9-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>YOLO</span>
<a href=#__codelineno-9-2 id=__codelineno-9-2 name=__codelineno-9-2></a>
<a href=#__codelineno-9-3 id=__codelineno-9-3 name=__codelineno-9-3></a><span class=c1># Initialize a YOLO-World model</span>
<a href=#__codelineno-9-4 id=__codelineno-9-4 name=__codelineno-9-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>YOLO</span><span class=p>(</span><span class=s2>"yolov8s-world.pt"</span><span class=p>)</span>  <span class=c1># or select yolov8m/l-world.pt</span>
<a href=#__codelineno-9-5 id=__codelineno-9-5 name=__codelineno-9-5></a>
<a href=#__codelineno-9-6 id=__codelineno-9-6 name=__codelineno-9-6></a><span class=c1># Define custom classes</span>
<a href=#__codelineno-9-7 id=__codelineno-9-7 name=__codelineno-9-7></a><span class=n>model</span><span class=o>.</span><span class=n>set_classes</span><span class=p>([</span><span class=s2>"person"</span><span class=p>,</span> <span class=s2>"bus"</span><span class=p>])</span>
<a href=#__codelineno-9-8 id=__codelineno-9-8 name=__codelineno-9-8></a>
<a href=#__codelineno-9-9 id=__codelineno-9-9 name=__codelineno-9-9></a><span class=c1># Save the model with the defined offline vocabulary</span>
<a href=#__codelineno-9-10 id=__codelineno-9-10 name=__codelineno-9-10></a><span class=n>model</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>"custom_yolov8s.pt"</span><span class=p>)</span>
</code></pre></div><p>After saving, the custom_yolov8s.pt model behaves like any other pre-trained YOLOv8 model but with a key difference: it is now optimized to detect only the classes you have defined. This customization can significantly improve detection performance and efficiency for your specific application scenarios.<div class=highlight><pre><span></span><code><a href=#__codelineno-10-1 id=__codelineno-10-1 name=__codelineno-10-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>YOLO</span>
<a href=#__codelineno-10-2 id=__codelineno-10-2 name=__codelineno-10-2></a>
<a href=#__codelineno-10-3 id=__codelineno-10-3 name=__codelineno-10-3></a><span class=c1># Load your custom model</span>
<a href=#__codelineno-10-4 id=__codelineno-10-4 name=__codelineno-10-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>YOLO</span><span class=p>(</span><span class=s2>"custom_yolov8s.pt"</span><span class=p>)</span>
<a href=#__codelineno-10-5 id=__codelineno-10-5 name=__codelineno-10-5></a>
<a href=#__codelineno-10-6 id=__codelineno-10-6 name=__codelineno-10-6></a><span class=c1># Run inference to detect your custom classes</span>
<a href=#__codelineno-10-7 id=__codelineno-10-7 name=__codelineno-10-7></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=s2>"path/to/image.jpg"</span><span class=p>)</span>
<a href=#__codelineno-10-8 id=__codelineno-10-8 name=__codelineno-10-8></a>
<a href=#__codelineno-10-9 id=__codelineno-10-9 name=__codelineno-10-9></a><span class=c1># Show results</span>
<a href=#__codelineno-10-10 id=__codelineno-10-10 name=__codelineno-10-10></a><span class=n>results</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div></div></div></div></div><h3 id=benefits-of-saving-with-custom-vocabulary>Benefits of Saving with Custom Vocabulary</h3><ul><li><strong>Efficiency</strong>: Streamlines the detection process by focusing on relevant objects, reducing computational overhead and speeding up inference.<li><strong>Flexibility</strong>: Allows for easy adaptation of the model to new or niche detection tasks without the need for extensive retraining or data collection.<li><strong>Simplicity</strong>: Simplifies deployment by eliminating the need to repeatedly specify custom classes at runtime, making the model directly usable with its embedded vocabulary.<li><strong>Performance</strong>: Enhances detection <a href=https://www.ultralytics.com/glossary/accuracy>accuracy</a> for specified classes by focusing the model's attention and resources on recognizing the defined objects.</ul><p>This approach provides a powerful means of customizing state-of-the-art object detection models for specific tasks, making advanced AI more accessible and applicable to a broader range of practical applications.<h2 id=reproduce-official-results-from-scratchexperimental>Reproduce official results from scratch(Experimental)</h2><h3 id=prepare-datasets>Prepare datasets</h3><ul><li>Train data</ul><table><thead><tr><th>Dataset<th>Type<th>Samples<th>Boxes<th>Annotation Files<tbody><tr><td><a href=https://opendatalab.com/OpenDataLab/Objects365_v1>Objects365v1</a><td>Detection<td>609k<td>9621k<td><a href=https://opendatalab.com/OpenDataLab/Objects365_v1>objects365_train.json</a><tr><td><a href=https://downloads.cs.stanford.edu/nlp/data/gqa/images.zip>GQA</a><td>Grounding<td>621k<td>3681k<td><a href=https://huggingface.co/GLIPModel/GLIP/blob/main/mdetr_annotations/final_mixed_train_no_coco.json>final_mixed_train_no_coco.json</a><tr><td><a href=https://shannon.cs.illinois.edu/DenotationGraph/>Flickr30k</a><td>Grounding<td>149k<td>641k<td><a href=https://huggingface.co/GLIPModel/GLIP/blob/main/mdetr_annotations/final_flickr_separateGT_train.json>final_flickr_separateGT_train.json</a></table><ul><li>Val data</ul><table><thead><tr><th>Dataset<th>Type<th>Annotation Files<tbody><tr><td><a href=https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/lvis.yaml>LVIS minival</a><td>Detection<td><a href=https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/lvis.yaml>minival.txt</a></table><h3 id=launch-training-from-scratch>Launch training from scratch</h3><div class="admonition note"><p class=admonition-title>Note<p><code>WorldTrainerFromScratch</code> is highly customized to allow training yolo-world models on both detection datasets and grounding datasets simultaneously. More details please checkout <a href=https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/world/train_world.py>ultralytics.model.yolo.world.train_world.py</a>.</div><div class="admonition example"><p class=admonition-title>Example<div class="tabbed-set tabbed-alternate"data-tabs=7:1><input checked id=__tabbed_7_1 name=__tabbed_7 type=radio><div class=tabbed-labels><label for=__tabbed_7_1>Python</label></div><div class=tabbed-content><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-11-1 id=__codelineno-11-1 name=__codelineno-11-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>YOLOWorld</span>
<a href=#__codelineno-11-2 id=__codelineno-11-2 name=__codelineno-11-2></a><span class=kn>from</span> <span class=nn>ultralytics.models.yolo.world.train_world</span> <span class=kn>import</span> <span class=n>WorldTrainerFromScratch</span>
<a href=#__codelineno-11-3 id=__codelineno-11-3 name=__codelineno-11-3></a>
<a href=#__codelineno-11-4 id=__codelineno-11-4 name=__codelineno-11-4></a><span class=n>data</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span>
<a href=#__codelineno-11-5 id=__codelineno-11-5 name=__codelineno-11-5></a>    <span class=n>train</span><span class=o>=</span><span class=nb>dict</span><span class=p>(</span>
<a href=#__codelineno-11-6 id=__codelineno-11-6 name=__codelineno-11-6></a>        <span class=n>yolo_data</span><span class=o>=</span><span class=p>[</span><span class=s2>"Objects365.yaml"</span><span class=p>],</span>
<a href=#__codelineno-11-7 id=__codelineno-11-7 name=__codelineno-11-7></a>        <span class=n>grounding_data</span><span class=o>=</span><span class=p>[</span>
<a href=#__codelineno-11-8 id=__codelineno-11-8 name=__codelineno-11-8></a>            <span class=nb>dict</span><span class=p>(</span>
<a href=#__codelineno-11-9 id=__codelineno-11-9 name=__codelineno-11-9></a>                <span class=n>img_path</span><span class=o>=</span><span class=s2>"../datasets/flickr30k/images"</span><span class=p>,</span>
<a href=#__codelineno-11-10 id=__codelineno-11-10 name=__codelineno-11-10></a>                <span class=n>json_file</span><span class=o>=</span><span class=s2>"../datasets/flickr30k/final_flickr_separateGT_train.json"</span><span class=p>,</span>
<a href=#__codelineno-11-11 id=__codelineno-11-11 name=__codelineno-11-11></a>            <span class=p>),</span>
<a href=#__codelineno-11-12 id=__codelineno-11-12 name=__codelineno-11-12></a>            <span class=nb>dict</span><span class=p>(</span>
<a href=#__codelineno-11-13 id=__codelineno-11-13 name=__codelineno-11-13></a>                <span class=n>img_path</span><span class=o>=</span><span class=s2>"../datasets/GQA/images"</span><span class=p>,</span>
<a href=#__codelineno-11-14 id=__codelineno-11-14 name=__codelineno-11-14></a>                <span class=n>json_file</span><span class=o>=</span><span class=s2>"../datasets/GQA/final_mixed_train_no_coco.json"</span><span class=p>,</span>
<a href=#__codelineno-11-15 id=__codelineno-11-15 name=__codelineno-11-15></a>            <span class=p>),</span>
<a href=#__codelineno-11-16 id=__codelineno-11-16 name=__codelineno-11-16></a>        <span class=p>],</span>
<a href=#__codelineno-11-17 id=__codelineno-11-17 name=__codelineno-11-17></a>    <span class=p>),</span>
<a href=#__codelineno-11-18 id=__codelineno-11-18 name=__codelineno-11-18></a>    <span class=n>val</span><span class=o>=</span><span class=nb>dict</span><span class=p>(</span><span class=n>yolo_data</span><span class=o>=</span><span class=p>[</span><span class=s2>"lvis.yaml"</span><span class=p>]),</span>
<a href=#__codelineno-11-19 id=__codelineno-11-19 name=__codelineno-11-19></a><span class=p>)</span>
<a href=#__codelineno-11-20 id=__codelineno-11-20 name=__codelineno-11-20></a><span class=n>model</span> <span class=o>=</span> <span class=n>YOLOWorld</span><span class=p>(</span><span class=s2>"yolov8s-worldv2.yaml"</span><span class=p>)</span>
<a href=#__codelineno-11-21 id=__codelineno-11-21 name=__codelineno-11-21></a><span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>data</span><span class=o>=</span><span class=n>data</span><span class=p>,</span> <span class=n>batch</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>trainer</span><span class=o>=</span><span class=n>WorldTrainerFromScratch</span><span class=p>)</span>
</code></pre></div></div></div></div></div><h2 id=citations-and-acknowledgements>Citations and Acknowledgements</h2><p>We extend our gratitude to the <a href=https://www.tencent.com/>Tencent AILab Computer Vision Center</a> for their pioneering work in real-time open-vocabulary object detection with YOLO-World:<div class="admonition quote"><div class="tabbed-set tabbed-alternate"data-tabs=8:1><input checked id=__tabbed_8_1 name=__tabbed_8 type=radio><div class=tabbed-labels><label for=__tabbed_8_1>BibTeX</label></div><div class=tabbed-content><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-12-1 id=__codelineno-12-1 name=__codelineno-12-1></a><span class=nc>@article</span><span class=p>{</span><span class=nl>cheng2024yolow</span><span class=p>,</span>
<a href=#__codelineno-12-2 id=__codelineno-12-2 name=__codelineno-12-2></a><span class=na>title</span><span class=p>=</span><span class=s>{YOLO-World: Real-Time Open-Vocabulary Object Detection}</span><span class=p>,</span>
<a href=#__codelineno-12-3 id=__codelineno-12-3 name=__codelineno-12-3></a><span class=na>author</span><span class=p>=</span><span class=s>{Cheng, Tianheng and Song, Lin and Ge, Yixiao and Liu, Wenyu and Wang, Xinggang and Shan, Ying}</span><span class=p>,</span>
<a href=#__codelineno-12-4 id=__codelineno-12-4 name=__codelineno-12-4></a><span class=na>journal</span><span class=p>=</span><span class=s>{arXiv preprint arXiv:2401.17270}</span><span class=p>,</span>
<a href=#__codelineno-12-5 id=__codelineno-12-5 name=__codelineno-12-5></a><span class=na>year</span><span class=p>=</span><span class=s>{2024}</span>
<a href=#__codelineno-12-6 id=__codelineno-12-6 name=__codelineno-12-6></a><span class=p>}</span>
</code></pre></div></div></div></div></div><p>For further reading, the original YOLO-World paper is available on <a href=https://arxiv.org/pdf/2401.17270v2.pdf>arXiv</a>. The project's source code and additional resources can be accessed via their <a href=https://github.com/AILab-CVC/YOLO-World>GitHub repository</a>. We appreciate their commitment to advancing the field and sharing their valuable insights with the community.<h2 id=faq>FAQ</h2><h3 id=what-is-the-yolo-world-model-and-how-does-it-work>What is the YOLO-World model and how does it work?</h3><p>The YOLO-World model is an advanced, real-time object detection approach based on the <a href=../yolov8/>Ultralytics YOLOv8</a> framework. It excels in Open-Vocabulary Detection tasks by identifying objects within an image based on descriptive texts. Using vision-language modeling and pre-training on large datasets, YOLO-World achieves high efficiency and performance with significantly reduced computational demands, making it ideal for real-time applications across various industries.<h3 id=how-does-yolo-world-handle-inference-with-custom-prompts>How does YOLO-World handle inference with custom prompts?</h3><p>YOLO-World supports a "prompt-then-detect" strategy, which utilizes an offline vocabulary to enhance efficiency. Custom prompts like captions or specific object categories are pre-encoded and stored as offline vocabulary <a href=https://www.ultralytics.com/glossary/embeddings>embeddings</a>. This approach streamlines the detection process without the need for retraining. You can dynamically set these prompts within the model to tailor it to specific detection tasks, as shown below:<div class=highlight><pre><span></span><code><a href=#__codelineno-13-1 id=__codelineno-13-1 name=__codelineno-13-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>YOLOWorld</span>
<a href=#__codelineno-13-2 id=__codelineno-13-2 name=__codelineno-13-2></a>
<a href=#__codelineno-13-3 id=__codelineno-13-3 name=__codelineno-13-3></a><span class=c1># Initialize a YOLO-World model</span>
<a href=#__codelineno-13-4 id=__codelineno-13-4 name=__codelineno-13-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>YOLOWorld</span><span class=p>(</span><span class=s2>"yolov8s-world.pt"</span><span class=p>)</span>
<a href=#__codelineno-13-5 id=__codelineno-13-5 name=__codelineno-13-5></a>
<a href=#__codelineno-13-6 id=__codelineno-13-6 name=__codelineno-13-6></a><span class=c1># Define custom classes</span>
<a href=#__codelineno-13-7 id=__codelineno-13-7 name=__codelineno-13-7></a><span class=n>model</span><span class=o>.</span><span class=n>set_classes</span><span class=p>([</span><span class=s2>"person"</span><span class=p>,</span> <span class=s2>"bus"</span><span class=p>])</span>
<a href=#__codelineno-13-8 id=__codelineno-13-8 name=__codelineno-13-8></a>
<a href=#__codelineno-13-9 id=__codelineno-13-9 name=__codelineno-13-9></a><span class=c1># Execute prediction on an image</span>
<a href=#__codelineno-13-10 id=__codelineno-13-10 name=__codelineno-13-10></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=s2>"path/to/image.jpg"</span><span class=p>)</span>
<a href=#__codelineno-13-11 id=__codelineno-13-11 name=__codelineno-13-11></a>
<a href=#__codelineno-13-12 id=__codelineno-13-12 name=__codelineno-13-12></a><span class=c1># Show results</span>
<a href=#__codelineno-13-13 id=__codelineno-13-13 name=__codelineno-13-13></a><span class=n>results</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div><h3 id=why-should-i-choose-yolo-world-over-traditional-open-vocabulary-detection-models>Why should I choose YOLO-World over traditional Open-Vocabulary detection models?</h3><p>YOLO-World provides several advantages over traditional Open-Vocabulary detection models:<ul><li><strong>Real-Time Performance:</strong> It leverages the computational speed of CNNs to offer quick, efficient detection.<li><strong>Efficiency and Low Resource Requirement:</strong> YOLO-World maintains high performance while significantly reducing computational and resource demands.<li><strong>Customizable Prompts:</strong> The model supports dynamic prompt setting, allowing users to specify custom detection classes without retraining.<li><strong>Benchmark Excellence:</strong> It outperforms other open-vocabulary detectors like MDETR and GLIP in both speed and efficiency on standard benchmarks.</ul><h3 id=how-do-i-train-a-yolo-world-model-on-my-dataset>How do I train a YOLO-World model on my dataset?</h3><p>Training a YOLO-World model on your dataset is straightforward through the provided Python API or CLI commands. Here's how to start training using Python:<div class=highlight><pre><span></span><code><a href=#__codelineno-14-1 id=__codelineno-14-1 name=__codelineno-14-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>YOLOWorld</span>
<a href=#__codelineno-14-2 id=__codelineno-14-2 name=__codelineno-14-2></a>
<a href=#__codelineno-14-3 id=__codelineno-14-3 name=__codelineno-14-3></a><span class=c1># Load a pretrained YOLOv8s-worldv2 model</span>
<a href=#__codelineno-14-4 id=__codelineno-14-4 name=__codelineno-14-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>YOLOWorld</span><span class=p>(</span><span class=s2>"yolov8s-worldv2.pt"</span><span class=p>)</span>
<a href=#__codelineno-14-5 id=__codelineno-14-5 name=__codelineno-14-5></a>
<a href=#__codelineno-14-6 id=__codelineno-14-6 name=__codelineno-14-6></a><span class=c1># Train the model on the COCO8 dataset for 100 epochs</span>
<a href=#__codelineno-14-7 id=__codelineno-14-7 name=__codelineno-14-7></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>data</span><span class=o>=</span><span class=s2>"coco8.yaml"</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>imgsz</span><span class=o>=</span><span class=mi>640</span><span class=p>)</span>
</code></pre></div><p>Or using CLI:<div class=highlight><pre><span></span><code><a href=#__codelineno-15-1 id=__codelineno-15-1 name=__codelineno-15-1></a>yolo<span class=w> </span>train<span class=w> </span><span class=nv>model</span><span class=o>=</span>yolov8s-worldv2.yaml<span class=w> </span><span class=nv>data</span><span class=o>=</span>coco8.yaml<span class=w> </span><span class=nv>epochs</span><span class=o>=</span><span class=m>100</span><span class=w> </span><span class=nv>imgsz</span><span class=o>=</span><span class=m>640</span>
</code></pre></div><h3 id=what-are-the-available-pre-trained-yolo-world-models-and-their-supported-tasks>What are the available pre-trained YOLO-World models and their supported tasks?</h3><p>Ultralytics offers multiple pre-trained YOLO-World models supporting various tasks and operating modes:<table><thead><tr><th>Model Type<th>Pre-trained Weights<th>Tasks Supported<th>Inference<th>Validation<th>Training<th>Export<tbody><tr><td>YOLOv8s-world<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s-world.pt>yolov8s-world.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>❌<tr><td>YOLOv8s-worldv2<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s-worldv2.pt>yolov8s-worldv2.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>✅<tr><td>YOLOv8m-world<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m-world.pt>yolov8m-world.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>❌<tr><td>YOLOv8m-worldv2<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m-worldv2.pt>yolov8m-worldv2.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>✅<tr><td>YOLOv8l-world<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8l-world.pt>yolov8l-world.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>❌<tr><td>YOLOv8l-worldv2<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8l-worldv2.pt>yolov8l-worldv2.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>✅<tr><td>YOLOv8x-world<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x-world.pt>yolov8x-world.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>❌<tr><td>YOLOv8x-worldv2<td><a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x-worldv2.pt>yolov8x-worldv2.pt</a><td><a href=../../tasks/detect/>Object Detection</a><td>✅<td>✅<td>✅<td>✅</table><h3 id=how-do-i-reproduce-the-official-results-of-yolo-world-from-scratch>How do I reproduce the official results of YOLO-World from scratch?</h3><p>To reproduce the official results from scratch, you need to prepare the datasets and launch the training using the provided code. The training procedure involves creating a data dictionary and running the <code>train</code> method with a custom trainer:<div class=highlight><pre><span></span><code><a href=#__codelineno-16-1 id=__codelineno-16-1 name=__codelineno-16-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>YOLOWorld</span>
<a href=#__codelineno-16-2 id=__codelineno-16-2 name=__codelineno-16-2></a><span class=kn>from</span> <span class=nn>ultralytics.models.yolo.world.train_world</span> <span class=kn>import</span> <span class=n>WorldTrainerFromScratch</span>
<a href=#__codelineno-16-3 id=__codelineno-16-3 name=__codelineno-16-3></a>
<a href=#__codelineno-16-4 id=__codelineno-16-4 name=__codelineno-16-4></a><span class=n>data</span> <span class=o>=</span> <span class=p>{</span>
<a href=#__codelineno-16-5 id=__codelineno-16-5 name=__codelineno-16-5></a>    <span class=s2>"train"</span><span class=p>:</span> <span class=p>{</span>
<a href=#__codelineno-16-6 id=__codelineno-16-6 name=__codelineno-16-6></a>        <span class=s2>"yolo_data"</span><span class=p>:</span> <span class=p>[</span><span class=s2>"Objects365.yaml"</span><span class=p>],</span>
<a href=#__codelineno-16-7 id=__codelineno-16-7 name=__codelineno-16-7></a>        <span class=s2>"grounding_data"</span><span class=p>:</span> <span class=p>[</span>
<a href=#__codelineno-16-8 id=__codelineno-16-8 name=__codelineno-16-8></a>            <span class=p>{</span>
<a href=#__codelineno-16-9 id=__codelineno-16-9 name=__codelineno-16-9></a>                <span class=s2>"img_path"</span><span class=p>:</span> <span class=s2>"../datasets/flickr30k/images"</span><span class=p>,</span>
<a href=#__codelineno-16-10 id=__codelineno-16-10 name=__codelineno-16-10></a>                <span class=s2>"json_file"</span><span class=p>:</span> <span class=s2>"../datasets/flickr30k/final_flickr_separateGT_train.json"</span><span class=p>,</span>
<a href=#__codelineno-16-11 id=__codelineno-16-11 name=__codelineno-16-11></a>            <span class=p>},</span>
<a href=#__codelineno-16-12 id=__codelineno-16-12 name=__codelineno-16-12></a>            <span class=p>{</span>
<a href=#__codelineno-16-13 id=__codelineno-16-13 name=__codelineno-16-13></a>                <span class=s2>"img_path"</span><span class=p>:</span> <span class=s2>"../datasets/GQA/images"</span><span class=p>,</span>
<a href=#__codelineno-16-14 id=__codelineno-16-14 name=__codelineno-16-14></a>                <span class=s2>"json_file"</span><span class=p>:</span> <span class=s2>"../datasets/GQA/final_mixed_train_no_coco.json"</span><span class=p>,</span>
<a href=#__codelineno-16-15 id=__codelineno-16-15 name=__codelineno-16-15></a>            <span class=p>},</span>
<a href=#__codelineno-16-16 id=__codelineno-16-16 name=__codelineno-16-16></a>        <span class=p>],</span>
<a href=#__codelineno-16-17 id=__codelineno-16-17 name=__codelineno-16-17></a>    <span class=p>},</span>
<a href=#__codelineno-16-18 id=__codelineno-16-18 name=__codelineno-16-18></a>    <span class=s2>"val"</span><span class=p>:</span> <span class=p>{</span><span class=s2>"yolo_data"</span><span class=p>:</span> <span class=p>[</span><span class=s2>"lvis.yaml"</span><span class=p>]},</span>
<a href=#__codelineno-16-19 id=__codelineno-16-19 name=__codelineno-16-19></a><span class=p>}</span>
<a href=#__codelineno-16-20 id=__codelineno-16-20 name=__codelineno-16-20></a>
<a href=#__codelineno-16-21 id=__codelineno-16-21 name=__codelineno-16-21></a><span class=n>model</span> <span class=o>=</span> <span class=n>YOLOWorld</span><span class=p>(</span><span class=s2>"yolov8s-worldv2.yaml"</span><span class=p>)</span>
<a href=#__codelineno-16-22 id=__codelineno-16-22 name=__codelineno-16-22></a><span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>data</span><span class=o>=</span><span class=n>data</span><span class=p>,</span> <span class=n>batch</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>trainer</span><span class=o>=</span><span class=n>WorldTrainerFromScratch</span><span class=p>)</span>
</code></pre></div><div class=git-info><div class=dates-container><span title="This page was first created on February 14, 2024"class=date-item> <span class=hover-item>📅</span> Created 8 months ago </span><span title="This page was last updated on October 16, 2024"class=date-item> <span class=hover-item>✏️</span> Updated 12 days ago </span></div><div class=authors-container><a title="glenn-jocher (11 changes)"class=author-link href=https://github.com/glenn-jocher> <img alt=glenn-jocher class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/26833433?v=4&s=96> </a><a title="jk4e (1 change)"class=author-link href=https://github.com/jk4e> <img alt=jk4e class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/116908874?v=4&s=96> </a><a title="MatthewNoyce (1 change)"class=author-link href=https://github.com/MatthewNoyce> <img alt=MatthewNoyce class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/131261051?v=4&s=96> </a><a title="UltralyticsAssistant (1 change)"class=author-link href=https://github.com/UltralyticsAssistant> <img alt=UltralyticsAssistant class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/135830346?v=4&s=96> </a><a title="RizwanMunawar (4 changes)"class=author-link href=https://github.com/RizwanMunawar> <img alt=RizwanMunawar class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/62513924?v=4&s=96> </a><a title="Burhan-Q (1 change)"class=author-link href=https://github.com/Burhan-Q> <img alt=Burhan-Q class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/62214284?v=4&s=96> </a><a title="Laughing-q (4 changes)"class=author-link href=https://github.com/Laughing-q> <img alt=Laughing-q class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/61612323?v=4&s=96> </a></div></div><div class=share-buttons><button class="share-button hover-item"onclick="window.open('https://twitter.com/intent/tweet?url=https://docs.ultralytics.com/models/yolo-world', 'TwitterShare', 'width=550,height=680,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-x-twitter"></i> Tweet</button><button class="share-button hover-item linkedin"onclick="window.open('https://www.linkedin.com/shareArticle?url=https://docs.ultralytics.com/models/yolo-world', 'LinkedinShare', 'width=550,height=730,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-linkedin-in"></i> Share</button></div><h2 id=__comments>Comments</h2><div id=giscus-container></div></article></div><script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script><script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script></div><button class="md-top md-icon"data-md-component=top hidden type=button><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top</button></main><footer class=md-footer><nav class="md-footer__inner md-grid"aria-label=Footer><a aria-label="Previous: RT-DETR (Realtime Detection Transformer)"class="md-footer__link md-footer__link--prev"href=../rtdetr/> <div class="md-footer__button md-icon"><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg></div> <div class=md-footer__title><span class=md-footer__direction> Previous </span><div class=md-ellipsis>RT-DETR (Realtime Detection Transformer)</div></div> </a><a aria-label="Next: Datasets Overview"class="md-footer__link md-footer__link--next"href=../../datasets/> <div class=md-footer__title><span class=md-footer__direction> Next </span><div class=md-ellipsis>Datasets Overview</div></div> <div class="md-footer__button md-icon"><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg></div> </a></nav><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class=md-copyright><div class=md-copyright__highlight><a href=https://ultralytics.com target=_blank>© 2024 Ultralytics Inc.</a> All rights reserved.</div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ rel=noopener target=_blank> Material for MkDocs </a></div><div class=md-social><a class=md-social__link href=https://github.com/ultralytics rel=noopener target=_blank title=github.com> <svg viewbox="0 0 496 512"xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </a><a class=md-social__link href=https://www.linkedin.com/company/ultralytics/ rel=noopener target=_blank title=www.linkedin.com> <svg viewbox="0 0 448 512"xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"></path></svg> </a><a class=md-social__link href=https://twitter.com/ultralytics rel=noopener target=_blank title=twitter.com> <svg viewbox="0 0 512 512"xmlns=http://www.w3.org/2000/svg><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"></path></svg> </a><a class=md-social__link href=https://youtube.com/ultralytics?sub_confirmation=1 rel=noopener target=_blank title=youtube.com> <svg viewbox="0 0 576 512"xmlns=http://www.w3.org/2000/svg><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305m-317.51 213.508V175.185l142.739 81.205z"></path></svg> </a><a class=md-social__link href=https://hub.docker.com/r/ultralytics/ultralytics/ rel=noopener target=_blank title=hub.docker.com> <svg viewbox="0 0 640 512"xmlns=http://www.w3.org/2000/svg><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"></path></svg> </a><a class=md-social__link href=https://pypi.org/project/ultralytics/ rel=noopener target=_blank title=pypi.org> <svg viewbox="0 0 448 512"xmlns=http://www.w3.org/2000/svg><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3"></path></svg> </a><a class=md-social__link href=https://ultralytics.com/discord rel=noopener target=_blank title=ultralytics.com> <svg viewbox="0 0 640 512"xmlns=http://www.w3.org/2000/svg><path d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485 485 0 0 0 404.081 32.03a1.82 1.82 0 0 0-1.923.91 338 338 0 0 0-14.9 30.6 447.9 447.9 0 0 0-134.426 0 310 310 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.7 483.7 0 0 0-119.688 37.107 1.7 1.7 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.02 2.02 0 0 0 .765 1.375 487.7 487.7 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348 348 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321 321 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251 251 0 0 0 9.109-7.137 1.82 1.82 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.81 1.81 0 0 1 1.924.233 235 235 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.4 301.4 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391 391 0 0 0 30.014 48.815 1.86 1.86 0 0 0 2.063.7A486 486 0 0 0 610.7 405.729a1.88 1.88 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541M222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241m195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241"></path></svg> </a><a class=md-social__link href=https://reddit.com/r/ultralytics rel=noopener target=_blank title=reddit.com> <svg viewbox="0 0 512 512"xmlns=http://www.w3.org/2000/svg><path d="M0 256C0 114.6 114.6 0 256 0s256 114.6 256 256-114.6 256-256 256H37.1c-13.7 0-20.5-16.5-10.9-26.2L75 437C28.7 390.7 0 326.7 0 256m349.6-102.4c23.6 0 42.7-19.1 42.7-42.7s-19.1-42.7-42.7-42.7c-20.6 0-37.8 14.6-41.8 34-34.5 3.7-61.4 33-61.4 68.4v.2c-37.5 1.6-71.8 12.3-99 29.1-10.1-7.8-22.8-12.5-36.5-12.5-33 0-59.8 26.8-59.8 59.8 0 24 14.1 44.6 34.4 54.1 2 69.4 77.6 125.2 170.6 125.2s168.7-55.9 170.6-125.3c20.2-9.6 34.1-30.2 34.1-54 0-33-26.8-59.8-59.8-59.8-13.7 0-26.3 4.6-36.4 12.4-27.4-17-62.1-27.7-100-29.1v-.2c0-25.4 18.9-46.5 43.4-49.9 4.4 18.8 21.3 32.8 41.5 32.8zm-172.5 93.3c16.7 0 29.5 17.6 28.5 39.3s-13.5 29.6-30.3 29.6-31.4-8.8-30.4-30.5S160.3 247 177 247zm190.1 38.3c1 21.7-13.7 30.5-30.4 30.5s-29.3-7.9-30.3-29.6 11.8-39.3 28.5-39.3 31.2 16.6 32.1 38.3zm-48.1 56.7c-10.3 24.6-34.6 41.9-63 41.9s-52.7-17.3-63-41.9c-1.2-2.9.8-6.2 3.9-6.5 18.4-1.9 38.3-2.9 59.1-2.9s40.7 1 59.1 2.9c3.1.3 5.1 3.6 3.9 6.5"></path></svg> </a></div></div></div></footer></div><div class=md-dialog data-md-component=dialog><div class="md-dialog__inner md-typeset"></div></div><div class=md-progress data-md-component=progress role=progressbar></div><script id=__config type=application/json>{"base": "../..", "features": ["content.action.edit", "content.code.annotate", "content.code.copy", "content.tooltips", "search.highlight", "search.share", "search.suggest", "toc.follow", "navigation.top", "navigation.tabs", "navigation.tabs.sticky", "navigation.prune", "navigation.footer", "navigation.tracking", "navigation.instant", "navigation.instant.progress", "navigation.indexes", "navigation.sections", "content.tabs.link"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script><script src=../../assets/javascripts/bundle.83f73b43.min.js></script><script src=../../javascript/extra.js></script><script src=../../javascript/giscus.js></script>