<!doctypehtml><html class=no-js lang=en><meta charset=utf-8><meta content=width=device-width,initial-scale=1 name=viewport><meta content="Learn to deploy Ultralytics YOLO11 on NVIDIA Jetson devices with our detailed guide. Explore performance benchmarks and maximize AI capabilities."name=description><meta content=Ultralytics name=author><link href=https://docs.ultralytics.com/guides/nvidia-jetson/ rel=canonical><link href=../raspberry-pi/ rel=prev><link href=../deepstream-nvidia-jetson/ rel=next><link href=../../assets/favicon.ico rel=icon><meta content="mkdocs-1.6.1, mkdocs-material-9.5.43"name=generator><title>NVIDIA Jetson - Ultralytics YOLO Docs</title><link href=../../assets/stylesheets/main.0253249f.min.css rel=stylesheet><link href=../../assets/stylesheets/palette.06af60db.min.css rel=stylesheet><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href=https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback rel=stylesheet><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link href=../../assets/_mkdocstrings.css rel=stylesheet><link href=../../stylesheets/style.css rel=stylesheet><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-2M5EHKC0BH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-2M5EHKC0BH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-2M5EHKC0BH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta content="NVIDIA Jetson"name=title><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css rel=stylesheet><meta content="Ultralytics, YOLO11, NVIDIA Jetson, JetPack, AI deployment, performance benchmarks, embedded systems, deep learning, TensorRT, computer vision"name=keywords><meta content=website property=og:type><meta content=https://docs.ultralytics.com/guides/nvidia-jetson property=og:url><meta content="NVIDIA Jetson"property=og:title><meta content="Learn to deploy Ultralytics YOLO11 on NVIDIA Jetson devices with our detailed guide. Explore performance benchmarks and maximize AI capabilities."property=og:description><meta content=https://github.com/ultralytics/docs/releases/download/0/nvidia-jetson-ecosystem.avif property=og:image><meta content=summary_large_image property=twitter:card><meta content=https://docs.ultralytics.com/guides/nvidia-jetson property=twitter:url><meta content="NVIDIA Jetson"property=twitter:title><meta content="Learn to deploy Ultralytics YOLO11 on NVIDIA Jetson devices with our detailed guide. Explore performance benchmarks and maximize AI capabilities."property=twitter:description><meta content=https://github.com/ultralytics/docs/releases/download/0/nvidia-jetson-ecosystem.avif property=twitter:image><script type=application/ld+json>{"@context": "https://schema.org", "@type": ["Article", "FAQPage"], "headline": "NVIDIA Jetson", "image": ["https://github.com/ultralytics/docs/releases/download/0/nvidia-jetson-ecosystem.avif"], "datePublished": "2024-04-02 01:31:15 -0700", "dateModified": "2024-10-28 14:46:36 -0700", "author": [{"@type": "Organization", "name": "Ultralytics", "url": "https://ultralytics.com/"}], "abstract": "Learn to deploy Ultralytics YOLO11 on NVIDIA Jetson devices with our detailed guide. Explore performance benchmarks and maximize AI capabilities.", "mainEntity": [{"@type": "Question", "name": "How do I deploy Ultralytics YOLO11 on NVIDIA Jetson devices?", "acceptedAnswer": {"@type": "Answer", "text": "Deploying Ultralytics YOLO11 on NVIDIA Jetson devices is a straightforward process. First, flash your Jetson device with the NVIDIA JetPack SDK. Then, either use a pre-built Docker image for quick setup or manually install the required packages. Detailed steps for each approach can be found in sections Quick Start with Docker and Start with Native Installation."}}, {"@type": "Question", "name": "What performance benchmarks can I expect from YOLO11 models on NVIDIA Jetson devices?", "acceptedAnswer": {"@type": "Answer", "text": "YOLO11 models have been benchmarked on various NVIDIA Jetson devices showing significant performance improvements. For example, the TensorRT format delivers the best inference performance. The table in the Detailed Comparison Table section provides a comprehensive view of performance metrics like mAP50-95 and inference time across different model formats."}}, {"@type": "Question", "name": "Why should I use TensorRT for deploying YOLO11 on NVIDIA Jetson?", "acceptedAnswer": {"@type": "Answer", "text": "TensorRT is highly recommended for deploying YOLO11 models on NVIDIA Jetson due to its optimal performance. It accelerates inference by leveraging the Jetson's GPU capabilities, ensuring maximum efficiency and speed. Learn more about how to convert to TensorRT and run inference in the Use TensorRT on NVIDIA Jetson section."}}, {"@type": "Question", "name": "How can I install PyTorch and Torchvision on NVIDIA Jetson?", "acceptedAnswer": {"@type": "Answer", "text": "To install PyTorch and Torchvision on NVIDIA Jetson, first uninstall any existing versions that may have been installed via pip. Then, manually install the compatible PyTorch and Torchvision versions for the Jetson's ARM64 architecture. Detailed instructions for this process are provided in the Install PyTorch and Torchvision section."}}, {"@type": "Question", "name": "What are the best practices for maximizing performance on NVIDIA Jetson when using YOLO11?", "acceptedAnswer": {"@type": "Answer", "text": "To maximize performance on NVIDIA Jetson with YOLO11, follow these best practices: For commands and additional details, refer to the Best Practices when using NVIDIA Jetson section."}}]}</script><body data-md-color-accent=indigo data-md-color-primary=indigo data-md-color-scheme=default dir=ltr><input autocomplete=off class=md-toggle data-md-toggle=drawer id=__drawer type=checkbox><input autocomplete=off class=md-toggle data-md-toggle=search id=__search type=checkbox><label class=md-overlay for=__drawer></label><div data-md-component=skip><a class=md-skip href=#quick-start-guide-nvidia-jetson-with-ultralytics-yolo11> Skip to content </a></div><div data-md-component=announce><aside class=md-banner><div class="md-banner__inner md-grid md-typeset"><div class=banner-wrapper><div class=banner-content-wrapper><p>YOLO Vision 2024 is here!<div class=banner-info-wrapper><img alt="YOLO Vision 24"height=20 loading=lazy src=https://assets-global.website-files.com/646dd1f1a3703e451ba81ecc/66e9a87cfc78245ffa51d6f0_w_yv24.svg width=20><p>September 27, 2024</div><div class=banner-info-wrapper><img alt="YOLO Vision 24"height=20 loading=lazy src=https://assets-global.website-files.com/646dd1f1a3703e451ba81ecc/66e9a87cdfbd25e409560ed8_l_yv24.svg width=20><p>Free hybrid event</div></div><div class=banner-button-wrapper><div class="banner-button-wrapper large"><button onclick="window.open('https://www.ultralytics.com/events/yolovision', '_blank')">Register now</button></div></div></div></div></aside></div><header class="md-header md-header--shadow md-header--lifted"data-md-component=header><nav class="md-header__inner md-grid"aria-label=Header><a aria-label="Ultralytics YOLO Docs"class="md-header__button md-logo"title="Ultralytics YOLO Docs"data-md-component=logo href=../..> <img alt=logo src=https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg> </a><label class="md-header__button md-icon"for=__drawer><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg></label><div class=md-header__title data-md-component=header-title><div class=md-header__ellipsis><div class=md-header__topic><span class=md-ellipsis> Ultralytics YOLO Docs </span></div><div class=md-header__topic data-md-component=header-topic><span class=md-ellipsis> NVIDIA Jetson </span></div></div></div><form class=md-header__option data-md-component=palette><input aria-label="Switch to light mode"class=md-option data-md-color-accent=indigo data-md-color-media=(prefers-color-scheme) data-md-color-primary=indigo data-md-color-scheme=default id=__palette_0 name=__palette type=radio><label class="md-header__button md-icon"title="Switch to light mode"for=__palette_1 hidden><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg></label><input aria-label="Switch to system preference"data-md-color-media="(prefers-color-scheme: dark)"class=md-option data-md-color-accent=indigo data-md-color-primary=black data-md-color-scheme=slate id=__palette_1 name=__palette type=radio><label class="md-header__button md-icon"title="Switch to system preference"for=__palette_2 hidden><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label><input aria-label="Switch to dark mode"data-md-color-media="(prefers-color-scheme: light)"class=md-option data-md-color-accent=indigo data-md-color-primary=indigo data-md-color-scheme=default id=__palette_2 name=__palette type=radio><label class="md-header__button md-icon"title="Switch to dark mode"for=__palette_0 hidden><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label></form><script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script><label class="md-header__button md-icon"for=__search><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg></label><div class=md-search data-md-component=search role=dialog><label class=md-search__overlay for=__search></label><div class=md-search__inner role=search><form class=md-search__form name=search><input aria-label=Search autocapitalize=off autocomplete=off autocorrect=off class=md-search__input data-md-component=search-query name=query placeholder=Search required spellcheck=false><label class="md-search__icon md-icon"for=__search><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg></label><nav aria-label=Search class=md-search__options><a class="md-search__icon md-icon"aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share href=javascript:void(0) tabindex=-1 title=Share> <svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a><button class="md-search__icon md-icon"aria-label=Clear tabindex=-1 title=Clear type=reset><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg></button></nav><div class=md-search__suggest data-md-component=search-suggest></div></form><div class=md-search__output><div class=md-search__scrollwrap data-md-scrollfix tabindex=0><div class=md-search-result data-md-component=search-result><div class=md-search-result__meta>Initializing search</div><ol class=md-search-result__list role=presentation></ol></div></div></div></div></div><div class=md-header__source><a title="Go to repository"class=md-source data-md-component=source href=https://github.com/ultralytics/ultralytics> <div class="md-source__icon md-icon"><svg viewbox="0 0 496 512"xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg></div> <div class=md-source__repository>ultralytics/ultralytics</div> </a></div></nav><nav aria-label=Tabs class=md-tabs data-md-component=tabs><div class=md-grid><ul class=md-tabs__list><li class=md-tabs__item><a class=md-tabs__link href=../..> Home </a><li class=md-tabs__item><a class=md-tabs__link href=../../quickstart/> Quickstart </a><li class=md-tabs__item><a class=md-tabs__link href=../../modes/> Modes </a><li class=md-tabs__item><a class=md-tabs__link href=../../tasks/> Tasks </a><li class=md-tabs__item><a class=md-tabs__link href=../../models/> Models </a><li class=md-tabs__item><a class=md-tabs__link href=../../datasets/> Datasets </a><li class=md-tabs__item><a class=md-tabs__link href=../../solutions/> Solutions üöÄ NEW </a><li class="md-tabs__item md-tabs__item--active"><a class=md-tabs__link href=../> Guides </a><li class=md-tabs__item><a class=md-tabs__link href=../../integrations/> Integrations </a><li class=md-tabs__item><a class=md-tabs__link href=../../hub/> HUB </a><li class=md-tabs__item><a class=md-tabs__link href=../../reference/cfg/__init__/> Reference </a><li class=md-tabs__item><a class=md-tabs__link href=../../help/> Help </a></ul></div></nav></header><div class=md-container data-md-component=container><main class=md-main data-md-component=main><div class="md-main__inner md-grid"><div class="md-sidebar md-sidebar--primary"data-md-component=sidebar data-md-type=navigation><div class=md-sidebar__scrollwrap><div class=md-sidebar__inner><nav class="md-nav md-nav--primary md-nav--lifted"aria-label=Navigation data-md-level=0><label class=md-nav__title for=__drawer><a aria-label="Ultralytics YOLO Docs"class="md-nav__button md-logo"title="Ultralytics YOLO Docs"data-md-component=logo href=../..> <img alt=logo src=https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg> </a> Ultralytics YOLO Docs</label><div class=md-nav__source><a title="Go to repository"class=md-source data-md-component=source href=https://github.com/ultralytics/ultralytics> <div class="md-source__icon md-icon"><svg viewbox="0 0 496 512"xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg></div> <div class=md-source__repository>ultralytics/ultralytics</div> </a></div><ul class=md-nav__list data-md-scrollfix><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../..> <span class=md-ellipsis> Home </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../quickstart/> <span class=md-ellipsis> Quickstart </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../modes/> <span class=md-ellipsis> Modes </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../tasks/> <span class=md-ellipsis> Tasks </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../models/> <span class=md-ellipsis> Models </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../datasets/> <span class=md-ellipsis> Datasets </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../solutions/> <span class=md-ellipsis> Solutions üöÄ NEW </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle"checked id=__nav_8 type=checkbox> <div class="md-nav__link md-nav__container"><a class=md-nav__link href=../> <span class=md-ellipsis> Guides </span> </a><label class=md-nav__link for=__nav_8 id=__nav_8_label><span class="md-nav__icon md-icon"></span></label></div> <nav aria-expanded=true aria-labelledby=__nav_8_label class=md-nav data-md-level=1><label class=md-nav__title for=__nav_8><span class="md-nav__icon md-icon"></span> Guides</label><ul class=md-nav__list data-md-scrollfix><li class=md-nav__item><a class=md-nav__link href=../yolo-common-issues/> <span class=md-ellipsis> YOLO Common Issues </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolo-performance-metrics/> <span class=md-ellipsis> YOLO Performance Metrics </span> </a><li class=md-nav__item><a class=md-nav__link href=../yolo-thread-safe-inference/> <span class=md-ellipsis> YOLO Thread-Safe Inference </span> </a><li class=md-nav__item><a class=md-nav__link href=../model-deployment-options/> <span class=md-ellipsis> Model Deployment Options </span> </a><li class=md-nav__item><a class=md-nav__link href=../kfold-cross-validation/> <span class=md-ellipsis> K-Fold Cross Validation </span> </a><li class=md-nav__item><a class=md-nav__link href=../hyperparameter-tuning/> <span class=md-ellipsis> Hyperparameter Tuning </span> </a><li class=md-nav__item><a class=md-nav__link href=../sahi-tiled-inference/> <span class=md-ellipsis> SAHI Tiled Inference </span> </a><li class=md-nav__item><a class=md-nav__link href=../azureml-quickstart/> <span class=md-ellipsis> AzureML Quickstart </span> </a><li class=md-nav__item><a class=md-nav__link href=../conda-quickstart/> <span class=md-ellipsis> Conda Quickstart </span> </a><li class=md-nav__item><a class=md-nav__link href=../docker-quickstart/> <span class=md-ellipsis> Docker Quickstart </span> </a><li class=md-nav__item><a class=md-nav__link href=../raspberry-pi/> <span class=md-ellipsis> Raspberry Pi </span> </a><li class="md-nav__item md-nav__item--active"><input class="md-nav__toggle md-toggle"id=__toc type=checkbox> <label class="md-nav__link md-nav__link--active"for=__toc><span class=md-ellipsis> NVIDIA Jetson </span> <span class="md-nav__icon md-icon"></span></label> <a class="md-nav__link md-nav__link--active"href=./> <span class=md-ellipsis> NVIDIA Jetson </span> </a> <nav aria-label="Table of contents"class="md-nav md-nav--secondary"><label class=md-nav__title for=__toc><span class="md-nav__icon md-icon"></span> Table of contents</label><ul class=md-nav__list data-md-component=toc data-md-scrollfix><li class=md-nav__item><a class=md-nav__link href=#what-is-nvidia-jetson> <span class=md-ellipsis> What is NVIDIA Jetson? </span> </a><li class=md-nav__item><a class=md-nav__link href=#nvidia-jetson-series-comparison> <span class=md-ellipsis> NVIDIA Jetson Series Comparison </span> </a><li class=md-nav__item><a class=md-nav__link href=#what-is-nvidia-jetpack> <span class=md-ellipsis> What is NVIDIA JetPack? </span> </a><li class=md-nav__item><a class=md-nav__link href=#flash-jetpack-to-nvidia-jetson> <span class=md-ellipsis> Flash JetPack to NVIDIA Jetson </span> </a><li class=md-nav__item><a class=md-nav__link href=#jetpack-support-based-on-jetson-device> <span class=md-ellipsis> JetPack Support Based on Jetson Device </span> </a><li class=md-nav__item><a class=md-nav__link href=#quick-start-with-docker> <span class=md-ellipsis> Quick Start with Docker </span> </a><li class=md-nav__item><a class=md-nav__link href=#start-with-native-installation> <span class=md-ellipsis> Start with Native Installation </span> </a> <nav aria-label="Start with Native Installation"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#run-on-jetpack-6x> <span class=md-ellipsis> Run on JetPack 6.x </span> </a> <nav aria-label="Run on JetPack 6.x"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#install-ultralytics-package> <span class=md-ellipsis> Install Ultralytics Package </span> </a><li class=md-nav__item><a class=md-nav__link href=#install-pytorch-and-torchvision> <span class=md-ellipsis> Install PyTorch and Torchvision </span> </a><li class=md-nav__item><a class=md-nav__link href=#install-onnxruntime-gpu> <span class=md-ellipsis> Install onnxruntime-gpu </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#run-on-jetpack-5x> <span class=md-ellipsis> Run on JetPack 5.x </span> </a> <nav aria-label="Run on JetPack 5.x"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#install-ultralytics-package_1> <span class=md-ellipsis> Install Ultralytics Package </span> </a><li class=md-nav__item><a class=md-nav__link href=#install-pytorch-and-torchvision_1> <span class=md-ellipsis> Install PyTorch and Torchvision </span> </a><li class=md-nav__item><a class=md-nav__link href=#install-onnxruntime-gpu_1> <span class=md-ellipsis> Install onnxruntime-gpu </span> </a></ul></nav></ul></nav><li class=md-nav__item><a class=md-nav__link href=#use-tensorrt-on-nvidia-jetson> <span class=md-ellipsis> Use TensorRT on NVIDIA Jetson </span> </a> <nav aria-label="Use TensorRT on NVIDIA Jetson"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#convert-model-to-tensorrt-and-run-inference> <span class=md-ellipsis> Convert Model to TensorRT and Run Inference </span> </a><li class=md-nav__item><a class=md-nav__link href=#use-nvidia-deep-learning-accelerator-dla> <span class=md-ellipsis> Use NVIDIA Deep Learning Accelerator (DLA) </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#nvidia-jetson-orin-yolo11-benchmarks> <span class=md-ellipsis> NVIDIA Jetson Orin YOLO11 Benchmarks </span> </a> <nav aria-label="NVIDIA Jetson Orin YOLO11 Benchmarks"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#comparison-chart> <span class=md-ellipsis> Comparison Chart </span> </a><li class=md-nav__item><a class=md-nav__link href=#detailed-comparison-table> <span class=md-ellipsis> Detailed Comparison Table </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#reproduce-our-results> <span class=md-ellipsis> Reproduce Our Results </span> </a><li class=md-nav__item><a class=md-nav__link href=#best-practices-when-using-nvidia-jetson> <span class=md-ellipsis> Best Practices when using NVIDIA Jetson </span> </a><li class=md-nav__item><a class=md-nav__link href=#next-steps> <span class=md-ellipsis> Next Steps </span> </a><li class=md-nav__item><a class=md-nav__link href=#faq> <span class=md-ellipsis> FAQ </span> </a> <nav aria-label=FAQ class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#how-do-i-deploy-ultralytics-yolo11-on-nvidia-jetson-devices> <span class=md-ellipsis> How do I deploy Ultralytics YOLO11 on NVIDIA Jetson devices? </span> </a><li class=md-nav__item><a class=md-nav__link href=#what-performance-benchmarks-can-i-expect-from-yolo11-models-on-nvidia-jetson-devices> <span class=md-ellipsis> What performance benchmarks can I expect from YOLO11 models on NVIDIA Jetson devices? </span> </a><li class=md-nav__item><a class=md-nav__link href=#why-should-i-use-tensorrt-for-deploying-yolo11-on-nvidia-jetson> <span class=md-ellipsis> Why should I use TensorRT for deploying YOLO11 on NVIDIA Jetson? </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-can-i-install-pytorch-and-torchvision-on-nvidia-jetson> <span class=md-ellipsis> How can I install PyTorch and Torchvision on NVIDIA Jetson? </span> </a><li class=md-nav__item><a class=md-nav__link href=#what-are-the-best-practices-for-maximizing-performance-on-nvidia-jetson-when-using-yolo11> <span class=md-ellipsis> What are the best practices for maximizing performance on NVIDIA Jetson when using YOLO11? </span> </a></ul></nav></ul></nav><li class=md-nav__item><a class=md-nav__link href=../deepstream-nvidia-jetson/> <span class=md-ellipsis> DeepStream on NVIDIA Jetson </span> </a><li class=md-nav__item><a class=md-nav__link href=../triton-inference-server/> <span class=md-ellipsis> Triton Inference Server </span> </a><li class=md-nav__item><a class=md-nav__link href=../isolating-segmentation-objects/> <span class=md-ellipsis> Isolating Segmentation Objects </span> </a><li class=md-nav__item><a class=md-nav__link href=../coral-edge-tpu-on-raspberry-pi/> <span class=md-ellipsis> Edge TPU on Raspberry Pi </span> </a><li class=md-nav__item><a class=md-nav__link href=../view-results-in-terminal/> <span class=md-ellipsis> Viewing Inference Images in a Terminal </span> </a><li class=md-nav__item><a class=md-nav__link href=../optimizing-openvino-latency-vs-throughput-modes/> <span class=md-ellipsis> OpenVINO Latency vs Throughput modes </span> </a><li class=md-nav__item><a class=md-nav__link href=../ros-quickstart/> <span class=md-ellipsis> ROS Quickstart </span> </a><li class=md-nav__item><a class=md-nav__link href=../steps-of-a-cv-project/> <span class=md-ellipsis> Steps of a Computer Vision Project </span> </a><li class=md-nav__item><a class=md-nav__link href=../defining-project-goals/> <span class=md-ellipsis> Defining A Computer Vision Project's Goals </span> </a><li class=md-nav__item><a class=md-nav__link href=../data-collection-and-annotation/> <span class=md-ellipsis> Data Collection and Annotation </span> </a><li class=md-nav__item><a class=md-nav__link href=../preprocessing_annotated_data/> <span class=md-ellipsis> Preprocessing Annotated Data </span> </a><li class=md-nav__item><a class=md-nav__link href=../model-training-tips/> <span class=md-ellipsis> Tips for Model Training </span> </a><li class=md-nav__item><a class=md-nav__link href=../model-evaluation-insights/> <span class=md-ellipsis> Insights on Model Evaluation and Fine-Tuning </span> </a><li class=md-nav__item><a class=md-nav__link href=../model-testing/> <span class=md-ellipsis> A Guide on Model Testing </span> </a><li class=md-nav__item><a class=md-nav__link href=../model-deployment-practices/> <span class=md-ellipsis> Best Practices for Model Deployment </span> </a><li class=md-nav__item><a class=md-nav__link href=../model-monitoring-and-maintenance/> <span class=md-ellipsis> Maintaining Your Computer Vision Model </span> </a><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle"id=__nav_8_30 type=checkbox> <div class="md-nav__link md-nav__container"><a class=md-nav__link href=../../datasets/explorer/> <span class=md-ellipsis> Explorer </span> </a><label class=md-nav__link for=__nav_8_30 id=__nav_8_30_label><span class="md-nav__icon md-icon"></span></label></div> <nav aria-expanded=false aria-labelledby=__nav_8_30_label class=md-nav data-md-level=2><label class=md-nav__title for=__nav_8_30><span class="md-nav__icon md-icon"></span> Explorer</label><ul class=md-nav__list data-md-scrollfix><li class=md-nav__item><a class=md-nav__link href=../../datasets/explorer/api/> <span class=md-ellipsis> Explorer API </span> </a><li class=md-nav__item><a class=md-nav__link href=../../datasets/explorer/dashboard/> <span class=md-ellipsis> Explorer Dashboard Demo </span> </a><li class=md-nav__item><a class=md-nav__link href=../../datasets/explorer/explorer/> <span class=md-ellipsis> VOC Exploration Example </span> </a></ul></nav><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle"id=__nav_8_31 type=checkbox> <div class="md-nav__link md-nav__container"><a class=md-nav__link href=../../yolov5/> <span class=md-ellipsis> YOLOv5 </span> </a><label class=md-nav__link for=__nav_8_31 id=__nav_8_31_label><span class="md-nav__icon md-icon"></span></label></div> <nav aria-expanded=false aria-labelledby=__nav_8_31_label class=md-nav data-md-level=2><label class=md-nav__title for=__nav_8_31><span class="md-nav__icon md-icon"></span> YOLOv5</label><ul class=md-nav__list data-md-scrollfix><li class=md-nav__item><a class=md-nav__link href=../../yolov5/quickstart_tutorial/> <span class=md-ellipsis> Quickstart </span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../yolov5/environments/aws_quickstart_tutorial/> <span class=md-ellipsis> Environments </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../yolov5/tutorials/train_custom_data/> <span class=md-ellipsis> Tutorials </span> <span class="md-nav__icon md-icon"></span> </a></ul></nav></ul></nav><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../integrations/> <span class=md-ellipsis> Integrations </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../hub/> <span class=md-ellipsis> HUB </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../reference/cfg/__init__/> <span class=md-ellipsis> Reference </span> <span class="md-nav__icon md-icon"></span> </a><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class=md-nav__link href=../../help/> <span class=md-ellipsis> Help </span> <span class="md-nav__icon md-icon"></span> </a></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary"data-md-component=sidebar data-md-type=toc><div class=md-sidebar__scrollwrap><div class=md-sidebar__inner><nav aria-label="Table of contents"class="md-nav md-nav--secondary"><label class=md-nav__title for=__toc><span class="md-nav__icon md-icon"></span> Table of contents</label><ul class=md-nav__list data-md-component=toc data-md-scrollfix><li class=md-nav__item><a class=md-nav__link href=#what-is-nvidia-jetson> <span class=md-ellipsis> What is NVIDIA Jetson? </span> </a><li class=md-nav__item><a class=md-nav__link href=#nvidia-jetson-series-comparison> <span class=md-ellipsis> NVIDIA Jetson Series Comparison </span> </a><li class=md-nav__item><a class=md-nav__link href=#what-is-nvidia-jetpack> <span class=md-ellipsis> What is NVIDIA JetPack? </span> </a><li class=md-nav__item><a class=md-nav__link href=#flash-jetpack-to-nvidia-jetson> <span class=md-ellipsis> Flash JetPack to NVIDIA Jetson </span> </a><li class=md-nav__item><a class=md-nav__link href=#jetpack-support-based-on-jetson-device> <span class=md-ellipsis> JetPack Support Based on Jetson Device </span> </a><li class=md-nav__item><a class=md-nav__link href=#quick-start-with-docker> <span class=md-ellipsis> Quick Start with Docker </span> </a><li class=md-nav__item><a class=md-nav__link href=#start-with-native-installation> <span class=md-ellipsis> Start with Native Installation </span> </a> <nav aria-label="Start with Native Installation"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#run-on-jetpack-6x> <span class=md-ellipsis> Run on JetPack 6.x </span> </a> <nav aria-label="Run on JetPack 6.x"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#install-ultralytics-package> <span class=md-ellipsis> Install Ultralytics Package </span> </a><li class=md-nav__item><a class=md-nav__link href=#install-pytorch-and-torchvision> <span class=md-ellipsis> Install PyTorch and Torchvision </span> </a><li class=md-nav__item><a class=md-nav__link href=#install-onnxruntime-gpu> <span class=md-ellipsis> Install onnxruntime-gpu </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#run-on-jetpack-5x> <span class=md-ellipsis> Run on JetPack 5.x </span> </a> <nav aria-label="Run on JetPack 5.x"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#install-ultralytics-package_1> <span class=md-ellipsis> Install Ultralytics Package </span> </a><li class=md-nav__item><a class=md-nav__link href=#install-pytorch-and-torchvision_1> <span class=md-ellipsis> Install PyTorch and Torchvision </span> </a><li class=md-nav__item><a class=md-nav__link href=#install-onnxruntime-gpu_1> <span class=md-ellipsis> Install onnxruntime-gpu </span> </a></ul></nav></ul></nav><li class=md-nav__item><a class=md-nav__link href=#use-tensorrt-on-nvidia-jetson> <span class=md-ellipsis> Use TensorRT on NVIDIA Jetson </span> </a> <nav aria-label="Use TensorRT on NVIDIA Jetson"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#convert-model-to-tensorrt-and-run-inference> <span class=md-ellipsis> Convert Model to TensorRT and Run Inference </span> </a><li class=md-nav__item><a class=md-nav__link href=#use-nvidia-deep-learning-accelerator-dla> <span class=md-ellipsis> Use NVIDIA Deep Learning Accelerator (DLA) </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#nvidia-jetson-orin-yolo11-benchmarks> <span class=md-ellipsis> NVIDIA Jetson Orin YOLO11 Benchmarks </span> </a> <nav aria-label="NVIDIA Jetson Orin YOLO11 Benchmarks"class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#comparison-chart> <span class=md-ellipsis> Comparison Chart </span> </a><li class=md-nav__item><a class=md-nav__link href=#detailed-comparison-table> <span class=md-ellipsis> Detailed Comparison Table </span> </a></ul></nav><li class=md-nav__item><a class=md-nav__link href=#reproduce-our-results> <span class=md-ellipsis> Reproduce Our Results </span> </a><li class=md-nav__item><a class=md-nav__link href=#best-practices-when-using-nvidia-jetson> <span class=md-ellipsis> Best Practices when using NVIDIA Jetson </span> </a><li class=md-nav__item><a class=md-nav__link href=#next-steps> <span class=md-ellipsis> Next Steps </span> </a><li class=md-nav__item><a class=md-nav__link href=#faq> <span class=md-ellipsis> FAQ </span> </a> <nav aria-label=FAQ class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#how-do-i-deploy-ultralytics-yolo11-on-nvidia-jetson-devices> <span class=md-ellipsis> How do I deploy Ultralytics YOLO11 on NVIDIA Jetson devices? </span> </a><li class=md-nav__item><a class=md-nav__link href=#what-performance-benchmarks-can-i-expect-from-yolo11-models-on-nvidia-jetson-devices> <span class=md-ellipsis> What performance benchmarks can I expect from YOLO11 models on NVIDIA Jetson devices? </span> </a><li class=md-nav__item><a class=md-nav__link href=#why-should-i-use-tensorrt-for-deploying-yolo11-on-nvidia-jetson> <span class=md-ellipsis> Why should I use TensorRT for deploying YOLO11 on NVIDIA Jetson? </span> </a><li class=md-nav__item><a class=md-nav__link href=#how-can-i-install-pytorch-and-torchvision-on-nvidia-jetson> <span class=md-ellipsis> How can I install PyTorch and Torchvision on NVIDIA Jetson? </span> </a><li class=md-nav__item><a class=md-nav__link href=#what-are-the-best-practices-for-maximizing-performance-on-nvidia-jetson-when-using-yolo11> <span class=md-ellipsis> What are the best practices for maximizing performance on NVIDIA Jetson when using YOLO11? </span> </a></ul></nav></ul></nav></div></div></div><div class=md-content data-md-component=content><article class="md-content__inner md-typeset"><a class="md-content__button md-icon"title="Edit this page"href=https://github.com/ultralytics/ultralytics/tree/main/docs/en/guides/nvidia-jetson.md> <svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg> </a><h1 id=quick-start-guide-nvidia-jetson-with-ultralytics-yolo11>Quick Start Guide: NVIDIA Jetson with Ultralytics YOLO11</h1><p>This comprehensive guide provides a detailed walkthrough for deploying Ultralytics YOLO11 on <a href=https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/>NVIDIA Jetson</a> devices. Additionally, it showcases performance benchmarks to demonstrate the capabilities of YOLO11 on these small and powerful devices.<p align=center><br> <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"title="YouTube video player"allowfullscreen frameborder=0 height=405 loading=lazy src=https://www.youtube.com/embed/mUybgOlSxxA width=720></iframe> <br> <strong>Watch:</strong> How to Setup NVIDIA Jetson with Ultralytics YOLO11<p><img alt="NVIDIA Jetson Ecosystem"src=https://github.com/ultralytics/docs/releases/download/0/nvidia-jetson-ecosystem.avif width=1024><div class="admonition note"><p class=admonition-title>Note<p>This guide has been tested with both <a href=https://www.seeedstudio.com/reComputer-J4012-p-5586.html>Seeed Studio reComputer J4012</a> which is based on NVIDIA Jetson Orin NX 16GB running the latest stable JetPack release of <a href=https://developer.nvidia.com/embedded/jetpack-sdk-60>JP6.0</a>, JetPack release of <a href=https://developer.nvidia.com/embedded/jetpack-sdk-513>JP5.1.3</a> and <a href=https://www.seeedstudio.com/reComputer-J1020-v2-p-5498.html>Seeed Studio reComputer J1020 v2</a> which is based on NVIDIA Jetson Nano 4GB running JetPack release of <a href=https://developer.nvidia.com/embedded/jetpack-sdk-461>JP4.6.1</a>. It is expected to work across all the NVIDIA Jetson hardware lineup including latest and legacy.</div><h2 id=what-is-nvidia-jetson>What is NVIDIA Jetson?</h2><p>NVIDIA Jetson is a series of embedded computing boards designed to bring accelerated AI (artificial intelligence) computing to edge devices. These compact and powerful devices are built around NVIDIA's GPU architecture and are capable of running complex AI algorithms and <a href=https://www.ultralytics.com/glossary/deep-learning-dl>deep learning</a> models directly on the device, without needing to rely on <a href=https://www.ultralytics.com/glossary/cloud-computing>cloud computing</a> resources. Jetson boards are often used in robotics, autonomous vehicles, industrial automation, and other applications where AI inference needs to be performed locally with low latency and high efficiency. Additionally, these boards are based on the ARM64 architecture and runs on lower power compared to traditional GPU computing devices.<h2 id=nvidia-jetson-series-comparison>NVIDIA Jetson Series Comparison</h2><p><a href=https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/>Jetson Orin</a> is the latest iteration of the NVIDIA Jetson family based on NVIDIA Ampere architecture which brings drastically improved AI performance when compared to the previous generations. Below table compared few of the Jetson devices in the ecosystem.<table><thead><tr><th><th>Jetson AGX Orin 64GB<th>Jetson Orin NX 16GB<th>Jetson Orin Nano 8GB<th>Jetson AGX Xavier<th>Jetson Xavier NX<th>Jetson Nano<tbody><tr><td>AI Performance<td>275 TOPS<td>100 TOPS<td>40 TOPs<td>32 TOPS<td>21 TOPS<td>472 GFLOPS<tr><td>GPU<td>2048-core NVIDIA Ampere architecture GPU with 64 Tensor Cores<td>1024-core NVIDIA Ampere architecture GPU with 32 Tensor Cores<td>1024-core NVIDIA Ampere architecture GPU with 32 Tensor Cores<td>512-core NVIDIA Volta architecture GPU with 64 Tensor Cores<td>384-core NVIDIA Volta‚Ñ¢ architecture GPU with 48 Tensor Cores<td>128-core NVIDIA Maxwell‚Ñ¢ architecture GPU<tr><td>GPU Max Frequency<td>1.3 GHz<td>918 MHz<td>625 MHz<td>1377 MHz<td>1100 MHz<td>921MHz<tr><td>CPU<td>12-core NVIDIA Arm¬Æ Cortex A78AE v8.2 64-bit CPU 3MB L2 + 6MB L3<td>8-core NVIDIA Arm¬Æ Cortex A78AE v8.2 64-bit CPU 2MB L2 + 4MB L3<td>6-core Arm¬Æ Cortex¬Æ-A78AE v8.2 64-bit CPU 1.5MB L2 + 4MB L3<td>8-core NVIDIA Carmel Arm¬Æv8.2 64-bit CPU 8MB L2 + 4MB L3<td>6-core NVIDIA Carmel Arm¬Æv8.2 64-bit CPU 6MB L2 + 4MB L3<td>Quad-Core Arm¬Æ Cortex¬Æ-A57 MPCore processor<tr><td>CPU Max Frequency<td>2.2 GHz<td>2.0 GHz<td>1.5 GHz<td>2.2 GHz<td>1.9 GHz<td>1.43GHz<tr><td>Memory<td>64GB 256-bit LPDDR5 204.8GB/s<td>16GB 128-bit LPDDR5 102.4GB/s<td>8GB 128-bit LPDDR5 68 GB/s<td>32GB 256-bit LPDDR4x 136.5GB/s<td>8GB 128-bit LPDDR4x 59.7GB/s<td>4GB 64-bit LPDDR4 25.6GB/s"</table><p>For a more detailed comparison table, please visit the <strong>Technical Specifications</strong> section of <a href=https://developer.nvidia.com/embedded/jetson-modules>official NVIDIA Jetson page</a>.<h2 id=what-is-nvidia-jetpack>What is NVIDIA JetPack?</h2><p><a href=https://developer.nvidia.com/embedded/jetpack>NVIDIA JetPack SDK</a> powering the Jetson modules is the most comprehensive solution and provides full development environment for building end-to-end accelerated AI applications and shortens time to market. JetPack includes Jetson Linux with bootloader, Linux kernel, Ubuntu desktop environment, and a complete set of libraries for acceleration of GPU computing, multimedia, graphics, and <a href=https://www.ultralytics.com/glossary/computer-vision-cv>computer vision</a>. It also includes samples, documentation, and developer tools for both host computer and developer kit, and supports higher level SDKs such as DeepStream for streaming video analytics, Isaac for robotics, and Riva for conversational AI.<h2 id=flash-jetpack-to-nvidia-jetson>Flash JetPack to NVIDIA Jetson</h2><p>The first step after getting your hands on an NVIDIA Jetson device is to flash NVIDIA JetPack to the device. There are several different way of flashing NVIDIA Jetson devices.<ol><li>If you own an official NVIDIA Development Kit such as the Jetson Orin Nano Developer Kit, you can <a href=https://developer.nvidia.com/embedded/learn/get-started-jetson-orin-nano-devkit>download an image and prepare an SD card with JetPack for booting the device</a>.<li>If you own any other NVIDIA Development Kit, you can <a href=https://docs.nvidia.com/sdk-manager/install-with-sdkm-jetson/index.html>flash JetPack to the device using SDK Manager</a>.<li>If you own a Seeed Studio reComputer J4012 device, you can <a href=https://wiki.seeedstudio.com/reComputer_J4012_Flash_Jetpack/>flash JetPack to the included SSD</a> and if you own a Seeed Studio reComputer J1020 v2 device, you can <a href=https://wiki.seeedstudio.com/reComputer_J2021_J202_Flash_Jetpack/>flash JetPack to the eMMC/ SSD</a>.<li>If you own any other third party device powered by the NVIDIA Jetson module, it is recommended to follow <a href=https://docs.nvidia.com/jetson/archives/r35.5.0/DeveloperGuide/IN/QuickStart.html>command-line flashing</a>.</ol><div class="admonition note"><p class=admonition-title>Note<p>For methods 3 and 4 above, after flashing the system and booting the device, please enter "sudo apt update && sudo apt install nvidia-jetpack -y" on the device terminal to install all the remaining JetPack components needed.</div><h2 id=jetpack-support-based-on-jetson-device>JetPack Support Based on Jetson Device</h2><p>The below table highlights NVIDIA JetPack versions supported by different NVIDIA Jetson devices.<table><thead><tr><th><th>JetPack 4<th>JetPack 5<th>JetPack 6<tbody><tr><td>Jetson Nano<td>‚úÖ<td>‚ùå<td>‚ùå<tr><td>Jetson TX2<td>‚úÖ<td>‚ùå<td>‚ùå<tr><td>Jetson Xavier NX<td>‚úÖ<td>‚úÖ<td>‚ùå<tr><td>Jetson AGX Xavier<td>‚úÖ<td>‚úÖ<td>‚ùå<tr><td>Jetson AGX Orin<td>‚ùå<td>‚úÖ<td>‚úÖ<tr><td>Jetson Orin NX<td>‚ùå<td>‚úÖ<td>‚úÖ<tr><td>Jetson Orin Nano<td>‚ùå<td>‚úÖ<td>‚úÖ</table><h2 id=quick-start-with-docker>Quick Start with Docker</h2><p>The fastest way to get started with Ultralytics YOLO11 on NVIDIA Jetson is to run with pre-built docker images for Jetson. Refer to the table above and choose the JetPack version according to the Jetson device you own.<div class="tabbed-set tabbed-alternate"data-tabs=1:3><input checked id=__tabbed_1_1 name=__tabbed_1 type=radio><input id=__tabbed_1_2 name=__tabbed_1 type=radio><input id=__tabbed_1_3 name=__tabbed_1 type=radio><div class=tabbed-labels><label for=__tabbed_1_1>JetPack 4</label><label for=__tabbed_1_2>JetPack 5</label><label for=__tabbed_1_3>JetPack 6</label></div><div class=tabbed-content><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-0-1 id=__codelineno-0-1 name=__codelineno-0-1></a><span class=nv>t</span><span class=o>=</span>ultralytics/ultralytics:latest-jetson-jetpack4
<a href=#__codelineno-0-2 id=__codelineno-0-2 name=__codelineno-0-2></a>sudo<span class=w> </span>docker<span class=w> </span>pull<span class=w> </span><span class=nv>$t</span><span class=w> </span><span class=o>&&</span><span class=w> </span>sudo<span class=w> </span>docker<span class=w> </span>run<span class=w> </span>-it<span class=w> </span>--ipc<span class=o>=</span>host<span class=w> </span>--runtime<span class=o>=</span>nvidia<span class=w> </span><span class=nv>$t</span>
</code></pre></div></div><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-1-1 id=__codelineno-1-1 name=__codelineno-1-1></a><span class=nv>t</span><span class=o>=</span>ultralytics/ultralytics:latest-jetson-jetpack5
<a href=#__codelineno-1-2 id=__codelineno-1-2 name=__codelineno-1-2></a>sudo<span class=w> </span>docker<span class=w> </span>pull<span class=w> </span><span class=nv>$t</span><span class=w> </span><span class=o>&&</span><span class=w> </span>sudo<span class=w> </span>docker<span class=w> </span>run<span class=w> </span>-it<span class=w> </span>--ipc<span class=o>=</span>host<span class=w> </span>--runtime<span class=o>=</span>nvidia<span class=w> </span><span class=nv>$t</span>
</code></pre></div></div><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-2-1 id=__codelineno-2-1 name=__codelineno-2-1></a><span class=nv>t</span><span class=o>=</span>ultralytics/ultralytics:latest-jetson-jetpack6
<a href=#__codelineno-2-2 id=__codelineno-2-2 name=__codelineno-2-2></a>sudo<span class=w> </span>docker<span class=w> </span>pull<span class=w> </span><span class=nv>$t</span><span class=w> </span><span class=o>&&</span><span class=w> </span>sudo<span class=w> </span>docker<span class=w> </span>run<span class=w> </span>-it<span class=w> </span>--ipc<span class=o>=</span>host<span class=w> </span>--runtime<span class=o>=</span>nvidia<span class=w> </span><span class=nv>$t</span>
</code></pre></div></div></div></div><p>After this is done, skip to <a href=#use-tensorrt-on-nvidia-jetson>Use TensorRT on NVIDIA Jetson section</a>.<h2 id=start-with-native-installation>Start with Native Installation</h2><p>For a native installation without Docker, please refer to the steps below.<h3 id=run-on-jetpack-6x>Run on JetPack 6.x</h3><h4 id=install-ultralytics-package>Install Ultralytics Package</h4><p>Here we will install Ultralytics package on the Jetson with optional dependencies so that we can export the <a href=https://www.ultralytics.com/glossary/pytorch>PyTorch</a> models to other different formats. We will mainly focus on <a href=../../integrations/tensorrt/>NVIDIA TensorRT exports</a> because TensorRT will make sure we can get the maximum performance out of the Jetson devices.<ol><li><p>Update packages list, install pip and upgrade to latest</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-3-1 id=__codelineno-3-1 name=__codelineno-3-1></a>sudo<span class=w> </span>apt<span class=w> </span>update
<a href=#__codelineno-3-2 id=__codelineno-3-2 name=__codelineno-3-2></a>sudo<span class=w> </span>apt<span class=w> </span>install<span class=w> </span>python3-pip<span class=w> </span>-y
<a href=#__codelineno-3-3 id=__codelineno-3-3 name=__codelineno-3-3></a>pip<span class=w> </span>install<span class=w> </span>-U<span class=w> </span>pip
</code></pre></div><li><p>Install <code>ultralytics</code> pip package with optional dependencies</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-4-1 id=__codelineno-4-1 name=__codelineno-4-1></a>pip<span class=w> </span>install<span class=w> </span>ultralytics<span class=o>[</span>export<span class=o>]</span>
</code></pre></div><li><p>Reboot the device</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-5-1 id=__codelineno-5-1 name=__codelineno-5-1></a>sudo<span class=w> </span>reboot
</code></pre></div></ol><h4 id=install-pytorch-and-torchvision>Install PyTorch and Torchvision</h4><p>The above ultralytics installation will install Torch and Torchvision. However, these 2 packages installed via pip are not compatible to run on Jetson platform which is based on ARM64 architecture. Therefore, we need to manually install pre-built PyTorch pip wheel and compile/ install Torchvision from source.<p>Install <code>torch 2.3.0</code> and <code>torchvision 0.18</code> according to JP6.0<div class=highlight><pre><span></span><code><a href=#__codelineno-6-1 id=__codelineno-6-1 name=__codelineno-6-1></a>sudo<span class=w> </span>apt-get<span class=w> </span>install<span class=w> </span>libopenmpi-dev<span class=w> </span>libopenblas-base<span class=w> </span>libomp-dev<span class=w> </span>-y
<a href=#__codelineno-6-2 id=__codelineno-6-2 name=__codelineno-6-2></a>pip<span class=w> </span>install<span class=w> </span>https://github.com/ultralytics/assets/releases/download/v0.0.0/torch-2.3.0-cp310-cp310-linux_aarch64.whl
<a href=#__codelineno-6-3 id=__codelineno-6-3 name=__codelineno-6-3></a>pip<span class=w> </span>install<span class=w> </span>https://github.com/ultralytics/assets/releases/download/v0.0.0/torchvision-0.18.0a0+6043bc2-cp310-cp310-linux_aarch64.whl
</code></pre></div><p>Visit the <a href=https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048>PyTorch for Jetson page</a> to access all different versions of PyTorch for different JetPack versions. For a more detailed list on the PyTorch, Torchvision compatibility, visit the <a href=https://github.com/pytorch/vision>PyTorch and Torchvision compatibility page</a>.<h4 id=install-onnxruntime-gpu>Install <code>onnxruntime-gpu</code></h4><p>The <a href=https://pypi.org/project/onnxruntime-gpu/>onnxruntime-gpu</a> package hosted in PyPI does not have <code>aarch64</code> binaries for the Jetson. So we need to manually install this package. This package is needed for some of the exports.<p>All different <code>onnxruntime-gpu</code> packages corresponding to different JetPack and Python versions are listed <a href=https://elinux.org/Jetson_Zoo#ONNX_Runtime>here</a>. However, here we will download and install <code>onnxruntime-gpu 1.18.0</code> with <code>Python3.10</code> support.<div class=highlight><pre><span></span><code><a href=#__codelineno-7-1 id=__codelineno-7-1 name=__codelineno-7-1></a>wget<span class=w> </span>https://nvidia.box.com/shared/static/48dtuob7meiw6ebgfsfqakc9vse62sg4.whl<span class=w> </span>-O<span class=w> </span>onnxruntime_gpu-1.18.0-cp310-cp310-linux_aarch64.whl
<a href=#__codelineno-7-2 id=__codelineno-7-2 name=__codelineno-7-2></a>pip<span class=w> </span>install<span class=w> </span>onnxruntime_gpu-1.18.0-cp310-cp310-linux_aarch64.whl
</code></pre></div><div class="admonition note"><p class=admonition-title>Note<p><code>onnxruntime-gpu</code> will automatically revert back the numpy version to latest. So we need to reinstall numpy to <code>1.23.5</code> to fix an issue by executing:<p><code>pip install numpy==1.23.5</code></div><h3 id=run-on-jetpack-5x>Run on JetPack 5.x</h3><h4 id=install-ultralytics-package_1>Install Ultralytics Package</h4><p>Here we will install Ultralytics package on the Jetson with optional dependencies so that we can export the PyTorch models to other different formats. We will mainly focus on <a href=../../integrations/tensorrt/>NVIDIA TensorRT exports</a> because TensorRT will make sure we can get the maximum performance out of the Jetson devices.<ol><li><p>Update packages list, install pip and upgrade to latest</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-8-1 id=__codelineno-8-1 name=__codelineno-8-1></a>sudo<span class=w> </span>apt<span class=w> </span>update
<a href=#__codelineno-8-2 id=__codelineno-8-2 name=__codelineno-8-2></a>sudo<span class=w> </span>apt<span class=w> </span>install<span class=w> </span>python3-pip<span class=w> </span>-y
<a href=#__codelineno-8-3 id=__codelineno-8-3 name=__codelineno-8-3></a>pip<span class=w> </span>install<span class=w> </span>-U<span class=w> </span>pip
</code></pre></div><li><p>Install <code>ultralytics</code> pip package with optional dependencies</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-9-1 id=__codelineno-9-1 name=__codelineno-9-1></a>pip<span class=w> </span>install<span class=w> </span>ultralytics<span class=o>[</span>export<span class=o>]</span>
</code></pre></div><li><p>Reboot the device</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-10-1 id=__codelineno-10-1 name=__codelineno-10-1></a>sudo<span class=w> </span>reboot
</code></pre></div></ol><h4 id=install-pytorch-and-torchvision_1>Install PyTorch and Torchvision</h4><p>The above ultralytics installation will install Torch and Torchvision. However, these 2 packages installed via pip are not compatible to run on Jetson platform which is based on ARM64 architecture. Therefore, we need to manually install pre-built PyTorch pip wheel and compile/ install Torchvision from source.<ol><li><p>Uninstall currently installed PyTorch and Torchvision</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-11-1 id=__codelineno-11-1 name=__codelineno-11-1></a>pip<span class=w> </span>uninstall<span class=w> </span>torch<span class=w> </span>torchvision
</code></pre></div><li><p>Install PyTorch 2.1.0 according to JP5.1.3</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-12-1 id=__codelineno-12-1 name=__codelineno-12-1></a>sudo<span class=w> </span>apt-get<span class=w> </span>install<span class=w> </span>-y<span class=w> </span>libopenblas-base<span class=w> </span>libopenmpi-dev
<a href=#__codelineno-12-2 id=__codelineno-12-2 name=__codelineno-12-2></a>wget<span class=w> </span>https://developer.download.nvidia.com/compute/redist/jp/v512/pytorch/torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl<span class=w> </span>-O<span class=w> </span>torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl
<a href=#__codelineno-12-3 id=__codelineno-12-3 name=__codelineno-12-3></a>pip<span class=w> </span>install<span class=w> </span>torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl
</code></pre></div><li><p>Install Torchvision v0.16.2 according to PyTorch v2.1.0</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-13-1 id=__codelineno-13-1 name=__codelineno-13-1></a>sudo<span class=w> </span>apt<span class=w> </span>install<span class=w> </span>-y<span class=w> </span>libjpeg-dev<span class=w> </span>zlib1g-dev
<a href=#__codelineno-13-2 id=__codelineno-13-2 name=__codelineno-13-2></a>git<span class=w> </span>clone<span class=w> </span>https://github.com/pytorch/vision<span class=w> </span>torchvision
<a href=#__codelineno-13-3 id=__codelineno-13-3 name=__codelineno-13-3></a><span class=nb>cd</span><span class=w> </span>torchvision
<a href=#__codelineno-13-4 id=__codelineno-13-4 name=__codelineno-13-4></a>git<span class=w> </span>checkout<span class=w> </span>v0.16.2
<a href=#__codelineno-13-5 id=__codelineno-13-5 name=__codelineno-13-5></a>python3<span class=w> </span>setup.py<span class=w> </span>install<span class=w> </span>--user
</code></pre></div></ol><p>Visit the <a href=https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048>PyTorch for Jetson page</a> to access all different versions of PyTorch for different JetPack versions. For a more detailed list on the PyTorch, Torchvision compatibility, visit the <a href=https://github.com/pytorch/vision>PyTorch and Torchvision compatibility page</a>.<h4 id=install-onnxruntime-gpu_1>Install <code>onnxruntime-gpu</code></h4><p>The <a href=https://pypi.org/project/onnxruntime-gpu/>onnxruntime-gpu</a> package hosted in PyPI does not have <code>aarch64</code> binaries for the Jetson. So we need to manually install this package. This package is needed for some of the exports.<p>All different <code>onnxruntime-gpu</code> packages corresponding to different JetPack and Python versions are listed <a href=https://elinux.org/Jetson_Zoo#ONNX_Runtime>here</a>. However, here we will download and install <code>onnxruntime-gpu 1.17.0</code> with <code>Python3.8</code> support.<div class=highlight><pre><span></span><code><a href=#__codelineno-14-1 id=__codelineno-14-1 name=__codelineno-14-1></a>wget<span class=w> </span>https://nvidia.box.com/shared/static/zostg6agm00fb6t5uisw51qi6kpcuwzd.whl<span class=w> </span>-O<span class=w> </span>onnxruntime_gpu-1.17.0-cp38-cp38-linux_aarch64.whl
<a href=#__codelineno-14-2 id=__codelineno-14-2 name=__codelineno-14-2></a>pip<span class=w> </span>install<span class=w> </span>onnxruntime_gpu-1.17.0-cp38-cp38-linux_aarch64.whl
</code></pre></div><div class="admonition note"><p class=admonition-title>Note<p><code>onnxruntime-gpu</code> will automatically revert back the numpy version to latest. So we need to reinstall numpy to <code>1.23.5</code> to fix an issue by executing:<p><code>pip install numpy==1.23.5</code></div><h2 id=use-tensorrt-on-nvidia-jetson>Use TensorRT on NVIDIA Jetson</h2><p>Out of all the model export formats supported by Ultralytics, TensorRT delivers the best inference performance when working with NVIDIA Jetson devices and our recommendation is to use TensorRT with Jetson. We also have a detailed document on TensorRT <a href=../../integrations/tensorrt/>here</a>.<h3 id=convert-model-to-tensorrt-and-run-inference>Convert Model to TensorRT and Run Inference</h3><p>The YOLO11n model in PyTorch format is converted to TensorRT to run inference with the exported model.<div class="admonition example"><p class=admonition-title>Example<div class="tabbed-set tabbed-alternate"data-tabs=2:2><input checked id=__tabbed_2_1 name=__tabbed_2 type=radio><input id=__tabbed_2_2 name=__tabbed_2 type=radio><div class=tabbed-labels><label for=__tabbed_2_1>Python</label><label for=__tabbed_2_2>CLI</label></div><div class=tabbed-content><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-15-1 id=__codelineno-15-1 name=__codelineno-15-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>YOLO</span>
<a href=#__codelineno-15-2 id=__codelineno-15-2 name=__codelineno-15-2></a>
<a href=#__codelineno-15-3 id=__codelineno-15-3 name=__codelineno-15-3></a><span class=c1># Load a YOLO11n PyTorch model</span>
<a href=#__codelineno-15-4 id=__codelineno-15-4 name=__codelineno-15-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>YOLO</span><span class=p>(</span><span class=s2>"yolo11n.pt"</span><span class=p>)</span>
<a href=#__codelineno-15-5 id=__codelineno-15-5 name=__codelineno-15-5></a>
<a href=#__codelineno-15-6 id=__codelineno-15-6 name=__codelineno-15-6></a><span class=c1># Export the model to TensorRT</span>
<a href=#__codelineno-15-7 id=__codelineno-15-7 name=__codelineno-15-7></a><span class=n>model</span><span class=o>.</span><span class=n>export</span><span class=p>(</span><span class=nb>format</span><span class=o>=</span><span class=s2>"engine"</span><span class=p>)</span>  <span class=c1># creates 'yolo11n.engine'</span>
<a href=#__codelineno-15-8 id=__codelineno-15-8 name=__codelineno-15-8></a>
<a href=#__codelineno-15-9 id=__codelineno-15-9 name=__codelineno-15-9></a><span class=c1># Load the exported TensorRT model</span>
<a href=#__codelineno-15-10 id=__codelineno-15-10 name=__codelineno-15-10></a><span class=n>trt_model</span> <span class=o>=</span> <span class=n>YOLO</span><span class=p>(</span><span class=s2>"yolo11n.engine"</span><span class=p>)</span>
<a href=#__codelineno-15-11 id=__codelineno-15-11 name=__codelineno-15-11></a>
<a href=#__codelineno-15-12 id=__codelineno-15-12 name=__codelineno-15-12></a><span class=c1># Run inference</span>
<a href=#__codelineno-15-13 id=__codelineno-15-13 name=__codelineno-15-13></a><span class=n>results</span> <span class=o>=</span> <span class=n>trt_model</span><span class=p>(</span><span class=s2>"https://ultralytics.com/images/bus.jpg"</span><span class=p>)</span>
</code></pre></div></div><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-16-1 id=__codelineno-16-1 name=__codelineno-16-1></a><span class=c1># Export a YOLO11n PyTorch model to TensorRT format</span>
<a href=#__codelineno-16-2 id=__codelineno-16-2 name=__codelineno-16-2></a>yolo<span class=w> </span><span class=nb>export</span><span class=w> </span><span class=nv>model</span><span class=o>=</span>yolo11n.pt<span class=w> </span><span class=nv>format</span><span class=o>=</span>engine<span class=w>  </span><span class=c1># creates 'yolo11n.engine'</span>
<a href=#__codelineno-16-3 id=__codelineno-16-3 name=__codelineno-16-3></a>
<a href=#__codelineno-16-4 id=__codelineno-16-4 name=__codelineno-16-4></a><span class=c1># Run inference with the exported model</span>
<a href=#__codelineno-16-5 id=__codelineno-16-5 name=__codelineno-16-5></a>yolo<span class=w> </span>predict<span class=w> </span><span class=nv>model</span><span class=o>=</span>yolo11n.engine<span class=w> </span><span class=nv>source</span><span class=o>=</span><span class=s1>'https://ultralytics.com/images/bus.jpg'</span>
</code></pre></div></div></div></div></div><h3 id=use-nvidia-deep-learning-accelerator-dla>Use NVIDIA Deep Learning Accelerator (DLA)</h3><p><a href=https://developer.nvidia.com/deep-learning-accelerator>NVIDIA Deep Learning Accelerator (DLA)</a> is a specialized hardware component built into NVIDIA Jetson devices that optimizes deep learning inference for energy efficiency and performance. By offloading tasks from the GPU (freeing it up for more intensive processes), DLA enables models to run with lower power consumption while maintaining high throughput, ideal for embedded systems and real-time AI applications.<p>The following Jetson devices are equipped with DLA hardware:<ul><li>Jetson Orin NX 16GB<li>Jetson AGX Orin Series<li>Jetson AGX Xavier Series<li>Jetson Xavier NX Series</ul><div class="admonition example"><p class=admonition-title>Example<div class="tabbed-set tabbed-alternate"data-tabs=3:2><input checked id=__tabbed_3_1 name=__tabbed_3 type=radio><input id=__tabbed_3_2 name=__tabbed_3 type=radio><div class=tabbed-labels><label for=__tabbed_3_1>Python</label><label for=__tabbed_3_2>CLI</label></div><div class=tabbed-content><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-17-1 id=__codelineno-17-1 name=__codelineno-17-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>YOLO</span>
<a href=#__codelineno-17-2 id=__codelineno-17-2 name=__codelineno-17-2></a>
<a href=#__codelineno-17-3 id=__codelineno-17-3 name=__codelineno-17-3></a><span class=c1># Load a YOLO11n PyTorch model</span>
<a href=#__codelineno-17-4 id=__codelineno-17-4 name=__codelineno-17-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>YOLO</span><span class=p>(</span><span class=s2>"yolo11n.pt"</span><span class=p>)</span>
<a href=#__codelineno-17-5 id=__codelineno-17-5 name=__codelineno-17-5></a>
<a href=#__codelineno-17-6 id=__codelineno-17-6 name=__codelineno-17-6></a><span class=c1># Export the model to TensorRT with DLA enabled (only works with FP16 or INT8)</span>
<a href=#__codelineno-17-7 id=__codelineno-17-7 name=__codelineno-17-7></a><span class=n>model</span><span class=o>.</span><span class=n>export</span><span class=p>(</span><span class=nb>format</span><span class=o>=</span><span class=s2>"engine"</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s2>"dla:0"</span><span class=p>,</span> <span class=n>half</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>  <span class=c1># dla:0 or dla:1 corresponds to the DLA cores</span>
<a href=#__codelineno-17-8 id=__codelineno-17-8 name=__codelineno-17-8></a>
<a href=#__codelineno-17-9 id=__codelineno-17-9 name=__codelineno-17-9></a><span class=c1># Load the exported TensorRT model</span>
<a href=#__codelineno-17-10 id=__codelineno-17-10 name=__codelineno-17-10></a><span class=n>trt_model</span> <span class=o>=</span> <span class=n>YOLO</span><span class=p>(</span><span class=s2>"yolo11n.engine"</span><span class=p>)</span>
<a href=#__codelineno-17-11 id=__codelineno-17-11 name=__codelineno-17-11></a>
<a href=#__codelineno-17-12 id=__codelineno-17-12 name=__codelineno-17-12></a><span class=c1># Run inference</span>
<a href=#__codelineno-17-13 id=__codelineno-17-13 name=__codelineno-17-13></a><span class=n>results</span> <span class=o>=</span> <span class=n>trt_model</span><span class=p>(</span><span class=s2>"https://ultralytics.com/images/bus.jpg"</span><span class=p>)</span>
</code></pre></div></div><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-18-1 id=__codelineno-18-1 name=__codelineno-18-1></a><span class=c1># Export a YOLO11n PyTorch model to TensorRT format with DLA enabled (only works with FP16 or INT8)</span>
<a href=#__codelineno-18-2 id=__codelineno-18-2 name=__codelineno-18-2></a>yolo<span class=w> </span><span class=nb>export</span><span class=w> </span><span class=nv>model</span><span class=o>=</span>yolo11n.pt<span class=w> </span><span class=nv>format</span><span class=o>=</span>engine<span class=w> </span><span class=nv>device</span><span class=o>=</span><span class=s2>"dla:0"</span><span class=w> </span><span class=nv>half</span><span class=o>=</span>True<span class=w>  </span><span class=c1># dla:0 or dla:1 corresponds to the DLA cores</span>
<a href=#__codelineno-18-3 id=__codelineno-18-3 name=__codelineno-18-3></a>
<a href=#__codelineno-18-4 id=__codelineno-18-4 name=__codelineno-18-4></a><span class=c1># Run inference with the exported model on the DLA</span>
<a href=#__codelineno-18-5 id=__codelineno-18-5 name=__codelineno-18-5></a>yolo<span class=w> </span>predict<span class=w> </span><span class=nv>model</span><span class=o>=</span>yolo11n.engine<span class=w> </span><span class=nv>source</span><span class=o>=</span><span class=s1>'https://ultralytics.com/images/bus.jpg'</span>
</code></pre></div></div></div></div></div><div class="admonition note"><p class=admonition-title>Note<p>Visit the <a href=../../modes/export/#arguments>Export page</a> to access additional arguments when exporting models to different model formats</div><h2 id=nvidia-jetson-orin-yolo11-benchmarks>NVIDIA Jetson Orin YOLO11 Benchmarks</h2><p>YOLO11 benchmarks were run by the Ultralytics team on 10 different model formats measuring speed and <a href=https://www.ultralytics.com/glossary/accuracy>accuracy</a>: PyTorch, TorchScript, ONNX, OpenVINO, TensorRT, TF SavedModel, TF GraphDef, TF Lite, PaddlePaddle, NCNN. Benchmarks were run on Seeed Studio reComputer J4012 powered by Jetson Orin NX 16GB device at FP32 <a href=https://www.ultralytics.com/glossary/precision>precision</a> with default input image size of 640.<h3 id=comparison-chart>Comparison Chart</h3><p>Even though all model exports are working with NVIDIA Jetson, we have only included <strong>PyTorch, TorchScript, TensorRT</strong> for the comparison chart below because, they make use of the GPU on the Jetson and are guaranteed to produce the best results. All the other exports only utilize the CPU and the performance is not as good as the above three. You can find benchmarks for all exports in the section after this chart.<div style="text-align: center;"><img alt="NVIDIA Jetson Ecosystem"src=https://github.com/ultralytics/docs/releases/download/0/nvidia-jetson-benchmarks.avif></div><h3 id=detailed-comparison-table>Detailed Comparison Table</h3><p>The below table represents the benchmark results for five different models (YOLO11n, YOLO11s, YOLO11m, YOLO11l, YOLO11x) across ten different formats (PyTorch, TorchScript, ONNX, OpenVINO, TensorRT, TF SavedModel, TF GraphDef, TF Lite, PaddlePaddle, NCNN), giving us the status, size, mAP50-95(B) metric, and inference time for each combination.<div class="admonition performance"><p class=admonition-title>Performance<div class="tabbed-set tabbed-alternate"data-tabs=4:5><input checked id=__tabbed_4_1 name=__tabbed_4 type=radio><input id=__tabbed_4_2 name=__tabbed_4 type=radio><input id=__tabbed_4_3 name=__tabbed_4 type=radio><input id=__tabbed_4_4 name=__tabbed_4 type=radio><input id=__tabbed_4_5 name=__tabbed_4 type=radio><div class=tabbed-labels><label for=__tabbed_4_1>YOLO11n</label><label for=__tabbed_4_2>YOLO11s</label><label for=__tabbed_4_3>YOLO11m</label><label for=__tabbed_4_4>YOLO11l</label><label for=__tabbed_4_5>YOLO11x</label></div><div class=tabbed-content><div class=tabbed-block><table><thead><tr><th>Format<th>Status<th>Size on disk (MB)<th>mAP50-95(B)<th>Inference time (ms/im)<tbody><tr><td>PyTorch<td>‚úÖ<td>5.4<td>0.6176<td>19.80<tr><td>TorchScript<td>‚úÖ<td>10.5<td>0.6100<td>13.30<tr><td>ONNX<td>‚úÖ<td>10.2<td>0.6082<td>67.92<tr><td>OpenVINO<td>‚úÖ<td>10.4<td>0.6082<td>118.21<tr><td>TensorRT (FP32)<td>‚úÖ<td>14.1<td>0.6100<td>7.94<tr><td>TensorRT (FP16)<td>‚úÖ<td>8.3<td>0.6082<td>4.80<tr><td>TensorRT (INT8)<td>‚úÖ<td>6.6<td>0.3256<td>4.17<tr><td>TF SavedModel<td>‚úÖ<td>25.8<td>0.6082<td>185.88<tr><td>TF GraphDef<td>‚úÖ<td>10.3<td>0.6082<td>256.66<tr><td>TF Lite<td>‚úÖ<td>10.3<td>0.6082<td>284.64<tr><td>PaddlePaddle<td>‚úÖ<td>20.4<td>0.6082<td>477.41<tr><td>NCNN<td>‚úÖ<td>10.2<td>0.6106<td>32.18</table></div><div class=tabbed-block><table><thead><tr><th>Format<th>Status<th>Size on disk (MB)<th>mAP50-95(B)<th>Inference time (ms/im)<tbody><tr><td>PyTorch<td>‚úÖ<td>18.4<td>0.7526<td>20.20<tr><td>TorchScript<td>‚úÖ<td>36.5<td>0.7416<td>23.42<tr><td>ONNX<td>‚úÖ<td>36.3<td>0.7416<td>162.01<tr><td>OpenVINO<td>‚úÖ<td>36.4<td>0.7416<td>159.61<tr><td>TensorRT (FP32)<td>‚úÖ<td>40.3<td>0.7416<td>13.93<tr><td>TensorRT (FP16)<td>‚úÖ<td>21.7<td>0.7416<td>7.47<tr><td>TensorRT (INT8)<td>‚úÖ<td>13.6<td>0.3179<td>5.66<tr><td>TF SavedModel<td>‚úÖ<td>91.1<td>0.7416<td>316.46<tr><td>TF GraphDef<td>‚úÖ<td>36.4<td>0.7416<td>506.71<tr><td>TF Lite<td>‚úÖ<td>36.4<td>0.7416<td>842.97<tr><td>PaddlePaddle<td>‚úÖ<td>72.5<td>0.7416<td>1172.57<tr><td>NCNN<td>‚úÖ<td>36.2<td>0.7419<td>66.00</table></div><div class=tabbed-block><table><thead><tr><th>Format<th>Status<th>Size on disk (MB)<th>mAP50-95(B)<th>Inference time (ms/im)<tbody><tr><td>PyTorch<td>‚úÖ<td>38.8<td>0.7595<td>36.70<tr><td>TorchScript<td>‚úÖ<td>77.3<td>0.7643<td>50.95<tr><td>ONNX<td>‚úÖ<td>76.9<td>0.7643<td>416.34<tr><td>OpenVINO<td>‚úÖ<td>77.1<td>0.7643<td>370.99<tr><td>TensorRT (FP32)<td>‚úÖ<td>81.5<td>0.7640<td>30.49<tr><td>TensorRT (FP16)<td>‚úÖ<td>42.2<td>0.7658<td>14.93<tr><td>TensorRT (INT8)<td>‚úÖ<td>24.3<td>0.4118<td>10.32<tr><td>TF SavedModel<td>‚úÖ<td>192.7<td>0.7643<td>597.08<tr><td>TF GraphDef<td>‚úÖ<td>77.0<td>0.7643<td>1016.12<tr><td>TF Lite<td>‚úÖ<td>77.0<td>0.7643<td>2494.60<tr><td>PaddlePaddle<td>‚úÖ<td>153.8<td>0.7643<td>3218.99<tr><td>NCNN<td>‚úÖ<td>76.8<td>0.7691<td>192.77</table></div><div class=tabbed-block><table><thead><tr><th>Format<th>Status<th>Size on disk (MB)<th>mAP50-95(B)<th>Inference time (ms/im)<tbody><tr><td>PyTorch<td>‚úÖ<td>49.0<td>0.7475<td>47.6<tr><td>TorchScript<td>‚úÖ<td>97.6<td>0.7250<td>66.36<tr><td>ONNX<td>‚úÖ<td>97.0<td>0.7250<td>532.58<tr><td>OpenVINO<td>‚úÖ<td>97.3<td>0.7250<td>477.55<tr><td>TensorRT (FP32)<td>‚úÖ<td>101.6<td>0.7250<td>38.71<tr><td>TensorRT (FP16)<td>‚úÖ<td>52.6<td>0.7265<td>19.35<tr><td>TensorRT (INT8)<td>‚úÖ<td>31.6<td>0.3856<td>13.50<tr><td>TF SavedModel<td>‚úÖ<td>243.3<td>0.7250<td>895.24<tr><td>TF GraphDef<td>‚úÖ<td>97.2<td>0.7250<td>1301.19<tr><td>TF Lite<td>‚úÖ<td>97.2<td>0.7250<td>3202.93<tr><td>PaddlePaddle<td>‚úÖ<td>193.9<td>0.7250<td>4206.98<tr><td>NCNN<td>‚úÖ<td>96.9<td>0.7252<td>225.75</table></div><div class=tabbed-block><table><thead><tr><th>Format<th>Status<th>Size on disk (MB)<th>mAP50-95(B)<th>Inference time (ms/im)<tbody><tr><td>PyTorch<td>‚úÖ<td>109.3<td>0.8288<td>85.60<tr><td>TorchScript<td>‚úÖ<td>218.1<td>0.8308<td>121.67<tr><td>ONNX<td>‚úÖ<td>217.5<td>0.8308<td>1073.14<tr><td>OpenVINO<td>‚úÖ<td>217.8<td>0.8308<td>955.60<tr><td>TensorRT (FP32)<td>‚úÖ<td>221.6<td>0.8307<td>75.84<tr><td>TensorRT (FP16)<td>‚úÖ<td>113.1<td>0.8295<td>35.75<tr><td>TensorRT (INT8)<td>‚úÖ<td>62.2<td>0.4783<td>22.23<tr><td>TF SavedModel<td>‚úÖ<td>545.0<td>0.8308<td>1497.40<tr><td>TF GraphDef<td>‚úÖ<td>217.8<td>0.8308<td>2552.42<tr><td>TF Lite<td>‚úÖ<td>217.8<td>0.8308<td>7044.58<tr><td>PaddlePaddle<td>‚úÖ<td>434.9<td>0.8308<td>8386.73<tr><td>NCNN<td>‚úÖ<td>217.3<td>0.8304<td>486.36</table></div></div></div></div><p><a href=https://www.seeedstudio.com/blog/2023/03/30/yolov8-performance-benchmarks-on-nvidia-jetson-devices>Explore more benchmarking efforts by Seeed Studio</a> running on different versions of NVIDIA Jetson hardware.<h2 id=reproduce-our-results>Reproduce Our Results</h2><p>To reproduce the above Ultralytics benchmarks on all export <a href=../../modes/export/>formats</a> run this code:<div class="admonition example"><p class=admonition-title>Example<div class="tabbed-set tabbed-alternate"data-tabs=5:2><input checked id=__tabbed_5_1 name=__tabbed_5 type=radio><input id=__tabbed_5_2 name=__tabbed_5 type=radio><div class=tabbed-labels><label for=__tabbed_5_1>Python</label><label for=__tabbed_5_2>CLI</label></div><div class=tabbed-content><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-19-1 id=__codelineno-19-1 name=__codelineno-19-1></a><span class=kn>from</span> <span class=nn>ultralytics</span> <span class=kn>import</span> <span class=n>YOLO</span>
<a href=#__codelineno-19-2 id=__codelineno-19-2 name=__codelineno-19-2></a>
<a href=#__codelineno-19-3 id=__codelineno-19-3 name=__codelineno-19-3></a><span class=c1># Load a YOLO11n PyTorch model</span>
<a href=#__codelineno-19-4 id=__codelineno-19-4 name=__codelineno-19-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>YOLO</span><span class=p>(</span><span class=s2>"yolo11n.pt"</span><span class=p>)</span>
<a href=#__codelineno-19-5 id=__codelineno-19-5 name=__codelineno-19-5></a>
<a href=#__codelineno-19-6 id=__codelineno-19-6 name=__codelineno-19-6></a><span class=c1># Benchmark YOLO11n speed and accuracy on the COCO8 dataset for all all export formats</span>
<a href=#__codelineno-19-7 id=__codelineno-19-7 name=__codelineno-19-7></a><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>benchmarks</span><span class=p>(</span><span class=n>data</span><span class=o>=</span><span class=s2>"coco8.yaml"</span><span class=p>,</span> <span class=n>imgsz</span><span class=o>=</span><span class=mi>640</span><span class=p>)</span>
</code></pre></div></div><div class=tabbed-block><div class=highlight><pre><span></span><code><a href=#__codelineno-20-1 id=__codelineno-20-1 name=__codelineno-20-1></a><span class=c1># Benchmark YOLO11n speed and accuracy on the COCO8 dataset for all all export formats</span>
<a href=#__codelineno-20-2 id=__codelineno-20-2 name=__codelineno-20-2></a>yolo<span class=w> </span>benchmark<span class=w> </span><span class=nv>model</span><span class=o>=</span>yolo11n.pt<span class=w> </span><span class=nv>data</span><span class=o>=</span>coco8.yaml<span class=w> </span><span class=nv>imgsz</span><span class=o>=</span><span class=m>640</span>
</code></pre></div></div></div></div><p>Note that benchmarking results might vary based on the exact hardware and software configuration of a system, as well as the current workload of the system at the time the benchmarks are run. For the most reliable results use a dataset with a large number of images, i.e. <code>data='coco8.yaml' (4 val images), or</code>data='coco.yaml'` (5000 val images).</div><h2 id=best-practices-when-using-nvidia-jetson>Best Practices when using NVIDIA Jetson</h2><p>When using NVIDIA Jetson, there are a couple of best practices to follow in order to enable maximum performance on the NVIDIA Jetson running YOLO11.<ol><li><p>Enable MAX Power Mode</p> <p>Enabling MAX Power Mode on the Jetson will make sure all CPU, GPU cores are turned on.</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-21-1 id=__codelineno-21-1 name=__codelineno-21-1></a>sudo<span class=w> </span>nvpmodel<span class=w> </span>-m<span class=w> </span><span class=m>0</span>
</code></pre></div><li><p>Enable Jetson Clocks</p> <p>Enabling Jetson Clocks will make sure all CPU, GPU cores are clocked at their maximum frequency.</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-22-1 id=__codelineno-22-1 name=__codelineno-22-1></a>sudo<span class=w> </span>jetson_clocks
</code></pre></div><li><p>Install Jetson Stats Application</p> <p>We can use jetson stats application to monitor the temperatures of the system components and check other system details such as view CPU, GPU, RAM utilization, change power modes, set to max clocks, check JetPack information</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-23-1 id=__codelineno-23-1 name=__codelineno-23-1></a>sudo<span class=w> </span>apt<span class=w> </span>update
<a href=#__codelineno-23-2 id=__codelineno-23-2 name=__codelineno-23-2></a>sudo<span class=w> </span>pip<span class=w> </span>install<span class=w> </span>jetson-stats
<a href=#__codelineno-23-3 id=__codelineno-23-3 name=__codelineno-23-3></a>sudo<span class=w> </span>reboot
<a href=#__codelineno-23-4 id=__codelineno-23-4 name=__codelineno-23-4></a>jtop
</code></pre></div></ol><p><img alt="Jetson Stats"src=https://github.com/ultralytics/docs/releases/download/0/jetson-stats-application.avif width=1024><h2 id=next-steps>Next Steps</h2><p>Congratulations on successfully setting up YOLO11 on your NVIDIA Jetson! For further learning and support, visit more guide at <a href=../../>Ultralytics YOLO11 Docs</a>!<h2 id=faq>FAQ</h2><h3 id=how-do-i-deploy-ultralytics-yolo11-on-nvidia-jetson-devices>How do I deploy Ultralytics YOLO11 on NVIDIA Jetson devices?</h3><p>Deploying Ultralytics YOLO11 on NVIDIA Jetson devices is a straightforward process. First, flash your Jetson device with the NVIDIA JetPack SDK. Then, either use a pre-built Docker image for quick setup or manually install the required packages. Detailed steps for each approach can be found in sections <a href=#quick-start-with-docker>Quick Start with Docker</a> and <a href=#start-with-native-installation>Start with Native Installation</a>.<h3 id=what-performance-benchmarks-can-i-expect-from-yolo11-models-on-nvidia-jetson-devices>What performance benchmarks can I expect from YOLO11 models on NVIDIA Jetson devices?</h3><p>YOLO11 models have been benchmarked on various NVIDIA Jetson devices showing significant performance improvements. For example, the TensorRT format delivers the best inference performance. The table in the <a href=#detailed-comparison-table>Detailed Comparison Table</a> section provides a comprehensive view of performance metrics like mAP50-95 and inference time across different model formats.<h3 id=why-should-i-use-tensorrt-for-deploying-yolo11-on-nvidia-jetson>Why should I use TensorRT for deploying YOLO11 on NVIDIA Jetson?</h3><p>TensorRT is highly recommended for deploying YOLO11 models on NVIDIA Jetson due to its optimal performance. It accelerates inference by leveraging the Jetson's GPU capabilities, ensuring maximum efficiency and speed. Learn more about how to convert to TensorRT and run inference in the <a href=#use-tensorrt-on-nvidia-jetson>Use TensorRT on NVIDIA Jetson</a> section.<h3 id=how-can-i-install-pytorch-and-torchvision-on-nvidia-jetson>How can I install PyTorch and Torchvision on NVIDIA Jetson?</h3><p>To install PyTorch and Torchvision on NVIDIA Jetson, first uninstall any existing versions that may have been installed via pip. Then, manually install the compatible PyTorch and Torchvision versions for the Jetson's ARM64 architecture. Detailed instructions for this process are provided in the <a href=#install-pytorch-and-torchvision>Install PyTorch and Torchvision</a> section.<h3 id=what-are-the-best-practices-for-maximizing-performance-on-nvidia-jetson-when-using-yolo11>What are the best practices for maximizing performance on NVIDIA Jetson when using YOLO11?</h3><p>To maximize performance on NVIDIA Jetson with YOLO11, follow these best practices:<ol><li>Enable MAX Power Mode to utilize all CPU and GPU cores.<li>Enable Jetson Clocks to run all cores at their maximum frequency.<li>Install the Jetson Stats application for monitoring system metrics.</ol><p>For commands and additional details, refer to the <a href=#best-practices-when-using-nvidia-jetson>Best Practices when using NVIDIA Jetson</a> section.<div class=git-info><div class=dates-container><span title="This page was first created on April 02, 2024"class=date-item> <span class=hover-item>üìÖ</span> Created 7 months ago </span><span title="This page was last updated on October 28, 2024"class=date-item> <span class=hover-item>‚úèÔ∏è</span> Updated 5 days ago </span></div><div class=authors-container><a title="lakshanthad (6 changes)"class=author-link href=https://github.com/lakshanthad> <img alt=lakshanthad class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/20147381?v=4&s=96> </a><a title="justincdavis (1 change)"class=author-link href=https://github.com/justincdavis> <img alt=justincdavis class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/23462437?v=4&s=96> </a><a title="glenn-jocher (10 changes)"class=author-link href=https://github.com/glenn-jocher> <img alt=glenn-jocher class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/26833433?v=4&s=96> </a><a title="MatthewNoyce (1 change)"class=author-link href=https://github.com/MatthewNoyce> <img alt=MatthewNoyce class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/131261051?v=4&s=96> </a><a title="UltralyticsAssistant (1 change)"class=author-link href=https://github.com/UltralyticsAssistant> <img alt=UltralyticsAssistant class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/135830346?v=4&s=96> </a><a title="RizwanMunawar (2 changes)"class=author-link href=https://github.com/RizwanMunawar> <img alt=RizwanMunawar class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/62513924?v=4&s=96> </a><a title="Ahelsamahy (1 change)"class=author-link href=https://github.com/Ahelsamahy> <img alt=Ahelsamahy class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/10195309?v=4&s=96> </a><a title="Burhan-Q (2 changes)"class=author-link href=https://github.com/Burhan-Q> <img alt=Burhan-Q class=hover-item loading=lazy src=https://avatars.githubusercontent.com/u/62214284?v=4&s=96> </a></div></div><div class=share-buttons><button class="share-button hover-item"onclick="window.open('https://twitter.com/intent/tweet?url=https://docs.ultralytics.com/guides/nvidia-jetson', 'TwitterShare', 'width=550,height=680,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-x-twitter"></i> Tweet</button><button class="share-button hover-item linkedin"onclick="window.open('https://www.linkedin.com/shareArticle?url=https://docs.ultralytics.com/guides/nvidia-jetson', 'LinkedinShare', 'width=550,height=730,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-linkedin-in"></i> Share</button></div><h2 id=__comments>Comments</h2><div id=giscus-container></div></article></div><script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script><script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script></div><button class="md-top md-icon"data-md-component=top hidden type=button><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top</button></main><footer class=md-footer><nav class="md-footer__inner md-grid"aria-label=Footer><a aria-label="Previous: Raspberry Pi"class="md-footer__link md-footer__link--prev"href=../raspberry-pi/> <div class="md-footer__button md-icon"><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg></div> <div class=md-footer__title><span class=md-footer__direction> Previous </span><div class=md-ellipsis>Raspberry Pi</div></div> </a><a aria-label="Next: DeepStream on NVIDIA Jetson"class="md-footer__link md-footer__link--next"href=../deepstream-nvidia-jetson/> <div class=md-footer__title><span class=md-footer__direction> Next </span><div class=md-ellipsis>DeepStream on NVIDIA Jetson</div></div> <div class="md-footer__button md-icon"><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg></div> </a></nav><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class=md-copyright><div class=md-copyright__highlight><a href=https://ultralytics.com target=_blank>¬© 2024 Ultralytics Inc.</a> All rights reserved.</div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ rel=noopener target=_blank> Material for MkDocs </a></div><div class=md-social><a class=md-social__link href=https://github.com/ultralytics rel=noopener target=_blank title=github.com> <svg viewbox="0 0 496 512"xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </a><a class=md-social__link href=https://www.linkedin.com/company/ultralytics/ rel=noopener target=_blank title=www.linkedin.com> <svg viewbox="0 0 448 512"xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"></path></svg> </a><a class=md-social__link href=https://twitter.com/ultralytics rel=noopener target=_blank title=twitter.com> <svg viewbox="0 0 512 512"xmlns=http://www.w3.org/2000/svg><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"></path></svg> </a><a class=md-social__link href=https://youtube.com/ultralytics?sub_confirmation=1 rel=noopener target=_blank title=youtube.com> <svg viewbox="0 0 576 512"xmlns=http://www.w3.org/2000/svg><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305m-317.51 213.508V175.185l142.739 81.205z"></path></svg> </a><a class=md-social__link href=https://hub.docker.com/r/ultralytics/ultralytics/ rel=noopener target=_blank title=hub.docker.com> <svg viewbox="0 0 640 512"xmlns=http://www.w3.org/2000/svg><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"></path></svg> </a><a class=md-social__link href=https://pypi.org/project/ultralytics/ rel=noopener target=_blank title=pypi.org> <svg viewbox="0 0 448 512"xmlns=http://www.w3.org/2000/svg><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3"></path></svg> </a><a class=md-social__link href=https://ultralytics.com/discord rel=noopener target=_blank title=ultralytics.com> <svg viewbox="0 0 640 512"xmlns=http://www.w3.org/2000/svg><path d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485 485 0 0 0 404.081 32.03a1.82 1.82 0 0 0-1.923.91 338 338 0 0 0-14.9 30.6 447.9 447.9 0 0 0-134.426 0 310 310 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.7 483.7 0 0 0-119.688 37.107 1.7 1.7 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.02 2.02 0 0 0 .765 1.375 487.7 487.7 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348 348 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321 321 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251 251 0 0 0 9.109-7.137 1.82 1.82 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.81 1.81 0 0 1 1.924.233 235 235 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.4 301.4 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391 391 0 0 0 30.014 48.815 1.86 1.86 0 0 0 2.063.7A486 486 0 0 0 610.7 405.729a1.88 1.88 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541M222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241m195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241"></path></svg> </a><a class=md-social__link href=https://reddit.com/r/ultralytics rel=noopener target=_blank title=reddit.com> <svg viewbox="0 0 512 512"xmlns=http://www.w3.org/2000/svg><path d="M0 256C0 114.6 114.6 0 256 0s256 114.6 256 256-114.6 256-256 256H37.1c-13.7 0-20.5-16.5-10.9-26.2L75 437C28.7 390.7 0 326.7 0 256m349.6-102.4c23.6 0 42.7-19.1 42.7-42.7s-19.1-42.7-42.7-42.7c-20.6 0-37.8 14.6-41.8 34-34.5 3.7-61.4 33-61.4 68.4v.2c-37.5 1.6-71.8 12.3-99 29.1-10.1-7.8-22.8-12.5-36.5-12.5-33 0-59.8 26.8-59.8 59.8 0 24 14.1 44.6 34.4 54.1 2 69.4 77.6 125.2 170.6 125.2s168.7-55.9 170.6-125.3c20.2-9.6 34.1-30.2 34.1-54 0-33-26.8-59.8-59.8-59.8-13.7 0-26.3 4.6-36.4 12.4-27.4-17-62.1-27.7-100-29.1v-.2c0-25.4 18.9-46.5 43.4-49.9 4.4 18.8 21.3 32.8 41.5 32.8zm-172.5 93.3c16.7 0 29.5 17.6 28.5 39.3s-13.5 29.6-30.3 29.6-31.4-8.8-30.4-30.5S160.3 247 177 247zm190.1 38.3c1 21.7-13.7 30.5-30.4 30.5s-29.3-7.9-30.3-29.6 11.8-39.3 28.5-39.3 31.2 16.6 32.1 38.3zm-48.1 56.7c-10.3 24.6-34.6 41.9-63 41.9s-52.7-17.3-63-41.9c-1.2-2.9.8-6.2 3.9-6.5 18.4-1.9 38.3-2.9 59.1-2.9s40.7 1 59.1 2.9c3.1.3 5.1 3.6 3.9 6.5"></path></svg> </a></div></div></div></footer></div><div class=md-dialog data-md-component=dialog><div class="md-dialog__inner md-typeset"></div></div><div class=md-progress data-md-component=progress role=progressbar></div><script id=__config type=application/json>{"base": "../..", "features": ["content.action.edit", "content.code.annotate", "content.code.copy", "content.tooltips", "search.highlight", "search.share", "search.suggest", "toc.follow", "navigation.top", "navigation.tabs", "navigation.tabs.sticky", "navigation.prune", "navigation.footer", "navigation.tracking", "navigation.instant", "navigation.instant.progress", "navigation.indexes", "navigation.sections", "content.tabs.link"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script><script src=../../assets/javascripts/bundle.83f73b43.min.js></script><script src=../../javascript/extra.js></script><script src=../../javascript/giscus.js></script>