<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="Learn how to integrate Ultralytics YOLO11 with NVIDIA Triton Inference Server for scalable, high-performance AI model deployment." name="description"/>
<meta content="Ultralytics" name="author"/>
<link href="https://docs.ultralytics.com/guides/triton-inference-server/" rel="canonical"/>
<link href="../deepstream-nvidia-jetson/" rel="prev"/>
<link href="../isolating-segmentation-objects/" rel="next"/>
<link href="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/logo/favicon-yolo.png" rel="icon"/>
<meta content="mkdocs-1.6.1, mkdocs-material-9.6.21" name="generator"/>
<title>Triton Inference Server with Ultralytics YOLO11</title>
<link href="../../assets/stylesheets/main.2a3383ac.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<link href="../../assets/_mkdocstrings.css" rel="stylesheet"/>
<link href="../../stylesheets/style.css" rel="stylesheet"/>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-2M5EHKC0BH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-2M5EHKC0BH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-2M5EHKC0BH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
<meta content="Triton Inference Server" name="title"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet"/><meta content="Triton Inference Server, YOLO11, Ultralytics, NVIDIA, deep learning, AI model deployment, ONNX, scalable inference" name="keywords"/><meta content="website" property="og:type"/><meta content="https://docs.ultralytics.com/guides/triton-inference-server" property="og:url"/><meta content="Triton Inference Server" property="og:title"/><meta content="Learn how to integrate Ultralytics YOLO11 with NVIDIA Triton Inference Server for scalable, high-performance AI model deployment." property="og:description"/><meta content="https://img.youtube.com/vi/NQDtfSi5QF4/maxresdefault.jpg" property="og:image"/><meta content="summary_large_image" property="twitter:card"/><meta content="https://docs.ultralytics.com/guides/triton-inference-server" property="twitter:url"/><meta content="Triton Inference Server" property="twitter:title"/><meta content="Learn how to integrate Ultralytics YOLO11 with NVIDIA Triton Inference Server for scalable, high-performance AI model deployment." property="twitter:description"/><meta content="https://img.youtube.com/vi/NQDtfSi5QF4/maxresdefault.jpg" property="twitter:image"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": ["Article", "FAQPage"], "headline": "Triton Inference Server", "image": ["https://img.youtube.com/vi/NQDtfSi5QF4/maxresdefault.jpg"], "datePublished": "2023-11-12 02:49:37 +0100", "dateModified": "2025-09-12 13:07:50 +0300", "author": [{"@type": "Organization", "name": "Ultralytics", "url": "https://ultralytics.com/"}], "abstract": "Learn how to integrate Ultralytics YOLO11 with NVIDIA Triton Inference Server for scalable, high-performance AI model deployment.", "mainEntity": [{"@type": "Question", "name": "How do I set up Ultralytics YOLO11 with NVIDIA Triton Inference Server?", "acceptedAnswer": {"@type": "Answer", "text": "Setting up Ultralytics YOLO11 with NVIDIA Triton Inference Server involves a few key steps: This setup can help you efficiently deploy YOLO11 models at scale on Triton Inference Server for high-performance AI model inference."}}, {"@type": "Question", "name": "What benefits does using Ultralytics YOLO11 with NVIDIA Triton Inference Server offer?", "acceptedAnswer": {"@type": "Answer", "text": "Integrating Ultralytics YOLO11 with NVIDIA Triton Inference Server provides several advantages: For detailed instructions on setting up and running YOLO11 with Triton, you can refer to the setup guide."}}, {"@type": "Question", "name": "Why should I export my YOLO11 model to ONNX format before using Triton Inference Server?", "acceptedAnswer": {"@type": "Answer", "text": "Using ONNX (Open Neural Network Exchange) format for your Ultralytics YOLO11 model before deploying it on NVIDIA Triton Inference Server offers several key benefits: To export your model, use: You can follow the steps in the ONNX integration guide to complete the process."}}, {"@type": "Question", "name": "Can I run inference using the Ultralytics YOLO11 model on Triton Inference Server?", "acceptedAnswer": {"@type": "Answer", "text": "Yes, you can run inference using the Ultralytics YOLO11 model on NVIDIA Triton Inference Server. Once your model is set up in the Triton Model Repository and the server is running, you can load and run inference on your model as follows: This approach allows you to leverage Triton's optimizations while using the familiar Ultralytics YOLO interface. For an in-depth guide on setting up and running Triton Server with YOLO11, refer to the running triton inference server section."}}, {"@type": "Question", "name": "How does Ultralytics YOLO11 compare to TensorFlow and PyTorch models for deployment?", "acceptedAnswer": {"@type": "Answer", "text": "Ultralytics YOLO11 offers several unique advantages compared to TensorFlow and PyTorch models for deployment: For more details, compare the deployment options in the model export guide."}}]}</script></head>
<body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="default" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#triton-inference-server-with-ultralytics-yolo11">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
<aside class="md-banner">
<div class="md-banner__inner md-grid md-typeset">
<a class="banner-wrapper" href="https://www.ultralytics.com/events/yolovision" target="_blank">
<div class="banner-content-wrapper">
<p>Register now for</p>
<img alt="Ultralytics YOLO Vision" height="40" loading="lazy" src="https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/6895ca9b3ece7244d9981ab9_yv25-logo.avif"/>
<p class="shenzhen-text">Shenzhen</p>
</div>
</a>
</div>
</aside>
</div>
<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Ultralytics YOLO Docs" class="md-header__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs">
<img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Ultralytics YOLO Docs
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
              Triton Inference Server
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_0" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg>
</label>
<input aria-label="Switch to system preference" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="black" data-md-color-scheme="slate" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to system preference">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
</label>
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_2" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_0" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
</label>
</form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<div class="md-header__option">
<div class="md-select">
<button aria-label="Select language" class="md-header__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"></path></svg>
</button>
<div class="md-select__inner">
<ul class="md-select__list">
<li class="md-select__item">
<a class="md-select__link" href="/" hreflang="en">
              🇬🇧 English
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/zh/" hreflang="zh">
              🇨🇳 简体中文
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/ko/" hreflang="ko">
              🇰🇷 한국어
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/ja/" hreflang="ja">
              🇯🇵 日本語
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/ru/" hreflang="ru">
              🇷🇺 Русский
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/de/" hreflang="de">
              🇩🇪 Deutsch
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/fr/" hreflang="fr">
              🇫🇷 Français
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/es/" hreflang="es">
              🇪🇸 Español
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/pt/" hreflang="pt">
              🇵🇹 Português
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/it/" hreflang="it">
              🇮🇹 Italiano
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/tr/" hreflang="tr">
              🇹🇷 Türkçe
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/vi/" hreflang="vi">
              🇻🇳 Tiếng Việt
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/ar/" hreflang="ar">
              🇸🇦 العربية
            </a>
</li>
</ul>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
</div>
<div class="md-source__repository">
    ultralytics/ultralytics
  </div>
</a>
</div>
</nav>
<nav aria-label="Tabs" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../..">
  Home
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../quickstart/">
  Quickstart
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../modes/">
  Modes
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../tasks/">
  Tasks
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../models/">
  Models
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../datasets/">
  Datasets
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../solutions/">
  Solutions 🚀
        </a>
</li>
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="../">
  Guides
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../integrations/">
  Integrations
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../hub/">
  HUB
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../reference/__init__/">
  Reference
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../help/">
  Help
        </a>
</li>
</ul>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Ultralytics YOLO Docs" class="md-nav__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs">
<img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/>
</a>
    Ultralytics YOLO Docs
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
</div>
<div class="md-source__repository">
    ultralytics/ultralytics
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../..">
<span class="md-ellipsis">
    Home
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../quickstart/">
<span class="md-ellipsis">
    Quickstart
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../modes/">
<span class="md-ellipsis">
    Modes
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../tasks/">
<span class="md-ellipsis">
    Tasks
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../models/">
<span class="md-ellipsis">
    Models
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../datasets/">
<span class="md-ellipsis">
    Datasets
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../solutions/">
<span class="md-ellipsis">
    Solutions 🚀
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_8" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../">
<span class="md-ellipsis">
    Guides
  </span>
</a>
<label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_8_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_8">
<span class="md-nav__icon md-icon"></span>
            Guides
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../yolo-common-issues/">
<span class="md-ellipsis">
    YOLO Common Issues
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../yolo-performance-metrics/">
<span class="md-ellipsis">
    YOLO Performance Metrics
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../yolo-thread-safe-inference/">
<span class="md-ellipsis">
    YOLO Thread-Safe Inference
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../yolo-data-augmentation/">
<span class="md-ellipsis">
    YOLO Data Augmentation
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../model-deployment-options/">
<span class="md-ellipsis">
    Model Deployment Options
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../model-yaml-config/">
<span class="md-ellipsis">
    Model YAML Configuration Guide
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../kfold-cross-validation/">
<span class="md-ellipsis">
    K-Fold Cross Validation
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../hyperparameter-tuning/">
<span class="md-ellipsis">
    Hyperparameter Tuning
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../sahi-tiled-inference/">
<span class="md-ellipsis">
    SAHI Tiled Inference
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../azureml-quickstart/">
<span class="md-ellipsis">
    AzureML Quickstart
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../conda-quickstart/">
<span class="md-ellipsis">
    Conda Quickstart
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../docker-quickstart/">
<span class="md-ellipsis">
    Docker Quickstart
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../raspberry-pi/">
<span class="md-ellipsis">
    Raspberry Pi
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../nvidia-jetson/">
<span class="md-ellipsis">
    NVIDIA Jetson
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../deepstream-nvidia-jetson/">
<span class="md-ellipsis">
    DeepStream on NVIDIA Jetson
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    Triton Inference Server
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
<span class="md-ellipsis">
    Triton Inference Server
  </span>
</a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#what-is-triton-inference-server">
<span class="md-ellipsis">
      What is Triton Inference Server?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#key-benefits-of-triton-inference-server">
<span class="md-ellipsis">
      Key Benefits of Triton Inference Server
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#prerequisites">
<span class="md-ellipsis">
      Prerequisites
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#exporting-yolo11-to-onnx-format">
<span class="md-ellipsis">
      Exporting YOLO11 to ONNX Format
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#setting-up-triton-model-repository">
<span class="md-ellipsis">
      Setting Up Triton Model Repository
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#running-triton-inference-server">
<span class="md-ellipsis">
      Running Triton Inference Server
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#tensorrt-optimization-optional">
<span class="md-ellipsis">
      TensorRT Optimization (Optional)
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#faq">
<span class="md-ellipsis">
      FAQ
    </span>
</a>
<nav aria-label="FAQ" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#how-do-i-set-up-ultralytics-yolo11-with-nvidia-triton-inference-server">
<span class="md-ellipsis">
      How do I set up Ultralytics YOLO11 with NVIDIA Triton Inference Server?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#what-benefits-does-using-ultralytics-yolo11-with-nvidia-triton-inference-server-offer">
<span class="md-ellipsis">
      What benefits does using Ultralytics YOLO11 with NVIDIA Triton Inference Server offer?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#why-should-i-export-my-yolo11-model-to-onnx-format-before-using-triton-inference-server">
<span class="md-ellipsis">
      Why should I export my YOLO11 model to ONNX format before using Triton Inference Server?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#can-i-run-inference-using-the-ultralytics-yolo11-model-on-triton-inference-server">
<span class="md-ellipsis">
      Can I run inference using the Ultralytics YOLO11 model on Triton Inference Server?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#how-does-ultralytics-yolo11-compare-to-tensorflow-and-pytorch-models-for-deployment">
<span class="md-ellipsis">
      How does Ultralytics YOLO11 compare to TensorFlow and PyTorch models for deployment?
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../isolating-segmentation-objects/">
<span class="md-ellipsis">
    Isolating Segmentation Objects
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../coral-edge-tpu-on-raspberry-pi/">
<span class="md-ellipsis">
    Edge TPU on Raspberry Pi
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../view-results-in-terminal/">
<span class="md-ellipsis">
    Viewing Inference Images in a Terminal
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../optimizing-openvino-latency-vs-throughput-modes/">
<span class="md-ellipsis">
    OpenVINO Latency vs Throughput modes
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../ros-quickstart/">
<span class="md-ellipsis">
    ROS Quickstart
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../steps-of-a-cv-project/">
<span class="md-ellipsis">
    Steps of a Computer Vision Project
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../defining-project-goals/">
<span class="md-ellipsis">
    Defining A Computer Vision Project's Goals
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../data-collection-and-annotation/">
<span class="md-ellipsis">
    Data Collection and Annotation
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../preprocessing_annotated_data/">
<span class="md-ellipsis">
    Preprocessing Annotated Data
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../model-training-tips/">
<span class="md-ellipsis">
    Tips for Model Training
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../model-evaluation-insights/">
<span class="md-ellipsis">
    Insights on Model Evaluation and Fine-Tuning
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../model-testing/">
<span class="md-ellipsis">
    A Guide on Model Testing
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../model-deployment-practices/">
<span class="md-ellipsis">
    Best Practices for Model Deployment
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../model-monitoring-and-maintenance/">
<span class="md-ellipsis">
    Maintaining Your Computer Vision Model
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../vertex-ai-deployment-with-docker/">
<span class="md-ellipsis">
    Deploying YOLO on Vertex AI in Docker container
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_8_33" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../datasets/explorer/">
<span class="md-ellipsis">
    Explorer
  </span>
</a>
<label class="md-nav__link" for="__nav_8_33" id="__nav_8_33_label" tabindex="">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_8_33_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_8_33">
<span class="md-nav__icon md-icon"></span>
            Explorer
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../datasets/explorer/api/">
<span class="md-ellipsis">
    Explorer API
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../datasets/explorer/dashboard/">
<span class="md-ellipsis">
    Explorer Dashboard Demo
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../datasets/explorer/explorer/">
<span class="md-ellipsis">
    VOC Exploration Example
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_8_34" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../yolov5/">
<span class="md-ellipsis">
    YOLOv5
  </span>
</a>
<label class="md-nav__link" for="__nav_8_34" id="__nav_8_34_label" tabindex="">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_8_34_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_8_34">
<span class="md-nav__icon md-icon"></span>
            YOLOv5
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../yolov5/quickstart_tutorial/">
<span class="md-ellipsis">
    Quickstart
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../yolov5/environments/aws_quickstart_tutorial/">
<span class="md-ellipsis">
    Environments
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../yolov5/tutorials/train_custom_data/">
<span class="md-ellipsis">
    Tutorials
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../integrations/">
<span class="md-ellipsis">
    Integrations
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../hub/">
<span class="md-ellipsis">
    HUB
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../reference/__init__/">
<span class="md-ellipsis">
    Reference
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../help/">
<span class="md-ellipsis">
    Help
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#what-is-triton-inference-server">
<span class="md-ellipsis">
      What is Triton Inference Server?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#key-benefits-of-triton-inference-server">
<span class="md-ellipsis">
      Key Benefits of Triton Inference Server
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#prerequisites">
<span class="md-ellipsis">
      Prerequisites
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#exporting-yolo11-to-onnx-format">
<span class="md-ellipsis">
      Exporting YOLO11 to ONNX Format
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#setting-up-triton-model-repository">
<span class="md-ellipsis">
      Setting Up Triton Model Repository
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#running-triton-inference-server">
<span class="md-ellipsis">
      Running Triton Inference Server
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#tensorrt-optimization-optional">
<span class="md-ellipsis">
      TensorRT Optimization (Optional)
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#faq">
<span class="md-ellipsis">
      FAQ
    </span>
</a>
<nav aria-label="FAQ" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#how-do-i-set-up-ultralytics-yolo11-with-nvidia-triton-inference-server">
<span class="md-ellipsis">
      How do I set up Ultralytics YOLO11 with NVIDIA Triton Inference Server?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#what-benefits-does-using-ultralytics-yolo11-with-nvidia-triton-inference-server-offer">
<span class="md-ellipsis">
      What benefits does using Ultralytics YOLO11 with NVIDIA Triton Inference Server offer?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#why-should-i-export-my-yolo11-model-to-onnx-format-before-using-triton-inference-server">
<span class="md-ellipsis">
      Why should I export my YOLO11 model to ONNX format before using Triton Inference Server?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#can-i-run-inference-using-the-ultralytics-yolo11-model-on-triton-inference-server">
<span class="md-ellipsis">
      Can I run inference using the Ultralytics YOLO11 model on Triton Inference Server?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#how-does-ultralytics-yolo11-compare-to-tensorflow-and-pytorch-models-for-deployment">
<span class="md-ellipsis">
      How does Ultralytics YOLO11 compare to TensorFlow and PyTorch models for deployment?
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<a class="md-content__button md-icon" href="https://github.com/ultralytics/ultralytics/tree/main/docs/en/guides/triton-inference-server.md" rel="edit" title="Edit this page">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
</a><a class="md-content__button md-icon" href="javascript:void(0)" onclick="copyMarkdownForLLM(this); return false;" title="Copy page in Markdown format"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg></a>
<h1 id="triton-inference-server-with-ultralytics-yolo11">Triton Inference Server with Ultralytics YOLO11</h1>
<p>The <a href="https://developer.nvidia.com/dynamo">Triton Inference Server</a> (formerly known as TensorRT Inference Server) is an open-source software solution developed by NVIDIA. It provides a cloud inference solution optimized for NVIDIA GPUs. Triton simplifies the deployment of AI models at scale in production. Integrating Ultralytics YOLO11 with Triton Inference Server allows you to deploy scalable, high-performance <a href="https://www.ultralytics.com/glossary/deep-learning-dl">deep learning</a> inference workloads. This guide provides steps to set up and test the integration.</p>
<p align="center">
<br/>
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="405" loading="lazy" src="https://www.youtube.com/embed/NQDtfSi5QF4" title="Getting Started with NVIDIA Triton Inference Server" width="720">
</iframe>
<br/>
<strong>Watch:</strong> Getting Started with NVIDIA Triton Inference Server.
</p>
<h2 id="what-is-triton-inference-server">What is Triton Inference Server?</h2>
<p>Triton Inference Server is designed to deploy a variety of AI models in production. It supports a wide range of deep learning and <a href="https://www.ultralytics.com/glossary/machine-learning-ml">machine learning</a> frameworks, including TensorFlow, <a href="https://www.ultralytics.com/glossary/pytorch">PyTorch</a>, ONNX Runtime, and many others. Its primary use cases are:</p>
<ul>
<li>Serving multiple models from a single server instance</li>
<li>Dynamic model loading and unloading without server restart</li>
<li>Ensemble inference, allowing multiple models to be used together to achieve results</li>
<li>Model versioning for A/B testing and rolling updates</li>
</ul>
<h2 id="key-benefits-of-triton-inference-server">Key Benefits of Triton Inference Server</h2>
<p>Using Triton Inference Server with Ultralytics YOLO11 provides several advantages:</p>
<ul>
<li><strong>Automatic batching</strong>: Groups multiple AI requests together before processing them, reducing latency and improving inference speed</li>
<li><strong>Kubernetes integration</strong>: Cloud-native design works seamlessly with Kubernetes for managing and scaling AI applications</li>
<li><strong>Hardware-specific optimizations</strong>: Takes full advantage of NVIDIA GPUs for maximum performance</li>
<li><strong>Framework flexibility</strong>: Supports multiple AI frameworks including TensorFlow, PyTorch, ONNX, and TensorRT</li>
<li><strong>Open-source and customizable</strong>: Can be modified to fit specific needs, ensuring flexibility for various AI applications</li>
</ul>
<h2 id="prerequisites">Prerequisites</h2>
<p>Ensure you have the following prerequisites before proceeding:</p>
<ul>
<li>Docker installed on your machine</li>
<li>Install <code>tritonclient</code>:
    <div class="highlight"><pre><span></span><code><span></span>pip<span class="w"> </span>install<span class="w"> </span>tritonclient<span class="o">[</span>all<span class="o">]</span>
</code></pre></div></li>
</ul>
<h2 id="exporting-yolo11-to-onnx-format">Exporting YOLO11 to ONNX Format</h2>
<p>Before deploying the model on Triton, it must be exported to the ONNX format. ONNX (Open Neural Network Exchange) is a format that allows models to be transferred between different deep learning frameworks. Use the <code>export</code> function from the <code>YOLO</code> class:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="c1"># Load a model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo11n.pt"</span><span class="p">)</span>  <span class="c1"># load an official model</span>
<span></span>
<span></span><span class="c1"># Retrieve metadata during export. Metadata needs to be added to config.pbtxt. See next section.</span>
<span></span><span class="n">metadata</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>
<span></span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">export_cb</span><span class="p">(</span><span class="n">exporter</span><span class="p">):</span>
<span></span>    <span class="n">metadata</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">exporter</span><span class="o">.</span><span class="n">metadata</span><span class="p">)</span>
<span></span>
<span></span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">add_callback</span><span class="p">(</span><span class="s2">"on_export_end"</span><span class="p">,</span> <span class="n">export_cb</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Export the model</span>
<span></span><span class="n">onnx_file</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">"onnx"</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<h2 id="setting-up-triton-model-repository">Setting Up Triton Model Repository</h2>
<p>The Triton Model Repository is a storage location where Triton can access and load models.</p>
<ol>
<li>
<p>Create the necessary directory structure:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span></span>
<span></span><span class="c1"># Define paths</span>
<span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">"yolo"</span>
<span></span><span class="n">triton_repo_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"tmp"</span><span class="p">)</span> <span class="o">/</span> <span class="s2">"triton_repo"</span>
<span></span><span class="n">triton_model_path</span> <span class="o">=</span> <span class="n">triton_repo_path</span> <span class="o">/</span> <span class="n">model_name</span>
<span></span>
<span></span><span class="c1"># Create directories</span>
<span></span><span class="p">(</span><span class="n">triton_model_path</span> <span class="o">/</span> <span class="s2">"1"</span><span class="p">)</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
</li>
<li>
<p>Move the exported ONNX model to the Triton repository:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span></span>
<span></span><span class="c1"># Move ONNX model to Triton Model path</span>
<span></span><span class="n">Path</span><span class="p">(</span><span class="n">onnx_file</span><span class="p">)</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">triton_model_path</span> <span class="o">/</span> <span class="s2">"1"</span> <span class="o">/</span> <span class="s2">"model.onnx"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Create config file</span>
<span></span><span class="p">(</span><span class="n">triton_model_path</span> <span class="o">/</span> <span class="s2">"config.pbtxt"</span><span class="p">)</span><span class="o">.</span><span class="n">touch</span><span class="p">()</span>
<span></span>
<span></span><span class="n">data</span> <span class="o">=</span> <span class="s2">"""</span>
<span></span><span class="s2"># Add metadata</span>
<span></span><span class="s2">parameters {</span>
<span></span><span class="s2">  key: "metadata"</span>
<span></span><span class="s2">  value {</span>
<span></span><span class="s2">    string_value: "</span><span class="si">%s</span><span class="s2">"</span>
<span></span><span class="s2">  }</span>
<span></span><span class="s2">}</span>
<span></span>
<span></span><span class="s2"># (Optional) Enable TensorRT for GPU inference</span>
<span></span><span class="s2"># First run will be slow due to TensorRT engine conversion</span>
<span></span><span class="s2">optimization {</span>
<span></span><span class="s2">  execution_accelerators {</span>
<span></span><span class="s2">    gpu_execution_accelerator {</span>
<span></span><span class="s2">      name: "tensorrt"</span>
<span></span><span class="s2">      parameters {</span>
<span></span><span class="s2">        key: "precision_mode"</span>
<span></span><span class="s2">        value: "FP16"</span>
<span></span><span class="s2">      }</span>
<span></span><span class="s2">      parameters {</span>
<span></span><span class="s2">        key: "max_workspace_size_bytes"</span>
<span></span><span class="s2">        value: "3221225472"</span>
<span></span><span class="s2">      }</span>
<span></span><span class="s2">      parameters {</span>
<span></span><span class="s2">        key: "trt_engine_cache_enable"</span>
<span></span><span class="s2">        value: "1"</span>
<span></span><span class="s2">      }</span>
<span></span><span class="s2">      parameters {</span>
<span></span><span class="s2">        key: "trt_engine_cache_path"</span>
<span></span><span class="s2">        value: "/models/yolo/1"</span>
<span></span><span class="s2">      }</span>
<span></span><span class="s2">    }</span>
<span></span><span class="s2">  }</span>
<span></span><span class="s2">}</span>
<span></span><span class="s2">"""</span> <span class="o">%</span> <span class="n">metadata</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># noqa</span>
<span></span>
<span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">triton_model_path</span> <span class="o">/</span> <span class="s2">"config.pbtxt"</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span></span>    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div>
</li>
</ol>
<h2 id="running-triton-inference-server">Running Triton Inference Server</h2>
<p>Run the Triton Inference Server using Docker:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">contextlib</span>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">subprocess</span>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span></span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tritonclient.http</span><span class="w"> </span><span class="kn">import</span> <span class="n">InferenceServerClient</span>
<span></span>
<span></span><span class="c1"># Define image https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver</span>
<span></span><span class="n">tag</span> <span class="o">=</span> <span class="s2">"nvcr.io/nvidia/tritonserver:24.09-py3"</span>  <span class="c1"># 8.57 GB</span>
<span></span>
<span></span><span class="c1"># Pull the image</span>
<span></span><span class="n">subprocess</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="sa">f</span><span class="s2">"docker pull </span><span class="si">{</span><span class="n">tag</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span> <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Run the Triton server and capture the container ID</span>
<span></span><span class="n">container_id</span> <span class="o">=</span> <span class="p">(</span>
<span></span>    <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">(</span>
<span></span>        <span class="sa">f</span><span class="s2">"docker run -d --rm --runtime=nvidia --gpus 0 -v </span><span class="si">{</span><span class="n">triton_repo_path</span><span class="si">}</span><span class="s2">:/models -p 8000:8000 </span><span class="si">{</span><span class="n">tag</span><span class="si">}</span><span class="s2"> tritonserver --model-repository=/models"</span><span class="p">,</span>
<span></span>        <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">"utf-8"</span><span class="p">)</span>
<span></span>    <span class="o">.</span><span class="n">strip</span><span class="p">()</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Wait for the Triton server to start</span>
<span></span><span class="n">triton_client</span> <span class="o">=</span> <span class="n">InferenceServerClient</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">"localhost:8000"</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ssl</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Wait until model is ready</span>
<span></span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
<span></span>    <span class="k">with</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">suppress</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
<span></span>        <span class="k">assert</span> <span class="n">triton_client</span><span class="o">.</span><span class="n">is_model_ready</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span></span>        <span class="k">break</span>
<span></span>    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<p>Then run inference using the Triton Server model:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="c1"># Load the Triton Server model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"http://localhost:8000/yolo"</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">"detect"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Run inference on the server</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">)</span>
</code></pre></div>
<p>Cleanup the container:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="c1"># Kill and remove the container at the end of the test</span>
<span></span><span class="n">subprocess</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="sa">f</span><span class="s2">"docker kill </span><span class="si">{</span><span class="n">container_id</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span> <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<h2 id="tensorrt-optimization-optional">TensorRT Optimization (Optional)</h2>
<p>For even greater performance, you can use <a href="https://docs.ultralytics.com/integrations/tensorrt/">TensorRT</a> with Triton Inference Server. TensorRT is a high-performance deep learning optimizer built specifically for NVIDIA GPUs that can significantly increase inference speed.</p>
<p>Key benefits of using TensorRT with Triton include:</p>
<ul>
<li>Up to 36x faster inference compared to unoptimized models</li>
<li>Hardware-specific optimizations for maximum GPU utilization</li>
<li>Support for reduced precision formats (INT8, FP16) while maintaining accuracy</li>
<li>Layer fusion to reduce computational overhead</li>
</ul>
<p>To use TensorRT directly, you can export your YOLO11 model to TensorRT format:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="c1"># Load the YOLO11 model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo11n.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Export the model to TensorRT format</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">"engine"</span><span class="p">)</span>  <span class="c1"># creates 'yolo11n.engine'</span>
</code></pre></div>
<p>For more information on TensorRT optimization, see the <a href="https://docs.ultralytics.com/integrations/tensorrt/">TensorRT integration guide</a>.</p>
<hr/>
<p>By following the above steps, you can deploy and run Ultralytics YOLO11 models efficiently on Triton Inference Server, providing a scalable and high-performance solution for deep learning inference tasks. If you face any issues or have further queries, refer to the <a href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html">official Triton documentation</a> or reach out to the Ultralytics community for support.</p>
<h2 id="faq">FAQ</h2>
<h3 id="how-do-i-set-up-ultralytics-yolo11-with-nvidia-triton-inference-server">How do I set up Ultralytics YOLO11 with NVIDIA Triton Inference Server?</h3>
<p>Setting up <a href="../../models/yolo11/">Ultralytics YOLO11</a> with <a href="https://developer.nvidia.com/dynamo">NVIDIA Triton Inference Server</a> involves a few key steps:</p>
<ol>
<li>
<p><strong>Export YOLO11 to ONNX format</strong>:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="c1"># Load a model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo11n.pt"</span><span class="p">)</span>  <span class="c1"># load an official model</span>
<span></span>
<span></span><span class="c1"># Export the model to ONNX format</span>
<span></span><span class="n">onnx_file</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">"onnx"</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
</li>
<li>
<p><strong>Set up Triton Model Repository</strong>:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span></span>
<span></span><span class="c1"># Define paths</span>
<span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">"yolo"</span>
<span></span><span class="n">triton_repo_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"tmp"</span><span class="p">)</span> <span class="o">/</span> <span class="s2">"triton_repo"</span>
<span></span><span class="n">triton_model_path</span> <span class="o">=</span> <span class="n">triton_repo_path</span> <span class="o">/</span> <span class="n">model_name</span>
<span></span>
<span></span><span class="c1"># Create directories</span>
<span></span><span class="p">(</span><span class="n">triton_model_path</span> <span class="o">/</span> <span class="s2">"1"</span><span class="p">)</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span></span><span class="n">Path</span><span class="p">(</span><span class="n">onnx_file</span><span class="p">)</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">triton_model_path</span> <span class="o">/</span> <span class="s2">"1"</span> <span class="o">/</span> <span class="s2">"model.onnx"</span><span class="p">)</span>
<span></span><span class="p">(</span><span class="n">triton_model_path</span> <span class="o">/</span> <span class="s2">"config.pbtxt"</span><span class="p">)</span><span class="o">.</span><span class="n">touch</span><span class="p">()</span>
</code></pre></div>
</li>
<li>
<p><strong>Run the Triton Server</strong>:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">contextlib</span>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">subprocess</span>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span></span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tritonclient.http</span><span class="w"> </span><span class="kn">import</span> <span class="n">InferenceServerClient</span>
<span></span>
<span></span><span class="c1"># Define image https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver</span>
<span></span><span class="n">tag</span> <span class="o">=</span> <span class="s2">"nvcr.io/nvidia/tritonserver:24.09-py3"</span>
<span></span>
<span></span><span class="n">subprocess</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="sa">f</span><span class="s2">"docker pull </span><span class="si">{</span><span class="n">tag</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span> <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span></span>
<span></span><span class="n">container_id</span> <span class="o">=</span> <span class="p">(</span>
<span></span>    <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">(</span>
<span></span>        <span class="sa">f</span><span class="s2">"docker run -d --rm --runtime=nvidia --gpus 0 -v </span><span class="si">{</span><span class="n">triton_repo_path</span><span class="si">}</span><span class="s2">:/models -p 8000:8000 </span><span class="si">{</span><span class="n">tag</span><span class="si">}</span><span class="s2"> tritonserver --model-repository=/models"</span><span class="p">,</span>
<span></span>        <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">"utf-8"</span><span class="p">)</span>
<span></span>    <span class="o">.</span><span class="n">strip</span><span class="p">()</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="n">triton_client</span> <span class="o">=</span> <span class="n">InferenceServerClient</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">"localhost:8000"</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ssl</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span></span>
<span></span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
<span></span>    <span class="k">with</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">suppress</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
<span></span>        <span class="k">assert</span> <span class="n">triton_client</span><span class="o">.</span><span class="n">is_model_ready</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span></span>        <span class="k">break</span>
<span></span>    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
</li>
</ol>
<p>This setup can help you efficiently deploy YOLO11 models at scale on Triton Inference Server for high-performance AI model inference.</p>
<h3 id="what-benefits-does-using-ultralytics-yolo11-with-nvidia-triton-inference-server-offer">What benefits does using Ultralytics YOLO11 with NVIDIA Triton Inference Server offer?</h3>
<p>Integrating Ultralytics YOLO11 with <a href="https://developer.nvidia.com/dynamo">NVIDIA Triton Inference Server</a> provides several advantages:</p>
<ul>
<li><strong>Scalable AI Inference</strong>: Triton allows serving multiple models from a single server instance, supporting dynamic model loading and unloading, making it highly scalable for diverse AI workloads.</li>
<li><strong>High Performance</strong>: Optimized for NVIDIA GPUs, Triton Inference Server ensures high-speed inference operations, perfect for real-time applications such as <a href="https://www.ultralytics.com/glossary/object-detection">object detection</a>.</li>
<li><strong>Ensemble and Model Versioning</strong>: Triton's ensemble mode enables combining multiple models to improve results, and its model versioning supports A/B testing and rolling updates.</li>
<li><strong>Automatic Batching</strong>: Triton automatically groups multiple inference requests together, significantly improving throughput and reducing latency.</li>
<li><strong>Simplified Deployment</strong>: Gradual optimization of AI workflows without requiring complete system overhauls, making it easier to scale efficiently.</li>
</ul>
<p>For detailed instructions on setting up and running YOLO11 with Triton, you can refer to the <a href="#setting-up-triton-model-repository">setup guide</a>.</p>
<h3 id="why-should-i-export-my-yolo11-model-to-onnx-format-before-using-triton-inference-server">Why should I export my YOLO11 model to ONNX format before using Triton Inference Server?</h3>
<p>Using ONNX (Open Neural Network Exchange) format for your Ultralytics YOLO11 model before deploying it on <a href="https://developer.nvidia.com/dynamo">NVIDIA Triton Inference Server</a> offers several key benefits:</p>
<ul>
<li><strong>Interoperability</strong>: ONNX format supports transfer between different deep learning frameworks (such as PyTorch, TensorFlow), ensuring broader compatibility.</li>
<li><strong>Optimization</strong>: Many deployment environments, including Triton, optimize for ONNX, enabling faster inference and better performance.</li>
<li><strong>Ease of Deployment</strong>: ONNX is widely supported across frameworks and platforms, simplifying the deployment process in various operating systems and hardware configurations.</li>
<li><strong>Framework Independence</strong>: Once converted to ONNX, your model is no longer tied to its original framework, making it more portable.</li>
<li><strong>Standardization</strong>: ONNX provides a standardized representation that helps overcome compatibility issues between different AI frameworks.</li>
</ul>
<p>To export your model, use:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo11n.pt"</span><span class="p">)</span>
<span></span><span class="n">onnx_file</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">"onnx"</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<p>You can follow the steps in the <a href="https://docs.ultralytics.com/integrations/onnx/">ONNX integration guide</a> to complete the process.</p>
<h3 id="can-i-run-inference-using-the-ultralytics-yolo11-model-on-triton-inference-server">Can I run inference using the Ultralytics YOLO11 model on Triton Inference Server?</h3>
<p>Yes, you can run inference using the Ultralytics YOLO11 model on <a href="https://developer.nvidia.com/dynamo">NVIDIA Triton Inference Server</a>. Once your model is set up in the Triton Model Repository and the server is running, you can load and run inference on your model as follows:</p>
<div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="c1"># Load the Triton Server model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"http://localhost:8000/yolo"</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">"detect"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Run inference on the server</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">)</span>
</code></pre></div>
<p>This approach allows you to leverage Triton's optimizations while using the familiar Ultralytics YOLO interface. For an in-depth guide on setting up and running Triton Server with YOLO11, refer to the <a href="#running-triton-inference-server">running triton inference server</a> section.</p>
<h3 id="how-does-ultralytics-yolo11-compare-to-tensorflow-and-pytorch-models-for-deployment">How does Ultralytics YOLO11 compare to TensorFlow and PyTorch models for deployment?</h3>
<p><a href="../../models/yolo11/">Ultralytics YOLO11</a> offers several unique advantages compared to <a href="https://www.ultralytics.com/glossary/tensorflow">TensorFlow</a> and PyTorch models for deployment:</p>
<ul>
<li><strong>Real-time Performance</strong>: Optimized for real-time object detection tasks, YOLO11 provides state-of-the-art <a href="https://www.ultralytics.com/glossary/accuracy">accuracy</a> and speed, making it ideal for applications requiring live video analytics.</li>
<li><strong>Ease of Use</strong>: YOLO11 integrates seamlessly with Triton Inference Server and supports diverse export formats (ONNX, TensorRT, CoreML), making it flexible for various deployment scenarios.</li>
<li><strong>Advanced Features</strong>: YOLO11 includes features like dynamic model loading, model versioning, and ensemble inference, which are crucial for scalable and reliable AI deployments.</li>
<li><strong>Simplified API</strong>: The Ultralytics API provides a consistent interface across different deployment targets, reducing the learning curve and development time.</li>
<li><strong>Edge Optimization</strong>: YOLO11 models are designed with edge deployment in mind, offering excellent performance even on resource-constrained devices.</li>
</ul>
<p>For more details, compare the deployment options in the <a href="../../modes/export/">model export guide</a>.</p>
<br/><br/>
<div class="git-info">
<div class="dates-container">
<span class="date-item" title="This page was first created on November 12, 2023">
<span class="hover-item">📅</span> Created 1 year ago
    </span>
<span class="date-item" title="This page was last updated on September 12, 2025">
<span class="hover-item">✏️</span> Updated 28 days ago
    </span>
</div>
<div class="authors-container">
<a class="author-link" href="https://github.com/glenn-jocher" title="glenn-jocher (15 changes)">
<img alt="glenn-jocher" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/26833433?v=4&amp;s=96"/>
</a>
<a class="author-link" href="https://github.com/Y-T-G" title="Y-T-G (4 changes)">
<img alt="Y-T-G" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/32206511?v=4&amp;s=96"/>
</a>
<a class="author-link" href="https://github.com/UltralyticsAssistant" title="UltralyticsAssistant (2 changes)">
<img alt="UltralyticsAssistant" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/135830346?v=4&amp;s=96"/>
</a>
<a class="author-link" href="https://github.com/onuralpszr" title="onuralpszr (1 change)">
<img alt="onuralpszr" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/1688848?v=4&amp;s=96"/>
</a>
<a class="author-link" href="https://github.com/leonnil" title="leonnil (1 change)">
<img alt="leonnil" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/146309319?v=4&amp;s=96"/>
</a>
</div></div><div class="share-buttons">
<button class="share-button hover-item" onclick="window.open('https://twitter.com/intent/tweet?url=https://docs.ultralytics.com/guides/triton-inference-server', 'TwitterShare', 'width=550,height=680,menubar=no,toolbar=no'); return false;">
<i class="fa-brands fa-x-twitter"></i> Tweet
    </button>
<button class="share-button hover-item linkedin" onclick="window.open('https://www.linkedin.com/shareArticle?url=https://docs.ultralytics.com/guides/triton-inference-server', 'LinkedinShare', 'width=550,height=730,menubar=no,toolbar=no'); return false;">
<i class="fa-brands fa-linkedin-in"></i> Share
    </button>
</div>
<br/>
<h2 id="__comments">Comments</h2>
<div id="giscus-container"></div>
</article>
</div>
<script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<button class="md-top md-icon" data-md-component="top" hidden="" type="button">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
</main>
<footer class="md-footer">
<nav aria-label="Footer" class="md-footer__inner md-grid">
<a aria-label="Previous: DeepStream on NVIDIA Jetson" class="md-footer__link md-footer__link--prev" href="../deepstream-nvidia-jetson/">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
</div>
<div class="md-footer__title">
<span class="md-footer__direction">
                Previous
              </span>
<div class="md-ellipsis">
                DeepStream on NVIDIA Jetson
              </div>
</div>
</a>
<a aria-label="Next: Isolating Segmentation Objects" class="md-footer__link md-footer__link--next" href="../isolating-segmentation-objects/">
<div class="md-footer__title">
<span class="md-footer__direction">
                Next
              </span>
<div class="md-ellipsis">
                Isolating Segmentation Objects
              </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
<div class="md-copyright__highlight">
<a href="https://www.ultralytics.com/" target="_blank">© 2025 Ultralytics Inc.</a> All rights reserved.
    </div>
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
<div class="md-social">
<a class="md-social__link" href="https://github.com/ultralytics" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
</a>
<a class="md-social__link" href="https://www.linkedin.com/company/ultralytics/" rel="noopener" target="_blank" title="www.linkedin.com">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"></path></svg>
</a>
<a class="md-social__link" href="https://x.com/ultralytics" rel="noopener" target="_blank" title="x.com">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z"></path></svg>
</a>
<a class="md-social__link" href="https://youtube.com/ultralytics?sub_confirmation=1" rel="noopener" target="_blank" title="youtube.com">
<svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M549.7 124.1c-6.2-23.7-24.8-42.3-48.3-48.6C458.9 64 288.1 64 288.1 64S117.3 64 74.7 75.5c-23.5 6.3-42 24.9-48.3 48.6C15 167 15 256.4 15 256.4s0 89.4 11.4 132.3c6.3 23.6 24.8 41.5 48.3 47.8C117.3 448 288.1 448 288.1 448s170.8 0 213.4-11.5c23.5-6.3 42-24.2 48.3-47.8 11.4-42.9 11.4-132.3 11.4-132.3s0-89.4-11.4-132.3zM232.2 337.6V175.2l142.7 81.2z"></path></svg>
</a>
<a class="md-social__link" href="https://hub.docker.com/r/ultralytics/ultralytics/" rel="noopener" target="_blank" title="hub.docker.com">
<svg viewbox="0 0 640 512" xmlns="http://www.w3.org/2000/svg"><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"></path></svg>
</a>
<a class="md-social__link" href="https://pypi.org/project/ultralytics/" rel="noopener" target="_blank" title="pypi.org">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 444.7a20.4 20.4 0 1 1 0-40.7 20.4 20.4 0 1 1 0 40.7M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.6-183.4a20.4 20.4 0 1 1 0 40.8 20.4 20.4 0 1 1 0-40.8"></path></svg>
</a>
<a class="md-social__link" href="https://discord.com/invite/ultralytics" rel="noopener" target="_blank" title="discord.com">
<svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M492.5 69.8c-.2-.3-.4-.6-.8-.7-38.1-17.5-78.4-30-119.7-37.1-.4-.1-.8 0-1.1.1s-.6.4-.8.8c-5.5 9.9-10.5 20.2-14.9 30.6-44.6-6.8-89.9-6.8-134.4 0-4.5-10.5-9.5-20.7-15.1-30.6-.2-.3-.5-.6-.8-.8s-.7-.2-1.1-.2C162.5 39 122.2 51.5 84.1 69c-.3.1-.6.4-.8.7C7.1 183.5-13.8 294.6-3.6 404.2c0 .3.1.5.2.8s.3.4.5.6c44.4 32.9 94 58 146.8 74.2.4.1.8.1 1.1 0s.7-.4.9-.7c11.3-15.4 21.4-31.8 30-48.8.1-.2.2-.5.2-.8s0-.5-.1-.8-.2-.5-.4-.6-.4-.3-.7-.4c-15.8-6.1-31.2-13.4-45.9-21.9-.3-.2-.5-.4-.7-.6s-.3-.6-.3-.9 0-.6.2-.9.3-.5.6-.7c3.1-2.3 6.2-4.7 9.1-7.1.3-.2.6-.4.9-.4s.7 0 1 .1c96.2 43.9 200.4 43.9 295.5 0 .3-.1.7-.2 1-.2s.7.2.9.4c2.9 2.4 6 4.9 9.1 7.2.2.2.4.4.6.7s.2.6.2.9-.1.6-.3.9-.4.5-.6.6c-14.7 8.6-30 15.9-45.9 21.8-.2.1-.5.2-.7.4s-.3.4-.4.7-.1.5-.1.8.1.5.2.8c8.8 17 18.8 33.3 30 48.8.2.3.6.6.9.7s.8.1 1.1 0c52.9-16.2 102.6-41.3 147.1-74.2.2-.2.4-.4.5-.6s.2-.5.2-.8c12.3-126.8-20.5-236.9-86.9-334.5zm-302 267.7c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.4 59.2-52.8 59.2m195.4 0c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.2 59.2-52.8 59.2"></path></svg>
</a>
<a class="md-social__link" href="https://reddit.com/r/ultralytics" rel="noopener" target="_blank" title="reddit.com">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M0 256C0 114.6 114.6 0 256 0s256 114.6 256 256-114.6 256-256 256H37.1c-13.7 0-20.5-16.5-10.9-26.2L75 437C28.7 390.7 0 326.7 0 256m349.6-102.4c23.6 0 42.7-19.1 42.7-42.7s-19.1-42.7-42.7-42.7c-20.6 0-37.8 14.6-41.8 34-34.5 3.7-61.4 33-61.4 68.4v.2c-37.5 1.6-71.8 12.3-99 29.1-10.1-7.8-22.8-12.5-36.5-12.5-33 0-59.8 26.8-59.8 59.8 0 24 14.1 44.6 34.4 54.1 2 69.4 77.6 125.2 170.6 125.2s168.7-55.9 170.6-125.3c20.2-9.6 34.1-30.2 34.1-54 0-33-26.8-59.8-59.8-59.8-13.7 0-26.3 4.6-36.4 12.4-27.4-17-62.1-27.7-100-29.1v-.2c0-25.4 18.9-46.5 43.4-49.9 4.4 18.8 21.3 32.8 41.5 32.8zm-172.5 93.3c16.7 0 29.5 17.6 28.5 39.3s-13.5 29.6-30.3 29.6-31.4-8.8-30.4-30.5S160.3 247 177 247zm190.1 38.3c1 21.7-13.7 30.5-30.4 30.5s-29.3-7.9-30.3-29.6 11.8-39.3 28.5-39.3 31.2 16.6 32.1 38.3zm-48.1 56.7c-10.3 24.6-34.6 41.9-63 41.9s-52.7-17.3-63-41.9c-1.2-2.9.8-6.2 3.9-6.5 18.4-1.9 38.3-2.9 59.1-2.9s40.7 1 59.1 2.9c3.1.3 5.1 3.6 3.9 6.5"></path></svg>
</a>
<a class="md-social__link" href="https://weixin.qq.com/r/mp/LxckPDfEgWr_rXNf90I9" rel="noopener" target="_blank" title="weixin.qq.com">
<svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M385.2 167.6c6.4 0 12.6.3 18.8 1.1C387.4 90.3 303.3 32 207.7 32 100.5 32 13 104.8 13 197.4c0 53.4 29.3 97.5 77.9 131.6l-19.3 58.6 68-34.1c24.4 4.8 43.8 9.7 68.2 9.7 6.2 0 12.1-.3 18.3-.8-4-12.9-6.2-26.6-6.2-40.8-.1-84.9 72.9-154 165.3-154m-104.5-52.9c14.5 0 24.2 9.7 24.2 24.4 0 14.5-9.7 24.2-24.2 24.2-14.8 0-29.3-9.7-29.3-24.2.1-14.7 14.6-24.4 29.3-24.4m-136.4 48.6c-14.5 0-29.3-9.7-29.3-24.2 0-14.8 14.8-24.4 29.3-24.4 14.8 0 24.4 9.7 24.4 24.4 0 14.6-9.6 24.2-24.4 24.2M563 319.4c0-77.9-77.9-141.3-165.4-141.3-92.7 0-165.4 63.4-165.4 141.3S305 460.7 397.6 460.7c19.3 0 38.9-5.1 58.6-9.9l53.4 29.3-14.8-48.6C534 402.1 563 363.2 563 319.4m-219.1-24.5c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.8 0 24.4 9.7 24.4 19.3 0 10-9.7 19.6-24.4 19.6m107.1 0c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.5 0 24.4 9.7 24.4 19.3.1 10-9.9 19.6-24.4 19.6"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<div class="md-progress" data-md-component="progress" role="progressbar"></div>
<script id="__config" type="application/json">{"base": "../..", "features": ["content.action.edit", "content.code.annotate", "content.code.copy", "content.tooltips", "toc.follow", "navigation.top", "navigation.tabs", "navigation.tabs.sticky", "navigation.prune", "navigation.footer", "navigation.tracking", "navigation.instant", "navigation.instant.progress", "navigation.indexes", "navigation.sections", "content.tabs.link"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
<script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
<script src="../../javascript/extra.js"></script>
<script src="../../javascript/giscus.js"></script>
<script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
<script src="../../javascript/tablesort.js"></script>
<script>
                    async function copyMarkdownForLLM(button) {
                        const editBtn = document.querySelector('a[title="Edit this page"]');
                        if (!editBtn) return;
                        const originalHTML = button.innerHTML;
                        const checkIcon = '<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 16.17L4.83 12l-1.42 1.41L9 19L21 7l-1.41-1.41L9 16.17z"></path></svg>';
                        // Handle both /blob/ and /tree/ in GitHub URLs
                        let rawUrl = editBtn.href.replace('github.com', 'raw.githubusercontent.com');
                        // Remove /blob/ or /tree/ from the URL
                        rawUrl = rawUrl.replace('/blob/', '/').replace('/tree/', '/');
                        try {
                            const response = await fetch(rawUrl);
                            let markdown = await response.text();
                            // Remove YAML front matter if present
                            if (markdown.startsWith('---')) {
                                const frontMatterEnd = markdown.indexOf('\n---\n', 3);
                                if (frontMatterEnd !== -1) {
                                    markdown = markdown.substring(frontMatterEnd + 5).trim();
                                }
                            }
                            const title = document.querySelector('h1')?.textContent || document.title;
                            const content = `# ${title}\n\nSource: ${window.location.href}\n\n---\n\n${markdown}`;
                            await navigator.clipboard.writeText(content);
                            button.innerHTML = checkIcon + ' Copied!';
                            setTimeout(() => { button.innerHTML = originalHTML; }, 2000);
                        } catch (err) {
                            button.innerHTML = '❌ Failed';
                            setTimeout(() => { button.innerHTML = originalHTML; }, 2000);
                        }
                    }
                    </script></body>
</html>