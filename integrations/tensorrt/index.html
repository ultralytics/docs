 <!DOCTYPE html><html class="no-js" lang="en"><head><meta charset="utf-8"/><meta content="width=device-width,initial-scale=1" name="viewport"/><meta content="Learn to convert YOLO26 models to TensorRT for high-speed NVIDIA GPU inference. Boost efficiency and deploy optimized models with our step-by-step guide." name="description"/><meta content="Ultralytics" name="author"/><link href="https://docs.ultralytics.com/integrations/tensorrt/" rel="canonical"/><link href="../tensorboard/" rel="prev"/><link href="../tf-graphdef/" rel="next"/><link href="/" hreflang="en" rel="alternate"/><link href="/zh/" hreflang="zh" rel="alternate"/><link href="/ko/" hreflang="ko" rel="alternate"/><link href="/ja/" hreflang="ja" rel="alternate"/><link href="/ru/" hreflang="ru" rel="alternate"/><link href="/de/" hreflang="de" rel="alternate"/><link href="/fr/" hreflang="fr" rel="alternate"/><link href="/es/" hreflang="es" rel="alternate"/><link href="/pt/" hreflang="pt" rel="alternate"/><link href="/it/" hreflang="it" rel="alternate"/><link href="/tr/" hreflang="tr" rel="alternate"/><link href="/vi/" hreflang="vi" rel="alternate"/><link href="/ar/" hreflang="ar" rel="alternate"/><link href="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/logo/favicon-yolo.png" rel="icon"/><meta content="zensical-0.0.16" name="generator"/><title>TensorRT Export for YOLO26 Models - Ultralytics YOLO Docs</title><link href="../../assets/stylesheets/modern/main.dfcb43b2.min.css" rel="stylesheet"/><link href="../../assets/stylesheets/modern/palette.dfe2e883.min.css" rel="stylesheet"/><link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/><link href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,500,500i,700,700i%7CJetBrains+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/><style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style><link href="../../stylesheets/style.css" rel="stylesheet"/>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,t)=>(e<<5)-e+t.charCodeAt(0)),0),__md_get=(e,t=localStorage,a=__md_scope)=>JSON.parse(t.getItem(a.pathname+"."+e)),__md_set=(e,t,a=localStorage,_=__md_scope)=>{try{a.setItem(_.pathname+"."+e,JSON.stringify(t))}catch(e){}},document.documentElement.setAttribute("data-platform",navigator.platform)</script>
<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-2M5EHKC0BH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-2M5EHKC0BH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-2M5EHKC0BH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
<style data-doc-kind="true">.doc-kind{display:inline-flex;align-items:center;gap:0.25em;padding:0.21em 0.59em;border-radius:999px;font-weight:700;font-size:0.81em;letter-spacing:0.06em;text-transform:uppercase;line-height:1;color:var(--doc-kind-color,#f8fafc);background:var(--doc-kind-bg,rgba(255,255,255,0.12));}.doc-kind-class{--doc-kind-color:#039dfc;--doc-kind-bg:rgba(3,157,252,0.22);}.doc-kind-function{--doc-kind-color:#fc9803;--doc-kind-bg:rgba(252,152,3,0.22);}.doc-kind-method{--doc-kind-color:#ef5eff;--doc-kind-bg:rgba(239,94,255,0.22);}.doc-kind-property{--doc-kind-color:#02e835;--doc-kind-bg:rgba(2,232,53,0.22);}</style><meta content="TensorRT Export for YOLO26 Models" name="title"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet"/><meta content="website" property="og:type"/><meta content="https://docs.ultralytics.com/integrations/tensorrt/" property="og:url"/><meta content="TensorRT Export for YOLO26 Models" property="og:title"/><meta content="" property="og:description"/><meta content="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" property="og:image"/><meta content="summary_large_image" property="twitter:card"/><meta content="https://docs.ultralytics.com/integrations/tensorrt/" property="twitter:url"/><meta content="TensorRT Export for YOLO26 Models" property="twitter:title"/><meta content="" property="twitter:description"/><meta content="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" property="twitter:image"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": ["Article", "FAQPage"], "headline": "TensorRT Export for YOLO26 Models", "image": ["https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png"], "datePublished": "2024-01-28 04:48:47 +0300", "dateModified": "2026-01-14 03:44:38 +0000", "author": [{"@type": "Organization", "name": "Ultralytics", "url": "https://ultralytics.com/"}], "abstract": "", "mainEntity": [{"@type": "Question", "name": "How do I convert YOLO26 models to TensorRT format?", "acceptedAnswer": {"@type": "Answer", "text": "To convert your Ultralytics YOLO26 models to TensorRT format for optimized NVIDIA GPU inference, follow these steps: For more details, visit the YOLO26 Installation guide and the export documentation."}}, {"@type": "Question", "name": "What are the benefits of using TensorRT for YOLO26 models?", "acceptedAnswer": {"@type": "Answer", "text": "Using TensorRT to optimize YOLO26 models offers several benefits: To learn more, explore the official TensorRT documentation from NVIDIA and our in-depth TensorRT overview."}}, {"@type": "Question", "name": "Can I use INT8 quantization with TensorRT for YOLO26 models?", "acceptedAnswer": {"@type": "Answer", "text": "Yes, you can export YOLO26 models using TensorRT with INT8 quantization. This process involves post-training quantization (PTQ) and calibration: For more details, refer to the exporting TensorRT with INT8 quantization section."}}, {"@type": "Question", "name": "How do I deploy YOLO26 TensorRT models on an NVIDIA Triton Inference Server?", "acceptedAnswer": {"@type": "Answer", "text": "Deploying YOLO26 TensorRT models on an NVIDIA Triton Inference Server can be done using the following resources: These guides will help you integrate YOLO26 models efficiently in various deployment environments."}}, {"@type": "Question", "name": "What are the performance improvements observed with YOLO26 models exported to TensorRT?", "acceptedAnswer": {"@type": "Answer", "text": "Performance improvements with TensorRT can vary based on the hardware used. Here are some typical benchmarks: Detailed performance benchmarks for different hardware configurations can be found in the performance section. For more comprehensive insights into TensorRT performance, refer to the Ultralytics documentation and our performance analysis reports."}}]}</script></head><body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="default" dir="ltr"><input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/><input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/><label class="md-overlay" for="__drawer"></label><div data-md-component="skip"><a class="md-skip" href="#tensorrt-export-for-yolo26-models"> Skip to content </a></div><div data-md-component="announce"><aside class="md-banner"><div class="md-banner__inner md-grid md-typeset"><a class="banner-wrapper" href="https://docs.ultralytics.com/models/yolo26"><div class="banner-content-wrapper"><img alt="Ultralytics YOLO26" height="40" src="https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/6967372a0d67914985d322e7_Ultralytics%20YOLO26%201.svg"/><p>Built End-to-End. Built for the Edge.</p></div></a></div></aside></div><header class="md-header md-header--shadow md-header--lifted" data-md-component="header"><nav aria-label="Header" class="md-header__inner md-grid"><a aria-label="Ultralytics YOLO Docs" class="md-header__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs"><img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/></a><label class="md-header__button md-icon" for="__drawer"><svg class="lucide lucide-menu" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 5h16M4 12h16M4 19h16"></path></svg></label><div class="md-header__title" data-md-component="header-title"><div class="md-header__ellipsis"><div class="md-header__topic"><span class="md-ellipsis"> Ultralytics YOLO Docs </span></div><div class="md-header__topic" data-md-component="header-topic"><span class="md-ellipsis"> TensorRT Export for YOLO26 Models </span></div></div></div><form class="md-header__option" data-md-component="palette"><input aria-label="Switch to light mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_0" name="__palette" type="radio"/><label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg></label><input aria-label="Switch to system preference" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="black" data-md-color-scheme="slate" id="__palette_1" name="__palette" type="radio"/><label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to system preference"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label><input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_2" name="__palette" type="radio"/><label class="md-header__button md-icon" for="__palette_0" hidden="" title="Switch to dark mode"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label></form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<div class="md-header__option"><div class="md-select"><button aria-label="Select language" class="md-header__button md-icon"><svg class="lucide lucide-languages" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m5 8 6 6M4 14l6-6 2-3M2 5h12M7 2h1M22 22l-5-10-5 10M14 18h6"></path></svg></button><div class="md-select__inner"><ul class="md-select__list"><li class="md-select__item"><a class="md-select__link" href="/" hreflang="en"> üá¨üáß English </a></li><li class="md-select__item"><a class="md-select__link" href="/zh/" hreflang="zh"> üá®üá≥ ÁÆÄ‰Ωì‰∏≠Êñá </a></li><li class="md-select__item"><a class="md-select__link" href="/ko/" hreflang="ko"> üá∞üá∑ ÌïúÍµ≠Ïñ¥ </a></li><li class="md-select__item"><a class="md-select__link" href="/ja/" hreflang="ja"> üáØüáµ Êó•Êú¨Ë™û </a></li><li class="md-select__item"><a class="md-select__link" href="/ru/" hreflang="ru"> üá∑üá∫ –†—É—Å—Å–∫–∏–π </a></li><li class="md-select__item"><a class="md-select__link" href="/de/" hreflang="de"> üá©üá™ Deutsch </a></li><li class="md-select__item"><a class="md-select__link" href="/fr/" hreflang="fr"> üá´üá∑ Fran√ßais </a></li><li class="md-select__item"><a class="md-select__link" href="/es/" hreflang="es"> üá™üá∏ Espa√±ol </a></li><li class="md-select__item"><a class="md-select__link" href="/pt/" hreflang="pt"> üáµüáπ Portugu√™s </a></li><li class="md-select__item"><a class="md-select__link" href="/it/" hreflang="it"> üáÆüáπ Italiano </a></li><li class="md-select__item"><a class="md-select__link" href="/tr/" hreflang="tr"> üáπüá∑ T√ºrk√ße </a></li><li class="md-select__item"><a class="md-select__link" href="/vi/" hreflang="vi"> üáªüá≥ Ti·∫øng Vi·ªát </a></li><li class="md-select__item"><a class="md-select__link" href="/ar/" hreflang="ar"> üá∏üá¶ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© </a></li></ul></div></div></div><div class="md-header__source"><a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository"><div class="md-source__icon md-icon"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg></div><div class="md-source__repository"> ultralytics/ultralytics </div></a></div></nav><nav aria-label="Tabs" class="md-tabs" data-md-component="tabs"><div class="md-grid"><ul class="md-tabs__list"><li class="md-tabs__item"><a class="md-tabs__link" href="../.."> Home </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../quickstart/"> Quickstart </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../modes/"> Modes </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../tasks/"> Tasks </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../models/"> Models </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../datasets/"> Datasets </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../solutions/"> Solutions </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../guides/"> Guides </a></li><li class="md-tabs__item md-tabs__item--active"><a class="md-tabs__link" href="../"> Integrations </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../platform/"> Platform </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../reference/__init__/"> Reference </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../help/"> Help </a></li></ul></div></nav></header><div class="md-container" data-md-component="container"><main class="md-main" data-md-component="main"><div class="md-main__inner md-grid"><div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0"><label class="md-nav__title" for="__drawer"><a aria-label="Ultralytics YOLO Docs" class="md-nav__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs"><img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/></a> Ultralytics YOLO Docs </label><div class="md-nav__source"><a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository"><div class="md-source__icon md-icon"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg></div><div class="md-source__repository"> ultralytics/ultralytics </div></a></div><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../.."><span class="md-ellipsis"> Home </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../quickstart/"><span class="md-ellipsis"> Quickstart </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../modes/"><span class="md-ellipsis"> Modes </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../tasks/"><span class="md-ellipsis"> Tasks </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../models/"><span class="md-ellipsis"> Models </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../datasets/"><span class="md-ellipsis"> Datasets </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../solutions/"><span class="md-ellipsis"> Solutions </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../guides/"><span class="md-ellipsis"> Guides </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"><input checked="" class="md-nav__toggle md-toggle" id="__nav_9" type="checkbox"/><div class="md-nav__link md-nav__container"><a class="md-nav__link" href="../"><span class="md-ellipsis"> Integrations </span></a><label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex=""><span class="md-nav__icon md-icon"></span></label></div><nav aria-expanded="true" aria-labelledby="__nav_9_label" class="md-nav" data-md-level="1"><label class="md-nav__title" for="__nav_9"><span class="md-nav__icon md-icon"></span> Integrations </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../albumentations/"><span class="md-ellipsis"> Albumentations </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../amazon-sagemaker/"><span class="md-ellipsis"> Amazon SageMaker </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../axelera/"><span class="md-ellipsis"> Axelera </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../clearml/"><span class="md-ellipsis"> ClearML </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../comet/"><span class="md-ellipsis"> Comet ML </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../coreml/"><span class="md-ellipsis"> CoreML </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../dvc/"><span class="md-ellipsis"> DVC </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../executorch/"><span class="md-ellipsis"> ExecuTorch </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../google-colab/"><span class="md-ellipsis"> Google Colab </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../gradio/"><span class="md-ellipsis"> Gradio </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../ibm-watsonx/"><span class="md-ellipsis"> IBM Watsonx </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../jupyterlab/"><span class="md-ellipsis"> JupyterLab </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../kaggle/"><span class="md-ellipsis"> Kaggle </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../mlflow/"><span class="md-ellipsis"> MLflow </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../mnn/"><span class="md-ellipsis"> MNN </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../ncnn/"><span class="md-ellipsis"> NCNN </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../neptune/"><span class="md-ellipsis"> Neptune </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../neural-magic/"><span class="md-ellipsis"> Neural Magic </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../onnx/"><span class="md-ellipsis"> ONNX </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../openvino/"><span class="md-ellipsis"> OpenVINO </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../paddlepaddle/"><span class="md-ellipsis"> PaddlePaddle </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../paperspace/"><span class="md-ellipsis"> Paperspace Gradient </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../ray-tune/"><span class="md-ellipsis"> Ray Tune </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../roboflow/"><span class="md-ellipsis"> Roboflow </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../rockchip-rknn/"><span class="md-ellipsis"> Rockchip RKNN </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../seeedstudio-recamera/"><span class="md-ellipsis"> Seeed Studio reCamera </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../sony-imx500/"><span class="md-ellipsis"> SONY IMX500 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../tensorboard/"><span class="md-ellipsis"> TensorBoard </span></a></li><li class="md-nav__item md-nav__item--active"><input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/><label class="md-nav__link md-nav__link--active" for="__toc"><span class="md-ellipsis"> TensorRT </span><span class="md-nav__icon md-icon"></span></label><a class="md-nav__link md-nav__link--active" href="./"><span class="md-ellipsis"> TensorRT </span></a><nav aria-label="On this page" class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc"><span class="md-nav__icon md-icon"></span> On this page </label><ul class="md-nav__list" data-md-component="toc" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="#tensorrt"><span class="md-ellipsis"> TensorRT </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#key-features-of-tensorrt-models"><span class="md-ellipsis"> Key Features of TensorRT Models </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#deployment-options-in-tensorrt"><span class="md-ellipsis"> Deployment Options in TensorRT </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#exporting-yolo26-models-to-tensorrt"><span class="md-ellipsis"> Exporting YOLO26 Models to TensorRT </span></a><nav aria-label="Exporting YOLO26 Models to TensorRT" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#installation"><span class="md-ellipsis"> Installation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#usage"><span class="md-ellipsis"> Usage </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#export-arguments"><span class="md-ellipsis"> Export Arguments </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#exporting-tensorrt-with-int8-quantization"><span class="md-ellipsis"> Exporting TensorRT with INT8 Quantization </span></a><nav aria-label="Exporting TensorRT with INT8 Quantization" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#configuring-int8-export"><span class="md-ellipsis"> Configuring INT8 Export </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#advantages-of-using-yolo-with-tensorrt-int8"><span class="md-ellipsis"> Advantages of using YOLO with TensorRT INT8 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#drawbacks-of-using-yolo-with-tensorrt-int8"><span class="md-ellipsis"> Drawbacks of using YOLO with TensorRT INT8 </span></a></li></ul></nav></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics-yolo-tensorrt-export-performance"><span class="md-ellipsis"> Ultralytics YOLO TensorRT Export Performance </span></a><nav aria-label="Ultralytics YOLO TensorRT Export Performance" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#nvidia-a100"><span class="md-ellipsis"> NVIDIA A100 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#consumer-gpus"><span class="md-ellipsis"> Consumer GPUs </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#embedded-devices"><span class="md-ellipsis"> Embedded Devices </span></a><nav aria-label="Embedded Devices" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#evaluation-methods"><span class="md-ellipsis"> Evaluation methods </span></a></li></ul></nav></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#deploying-exported-yolo26-tensorrt-models"><span class="md-ellipsis"> Deploying Exported YOLO26 TensorRT Models </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#summary"><span class="md-ellipsis"> Summary </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#faq"><span class="md-ellipsis"> FAQ </span></a><nav aria-label="FAQ" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#how-do-i-convert-yolo26-models-to-tensorrt-format"><span class="md-ellipsis"> How do I convert YOLO26 models to TensorRT format? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#what-are-the-benefits-of-using-tensorrt-for-yolo26-models"><span class="md-ellipsis"> What are the benefits of using TensorRT for YOLO26 models? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#can-i-use-int8-quantization-with-tensorrt-for-yolo26-models"><span class="md-ellipsis"> Can I use INT8 quantization with TensorRT for YOLO26 models? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#how-do-i-deploy-yolo26-tensorrt-models-on-an-nvidia-triton-inference-server"><span class="md-ellipsis"> How do I deploy YOLO26 TensorRT models on an NVIDIA Triton Inference Server? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#what-are-the-performance-improvements-observed-with-yolo26-models-exported-to-tensorrt"><span class="md-ellipsis"> What are the performance improvements observed with YOLO26 models exported to TensorRT? </span></a></li></ul></nav></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="../tf-graphdef/"><span class="md-ellipsis"> TF GraphDef </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../tf-savedmodel/"><span class="md-ellipsis"> TF SavedModel </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../tfjs/"><span class="md-ellipsis"> TF.js </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../tflite/"><span class="md-ellipsis"> TFLite </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../edge-tpu/"><span class="md-ellipsis"> TFLite Edge TPU </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../torchscript/"><span class="md-ellipsis"> TorchScript </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../vscode/"><span class="md-ellipsis"> VS Code </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../weights-biases/"><span class="md-ellipsis"> Weights &amp; Biases </span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../platform/"><span class="md-ellipsis"> Platform </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../reference/__init__/"><span class="md-ellipsis"> Reference </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../help/"><span class="md-ellipsis"> Help </span><span class="md-nav__icon md-icon"></span></a></li></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav aria-label="On this page" class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc"><span class="md-nav__icon md-icon"></span> On this page </label><ul class="md-nav__list" data-md-component="toc" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="#tensorrt"><span class="md-ellipsis"> TensorRT </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#key-features-of-tensorrt-models"><span class="md-ellipsis"> Key Features of TensorRT Models </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#deployment-options-in-tensorrt"><span class="md-ellipsis"> Deployment Options in TensorRT </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#exporting-yolo26-models-to-tensorrt"><span class="md-ellipsis"> Exporting YOLO26 Models to TensorRT </span></a><nav aria-label="Exporting YOLO26 Models to TensorRT" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#installation"><span class="md-ellipsis"> Installation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#usage"><span class="md-ellipsis"> Usage </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#export-arguments"><span class="md-ellipsis"> Export Arguments </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#exporting-tensorrt-with-int8-quantization"><span class="md-ellipsis"> Exporting TensorRT with INT8 Quantization </span></a><nav aria-label="Exporting TensorRT with INT8 Quantization" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#configuring-int8-export"><span class="md-ellipsis"> Configuring INT8 Export </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#advantages-of-using-yolo-with-tensorrt-int8"><span class="md-ellipsis"> Advantages of using YOLO with TensorRT INT8 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#drawbacks-of-using-yolo-with-tensorrt-int8"><span class="md-ellipsis"> Drawbacks of using YOLO with TensorRT INT8 </span></a></li></ul></nav></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics-yolo-tensorrt-export-performance"><span class="md-ellipsis"> Ultralytics YOLO TensorRT Export Performance </span></a><nav aria-label="Ultralytics YOLO TensorRT Export Performance" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#nvidia-a100"><span class="md-ellipsis"> NVIDIA A100 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#consumer-gpus"><span class="md-ellipsis"> Consumer GPUs </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#embedded-devices"><span class="md-ellipsis"> Embedded Devices </span></a><nav aria-label="Embedded Devices" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#evaluation-methods"><span class="md-ellipsis"> Evaluation methods </span></a></li></ul></nav></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#deploying-exported-yolo26-tensorrt-models"><span class="md-ellipsis"> Deploying Exported YOLO26 TensorRT Models </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#summary"><span class="md-ellipsis"> Summary </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#faq"><span class="md-ellipsis"> FAQ </span></a><nav aria-label="FAQ" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#how-do-i-convert-yolo26-models-to-tensorrt-format"><span class="md-ellipsis"> How do I convert YOLO26 models to TensorRT format? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#what-are-the-benefits-of-using-tensorrt-for-yolo26-models"><span class="md-ellipsis"> What are the benefits of using TensorRT for YOLO26 models? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#can-i-use-int8-quantization-with-tensorrt-for-yolo26-models"><span class="md-ellipsis"> Can I use INT8 quantization with TensorRT for YOLO26 models? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#how-do-i-deploy-yolo26-tensorrt-models-on-an-nvidia-triton-inference-server"><span class="md-ellipsis"> How do I deploy YOLO26 TensorRT models on an NVIDIA Triton Inference Server? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#what-are-the-performance-improvements-observed-with-yolo26-models-exported-to-tensorrt"><span class="md-ellipsis"> What are the performance improvements observed with YOLO26 models exported to TensorRT? </span></a></li></ul></nav></li></ul></nav></div></div></div><div class="md-content" data-md-component="content"><article class="md-content__inner md-typeset"><a class="md-content__button md-icon" href="https://github.com/ultralytics/ultralytics/tree/main/docs/en/integrations/tensorrt.md" rel="edit" title="Edit this page"><svg class="lucide lucide-file-pen" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12.659 22H18a2 2 0 0 0 2-2V8a2.4 2.4 0 0 0-.706-1.706l-3.588-3.588A2.4 2.4 0 0 0 14 2H6a2 2 0 0 0-2 2v9.34"></path><path d="M14 2v5a1 1 0 0 0 1 1h5M10.378 12.622a1 1 0 0 1 3 3.003L8.36 20.637a2 2 0 0 1-.854.506l-2.867.837a.5.5 0 0 1-.62-.62l.836-2.869a2 2 0 0 1 .506-.853z"></path></svg></a><a class="md-content__button md-icon" href="javascript:void(0)" onclick="copyMarkdownForLLM(this); return false;" title="Copy page in Markdown format"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg></a><h1 id="tensorrt-export-for-yolo26-models">TensorRT Export for YOLO26 Models</h1><p>Deploying <a href="https://www.ultralytics.com/glossary/computer-vision-cv">computer vision</a> models in high-performance environments can require a format that maximizes speed and efficiency. This is especially true when you are deploying your model on NVIDIA GPUs.</p><p>By using the TensorRT export format, you can enhance your <a href="https://github.com/ultralytics/ultralytics">Ultralytics YOLO26</a> models for swift and efficient inference on NVIDIA hardware. This guide will give you easy-to-follow steps for the conversion process and help you make the most of NVIDIA's advanced technology in your <a href="https://www.ultralytics.com/glossary/deep-learning-dl">deep learning</a> projects.</p><h2 id="tensorrt">TensorRT</h2><p align="center"><img alt="TensorRT Overview" src="https://github.com/ultralytics/docs/releases/download/0/tensorrt-overview.avif" width="100%"/></p><p><a href="https://developer.nvidia.com/tensorrt">TensorRT</a>, developed by NVIDIA, is an advanced software development kit (SDK) designed for high-speed deep learning inference. It's well-suited for real-time applications like <a href="https://www.ultralytics.com/glossary/object-detection">object detection</a>.</p><p>This toolkit optimizes deep learning models for NVIDIA GPUs and results in faster and more efficient operations. TensorRT models undergo TensorRT optimization, which includes techniques like layer fusion, precision calibration (INT8 and FP16), dynamic tensor memory management, and kernel auto-tuning. Converting deep learning models into the TensorRT format allows developers to realize the potential of NVIDIA GPUs fully.</p><p>TensorRT is known for its compatibility with various model formats, including TensorFlow, <a href="https://www.ultralytics.com/glossary/pytorch">PyTorch</a>, and ONNX, providing developers with a flexible solution for integrating and optimizing models from different frameworks. This versatility enables efficient <a href="https://www.ultralytics.com/glossary/model-deployment">model deployment</a> across diverse hardware and software environments.</p><h2 id="key-features-of-tensorrt-models">Key Features of TensorRT Models</h2><p>TensorRT models offer a range of key features that contribute to their efficiency and effectiveness in high-speed deep learning inference:</p><ul><li><p><strong>Precision Calibration</strong>: TensorRT supports precision calibration, allowing models to be fine-tuned for specific accuracy requirements. This includes support for reduced precision formats like INT8 and FP16, which can further boost inference speed while maintaining acceptable accuracy levels.</p></li><li><p><strong>Layer Fusion</strong>: The TensorRT optimization process includes layer fusion, where multiple layers of a <a href="https://www.ultralytics.com/glossary/neural-network-nn">neural network</a> are combined into a single operation. This reduces computational overhead and improves inference speed by minimizing memory access and computation.</p></li></ul><p align="center"><img alt="TensorRT Layer Fusion" src="https://github.com/ultralytics/docs/releases/download/0/tensorrt-layer-fusion.avif" width="100%"/></p><ul><li><p><strong>Dynamic Tensor Memory Management</strong>: TensorRT efficiently manages tensor memory usage during inference, reducing memory overhead and optimizing memory allocation. This results in more efficient GPU memory utilization.</p></li><li><p><strong>Automatic Kernel Tuning</strong>: TensorRT applies automatic kernel tuning to select the most optimized GPU kernel for each layer of the model. This adaptive approach ensures that the model takes full advantage of the GPU's computational power.</p></li></ul><h2 id="deployment-options-in-tensorrt">Deployment Options in TensorRT</h2><p>Before we look at the code for exporting YOLO26 models to the TensorRT format, let's understand where TensorRT models are normally used.</p><p>TensorRT offers several deployment options, and each option balances ease of integration, performance optimization, and flexibility differently:</p><ul><li><strong>Deploying within <a href="https://www.ultralytics.com/glossary/tensorflow">TensorFlow</a></strong>: This method integrates TensorRT into TensorFlow, allowing optimized models to run in a familiar TensorFlow environment. It's useful for models with a mix of supported and unsupported layers, as TF-TRT can handle these efficiently.</li></ul><p align="center"><img alt="TensorRT Overview" src="https://github.com/ultralytics/docs/releases/download/0/tf-trt-workflow.avif" width="100%"/></p><ul><li><p><strong>Standalone TensorRT Runtime API</strong>: Offers granular control, ideal for performance-critical applications. It's more complex but allows for custom implementation of unsupported operators.</p></li><li><p><strong>NVIDIA Triton Inference Server</strong>: An option that supports models from various frameworks. Particularly suited for cloud or edge inference, it provides features like concurrent model execution and model analysis.</p></li></ul><h2 id="exporting-yolo26-models-to-tensorrt">Exporting YOLO26 Models to TensorRT</h2><p>You can improve execution efficiency and optimize performance by converting YOLO26 models to TensorRT format.</p><h3 id="installation">Installation</h3><p>To install the required package, run:</p><div class="admonition tip"><p class="admonition-title">Installation</p><div class="tabbed-set tabbed-alternate" data-tabs="1:1"><input checked="checked" id="cli" name="__tabbed_1" type="radio"/><div class="tabbed-labels"><label for="cli">CLI</label></div><div class="tabbed-content"><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="c1"># Install the required package for YOLO26</span>
<span></span>pip<span class="w"> </span>install<span class="w"> </span>ultralytics
</code></pre></div></div></div></div></div><p>For detailed instructions and best practices related to the installation process, check our <a href="../../quickstart/">YOLO26 Installation guide</a>. While installing the required packages for YOLO26, if you encounter any difficulties, consult our <a href="../../guides/yolo-common-issues/">Common Issues guide</a> for solutions and tips.</p><h3 id="usage">Usage</h3><p>Before diving into the usage instructions, be sure to check out the range of <a href="../../models/">YOLO26 models offered by Ultralytics</a>. This will help you choose the most appropriate model for your project requirements.</p><div class="admonition example"><p class="admonition-title">Usage</p><div class="tabbed-set tabbed-alternate" data-tabs="2:2"><input checked="checked" id="python" name="__tabbed_2" type="radio"/><input id="cli_1" name="__tabbed_2" type="radio"/><div class="tabbed-labels"><label for="python">Python</label><label for="cli_1">CLI</label></div><div class="tabbed-content"><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="c1"># Load the YOLO26 model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo26n.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Export the model to TensorRT format</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">"engine"</span><span class="p">)</span>  <span class="c1"># creates 'yolo26n.engine'</span>
<span></span>
<span></span><span class="c1"># Load the exported TensorRT model</span>
<span></span><span class="n">tensorrt_model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo26n.engine"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Run inference</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">tensorrt_model</span><span class="p">(</span><span class="s2">"https://ultralytics.com/images/bus.jpg"</span><span class="p">)</span>
</code></pre></div></div><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="c1"># Export a YOLO26n PyTorch model to TensorRT format</span>
<span></span>yolo<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">model</span><span class="o">=</span>yolo26n.pt<span class="w"> </span><span class="nv">format</span><span class="o">=</span>engine<span class="w"> </span><span class="c1"># creates 'yolo26n.engine'</span>
<span></span>
<span></span><span class="c1"># Run inference with the exported model</span>
<span></span>yolo<span class="w"> </span>predict<span class="w"> </span><span class="nv">model</span><span class="o">=</span>yolo26n.engine<span class="w"> </span><span class="nv">source</span><span class="o">=</span><span class="s1">'https://ultralytics.com/images/bus.jpg'</span>
</code></pre></div></div></div></div></div><h3 id="export-arguments">Export Arguments</h3><table><thead><tr><th>Argument</th><th>Type</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><code>format</code></td><td><code>str</code></td><td><code>'engine'</code></td><td>Target format for the exported model, defining compatibility with various deployment environments.</td></tr><tr><td><code>imgsz</code></td><td><code>int</code> or <code>tuple</code></td><td><code>640</code></td><td>Desired image size for the model input. Can be an integer for square images or a tuple <code>(height, width)</code> for specific dimensions.</td></tr><tr><td><code>half</code></td><td><code>bool</code></td><td><code>False</code></td><td>Enables FP16 (half-precision) quantization, reducing model size and potentially speeding up inference on supported hardware.</td></tr><tr><td><code>int8</code></td><td><code>bool</code></td><td><code>False</code></td><td>Activates INT8 quantization, further compressing the model and speeding up inference with minimal <a href="https://www.ultralytics.com/glossary/accuracy">accuracy</a> loss, primarily for edge devices.</td></tr><tr><td><code>dynamic</code></td><td><code>bool</code></td><td><code>False</code></td><td>Allows dynamic input sizes, enhancing flexibility in handling varying image dimensions.</td></tr><tr><td><code>simplify</code></td><td><code>bool</code></td><td><code>True</code></td><td>Simplifies the model graph with <code>onnxslim</code>, potentially improving performance and compatibility.</td></tr><tr><td><code>workspace</code></td><td><code>float</code> or <code>None</code></td><td><code>None</code></td><td>Sets the maximum workspace size in GiB for TensorRT optimizations, balancing memory usage and performance; use <code>None</code> for auto-allocation by TensorRT up to device maximum.</td></tr><tr><td><code>nms</code></td><td><code>bool</code></td><td><code>False</code></td><td>Adds Non-Maximum Suppression (NMS), essential for accurate and efficient detection post-processing.</td></tr><tr><td><code>batch</code></td><td><code>int</code></td><td><code>1</code></td><td>Specifies export model batch inference size or the max number of images the exported model will process concurrently in <code>predict</code> mode.</td></tr><tr><td><code>data</code></td><td><code>str</code></td><td><code>'coco8.yaml'</code></td><td>Path to the <a href="https://docs.ultralytics.com/datasets/">dataset</a> configuration file (default: <code>coco8.yaml</code>), essential for quantization.</td></tr><tr><td><code>fraction</code></td><td><code>float</code></td><td><code>1.0</code></td><td>Specifies the fraction of the dataset to use for INT8 quantization calibration. Allows for calibrating on a subset of the full dataset, useful for experiments or when resources are limited. If not specified with INT8 enabled, the full dataset will be used.</td></tr><tr><td><code>device</code></td><td><code>str</code></td><td><code>None</code></td><td>Specifies the device for exporting: GPU (<code>device=0</code>), DLA for NVIDIA Jetson (<code>device=dla:0</code> or <code>device=dla:1</code>).</td></tr></tbody></table><div class="admonition tip"><p class="admonition-title">Tip</p><p>Please make sure to use a GPU with CUDA support when exporting to TensorRT.</p></div><p>For more details about the export process, visit the <a href="../../modes/export/">Ultralytics documentation page on exporting</a>.</p><h3 id="exporting-tensorrt-with-int8-quantization">Exporting TensorRT with INT8 Quantization</h3><p>Exporting Ultralytics YOLO models using TensorRT with INT8 <a href="https://www.ultralytics.com/glossary/precision">precision</a> executes post-training quantization (PTQ). TensorRT uses calibration for PTQ, which measures the distribution of activations within each activation tensor as the YOLO model processes inference on representative input data, and then uses that distribution to estimate scale values for each tensor. Each activation tensor that is a candidate for quantization has an associated scale that is deduced by a calibration process.</p><p>When processing implicitly quantized networks TensorRT uses INT8 opportunistically to optimize layer execution time. If a layer runs faster in INT8 and has assigned quantization scales on its data inputs and outputs, then a kernel with INT8 precision is assigned to that layer, otherwise TensorRT selects a precision of either FP32 or FP16 for the kernel based on whichever results in faster execution time for that layer.</p><div class="admonition tip"><p class="admonition-title">Tip</p><p>It is <strong>critical</strong> to ensure that the same device that will use the TensorRT model weights for deployment is used for exporting with INT8 precision, as the calibration results can vary across devices.</p></div><h4 id="configuring-int8-export">Configuring INT8 Export</h4><p>The arguments provided when using <a href="../../modes/export/">export</a> for an Ultralytics YOLO model will <strong>greatly</strong> influence the performance of the exported model. They will also need to be selected based on the device resources available, however the default arguments <em>should</em> work for most <a href="https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/">Ampere (or newer) NVIDIA discrete GPUs</a>. The calibration algorithm used is <code>"MINMAX_CALIBRATION"</code> and you can read more details about the options available <a href="https://docs.nvidia.com/deeplearning/tensorrt/latest/_static/python-api/infer/Int8/MinMaxCalibrator.html">in the TensorRT Developer Guide</a>. Ultralytics tests found that <code>"MINMAX_CALIBRATION"</code> was the best choice and exports are fixed to using this algorithm.</p><ul><li><p><code>workspace</code> : Controls the size (in GiB) of the device memory allocation while converting the model weights.</p><ul><li><p>Adjust the <code>workspace</code> value according to your calibration needs and resource availability. While a larger <code>workspace</code> may increase calibration time, it allows TensorRT to explore a wider range of optimization tactics, potentially enhancing model performance and <a href="https://www.ultralytics.com/glossary/accuracy">accuracy</a>. Conversely, a smaller <code>workspace</code> can reduce calibration time but may limit the optimization strategies, affecting the quality of the quantized model.</p></li><li><p>Default is <code>workspace=None</code>, which will allow for TensorRT to automatically allocate memory, when configuring manually, this value may need to be increased if calibration crashes (exits without warning).</p></li><li><p>TensorRT will report <code>UNSUPPORTED_STATE</code> during export if the value for <code>workspace</code> is larger than the memory available to the device, which means the value for <code>workspace</code> should be lowered or set to <code>None</code>.</p></li><li><p>If <code>workspace</code> is set to max value and calibration fails/crashes, consider using <code>None</code> for auto-allocation or by reducing the values for <code>imgsz</code> and <code>batch</code> to reduce memory requirements.</p></li><li><p><u><b>Remember</b> calibration for INT8 is specific to each device</u>, borrowing a "high-end" GPU for calibration, might result in poor performance when inference is run on another device.</p></li></ul></li><li><p><code>batch</code> : The maximum batch-size that will be used for inference. During inference smaller batches can be used, but inference will not accept batches any larger than what is specified.</p></li></ul><div class="admonition note"><p class="admonition-title">Note</p><p>During calibration, twice the <code>batch</code> size provided will be used. Using small batches can lead to inaccurate scaling during calibration. This is because the process adjusts based on the data it sees. Small batches might not capture the full range of values, leading to issues with the final calibration, so the <code>batch</code> size is doubled automatically. If no <a href="https://www.ultralytics.com/glossary/batch-size">batch size</a> is specified <code>batch=1</code>, calibration will be run at <code>batch=1 * 2</code> to reduce calibration scaling errors.</p></div><p>Experimentation by NVIDIA led them to recommend using at least 500 calibration images that are representative of the data for your model, with INT8 quantization calibration. This is a guideline and not a <em>hard</em> requirement, and <u><strong>you will need to experiment with what is required to perform well for your dataset</strong>.</u> Since the calibration data is required for INT8 calibration with TensorRT, make certain to use the <code>data</code> argument when <code>int8=True</code> for TensorRT and use <code>data="my_dataset.yaml"</code>, which will use the images from <a href="../../modes/val/">validation</a> to calibrate with. When no value is passed for <code>data</code> with export to TensorRT with INT8 quantization, the default will be to use one of the <a href="../../datasets/">"small" example datasets based on the model task</a> instead of throwing an error.</p><div class="admonition example"><p class="admonition-title">Example</p><div class="tabbed-set tabbed-alternate" data-tabs="3:2"><input checked="checked" id="python_1" name="__tabbed_3" type="radio"/><input id="cli_2" name="__tabbed_3" type="radio"/><div class="tabbed-labels"><label for="python_1">Python</label><label for="cli_2">CLI</label></div><div class="tabbed-content"><div class="tabbed-block"><div class="annotate highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo26n.pt"</span><span class="p">)</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
<span></span>    <span class="nb">format</span><span class="o">=</span><span class="s2">"engine"</span><span class="p">,</span>
<span></span>    <span class="n">dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># (1)!</span>
<span></span>    <span class="n">batch</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>  <span class="c1"># (2)!</span>
<span></span>    <span class="n">workspace</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>  <span class="c1"># (3)!</span>
<span></span>    <span class="n">int8</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span></span>    <span class="n">data</span><span class="o">=</span><span class="s2">"coco.yaml"</span><span class="p">,</span>  <span class="c1"># (4)!</span>
<span></span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Load the exported TensorRT INT8 model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo26n.engine"</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">"detect"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Run inference</span>
<span></span><span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s2">"https://ultralytics.com/images/bus.jpg"</span><span class="p">)</span>
</code></pre></div><ol><li>Exports with dynamic axes, this will be enabled by default when exporting with <code>int8=True</code> even when not explicitly set. See <a href="../../modes/export/#arguments">export arguments</a> for additional information.</li><li>Sets max batch size of 8 for exported model, which calibrates with <code>batch = 2 * 8</code> to avoid scaling errors during calibration.</li><li>Allocates 4 GiB of memory instead of allocating the entire device for conversion process.</li><li>Uses <a href="../../datasets/detect/coco/">COCO dataset</a> for calibration, specifically the images used for <a href="../../modes/val/">validation</a> (5,000 total).</li></ol></div><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="c1"># Export a YOLO26n PyTorch model to TensorRT format with INT8 quantization</span>
<span></span>yolo<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">model</span><span class="o">=</span>yolo26n.pt<span class="w"> </span><span class="nv">format</span><span class="o">=</span>engine<span class="w"> </span><span class="nv">batch</span><span class="o">=</span><span class="m">8</span><span class="w"> </span><span class="nv">workspace</span><span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="nv">int8</span><span class="o">=</span>True<span class="w"> </span><span class="nv">data</span><span class="o">=</span>coco.yaml<span class="w"> </span><span class="c1"># creates 'yolo26n.engine'</span>
<span></span>
<span></span><span class="c1"># Run inference with the exported TensorRT quantized model</span>
<span></span>yolo<span class="w"> </span>predict<span class="w"> </span><span class="nv">model</span><span class="o">=</span>yolo26n.engine<span class="w"> </span><span class="nv">source</span><span class="o">=</span><span class="s1">'https://ultralytics.com/images/bus.jpg'</span>
</code></pre></div></div></div></div></div><details class="warning" open="open"><summary>Calibration Cache</summary><p>TensorRT will generate a calibration <code>.cache</code> which can be reused to speed up export of future model weights using the same data, but this may result in poor calibration when the data is vastly different or if the <code>batch</code> value is changed drastically. In these circumstances, the existing <code>.cache</code> should be renamed and moved to a different directory or deleted entirely.</p></details><h4 id="advantages-of-using-yolo-with-tensorrt-int8">Advantages of using YOLO with TensorRT INT8</h4><ul><li><p><strong>Reduced model size:</strong> Quantization from FP32 to INT8 can reduce the model size by 4x (on disk or in memory), leading to faster download times. lower storage requirements, and reduced memory footprint when deploying a model.</p></li><li><p><strong>Lower power consumption:</strong> Reduced precision operations for INT8 exported YOLO models can consume less power compared to FP32 models, especially for battery-powered devices.</p></li><li><p><strong>Improved inference speeds:</strong> TensorRT optimizes the model for the target hardware, potentially leading to faster inference speeds on GPUs, embedded devices, and accelerators.</p></li></ul><details class="note"><summary>Note on Inference Speeds</summary><p>The first few inference calls with a model exported to TensorRT INT8 can be expected to have longer than usual preprocessing, inference, and/or postprocessing times. This may also occur when changing <code>imgsz</code> during inference, especially when <code>imgsz</code> is not the same as what was specified during export (export <code>imgsz</code> is set as TensorRT "optimal" profile).</p></details><h4 id="drawbacks-of-using-yolo-with-tensorrt-int8">Drawbacks of using YOLO with TensorRT INT8</h4><ul><li><p><strong>Decreases in evaluation metrics:</strong> Using a lower precision will mean that <code>mAP</code>, <code>Precision</code>, <code>Recall</code> or any <a href="../../guides/yolo-performance-metrics/">other metric used to evaluate model performance</a> is likely to be somewhat worse. See the <a href="#ultralytics-yolo-tensorrt-export-performance">Performance results section</a> to compare the differences in <code>mAP50</code> and <code>mAP50-95</code> when exporting with INT8 on small sample of various devices.</p></li><li><p><strong>Increased development times:</strong> Finding the "optimal" settings for INT8 calibration for dataset and device can take a significant amount of testing.</p></li><li><p><strong>Hardware dependency:</strong> Calibration and performance gains could be highly hardware dependent and model weights are less transferable.</p></li></ul><h2 id="ultralytics-yolo-tensorrt-export-performance">Ultralytics YOLO TensorRT Export Performance</h2><h3 id="nvidia-a100">NVIDIA A100</h3><div class="admonition tip"><p class="admonition-title">Performance</p><p>Tested with Ubuntu 22.04.3 LTS, <code>python 3.10.12</code>, <code>ultralytics==8.2.4</code>, <code>tensorrt==8.6.1.post1</code></p><div class="tabbed-set tabbed-alternate" data-tabs="4:5"><input checked="checked" id="detection-coco" name="__tabbed_4" type="radio"/><input id="segmentation-coco" name="__tabbed_4" type="radio"/><input id="classification-imagenet" name="__tabbed_4" type="radio"><input id="pose-coco" name="__tabbed_4" type="radio"><input id="obb-dotav1" name="__tabbed_4" type="radio"/><div class="tabbed-labels"><label for="detection-coco">Detection (COCO)</label><label for="segmentation-coco">Segmentation (COCO)</label><label for="classification-imagenet">Classification (ImageNet)</label><label for="pose-coco">Pose (COCO)</label><label for="obb-dotav1">OBB (DOTAv1)</label></div><div class="tabbed-content"><div class="tabbed-block"><p>See <a href="../../tasks/detect/">Detection Docs</a> for usage examples with these models trained on <a href="../../datasets/detect/coco/">COCO</a>, which include 80 pretrained classes.</p><div class="admonition note"><p class="admonition-title">Note</p><p>Inference times shown for <code>mean</code>, <code>min</code> (fastest), and <code>max</code> (slowest) for each test using pretrained weights <code>yolov8n.engine</code></p></div><table><thead><tr><th>Precision</th><th>Eval test</th><th>mean<br/>(ms)</th><th>min | max<br/>(ms)</th><th>mAP<sup>val</sup><br/>50(B)</th><th>mAP<sup>val</sup><br/>50-95(B)</th><th><code>batch</code></th><th>size<br/><sup>(pixels)</sup></th></tr></thead><tbody><tr><td>FP32</td><td>Predict</td><td>0.52</td><td>0.51 | 0.56</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP32</td><td>COCO<sup>val</sup></td><td>0.52</td><td></td><td>0.52</td><td>0.37</td><td>1</td><td>640</td></tr><tr><td>FP16</td><td>Predict</td><td>0.34</td><td>0.34 | 0.41</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP16</td><td>COCO<sup>val</sup></td><td>0.33</td><td></td><td>0.52</td><td>0.37</td><td>1</td><td>640</td></tr><tr><td>INT8</td><td>Predict</td><td>0.28</td><td>0.27 | 0.31</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>INT8</td><td>COCO<sup>val</sup></td><td>0.29</td><td></td><td>0.47</td><td>0.33</td><td>1</td><td>640</td></tr></tbody></table></div><div class="tabbed-block"><p>See <a href="../../tasks/segment/">Segmentation Docs</a> for usage examples with these models trained on <a href="../../datasets/segment/coco/">COCO</a>, which include 80 pretrained classes.</p><div class="admonition note"><p class="admonition-title">Note</p><p>Inference times shown for <code>mean</code>, <code>min</code> (fastest), and <code>max</code> (slowest) for each test using pretrained weights <code>yolov8n-seg.engine</code></p></div><table><thead><tr><th>Precision</th><th>Eval test</th><th>mean<br/>(ms)</th><th>min | max<br/>(ms)</th><th>mAP<sup>val</sup><br/>50(B)</th><th>mAP<sup>val</sup><br/>50-95(B)</th><th>mAP<sup>val</sup><br/>50(M)</th><th>mAP<sup>val</sup><br/>50-95(M)</th><th><code>batch</code></th><th>size<br/><sup>(pixels)</sup></th></tr></thead><tbody><tr><td>FP32</td><td>Predict</td><td>0.62</td><td>0.61 | 0.68</td><td></td><td></td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP32</td><td>COCO<sup>val</sup></td><td>0.63</td><td></td><td>0.52</td><td>0.36</td><td>0.49</td><td>0.31</td><td>1</td><td>640</td></tr><tr><td>FP16</td><td>Predict</td><td>0.40</td><td>0.39 | 0.44</td><td></td><td></td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP16</td><td>COCO<sup>val</sup></td><td>0.43</td><td></td><td>0.52</td><td>0.36</td><td>0.49</td><td>0.30</td><td>1</td><td>640</td></tr><tr><td>INT8</td><td>Predict</td><td>0.34</td><td>0.33 | 0.37</td><td></td><td></td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>INT8</td><td>COCO<sup>val</sup></td><td>0.36</td><td></td><td>0.46</td><td>0.32</td><td>0.43</td><td>0.27</td><td>1</td><td>640</td></tr></tbody></table></div><div class="tabbed-block"><p>See <a href="../../tasks/classify/">Classification Docs</a> for usage examples with these models trained on <a href="../../datasets/classify/imagenet/">ImageNet</a>, which include 1000 pretrained classes.</p><div class="admonition note"><p class="admonition-title">Note</p><p>Inference times shown for <code>mean</code>, <code>min</code> (fastest), and <code>max</code> (slowest) for each test using pretrained weights <code>yolov8n-cls.engine</code></p></div><table><thead><tr><th>Precision</th><th>Eval test</th><th>mean<br/>(ms)</th><th>min | max<br/>(ms)</th><th>top-1</th><th>top-5</th><th><code>batch</code></th><th>size<br/><sup>(pixels)</sup></th></tr></thead><tbody><tr><td>FP32</td><td>Predict</td><td>0.26</td><td>0.25 | 0.28</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP32</td><td>ImageNet<sup>val</sup></td><td>0.26</td><td></td><td>0.35</td><td>0.61</td><td>1</td><td>640</td></tr><tr><td>FP16</td><td>Predict</td><td>0.18</td><td>0.17 | 0.19</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP16</td><td>ImageNet<sup>val</sup></td><td>0.18</td><td></td><td>0.35</td><td>0.61</td><td>1</td><td>640</td></tr><tr><td>INT8</td><td>Predict</td><td>0.16</td><td>0.15 | 0.57</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>INT8</td><td>ImageNet<sup>val</sup></td><td>0.15</td><td></td><td>0.32</td><td>0.59</td><td>1</td><td>640</td></tr></tbody></table></div><div class="tabbed-block"><p>See <a href="../../tasks/pose/">Pose Estimation Docs</a> for usage examples with these models trained on <a href="../../datasets/pose/coco/">COCO</a>, which include 1 pretrained class, "person".</p><div class="admonition note"><p class="admonition-title">Note</p><p>Inference times shown for <code>mean</code>, <code>min</code> (fastest), and <code>max</code> (slowest) for each test using pretrained weights <code>yolov8n-pose.engine</code></p></div><table><thead><tr><th>Precision</th><th>Eval test</th><th>mean<br/>(ms)</th><th>min | max<br/>(ms)</th><th>mAP<sup>val</sup><br/>50(B)</th><th>mAP<sup>val</sup><br/>50-95(B)</th><th>mAP<sup>val</sup><br/>50(P)</th><th>mAP<sup>val</sup><br/>50-95(P)</th><th><code>batch</code></th><th>size<br/><sup>(pixels)</sup></th></tr></thead><tbody><tr><td>FP32</td><td>Predict</td><td>0.54</td><td>0.53 | 0.58</td><td></td><td></td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP32</td><td>COCO<sup>val</sup></td><td>0.55</td><td></td><td>0.91</td><td>0.69</td><td>0.80</td><td>0.51</td><td>1</td><td>640</td></tr><tr><td>FP16</td><td>Predict</td><td>0.37</td><td>0.35 | 0.41</td><td></td><td></td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP16</td><td>COCO<sup>val</sup></td><td>0.36</td><td></td><td>0.91</td><td>0.69</td><td>0.80</td><td>0.51</td><td>1</td><td>640</td></tr><tr><td>INT8</td><td>Predict</td><td>0.29</td><td>0.28 | 0.33</td><td></td><td></td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>INT8</td><td>COCO<sup>val</sup></td><td>0.30</td><td></td><td>0.90</td><td>0.68</td><td>0.78</td><td>0.47</td><td>1</td><td>640</td></tr></tbody></table></div><div class="tabbed-block"><p>See <a href="../../tasks/obb/">Oriented Detection Docs</a> for usage examples with these models trained on <a href="../../datasets/obb/dota-v2/#dota-v10">DOTAv1</a>, which include 15 pretrained classes.</p><div class="admonition note"><p class="admonition-title">Note</p><p>Inference times shown for <code>mean</code>, <code>min</code> (fastest), and <code>max</code> (slowest) for each test using pretrained weights <code>yolov8n-obb.engine</code></p></div><table><thead><tr><th>Precision</th><th>Eval test</th><th>mean<br/>(ms)</th><th>min | max<br/>(ms)</th><th>mAP<sup>val</sup><br/>50(B)</th><th>mAP<sup>val</sup><br/>50-95(B)</th><th><code>batch</code></th><th>size<br/><sup>(pixels)</sup></th></tr></thead><tbody><tr><td>FP32</td><td>Predict</td><td>0.52</td><td>0.51 | 0.59</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP32</td><td>DOTAv1<sup>val</sup></td><td>0.76</td><td></td><td>0.50</td><td>0.36</td><td>1</td><td>640</td></tr><tr><td>FP16</td><td>Predict</td><td>0.34</td><td>0.33 | 0.42</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP16</td><td>DOTAv1<sup>val</sup></td><td>0.59</td><td></td><td>0.50</td><td>0.36</td><td>1</td><td>640</td></tr><tr><td>INT8</td><td>Predict</td><td>0.29</td><td>0.28 | 0.33</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>INT8</td><td>DOTAv1<sup>val</sup></td><td>0.32</td><td></td><td>0.45</td><td>0.32</td><td>1</td><td>640</td></tr></tbody></table></div></div></input></input></div></div><h3 id="consumer-gpus">Consumer GPUs</h3><div class="admonition tip"><p class="admonition-title">Detection Performance (COCO)</p><div class="tabbed-set tabbed-alternate" data-tabs="5:3"><input checked="checked" id="rtx-3080-12-gb" name="__tabbed_5" type="radio"/><input id="rtx-3060-12-gb" name="__tabbed_5" type="radio"/><input id="rtx-2060-6-gb" name="__tabbed_5" type="radio"/><div class="tabbed-labels"><label for="rtx-3080-12-gb">RTX 3080 12 GB</label><label for="rtx-3060-12-gb">RTX 3060 12 GB</label><label for="rtx-2060-6-gb">RTX 2060 6 GB</label></div><div class="tabbed-content"><div class="tabbed-block"><p>Tested with Windows 10.0.19045, <code>python 3.10.9</code>, <code>ultralytics==8.2.4</code>, <code>tensorrt==10.0.0b6</code></p><div class="admonition note"><p class="admonition-title">Note</p><p>Inference times shown for <code>mean</code>, <code>min</code> (fastest), and <code>max</code> (slowest) for each test using pretrained weights <code>yolov8n.engine</code></p></div><table><thead><tr><th>Precision</th><th>Eval test</th><th>mean<br/>(ms)</th><th>min | max<br/>(ms)</th><th>mAP<sup>val</sup><br/>50(B)</th><th>mAP<sup>val</sup><br/>50-95(B)</th><th><code>batch</code></th><th>size<br/><sup>(pixels)</sup></th></tr></thead><tbody><tr><td>FP32</td><td>Predict</td><td>1.06</td><td>0.75 | 1.88</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP32</td><td>COCO<sup>val</sup></td><td>1.37</td><td></td><td>0.52</td><td>0.37</td><td>1</td><td>640</td></tr><tr><td>FP16</td><td>Predict</td><td>0.62</td><td>0.75 | 1.13</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP16</td><td>COCO<sup>val</sup></td><td>0.85</td><td></td><td>0.52</td><td>0.37</td><td>1</td><td>640</td></tr><tr><td>INT8</td><td>Predict</td><td>0.52</td><td>0.38 | 1.00</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>INT8</td><td>COCO<sup>val</sup></td><td>0.74</td><td></td><td>0.47</td><td>0.33</td><td>1</td><td>640</td></tr></tbody></table></div><div class="tabbed-block"><p>Tested with Windows 10.0.22631, <code>python 3.11.9</code>, <code>ultralytics==8.2.4</code>, <code>tensorrt==10.0.1</code></p><div class="admonition note"><p class="admonition-title">Note</p><p>Inference times shown for <code>mean</code>, <code>min</code> (fastest), and <code>max</code> (slowest) for each test using pretrained weights <code>yolov8n.engine</code></p></div><table><thead><tr><th>Precision</th><th>Eval test</th><th>mean<br/>(ms)</th><th>min | max<br/>(ms)</th><th>mAP<sup>val</sup><br/>50(B)</th><th>mAP<sup>val</sup><br/>50-95(B)</th><th><code>batch</code></th><th>size<br/><sup>(pixels)</sup></th></tr></thead><tbody><tr><td>FP32</td><td>Predict</td><td>1.76</td><td>1.69 | 1.87</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP32</td><td>COCO<sup>val</sup></td><td>1.94</td><td></td><td>0.52</td><td>0.37</td><td>1</td><td>640</td></tr><tr><td>FP16</td><td>Predict</td><td>0.86</td><td>0.75 | 1.00</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP16</td><td>COCO<sup>val</sup></td><td>1.43</td><td></td><td>0.52</td><td>0.37</td><td>1</td><td>640</td></tr><tr><td>INT8</td><td>Predict</td><td>0.80</td><td>0.75 | 1.00</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>INT8</td><td>COCO<sup>val</sup></td><td>1.35</td><td></td><td>0.47</td><td>0.33</td><td>1</td><td>640</td></tr></tbody></table></div><div class="tabbed-block"><p>Tested with Pop!_OS 22.04 LTS, <code>python 3.10.12</code>, <code>ultralytics==8.2.4</code>, <code>tensorrt==8.6.1.post1</code></p><div class="admonition note"><p class="admonition-title">Note</p><p>Inference times shown for <code>mean</code>, <code>min</code> (fastest), and <code>max</code> (slowest) for each test using pretrained weights <code>yolov8n.engine</code></p></div><table><thead><tr><th>Precision</th><th>Eval test</th><th>mean<br/>(ms)</th><th>min | max<br/>(ms)</th><th>mAP<sup>val</sup><br/>50(B)</th><th>mAP<sup>val</sup><br/>50-95(B)</th><th><code>batch</code></th><th>size<br/><sup>(pixels)</sup></th></tr></thead><tbody><tr><td>FP32</td><td>Predict</td><td>2.84</td><td>2.84 | 2.85</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP32</td><td>COCO<sup>val</sup></td><td>2.94</td><td></td><td>0.52</td><td>0.37</td><td>1</td><td>640</td></tr><tr><td>FP16</td><td>Predict</td><td>1.09</td><td>1.09 | 1.10</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP16</td><td>COCO<sup>val</sup></td><td>1.20</td><td></td><td>0.52</td><td>0.37</td><td>1</td><td>640</td></tr><tr><td>INT8</td><td>Predict</td><td>0.75</td><td>0.74 | 0.75</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>INT8</td><td>COCO<sup>val</sup></td><td>0.76</td><td></td><td>0.47</td><td>0.33</td><td>1</td><td>640</td></tr></tbody></table></div></div></div></div><h3 id="embedded-devices">Embedded Devices</h3><div class="admonition tip"><p class="admonition-title">Detection Performance (COCO)</p><div class="tabbed-set tabbed-alternate" data-tabs="6:1"><input checked="checked" id="jetson-orin-nx-16gb" name="__tabbed_6" type="radio"/><div class="tabbed-labels"><label for="jetson-orin-nx-16gb">Jetson Orin NX 16GB</label></div><div class="tabbed-content"><div class="tabbed-block"><p>Tested with JetPack 6.0 (L4T 36.3) Ubuntu 22.04.4 LTS, <code>python 3.10.12</code>, <code>ultralytics==8.2.16</code>, <code>tensorrt==10.0.1</code></p><div class="admonition note"><p class="admonition-title">Note</p><p>Inference times shown for <code>mean</code>, <code>min</code> (fastest), and <code>max</code> (slowest) for each test using pretrained weights <code>yolov8n.engine</code></p></div><table><thead><tr><th>Precision</th><th>Eval test</th><th>mean<br/>(ms)</th><th>min | max<br/>(ms)</th><th>mAP<sup>val</sup><br/>50(B)</th><th>mAP<sup>val</sup><br/>50-95(B)</th><th><code>batch</code></th><th>size<br/><sup>(pixels)</sup></th></tr></thead><tbody><tr><td>FP32</td><td>Predict</td><td>6.11</td><td>6.10 | 6.29</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP32</td><td>COCO<sup>val</sup></td><td>6.17</td><td></td><td>0.52</td><td>0.37</td><td>1</td><td>640</td></tr><tr><td>FP16</td><td>Predict</td><td>3.18</td><td>3.18 | 3.20</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>FP16</td><td>COCO<sup>val</sup></td><td>3.19</td><td></td><td>0.52</td><td>0.37</td><td>1</td><td>640</td></tr><tr><td>INT8</td><td>Predict</td><td>2.30</td><td>2.29 | 2.35</td><td></td><td></td><td>8</td><td>640</td></tr><tr><td>INT8</td><td>COCO<sup>val</sup></td><td>2.32</td><td></td><td>0.46</td><td>0.32</td><td>1</td><td>640</td></tr></tbody></table></div></div></div></div><div class="admonition info"><p class="admonition-title">Info</p><p>See our <a href="../../guides/nvidia-jetson/">quickstart guide on NVIDIA Jetson with Ultralytics YOLO</a> to learn more about setup and configuration.</p></div><div class="admonition info"><p class="admonition-title">Info</p><p>See our <a href="../../guides/nvidia-dgx-spark/">quickstart guide on NVIDIA DGX Spark with Ultralytics YOLO</a> to learn more about setup and configuration.</p></div><h4 id="evaluation-methods">Evaluation methods</h4><p>Expand sections below for information on how these models were exported and tested.</p><details class="example"><summary>Export configurations</summary><p>See <a href="../../modes/export/">export mode</a> for details regarding export configuration arguments.</p><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo26n.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># TensorRT FP32</span>
<span></span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">"engine"</span><span class="p">,</span> <span class="n">imgsz</span><span class="o">=</span><span class="mi">640</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">workspace</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># TensorRT FP16</span>
<span></span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">"engine"</span><span class="p">,</span> <span class="n">imgsz</span><span class="o">=</span><span class="mi">640</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">workspace</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">half</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># TensorRT INT8 with calibration `data` (i.e. COCO, ImageNet, or DOTAv1 for appropriate model task)</span>
<span></span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
<span></span>    <span class="nb">format</span><span class="o">=</span><span class="s2">"engine"</span><span class="p">,</span> <span class="n">imgsz</span><span class="o">=</span><span class="mi">640</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">workspace</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">int8</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="s2">"coco8.yaml"</span>
<span></span><span class="p">)</span>
</code></pre></div></details><details class="example"><summary>Predict loop</summary><p>See <a href="../../modes/predict/">predict mode</a> for additional information.</p><div class="highlight"><pre><span></span><code><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">cv2</span>
<span></span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo26n.engine"</span><span class="p">)</span>
<span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">)</span>
<span></span>
<span></span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span></span>    <span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
<span></span>        <span class="p">[</span><span class="n">img</span><span class="p">]</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span>  <span class="c1"># batch=8 of the same image</span>
<span></span>        <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
</code></pre></div></details><details class="example"><summary>Validation configuration</summary><p>See <a href="../../modes/val/"><code>val</code> mode</a> to learn more about validation configuration arguments.</p><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo26n.engine"</span><span class="p">)</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">val</span><span class="p">(</span>
<span></span>    <span class="n">data</span><span class="o">=</span><span class="s2">"data.yaml"</span><span class="p">,</span>  <span class="c1"># COCO, ImageNet, or DOTAv1 for appropriate model task</span>
<span></span>    <span class="n">batch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span></span>    <span class="n">imgsz</span><span class="o">=</span><span class="mi">640</span><span class="p">,</span>
<span></span>    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div></details><h2 id="deploying-exported-yolo26-tensorrt-models">Deploying Exported YOLO26 TensorRT Models</h2><p>Having successfully exported your Ultralytics YOLO26 models to TensorRT format, you're now ready to deploy them. For in-depth instructions on deploying your TensorRT models in various settings, take a look at the following resources:</p><ul><li><p><strong><a href="../../guides/triton-inference-server/">Deploy Ultralytics with a Triton Server</a></strong>: Our guide on how to use NVIDIA's Triton Inference (formerly TensorRT Inference) Server specifically for use with Ultralytics YOLO models.</p></li><li><p><strong><a href="https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/">Deploying Deep Neural Networks with NVIDIA TensorRT</a></strong>: This article explains how to use NVIDIA TensorRT to deploy deep neural networks on GPU-based deployment platforms efficiently.</p></li><li><p><strong><a href="https://developer.nvidia.com/blog/end-to-end-ai-for-nvidia-based-pcs-nvidia-tensorrt-deployment/">End-to-End AI for NVIDIA-Based PCs: NVIDIA TensorRT Deployment</a></strong>: This blog post explains the use of NVIDIA TensorRT for optimizing and deploying AI models on NVIDIA-based PCs.</p></li><li><p><strong><a href="https://github.com/NVIDIA/TensorRT">GitHub Repository for NVIDIA TensorRT:</a></strong>: This is the official GitHub repository that contains the source code and documentation for NVIDIA TensorRT.</p></li></ul><h2 id="summary">Summary</h2><p>In this guide, we focused on converting Ultralytics YOLO26 models to NVIDIA's TensorRT model format. This conversion step is crucial for improving the efficiency and speed of YOLO26 models, making them more effective and suitable for diverse deployment environments.</p><p>For more information on usage details, take a look at the <a href="https://docs.nvidia.com/deeplearning/tensorrt/">TensorRT official documentation</a>.</p><p>If you're curious about additional Ultralytics YOLO26 integrations, our <a href="../../integrations/">integration guide page</a> provides an extensive selection of informative resources and insights.</p><h2 id="faq">FAQ</h2><h3 id="how-do-i-convert-yolo26-models-to-tensorrt-format">How do I convert YOLO26 models to TensorRT format?</h3><p>To convert your Ultralytics YOLO26 models to TensorRT format for optimized NVIDIA GPU inference, follow these steps:</p><ol><li><p><strong>Install the required package</strong>:</p><div class="highlight"><pre><span></span><code><span></span>pip<span class="w"> </span>install<span class="w"> </span>ultralytics
</code></pre></div></li><li><p><strong>Export your YOLO26 model</strong>:</p><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo26n.pt"</span><span class="p">)</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">"engine"</span><span class="p">)</span>  <span class="c1"># creates 'yolo26n.engine'</span>
<span></span>
<span></span><span class="c1"># Run inference</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo26n.engine"</span><span class="p">)</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="s2">"https://ultralytics.com/images/bus.jpg"</span><span class="p">)</span>
</code></pre></div></li></ol><p>For more details, visit the <a href="../../quickstart/">YOLO26 Installation guide</a> and the <a href="../../modes/export/">export documentation</a>.</p><h3 id="what-are-the-benefits-of-using-tensorrt-for-yolo26-models">What are the benefits of using TensorRT for YOLO26 models?</h3><p>Using TensorRT to optimize YOLO26 models offers several benefits:</p><ul><li><strong>Faster Inference Speed</strong>: TensorRT optimizes the model layers and uses precision calibration (INT8 and FP16) to speed up inference without significantly sacrificing accuracy.</li><li><strong>Memory Efficiency</strong>: TensorRT manages tensor memory dynamically, reducing overhead and improving GPU memory utilization.</li><li><strong>Layer Fusion</strong>: Combines multiple layers into single operations, reducing computational complexity.</li><li><strong>Kernel Auto-Tuning</strong>: Automatically selects optimized GPU kernels for each model layer, ensuring maximum performance.</li></ul><p>To learn more, explore the <a href="https://developer.nvidia.com/tensorrt">official TensorRT documentation from NVIDIA</a> and our <a href="#tensorrt">in-depth TensorRT overview</a>.</p><h3 id="can-i-use-int8-quantization-with-tensorrt-for-yolo26-models">Can I use INT8 quantization with TensorRT for YOLO26 models?</h3><p>Yes, you can export YOLO26 models using TensorRT with INT8 quantization. This process involves post-training quantization (PTQ) and calibration:</p><ol><li><p><strong>Export with INT8</strong>:</p><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo26n.pt"</span><span class="p">)</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">"engine"</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">workspace</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">int8</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="s2">"coco.yaml"</span><span class="p">)</span>
</code></pre></div></li><li><p><strong>Run inference</strong>:</p><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo26n.engine"</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">"detect"</span><span class="p">)</span>
<span></span><span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s2">"https://ultralytics.com/images/bus.jpg"</span><span class="p">)</span>
</code></pre></div></li></ol><p>For more details, refer to the <a href="#exporting-tensorrt-with-int8-quantization">exporting TensorRT with INT8 quantization section</a>.</p><h3 id="how-do-i-deploy-yolo26-tensorrt-models-on-an-nvidia-triton-inference-server">How do I deploy YOLO26 TensorRT models on an NVIDIA Triton Inference Server?</h3><p>Deploying YOLO26 TensorRT models on an NVIDIA Triton Inference Server can be done using the following resources:</p><ul><li><strong><a href="../../guides/triton-inference-server/">Deploy Ultralytics YOLO26 with Triton Server</a></strong>: Step-by-step guidance on setting up and using Triton Inference Server.</li><li><strong><a href="https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/">NVIDIA Triton Inference Server Documentation</a></strong>: Official NVIDIA documentation for detailed deployment options and configurations.</li></ul><p>These guides will help you integrate YOLO26 models efficiently in various deployment environments.</p><h3 id="what-are-the-performance-improvements-observed-with-yolo26-models-exported-to-tensorrt">What are the performance improvements observed with YOLO26 models exported to TensorRT?</h3><p>Performance improvements with TensorRT can vary based on the hardware used. Here are some typical benchmarks:</p><ul><li><p><strong>NVIDIA A100</strong>:</p><ul><li><strong>FP32</strong> Inference: ~0.52 ms / image</li><li><strong>FP16</strong> Inference: ~0.34 ms / image</li><li><strong>INT8</strong> Inference: ~0.28 ms / image</li><li>Slight reduction in mAP with INT8 precision, but significant improvement in speed.</li></ul></li><li><p><strong>Consumer GPUs (e.g., RTX 3080)</strong>:</p><ul><li><strong>FP32</strong> Inference: ~1.06 ms / image</li><li><strong>FP16</strong> Inference: ~0.62 ms / image</li><li><strong>INT8</strong> Inference: ~0.52 ms / image</li></ul></li></ul><p>Detailed performance benchmarks for different hardware configurations can be found in the <a href="#ultralytics-yolo-tensorrt-export-performance">performance section</a>.</p><p>For more comprehensive insights into TensorRT performance, refer to the <a href="../../modes/export/">Ultralytics documentation</a> and our performance analysis reports.</p><br/><br/><div class="git-info"><div class="dates-container"><span class="date-item" title="This page was first created on January 28, 2024"><span class="hover-item">üìÖ</span> Created 1 year ago </span><span class="date-item" title="This page was last updated on January 14, 2026"><span class="hover-item">‚úèÔ∏è</span> Updated 4 days ago </span></div><div class="authors-container"><a class="author-link" href="https://github.com/glenn-jocher" title="glenn-jocher (19 changes)"><img alt="glenn-jocher" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/26833433?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/RizwanMunawar" title="RizwanMunawar (3 changes)"><img alt="RizwanMunawar" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/62513924?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/lakshanthad" title="lakshanthad (3 changes)"><img alt="lakshanthad" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/20147381?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/Burhan-Q" title="Burhan-Q (3 changes)"><img alt="Burhan-Q" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/62214284?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/lakshanthad" title="lakshanthad (2 changes)"><img alt="lakshanthad" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/20147381?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/ambitious-octopus" title="ambitious-octopus (2 changes)"><img alt="ambitious-octopus" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/3855193?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/onuralpszr" title="onuralpszr (1 change)"><img alt="onuralpszr" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/1688848?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/pderrenger" title="pderrenger (1 change)"><img alt="pderrenger" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/107626595?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/Y-T-G" title="Y-T-G (1 change)"><img alt="Y-T-G" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/32206511?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/ultralytics/ultralytics" title="willie.maddox@gmail.com (1 change)"><img alt="willie.maddox@gmail.com" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/9919?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/jk4e" title="jk4e (1 change)"><img alt="jk4e" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/116908874?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/MatthewNoyce" title="MatthewNoyce (1 change)"><img alt="MatthewNoyce" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/131261051?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/IvorZhu331" title="IvorZhu331 (1 change)"><img alt="IvorZhu331" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/130829914?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/abirami-vina" title="abirami-vina (1 change)"><img alt="abirami-vina" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/25847604?v=4&amp;s=96"/></a></div></div><div class="share-buttons"><button class="share-button hover-item" onclick="window.open('https://twitter.com/intent/tweet?url=https%3A%2F%2Fdocs.ultralytics.com%2Fintegrations%2Ftensorrt%2F', 'TwitterShare', 'width=550,height=680,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-x-twitter"></i> Tweet </button><button class="share-button hover-item linkedin" onclick="window.open('https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fdocs.ultralytics.com%2Fintegrations%2Ftensorrt%2F', 'LinkedinShare', 'width=550,height=730,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-linkedin-in"></i> Share </button></div><br/><h2 id="__comments">Comments</h2><div id="giscus-container"></div></article></div>
<script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div><button class="md-top md-icon" data-md-component="top" hidden="" type="button"><svg class="lucide lucide-circle-arrow-up" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><path d="m16 12-4-4-4 4M12 16V8"></path></svg> Back to top
</button></main><footer class="md-footer"><nav aria-label="Footer" class="md-footer__inner md-grid"><a aria-label="Previous: TensorBoard" class="md-footer__link md-footer__link--prev" href="../tensorboard/"><div class="md-footer__button md-icon"><svg class="lucide lucide-arrow-left" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m12 19-7-7 7-7M19 12H5"></path></svg></div><div class="md-footer__title"><span class="md-footer__direction"> Previous </span><div class="md-ellipsis"> TensorBoard </div></div></a><a aria-label="Next: TF GraphDef" class="md-footer__link md-footer__link--next" href="../tf-graphdef/"><div class="md-footer__title"><span class="md-footer__direction"> Next </span><div class="md-ellipsis"> TF GraphDef </div></div><div class="md-footer__button md-icon"><svg class="lucide lucide-arrow-right" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M5 12h14M12 5l7 7-7 7"></path></svg></div></a></nav><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class="md-copyright"><div class="md-copyright__highlight"><a href="https://www.ultralytics.com/" target="_blank">¬© 2026 Ultralytics Inc.</a> All rights reserved. </div> Made with <a href="https://zensical.org/" rel="noopener" target="_blank"> Zensical </a></div><div class="md-social"><a class="md-social__link" href="https://github.com/ultralytics" rel="noopener" target="_blank" title="github.com"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://www.linkedin.com/company/ultralytics/" rel="noopener" target="_blank" title="www.linkedin.com"><svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://x.com/ultralytics" rel="noopener" target="_blank" title="x.com"><svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://youtube.com/ultralytics?sub_confirmation=1" rel="noopener" target="_blank" title="youtube.com"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M549.7 124.1c-6.2-23.7-24.8-42.3-48.3-48.6C458.9 64 288.1 64 288.1 64S117.3 64 74.7 75.5c-23.5 6.3-42 24.9-48.3 48.6C15 167 15 256.4 15 256.4s0 89.4 11.4 132.3c6.3 23.6 24.8 41.5 48.3 47.8C117.3 448 288.1 448 288.1 448s170.8 0 213.4-11.5c23.5-6.3 42-24.2 48.3-47.8 11.4-42.9 11.4-132.3 11.4-132.3s0-89.4-11.4-132.3zM232.2 337.6V175.2l142.7 81.2z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://hub.docker.com/r/ultralytics/ultralytics/" rel="noopener" target="_blank" title="hub.docker.com"><svg viewbox="0 0 640 512" xmlns="http://www.w3.org/2000/svg"><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://pypi.org/project/ultralytics/" rel="noopener" target="_blank" title="pypi.org"><svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 444.7a20.4 20.4 0 1 1 0-40.7 20.4 20.4 0 1 1 0 40.7M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.6-183.4a20.4 20.4 0 1 1 0 40.8 20.4 20.4 0 1 1 0-40.8" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://discord.com/invite/ultralytics" rel="noopener" target="_blank" title="discord.com"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M492.5 69.8c-.2-.3-.4-.6-.8-.7-38.1-17.5-78.4-30-119.7-37.1-.4-.1-.8 0-1.1.1s-.6.4-.8.8c-5.5 9.9-10.5 20.2-14.9 30.6-44.6-6.8-89.9-6.8-134.4 0-4.5-10.5-9.5-20.7-15.1-30.6-.2-.3-.5-.6-.8-.8s-.7-.2-1.1-.2C162.5 39 122.2 51.5 84.1 69c-.3.1-.6.4-.8.7C7.1 183.5-13.8 294.6-3.6 404.2c0 .3.1.5.2.8s.3.4.5.6c44.4 32.9 94 58 146.8 74.2.4.1.8.1 1.1 0s.7-.4.9-.7c11.3-15.4 21.4-31.8 30-48.8.1-.2.2-.5.2-.8s0-.5-.1-.8-.2-.5-.4-.6-.4-.3-.7-.4c-15.8-6.1-31.2-13.4-45.9-21.9-.3-.2-.5-.4-.7-.6s-.3-.6-.3-.9 0-.6.2-.9.3-.5.6-.7c3.1-2.3 6.2-4.7 9.1-7.1.3-.2.6-.4.9-.4s.7 0 1 .1c96.2 43.9 200.4 43.9 295.5 0 .3-.1.7-.2 1-.2s.7.2.9.4c2.9 2.4 6 4.9 9.1 7.2.2.2.4.4.6.7s.2.6.2.9-.1.6-.3.9-.4.5-.6.6c-14.7 8.6-30 15.9-45.9 21.8-.2.1-.5.2-.7.4s-.3.4-.4.7-.1.5-.1.8.1.5.2.8c8.8 17 18.8 33.3 30 48.8.2.3.6.6.9.7s.8.1 1.1 0c52.9-16.2 102.6-41.3 147.1-74.2.2-.2.4-.4.5-.6s.2-.5.2-.8c12.3-126.8-20.5-236.9-86.9-334.5zm-302 267.7c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.4 59.2-52.8 59.2m195.4 0c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.2 59.2-52.8 59.2" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://reddit.com/r/ultralytics" rel="noopener" target="_blank" title="reddit.com"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M0 256C0 114.6 114.6 0 256 0s256 114.6 256 256-114.6 256-256 256H37.1c-13.7 0-20.5-16.5-10.9-26.2L75 437C28.7 390.7 0 326.7 0 256m349.6-102.4c23.6 0 42.7-19.1 42.7-42.7s-19.1-42.7-42.7-42.7c-20.6 0-37.8 14.6-41.8 34-34.5 3.7-61.4 33-61.4 68.4v.2c-37.5 1.6-71.8 12.3-99 29.1-10.1-7.8-22.8-12.5-36.5-12.5-33 0-59.8 26.8-59.8 59.8 0 24 14.1 44.6 34.4 54.1 2 69.4 77.6 125.2 170.6 125.2s168.7-55.9 170.6-125.3c20.2-9.6 34.1-30.2 34.1-54 0-33-26.8-59.8-59.8-59.8-13.7 0-26.3 4.6-36.4 12.4-27.4-17-62.1-27.7-100-29.1v-.2c0-25.4 18.9-46.5 43.4-49.9 4.4 18.8 21.3 32.8 41.5 32.8zm-172.5 93.3c16.7 0 29.5 17.6 28.5 39.3s-13.5 29.6-30.3 29.6-31.4-8.8-30.4-30.5S160.3 247 177 247zm190.1 38.3c1 21.7-13.7 30.5-30.4 30.5s-29.3-7.9-30.3-29.6 11.8-39.3 28.5-39.3 31.2 16.6 32.1 38.3zm-48.1 56.7c-10.3 24.6-34.6 41.9-63 41.9s-52.7-17.3-63-41.9c-1.2-2.9.8-6.2 3.9-6.5 18.4-1.9 38.3-2.9 59.1-2.9s40.7 1 59.1 2.9c3.1.3 5.1 3.6 3.9 6.5" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://weixin.qq.com/r/mp/LxckPDfEgWr_rXNf90I9" rel="noopener" target="_blank" title="weixin.qq.com"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M385.2 167.6c6.4 0 12.6.3 18.8 1.1C387.4 90.3 303.3 32 207.7 32 100.5 32 13 104.8 13 197.4c0 53.4 29.3 97.5 77.9 131.6l-19.3 58.6 68-34.1c24.4 4.8 43.8 9.7 68.2 9.7 6.2 0 12.1-.3 18.3-.8-4-12.9-6.2-26.6-6.2-40.8-.1-84.9 72.9-154 165.3-154m-104.5-52.9c14.5 0 24.2 9.7 24.2 24.4 0 14.5-9.7 24.2-24.2 24.2-14.8 0-29.3-9.7-29.3-24.2.1-14.7 14.6-24.4 29.3-24.4m-136.4 48.6c-14.5 0-29.3-9.7-29.3-24.2 0-14.8 14.8-24.4 29.3-24.4 14.8 0 24.4 9.7 24.4 24.4 0 14.6-9.6 24.2-24.4 24.2M563 319.4c0-77.9-77.9-141.3-165.4-141.3-92.7 0-165.4 63.4-165.4 141.3S305 460.7 397.6 460.7c19.3 0 38.9-5.1 58.6-9.9l53.4 29.3-14.8-48.6C534 402.1 563 363.2 563 319.4m-219.1-24.5c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.8 0 24.4 9.7 24.4 19.3 0 10-9.7 19.6-24.4 19.6m107.1 0c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.5 0 24.4 9.7 24.4 19.3.1 10-9.9 19.6-24.4 19.6" fill="currentColor"></path></svg></a></div></div></div></footer></div><div class="md-dialog" data-md-component="dialog"><div class="md-dialog__inner md-typeset"></div></div><div class="md-progress" data-md-component="progress" role="progressbar"></div>
<script id="__config" type="application/json">{"annotate":null,"base":"../..","features":["content.action.edit","content.code.annotate","content.code.copy","content.tooltips","toc.follow","navigation.top","navigation.tabs","navigation.tabs.sticky","navigation.prune","navigation.footer","navigation.tracking","navigation.instant","navigation.instant.progress","navigation.indexes","navigation.sections","content.tabs.link"],"search":"../../assets/javascripts/workers/search.e2d2d235.min.js","tags":null,"translations":{"clipboard.copied":"Copied to clipboard","clipboard.copy":"Copy to clipboard","search.result.more.one":"1 more on this page","search.result.more.other":"# more on this page","search.result.none":"No matching documents","search.result.one":"1 matching document","search.result.other":"# matching documents","search.result.placeholder":"Type to start searching","search.result.term.missing":"Missing","select.version":"Select version"},"version":null}</script>
<script src="../../assets/javascripts/bundle.8ffeb9c9.min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/ultralytics/llm@v0.1.8/js/chat.min.js"></script>
<script src="https://unpkg.com/tablesort@5.6.0/dist/tablesort.min.js"></script>
<script src="../../javascript/extra.js"></script>
<script src="../../javascript/giscus.js"></script>
<script src="../../javascript/tablesort.js"></script>
<script>
            async function copyMarkdownForLLM(button) {
                const editBtn = document.querySelector('a[title="Edit this page"]');
                if (!editBtn) return;

                const originalHTML = button.innerHTML;
                const checkIcon = '<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 16.17L4.83 12l-1.42 1.41L9 19L21 7l-1.41-1.41L9 16.17z"></path></svg>';

                let rawUrl = editBtn.href.replace('github.com', 'raw.githubusercontent.com');
                rawUrl = rawUrl.replace('/blob/', '/').replace('/tree/', '/');

                try {
                    const response = await fetch(rawUrl);
                    let markdown = await response.text();

                    if (markdown.startsWith('---')) {
                        const frontMatterEnd = markdown.indexOf('\n---\n', 3);
                        if (frontMatterEnd !== -1) {
                            markdown = markdown.substring(frontMatterEnd + 5).trim();
                        }
                    }

                    const title = document.querySelector('h1')?.textContent || document.title;
                    const content = `# ${title}\n\nSource: ${window.location.href}\n\n---\n\n${markdown}`;

                    await navigator.clipboard.writeText(content);
                    button.innerHTML = checkIcon + ' Copied!';
                    setTimeout(() => { button.innerHTML = originalHTML; }, 2000);
                } catch (err) {
                    button.innerHTML = '‚ùå Failed';
                    setTimeout(() => { button.innerHTML = originalHTML; }, 2000);
                }
            }
            </script></body></html>