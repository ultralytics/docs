 <!DOCTYPE html><html class="no-js" lang="en"><head><meta charset="utf-8"/><meta content="width=device-width,initial-scale=1" name="viewport"/><meta content="Learn to export Ultralytics YOLO11 models to Sony's IMX500 format for efficient edge AI deployment on Raspberry Pi AI Camera with on-chip processing." name="description"/><meta content="Ultralytics" name="author"/><link href="https://docs.ultralytics.com/integrations/sony-imx500/" rel="canonical"/><link href="../seeedstudio-recamera/" rel="prev"/><link href="../tensorboard/" rel="next"/><link href="/" hreflang="en" rel="alternate"/><link href="/zh/" hreflang="zh" rel="alternate"/><link href="/ko/" hreflang="ko" rel="alternate"/><link href="/ja/" hreflang="ja" rel="alternate"/><link href="/ru/" hreflang="ru" rel="alternate"/><link href="/de/" hreflang="de" rel="alternate"/><link href="/fr/" hreflang="fr" rel="alternate"/><link href="/es/" hreflang="es" rel="alternate"/><link href="/pt/" hreflang="pt" rel="alternate"/><link href="/it/" hreflang="it" rel="alternate"/><link href="/tr/" hreflang="tr" rel="alternate"/><link href="/vi/" hreflang="vi" rel="alternate"/><link href="/ar/" hreflang="ar" rel="alternate"/><link href="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/logo/favicon-yolo.png" rel="icon"/><meta content="zensical-0.0.11" name="generator"/><title>Sony IMX500 Export for Ultralytics YOLO11 - Ultralytics YOLO Docs</title><link href="../../assets/stylesheets/modern/main.bd6182e7.min.css" rel="stylesheet"/><link href="../../assets/stylesheets/modern/palette.dfe2e883.min.css" rel="stylesheet"/><style>:root{}</style><link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/><link href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,500,500i,700,700i%7CJetBrains+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/><style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style><link href="../../stylesheets/style.css" rel="stylesheet"/>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,t)=>(e<<5)-e+t.charCodeAt(0)),0),__md_get=(e,t=localStorage,a=__md_scope)=>JSON.parse(t.getItem(a.pathname+"."+e)),__md_set=(e,t,a=localStorage,_=__md_scope)=>{try{a.setItem(_.pathname+"."+e,JSON.stringify(t))}catch(e){}},document.documentElement.setAttribute("data-platform",navigator.platform)</script>
<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-2M5EHKC0BH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-2M5EHKC0BH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-2M5EHKC0BH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
<style data-doc-kind="true">.doc-kind{display:inline-flex;align-items:center;gap:0.25em;padding:0.21em 0.59em;border-radius:999px;font-weight:700;font-size:0.81em;letter-spacing:0.06em;text-transform:uppercase;line-height:1;color:var(--doc-kind-color,#f8fafc);background:var(--doc-kind-bg,rgba(255,255,255,0.12));}.doc-kind-class{--doc-kind-color:#039dfc;--doc-kind-bg:rgba(3,157,252,0.22);}.doc-kind-function{--doc-kind-color:#fc9803;--doc-kind-bg:rgba(252,152,3,0.22);}.doc-kind-method{--doc-kind-color:#ef5eff;--doc-kind-bg:rgba(239,94,255,0.22);}.doc-kind-property{--doc-kind-color:#02e835;--doc-kind-bg:rgba(2,232,53,0.22);}</style><meta content="Sony IMX500 Export for Ultralytics YOLO11" name="title"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet"/><meta content="website" property="og:type"/><meta content="https://docs.ultralytics.com/integrations/sony-imx500/" property="og:url"/><meta content="Sony IMX500 Export for Ultralytics YOLO11" property="og:title"/><meta content="" property="og:description"/><meta content="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" property="og:image"/><meta content="summary_large_image" property="twitter:card"/><meta content="https://docs.ultralytics.com/integrations/sony-imx500/" property="twitter:url"/><meta content="Sony IMX500 Export for Ultralytics YOLO11" property="twitter:title"/><meta content="" property="twitter:description"/><meta content="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" property="twitter:image"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": ["Article", "FAQPage"], "headline": "Sony IMX500 Export for Ultralytics YOLO11", "image": ["https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png"], "datePublished": "2024-11-11 00:52:54 +0100", "dateModified": "2025-12-02 12:27:39 +0100", "author": [{"@type": "Organization", "name": "Ultralytics", "url": "https://ultralytics.com/"}], "abstract": "", "mainEntity": [{"@type": "Question", "name": "How do I export a YOLO11 model to IMX500 format for Raspberry Pi AI Camera?", "acceptedAnswer": {"@type": "Answer", "text": "To export a YOLO11 model to IMX500 format, use either the Python API or CLI command: The export process will create a directory containing the necessary files for deployment, including packerOut.zip."}}, {"@type": "Question", "name": "What are the key benefits of using the IMX500 format for edge AI deployment?", "acceptedAnswer": {"@type": "Answer", "text": "The IMX500 format offers several important advantages for edge deployment:"}}, {"@type": "Question", "name": "What hardware and software prerequisites are needed for IMX500 deployment?", "acceptedAnswer": {"@type": "Answer", "text": "For deploying IMX500 models, you'll need: Hardware: Software:"}}, {"@type": "Question", "name": "What performance can I expect from YOLO11 models on the IMX500?", "acceptedAnswer": {"@type": "Answer", "text": "Based on Ultralytics benchmarks on Raspberry Pi AI Camera: This demonstrates that IMX500 format provides efficient real-time inference while maintaining good accuracy for edge AI applications."}}]}</script></head><body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="default" dir="ltr"><input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/><input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/><label class="md-overlay" for="__drawer"></label><div data-md-component="skip"><a class="md-skip" href="#sony-imx500-export-for-ultralytics-yolo11"> Skip to content </a></div><div data-md-component="announce"><aside class="md-banner"><div class="md-banner__inner md-grid md-typeset"><a class="banner-wrapper" href="https://www.ultralytics.com/news/ultralytics-raises-30m-series-a" target="_blank"><div class="banner-content-wrapper"><img alt="Ultralytics raises $30M Series A" height="40" src="https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/691dac336b5de2ae8b398bca_writting.svg"/><div class="vc-wrapper"><img alt="Elephant" height="28" src="https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/691dac33c95408144846afc7_image%201.png"/><img alt="SquareOne" height="28" src="https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/691dac3333068d9632cc6df8_image%202.png"/></div></div></a></div></aside></div><header class="md-header md-header--shadow md-header--lifted" data-md-component="header"><nav aria-label="Header" class="md-header__inner md-grid"><a aria-label="Ultralytics YOLO Docs" class="md-header__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs"><img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/></a><label class="md-header__button md-icon" for="__drawer"><svg class="lucide lucide-menu" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 5h16M4 12h16M4 19h16"></path></svg></label><div class="md-header__title" data-md-component="header-title"><div class="md-header__ellipsis"><div class="md-header__topic"><span class="md-ellipsis"> Ultralytics YOLO Docs </span></div><div class="md-header__topic" data-md-component="header-topic"><span class="md-ellipsis"> Sony IMX500 Export for Ultralytics YOLO11 </span></div></div></div><form class="md-header__option" data-md-component="palette"><input aria-label="Switch to light mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_0" name="__palette" type="radio"/><label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg></label><input aria-label="Switch to system preference" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="black" data-md-color-scheme="slate" id="__palette_1" name="__palette" type="radio"/><label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to system preference"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label><input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_2" name="__palette" type="radio"/><label class="md-header__button md-icon" for="__palette_0" hidden="" title="Switch to dark mode"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label></form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<div class="md-header__option"><div class="md-select"><button aria-label="Select language" class="md-header__button md-icon"><svg class="lucide lucide-languages" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m5 8 6 6M4 14l6-6 2-3M2 5h12M7 2h1M22 22l-5-10-5 10M14 18h6"></path></svg></button><div class="md-select__inner"><ul class="md-select__list"><li class="md-select__item"><a class="md-select__link" href="/" hreflang="en"> üá¨üáß English </a></li><li class="md-select__item"><a class="md-select__link" href="/zh/" hreflang="zh"> üá®üá≥ ÁÆÄ‰Ωì‰∏≠Êñá </a></li><li class="md-select__item"><a class="md-select__link" href="/ko/" hreflang="ko"> üá∞üá∑ ÌïúÍµ≠Ïñ¥ </a></li><li class="md-select__item"><a class="md-select__link" href="/ja/" hreflang="ja"> üáØüáµ Êó•Êú¨Ë™û </a></li><li class="md-select__item"><a class="md-select__link" href="/ru/" hreflang="ru"> üá∑üá∫ –†—É—Å—Å–∫–∏–π </a></li><li class="md-select__item"><a class="md-select__link" href="/de/" hreflang="de"> üá©üá™ Deutsch </a></li><li class="md-select__item"><a class="md-select__link" href="/fr/" hreflang="fr"> üá´üá∑ Fran√ßais </a></li><li class="md-select__item"><a class="md-select__link" href="/es/" hreflang="es"> üá™üá∏ Espa√±ol </a></li><li class="md-select__item"><a class="md-select__link" href="/pt/" hreflang="pt"> üáµüáπ Portugu√™s </a></li><li class="md-select__item"><a class="md-select__link" href="/it/" hreflang="it"> üáÆüáπ Italiano </a></li><li class="md-select__item"><a class="md-select__link" href="/tr/" hreflang="tr"> üáπüá∑ T√ºrk√ße </a></li><li class="md-select__item"><a class="md-select__link" href="/vi/" hreflang="vi"> üáªüá≥ Ti·∫øng Vi·ªát </a></li><li class="md-select__item"><a class="md-select__link" href="/ar/" hreflang="ar"> üá∏üá¶ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© </a></li></ul></div></div></div><div class="md-header__source"><a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository"><div class="md-source__icon md-icon"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg></div><div class="md-source__repository"> ultralytics/ultralytics </div></a></div></nav><nav aria-label="Tabs" class="md-tabs" data-md-component="tabs"><div class="md-grid"><ul class="md-tabs__list"><li class="md-tabs__item"><a class="md-tabs__link" href="../.."> Home </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../quickstart/"> Quickstart </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../modes/"> Modes </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../tasks/"> Tasks </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../models/"> Models </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../datasets/"> Datasets </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../solutions/"> Solutions </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../guides/"> Guides </a></li><li class="md-tabs__item md-tabs__item--active"><a class="md-tabs__link" href="../"> Integrations </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../hub/"> HUB </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../reference/__init__/"> Reference </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../help/"> Help </a></li></ul></div></nav></header><div class="md-container" data-md-component="container"><main class="md-main" data-md-component="main"><div class="md-main__inner md-grid"><div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0"><label class="md-nav__title" for="__drawer"><a aria-label="Ultralytics YOLO Docs" class="md-nav__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs"><img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/></a> Ultralytics YOLO Docs </label><div class="md-nav__source"><a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository"><div class="md-source__icon md-icon"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg></div><div class="md-source__repository"> ultralytics/ultralytics </div></a></div><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../.."><span class="md-ellipsis"> Home </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../quickstart/"><span class="md-ellipsis"> Quickstart </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../modes/"><span class="md-ellipsis"> Modes </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../tasks/"><span class="md-ellipsis"> Tasks </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../models/"><span class="md-ellipsis"> Models </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../datasets/"><span class="md-ellipsis"> Datasets </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../solutions/"><span class="md-ellipsis"> Solutions </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../guides/"><span class="md-ellipsis"> Guides </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"><input checked="" class="md-nav__toggle md-toggle" id="__nav_9" type="checkbox"/><div class="md-nav__link md-nav__container"><a class="md-nav__link" href="../"><span class="md-ellipsis"> Integrations </span></a><label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex=""><span class="md-nav__icon md-icon"></span></label></div><nav aria-expanded="true" aria-labelledby="__nav_9_label" class="md-nav" data-md-level="1"><label class="md-nav__title" for="__nav_9"><span class="md-nav__icon md-icon"></span> Integrations </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../albumentations/"><span class="md-ellipsis"> Albumentations </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../amazon-sagemaker/"><span class="md-ellipsis"> Amazon SageMaker </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../axelera/"><span class="md-ellipsis"> Axelera </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../clearml/"><span class="md-ellipsis"> ClearML </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../comet/"><span class="md-ellipsis"> Comet ML </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../coreml/"><span class="md-ellipsis"> CoreML </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../dvc/"><span class="md-ellipsis"> DVC </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../executorch/"><span class="md-ellipsis"> ExecuTorch </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../google-colab/"><span class="md-ellipsis"> Google Colab </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../gradio/"><span class="md-ellipsis"> Gradio </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../ibm-watsonx/"><span class="md-ellipsis"> IBM Watsonx </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../jupyterlab/"><span class="md-ellipsis"> JupyterLab </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../kaggle/"><span class="md-ellipsis"> Kaggle </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../mlflow/"><span class="md-ellipsis"> MLflow </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../mnn/"><span class="md-ellipsis"> MNN </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../ncnn/"><span class="md-ellipsis"> NCNN </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../neptune/"><span class="md-ellipsis"> Neptune </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../neural-magic/"><span class="md-ellipsis"> Neural Magic </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../onnx/"><span class="md-ellipsis"> ONNX </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../openvino/"><span class="md-ellipsis"> OpenVINO </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../paddlepaddle/"><span class="md-ellipsis"> PaddlePaddle </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../paperspace/"><span class="md-ellipsis"> Paperspace Gradient </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../ray-tune/"><span class="md-ellipsis"> Ray Tune </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../roboflow/"><span class="md-ellipsis"> Roboflow </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../rockchip-rknn/"><span class="md-ellipsis"> Rockchip RKNN </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../seeedstudio-recamera/"><span class="md-ellipsis"> Seeed Studio reCamera </span></a></li><li class="md-nav__item md-nav__item--active"><input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/><label class="md-nav__link md-nav__link--active" for="__toc"><span class="md-ellipsis"> SONY IMX500 </span><span class="md-nav__icon md-icon"></span></label><a class="md-nav__link md-nav__link--active" href="./"><span class="md-ellipsis"> SONY IMX500 </span></a><nav aria-label="On this page" class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc"><span class="md-nav__icon md-icon"></span> On this page </label><ul class="md-nav__list" data-md-component="toc" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="#why-should-you-export-to-imx500"><span class="md-ellipsis"> Why Should You Export to IMX500? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#sonys-imx500-export-for-yolo11-models"><span class="md-ellipsis"> Sony's IMX500 Export for YOLO11 Models </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#supported-tasks"><span class="md-ellipsis"> Supported Tasks </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#usage-examples"><span class="md-ellipsis"> Usage Examples </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#export-arguments"><span class="md-ellipsis"> Export Arguments </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#using-imx500-export-in-deployment"><span class="md-ellipsis"> Using IMX500 Export in Deployment </span></a><nav aria-label="Using IMX500 Export in Deployment" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#hardware-prerequisites"><span class="md-ellipsis"> Hardware Prerequisites </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#software-prerequisites"><span class="md-ellipsis"> Software Prerequisites </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#benchmarks"><span class="md-ellipsis"> Benchmarks </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#whats-under-the-hood"><span class="md-ellipsis"> What's Under the Hood? </span></a><nav aria-label="What's Under the Hood?" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#sony-model-compression-toolkit-mct"><span class="md-ellipsis"> Sony Model Compression Toolkit (MCT) </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#supported-features-of-mct"><span class="md-ellipsis"> Supported Features of MCT </span></a><nav aria-label="Supported Features of MCT" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#quantization"><span class="md-ellipsis"> Quantization </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#structured-pruning"><span class="md-ellipsis"> Structured Pruning </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#imx500-converter-tool-compiler"><span class="md-ellipsis"> IMX500 Converter Tool (Compiler) </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#real-world-use-cases"><span class="md-ellipsis"> Real-World Use Cases </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#conclusion"><span class="md-ellipsis"> Conclusion </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#faq"><span class="md-ellipsis"> FAQ </span></a><nav aria-label="FAQ" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#how-do-i-export-a-yolo11-model-to-imx500-format-for-raspberry-pi-ai-camera"><span class="md-ellipsis"> How do I export a YOLO11 model to IMX500 format for Raspberry Pi AI Camera? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#what-are-the-key-benefits-of-using-the-imx500-format-for-edge-ai-deployment"><span class="md-ellipsis"> What are the key benefits of using the IMX500 format for edge AI deployment? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#what-hardware-and-software-prerequisites-are-needed-for-imx500-deployment"><span class="md-ellipsis"> What hardware and software prerequisites are needed for IMX500 deployment? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#what-performance-can-i-expect-from-yolo11-models-on-the-imx500"><span class="md-ellipsis"> What performance can I expect from YOLO11 models on the IMX500? </span></a></li></ul></nav></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="../tensorboard/"><span class="md-ellipsis"> TensorBoard </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../tensorrt/"><span class="md-ellipsis"> TensorRT </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../tf-graphdef/"><span class="md-ellipsis"> TF GraphDef </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../tf-savedmodel/"><span class="md-ellipsis"> TF SavedModel </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../tfjs/"><span class="md-ellipsis"> TF.js </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../tflite/"><span class="md-ellipsis"> TFLite </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../edge-tpu/"><span class="md-ellipsis"> TFLite Edge TPU </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../torchscript/"><span class="md-ellipsis"> TorchScript </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../vscode/"><span class="md-ellipsis"> VS Code </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../weights-biases/"><span class="md-ellipsis"> Weights &amp; Biases </span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../hub/"><span class="md-ellipsis"> HUB </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../reference/__init__/"><span class="md-ellipsis"> Reference </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../help/"><span class="md-ellipsis"> Help </span><span class="md-nav__icon md-icon"></span></a></li></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav aria-label="On this page" class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc"><span class="md-nav__icon md-icon"></span> On this page </label><ul class="md-nav__list" data-md-component="toc" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="#why-should-you-export-to-imx500"><span class="md-ellipsis"> Why Should You Export to IMX500? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#sonys-imx500-export-for-yolo11-models"><span class="md-ellipsis"> Sony's IMX500 Export for YOLO11 Models </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#supported-tasks"><span class="md-ellipsis"> Supported Tasks </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#usage-examples"><span class="md-ellipsis"> Usage Examples </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#export-arguments"><span class="md-ellipsis"> Export Arguments </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#using-imx500-export-in-deployment"><span class="md-ellipsis"> Using IMX500 Export in Deployment </span></a><nav aria-label="Using IMX500 Export in Deployment" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#hardware-prerequisites"><span class="md-ellipsis"> Hardware Prerequisites </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#software-prerequisites"><span class="md-ellipsis"> Software Prerequisites </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#benchmarks"><span class="md-ellipsis"> Benchmarks </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#whats-under-the-hood"><span class="md-ellipsis"> What's Under the Hood? </span></a><nav aria-label="What's Under the Hood?" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#sony-model-compression-toolkit-mct"><span class="md-ellipsis"> Sony Model Compression Toolkit (MCT) </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#supported-features-of-mct"><span class="md-ellipsis"> Supported Features of MCT </span></a><nav aria-label="Supported Features of MCT" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#quantization"><span class="md-ellipsis"> Quantization </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#structured-pruning"><span class="md-ellipsis"> Structured Pruning </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#imx500-converter-tool-compiler"><span class="md-ellipsis"> IMX500 Converter Tool (Compiler) </span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#real-world-use-cases"><span class="md-ellipsis"> Real-World Use Cases </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#conclusion"><span class="md-ellipsis"> Conclusion </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#faq"><span class="md-ellipsis"> FAQ </span></a><nav aria-label="FAQ" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#how-do-i-export-a-yolo11-model-to-imx500-format-for-raspberry-pi-ai-camera"><span class="md-ellipsis"> How do I export a YOLO11 model to IMX500 format for Raspberry Pi AI Camera? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#what-are-the-key-benefits-of-using-the-imx500-format-for-edge-ai-deployment"><span class="md-ellipsis"> What are the key benefits of using the IMX500 format for edge AI deployment? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#what-hardware-and-software-prerequisites-are-needed-for-imx500-deployment"><span class="md-ellipsis"> What hardware and software prerequisites are needed for IMX500 deployment? </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#what-performance-can-i-expect-from-yolo11-models-on-the-imx500"><span class="md-ellipsis"> What performance can I expect from YOLO11 models on the IMX500? </span></a></li></ul></nav></li></ul></nav></div></div></div><div class="md-content" data-md-component="content"><article class="md-content__inner md-typeset"><a class="md-content__button md-icon" href="https://github.com/ultralytics/ultralytics/tree/main/docs/en/integrations/sony-imx500.md" rel="edit" title="Edit this page"><svg class="lucide lucide-file-pen" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12.5 22H18a2 2 0 0 0 2-2V7l-5-5H6a2 2 0 0 0-2 2v9.5"></path><path d="M14 2v4a2 2 0 0 0 2 2h4M13.378 15.626a1 1 0 1 0-3.004-3.004l-5.01 5.012a2 2 0 0 0-.506.854l-.837 2.87a.5.5 0 0 0 .62.62l2.87-.837a2 2 0 0 0 .854-.506z"></path></svg></a><a class="md-content__button md-icon" href="javascript:void(0)" onclick="copyMarkdownForLLM(this); return false;" title="Copy page in Markdown format"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg></a><h1 id="sony-imx500-export-for-ultralytics-yolo11">Sony IMX500 Export for Ultralytics YOLO11</h1><p>This guide covers exporting and deploying Ultralytics YOLO11 models to Raspberry Pi AI Cameras that feature the Sony IMX500 sensor.</p><p>Deploying computer vision models on devices with limited computational power, such as <a href="https://www.raspberrypi.com/products/ai-camera/">Raspberry Pi AI Camera</a>, can be tricky. Using a model format optimized for faster performance makes a huge difference.</p><p>The IMX500 model format is designed to use minimal power while delivering fast performance for neural networks. It allows you to optimize your <a href="https://github.com/ultralytics/ultralytics">Ultralytics YOLO11</a> models for high-speed and low-power inferencing. In this guide, we'll walk you through exporting and deploying your models to the IMX500 format while making it easier for your models to perform well on the <a href="https://www.raspberrypi.com/products/ai-camera/">Raspberry Pi AI Camera</a>.</p><p align="center"><img alt="Raspberry Pi AI Camera" src="https://github.com/ultralytics/assets/releases/download/v8.3.0/ai-camera.avif" width="100%"/></p><h2 id="why-should-you-export-to-imx500">Why Should You Export to IMX500?</h2><p>Sony's <a href="https://www.aitrios.sony-semicon.com/edge-ai-devices/raspberry-pi-ai-camera">IMX500 Intelligent Vision Sensor</a> is a game-changing piece of hardware in edge AI processing. It's the world's first intelligent vision sensor with on-chip AI capabilities. This sensor helps overcome many challenges in edge AI, including data processing bottlenecks, privacy concerns, and performance limitations.
While other sensors merely pass along images and frames, the IMX500 tells a whole story. It processes data directly on the sensor, allowing devices to generate insights in real-time.</p><h2 id="sonys-imx500-export-for-yolo11-models">Sony's IMX500 Export for YOLO11 Models</h2><p>The IMX500 is designed to transform how devices handle data directly on the sensor, without needing to send it off to the cloud for processing.</p><p>The IMX500 works with quantized models. Quantization makes models smaller and faster without losing much <a href="https://www.ultralytics.com/glossary/accuracy">accuracy</a>. It is ideal for the limited resources of edge computing, allowing applications to respond quickly by reducing latency and allowing for quick data processing locally, without cloud dependency. Local processing also keeps user data private and secure since it's not sent to a remote server.</p><p><strong>IMX500 Key Features:</strong></p><ul><li><strong>Metadata Output:</strong> Instead of transmitting images only, the IMX500 can output both image and metadata (inference result), and can output metadata only for minimizing data size, reducing bandwidth, and lowering costs.</li><li><strong>Addresses Privacy Concerns:</strong> By processing data on the device, the IMX500 addresses privacy concerns, ideal for human-centric applications like person counting and occupancy tracking.</li><li><strong>Real-time Processing:</strong> Fast, on-sensor processing supports real-time decisions, perfect for edge AI applications such as autonomous systems.</li></ul><p><strong>Before You Begin:</strong> For best results, ensure your YOLO11 model is well-prepared for export by following our <a href="https://docs.ultralytics.com/modes/train/">Model Training Guide</a>, <a href="https://docs.ultralytics.com/datasets/">Data Preparation Guide</a>, and <a href="https://docs.ultralytics.com/guides/hyperparameter-tuning/">Hyperparameter Tuning Guide</a>.</p><h2 id="supported-tasks">Supported Tasks</h2><p>Currently, you can only export models that include the following tasks to IMX500 format.</p><ul><li><a href="https://docs.ultralytics.com/tasks/detect/">Object Detection</a></li><li><a href="https://docs.ultralytics.com/tasks/pose/">Pose Estimation</a></li><li><a href="https://docs.ultralytics.com/tasks/classify/">Classification</a></li><li><a href="https://docs.ultralytics.com/tasks/segment/">Instance segmentation</a></li></ul><h2 id="usage-examples">Usage Examples</h2><p>Export an Ultralytics YOLO11 model to IMX500 format and run inference with the exported model.</p><div class="admonition note"><p class="admonition-title">Note</p><p>Here we perform inference just to make sure the model works as expected. However, for deployment and inference on the Raspberry Pi AI Camera, please jump to <a href="#using-imx500-export-in-deployment">Using IMX500 Export in Deployment</a> section.</p></div><div class="admonition example"><p class="admonition-title">Object Detection</p><div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="python" name="__tabbed_1" type="radio"/><input id="cli" name="__tabbed_1" type="radio"/><div class="tabbed-labels"><label for="python">Python</label><label for="cli">CLI</label></div><div class="tabbed-content"><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="c1"># Load a YOLO11n PyTorch model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo11n.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Export the model</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">"imx"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="s2">"coco8.yaml"</span><span class="p">)</span>  <span class="c1"># exports with PTQ quantization by default</span>
<span></span>
<span></span><span class="c1"># Load the exported model</span>
<span></span><span class="n">imx_model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo11n_imx_model"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Run inference</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">imx_model</span><span class="p">(</span><span class="s2">"https://ultralytics.com/images/bus.jpg"</span><span class="p">)</span>
</code></pre></div></div><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="c1"># Export a YOLO11n PyTorch model to imx format with Post-Training Quantization (PTQ)</span>
<span></span>yolo<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">model</span><span class="o">=</span>yolo11n.pt<span class="w"> </span><span class="nv">format</span><span class="o">=</span>imx<span class="w"> </span><span class="nv">data</span><span class="o">=</span>coco8.yaml
<span></span>
<span></span><span class="c1"># Run inference with the exported model</span>
<span></span>yolo<span class="w"> </span>predict<span class="w"> </span><span class="nv">model</span><span class="o">=</span>yolo11n_imx_model<span class="w"> </span><span class="nv">source</span><span class="o">=</span><span class="s1">'https://ultralytics.com/images/bus.jpg'</span>
</code></pre></div></div></div></div></div><div class="admonition example"><p class="admonition-title">Pose Estimation</p><div class="tabbed-set tabbed-alternate" data-tabs="2:2"><input checked="checked" id="python_1" name="__tabbed_2" type="radio"/><input id="cli_1" name="__tabbed_2" type="radio"/><div class="tabbed-labels"><label for="python_1">Python</label><label for="cli_1">CLI</label></div><div class="tabbed-content"><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="c1"># Load a YOLO11n-pose PyTorch model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo11n-pose.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Export the model</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">"imx"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="s2">"coco8-pose.yaml"</span><span class="p">)</span>  <span class="c1"># exports with PTQ quantization by default</span>
<span></span>
<span></span><span class="c1"># Load the exported model</span>
<span></span><span class="n">imx_model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo11n-pose_imx_model"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Run inference</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">imx_model</span><span class="p">(</span><span class="s2">"https://ultralytics.com/images/bus.jpg"</span><span class="p">)</span>
</code></pre></div></div><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="c1"># Export a YOLO11n-pose PyTorch model to imx format with Post-Training Quantization (PTQ)</span>
<span></span>yolo<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">model</span><span class="o">=</span>yolo11n-pose.pt<span class="w"> </span><span class="nv">format</span><span class="o">=</span>imx<span class="w"> </span><span class="nv">data</span><span class="o">=</span>coco8-pose.yaml
<span></span>
<span></span><span class="c1"># Run inference with the exported model</span>
<span></span>yolo<span class="w"> </span>predict<span class="w"> </span><span class="nv">model</span><span class="o">=</span>yolo11n-pose_imx_model<span class="w"> </span><span class="nv">source</span><span class="o">=</span><span class="s1">'https://ultralytics.com/images/bus.jpg'</span>
</code></pre></div></div></div></div></div><div class="admonition example"><p class="admonition-title">Classification</p><div class="tabbed-set tabbed-alternate" data-tabs="3:2"><input checked="checked" id="python_2" name="__tabbed_3" type="radio"/><input id="cli_2" name="__tabbed_3" type="radio"/><div class="tabbed-labels"><label for="python_2">Python</label><label for="cli_2">CLI</label></div><div class="tabbed-content"><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="c1"># Load a YOLO11n-cls PyTorch model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo11n-cls.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Export the model</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">"imx"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="s2">"imagenet10"</span><span class="p">)</span>  <span class="c1"># exports with PTQ quantization by default</span>
<span></span>
<span></span><span class="c1"># Load the exported model</span>
<span></span><span class="n">imx_model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo11n-cls_imx_model"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Run inference</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">imx_model</span><span class="p">(</span><span class="s2">"https://ultralytics.com/images/bus.jpg"</span><span class="p">,</span> <span class="n">imgsz</span><span class="o">=</span><span class="mi">224</span><span class="p">)</span>
</code></pre></div></div><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="c1"># Export a YOLO11n-cls PyTorch model to imx format with Post-Training Quantization (PTQ)</span>
<span></span>yolo<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">model</span><span class="o">=</span>yolo11n-cls.pt<span class="w"> </span><span class="nv">format</span><span class="o">=</span>imx<span class="w"> </span><span class="nv">data</span><span class="o">=</span>imagenet10
<span></span>
<span></span><span class="c1"># Run inference with the exported model</span>
<span></span>yolo<span class="w"> </span>predict<span class="w"> </span><span class="nv">model</span><span class="o">=</span>yolo11n-cls_imx_model<span class="w"> </span><span class="nv">source</span><span class="o">=</span><span class="s1">'https://ultralytics.com/images/bus.jpg'</span><span class="w"> </span><span class="nv">imgsz</span><span class="o">=</span><span class="m">224</span>
</code></pre></div></div></div></div></div><div class="admonition example"><p class="admonition-title">Instance Segmentation</p><div class="tabbed-set tabbed-alternate" data-tabs="4:2"><input checked="checked" id="python_3" name="__tabbed_4" type="radio"/><input id="cli_3" name="__tabbed_4" type="radio"><div class="tabbed-labels"><label for="python_3">Python</label><label for="cli_3">CLI</label></div><div class="tabbed-content"><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="c1"># Load a YOLO11n-seg PyTorch model</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo11n-seg.pt"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Export the model</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">"imx"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="s2">"coco8-seg.yaml"</span><span class="p">)</span>  <span class="c1"># exports with PTQ quantization by default</span>
<span></span>
<span></span><span class="c1"># Load the exported model</span>
<span></span><span class="n">imx_model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo11n-seg_imx_model"</span><span class="p">)</span>
<span></span>
<span></span><span class="c1"># Run inference</span>
<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">imx_model</span><span class="p">(</span><span class="s2">"https://ultralytics.com/images/bus.jpg"</span><span class="p">)</span>
</code></pre></div></div><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="c1"># Export a YOLO11n-seg PyTorch model to imx format with Post-Training Quantization (PTQ)</span>
<span></span>yolo<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">model</span><span class="o">=</span>yolo11n-seg.pt<span class="w"> </span><span class="nv">format</span><span class="o">=</span>imx<span class="w"> </span><span class="nv">data</span><span class="o">=</span>coco8-seg.yaml
<span></span>
<span></span><span class="c1"># Run inference with the exported model</span>
<span></span>yolo<span class="w"> </span>predict<span class="w"> </span><span class="nv">model</span><span class="o">=</span>yolo11n-seg_imx_model<span class="w"> </span><span class="nv">source</span><span class="o">=</span><span class="s1">'https://ultralytics.com/images/bus.jpg'</span>
</code></pre></div></div></div></input></div></div><div class="admonition warning"><p class="admonition-title">Warning</p><p>The Ultralytics package installs additional export dependencies at runtime. The first time you run the export command, you may need to restart your console to ensure it works correctly.</p></div><h2 id="export-arguments">Export Arguments</h2><table><thead><tr><th>Argument</th><th>Type</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><code>format</code></td><td><code>str</code></td><td><code>'imx'</code></td><td>Target format for the exported model, defining compatibility with various deployment environments.</td></tr><tr><td><code>imgsz</code></td><td><code>int</code> or <code>tuple</code></td><td><code>640</code></td><td>Desired image size for the model input. Can be an integer for square images or a tuple <code>(height, width)</code> for specific dimensions.</td></tr><tr><td><code>int8</code></td><td><code>bool</code></td><td><code>True</code></td><td>Activates INT8 quantization, further compressing the model and speeding up inference with minimal <a href="https://www.ultralytics.com/glossary/accuracy">accuracy</a> loss, primarily for edge devices.</td></tr><tr><td><code>data</code></td><td><code>str</code></td><td><code>'coco8.yaml'</code></td><td>Path to the <a href="https://docs.ultralytics.com/datasets/">dataset</a> configuration file (default: <code>coco8.yaml</code>), essential for quantization.</td></tr><tr><td><code>fraction</code></td><td><code>float</code></td><td><code>1.0</code></td><td>Specifies the fraction of the dataset to use for INT8 quantization calibration. Allows for calibrating on a subset of the full dataset, useful for experiments or when resources are limited. If not specified with INT8 enabled, the full dataset will be used.</td></tr><tr><td><code>device</code></td><td><code>str</code></td><td><code>None</code></td><td>Specifies the device for exporting: GPU (<code>device=0</code>), CPU (<code>device=cpu</code>).</td></tr></tbody></table><div class="admonition tip"><p class="admonition-title">Tip</p><p>If you are exporting on a GPU with CUDA support, please pass the argument <code>device=0</code> for faster export.</p></div><p>For more details about the export process, visit the <a href="../../modes/export/">Ultralytics documentation page on exporting</a>.</p><p>The export process will create an ONNX model for quantization validation, along with a directory named <code>&lt;model-name&gt;_imx_model</code>. This directory will include the <code>packerOut.zip</code> file, which is essential for packaging the model for deployment on the IMX500 hardware. Additionally, the <code>&lt;model-name&gt;_imx_model</code> folder will contain a text file (<code>labels.txt</code>) listing all the labels associated with the model.</p><div class="admonition example"><p class="admonition-title">Folder Structure</p><div class="tabbed-set tabbed-alternate" data-tabs="5:4"><input checked="checked" id="object-detection" name="__tabbed_5" type="radio"/><input id="pose-estimation" name="__tabbed_5" type="radio"/><input id="classification" name="__tabbed_5" type="radio"/><input id="instance-segmentation" name="__tabbed_5" type="radio"/><div class="tabbed-labels"><label for="object-detection">Object Detection</label><label for="pose-estimation">Pose Estimation</label><label for="classification">Classification</label><label for="instance-segmentation">Instance Segmentation</label></div><div class="tabbed-content"><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span>yolo11n_imx_model
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>dnnParams.xml
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>labels.txt
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>packerOut.zip
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>yolo11n_imx.onnx
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>yolo11n_imx_MemoryReport.json
<span></span>‚îî‚îÄ‚îÄ<span class="w"> </span>yolo11n_imx.pbtxt
</code></pre></div></div><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span>yolo11n-pose_imx_model
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>dnnParams.xml
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>labels.txt
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>packerOut.zip
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>yolo11n-pose_imx.onnx
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>yolo11n-pose_imx_MemoryReport.json
<span></span>‚îî‚îÄ‚îÄ<span class="w"> </span>yolo11n-pose_imx.pbtxt
</code></pre></div></div><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span>yolo11n-cls_imx_model
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>dnnParams.xml
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>labels.txt
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>packerOut.zip
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>yolo11n-cls_imx.onnx
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>yolo11n-cls_imx_MemoryReport.json
<span></span>‚îî‚îÄ‚îÄ<span class="w"> </span>yolo11n-cls_imx.pbtxt
</code></pre></div></div><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span>yolo11n-seg_imx_model
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>dnnParams.xml
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>labels.txt
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>packerOut.zip
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>yolo11n-seg_imx.onnx
<span></span>‚îú‚îÄ‚îÄ<span class="w"> </span>yolo11n-seg_imx_MemoryReport.json
<span></span>‚îî‚îÄ‚îÄ<span class="w"> </span>yolo11n-seg_imx.pbtxt
</code></pre></div></div></div></div></div><h2 id="using-imx500-export-in-deployment">Using IMX500 Export in Deployment</h2><p>After exporting Ultralytics YOLO11n model to IMX500 format, it can be deployed to Raspberry Pi AI Camera for inference.</p><h3 id="hardware-prerequisites">Hardware Prerequisites</h3><p>Make sure you have the below hardware:</p><ol><li>Raspberry Pi 5 or Raspberry Pi 4 Model B</li><li>Raspberry Pi AI Camera</li></ol><p>Connect the Raspberry Pi AI camera to the 15-pin MIPI CSI connector on the Raspberry Pi and power on the Raspberry Pi</p><h3 id="software-prerequisites">Software Prerequisites</h3><div class="admonition note"><p class="admonition-title">Note</p><p>This guide has been tested with Raspberry Pi OS Bookworm running on a Raspberry Pi 5</p></div><p>Step 1: Open a terminal window and execute the following commands to update the Raspberry Pi software to the latest version.</p><div class="highlight"><pre><span></span><code><span></span>sudo<span class="w"> </span>apt<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>sudo<span class="w"> </span>apt<span class="w"> </span>full-upgrade
</code></pre></div><p>Step 2: Install IMX500 firmware which is required to operate the IMX500 sensor.</p><div class="highlight"><pre><span></span><code><span></span>sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>imx500-all
</code></pre></div><p>Step 3: Reboot Raspberry Pi for the changes to take into effect</p><div class="highlight"><pre><span></span><code><span></span>sudo<span class="w"> </span>reboot
</code></pre></div><p>Step 4: Install <a href="https://github.com/SonySemiconductorSolutions/aitrios-rpi-application-module-library">Aitrios Raspberry Pi application module library</a></p><div class="highlight"><pre><span></span><code><span></span>pip<span class="w"> </span>install<span class="w"> </span>git+https://github.com/SonySemiconductorSolutions/aitrios-rpi-application-module-library.git
</code></pre></div><p>Step 5: Run YOLO11 object detection, pose estimation, classification and segmentation by using the below scripts which are available in <a href="https://github.com/SonySemiconductorSolutions/aitrios-rpi-application-module-library/tree/main/examples/aicam">aitrios-rpi-application-module-library examples</a>.</p><div class="admonition note"><p class="admonition-title">Note</p><p>Make sure to replace <code>model_file</code> and <code>labels.txt</code> directories according to your environment before running these scripts.</p></div><div class="admonition example"><p class="admonition-title">Python Scripts</p><div class="tabbed-set tabbed-alternate" data-tabs="6:4"><input checked="checked" id="object-detection_1" name="__tabbed_6" type="radio"/><input id="pose-estimation_1" name="__tabbed_6" type="radio"/><input id="classification_1" name="__tabbed_6" type="radio"/><input id="instance-segmentation_1" name="__tabbed_6" type="radio"/><div class="tabbed-labels"><label for="object-detection_1">Object Detection</label><label for="pose-estimation_1">Pose Estimation</label><label for="classification_1">Classification</label><label for="instance-segmentation_1">Instance Segmentation</label></div><div class="tabbed-content"><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.apps</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotator</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.devices</span><span class="w"> </span><span class="kn">import</span> <span class="n">AiCamera</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">COLOR_FORMAT</span><span class="p">,</span> <span class="n">MODEL_TYPE</span><span class="p">,</span> <span class="n">Model</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.models.post_processors</span><span class="w"> </span><span class="kn">import</span> <span class="n">pp_od_yolo_ultralytics</span>
<span></span>
<span></span>
<span></span><span class="k">class</span><span class="w"> </span><span class="nc">YOLO</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""YOLO model for IMX500 deployment."""</span>
<span></span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">        </span><span class="sd">"""Initialize the YOLO model for IMX500 deployment."""</span>
<span></span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
<span></span>            <span class="n">model_file</span><span class="o">=</span><span class="s2">"yolo11n_imx_model/packerOut.zip"</span><span class="p">,</span>  <span class="c1"># replace with proper directory</span>
<span></span>            <span class="n">model_type</span><span class="o">=</span><span class="n">MODEL_TYPE</span><span class="o">.</span><span class="n">CONVERTED</span><span class="p">,</span>
<span></span>            <span class="n">color_format</span><span class="o">=</span><span class="n">COLOR_FORMAT</span><span class="o">.</span><span class="n">RGB</span><span class="p">,</span>
<span></span>            <span class="n">preserve_aspect_ratio</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span>
<span></span>            <span class="s2">"yolo11n_imx_model/labels.txt"</span><span class="p">,</span>  <span class="c1"># replace with proper directory</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
<span></span>            <span class="n">delimiter</span><span class="o">=</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="nf">post_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_tensors</span><span class="p">):</span>
<span></span><span class="w">        </span><span class="sd">"""Post-process the output tensors for object detection."""</span>
<span></span>        <span class="k">return</span> <span class="n">pp_od_yolo_ultralytics</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">)</span>
<span></span>
<span></span>
<span></span><span class="n">device</span> <span class="o">=</span> <span class="n">AiCamera</span><span class="p">(</span><span class="n">frame_rate</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>  <span class="c1"># Optimal frame rate for maximum DPS of the YOLO model running on the AI Camera</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">()</span>
<span></span><span class="n">device</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span></span>
<span></span><span class="n">annotator</span> <span class="o">=</span> <span class="n">Annotator</span><span class="p">()</span>
<span></span>
<span></span><span class="k">with</span> <span class="n">device</span> <span class="k">as</span> <span class="n">stream</span><span class="p">:</span>
<span></span>    <span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
<span></span>        <span class="n">detections</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">detections</span><span class="p">[</span><span class="n">frame</span><span class="o">.</span><span class="n">detections</span><span class="o">.</span><span class="n">confidence</span> <span class="o">&gt;</span> <span class="mf">0.55</span><span class="p">]</span>
<span></span>        <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">class_id</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">class_id</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">detections</span><span class="p">]</span>
<span></span>
<span></span>        <span class="n">annotator</span><span class="o">.</span><span class="n">annotate_boxes</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">detections</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">corner_radius</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span></span>        <span class="n">frame</span><span class="o">.</span><span class="n">display</span><span class="p">()</span>
</code></pre></div></div><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.apps</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotator</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.devices</span><span class="w"> </span><span class="kn">import</span> <span class="n">AiCamera</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">COLOR_FORMAT</span><span class="p">,</span> <span class="n">MODEL_TYPE</span><span class="p">,</span> <span class="n">Model</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.models.post_processors</span><span class="w"> </span><span class="kn">import</span> <span class="n">pp_yolo_pose_ultralytics</span>
<span></span>
<span></span>
<span></span><span class="k">class</span><span class="w"> </span><span class="nc">YOLOPose</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""YOLO pose estimation model for IMX500 deployment."""</span>
<span></span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">        </span><span class="sd">"""Initialize the YOLO pose estimation model for IMX500 deployment."""</span>
<span></span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
<span></span>            <span class="n">model_file</span><span class="o">=</span><span class="s2">"yolo11n-pose_imx_model/packerOut.zip"</span><span class="p">,</span>  <span class="c1"># replace with proper directory</span>
<span></span>            <span class="n">model_type</span><span class="o">=</span><span class="n">MODEL_TYPE</span><span class="o">.</span><span class="n">CONVERTED</span><span class="p">,</span>
<span></span>            <span class="n">color_format</span><span class="o">=</span><span class="n">COLOR_FORMAT</span><span class="o">.</span><span class="n">RGB</span><span class="p">,</span>
<span></span>            <span class="n">preserve_aspect_ratio</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="nf">post_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_tensors</span><span class="p">):</span>
<span></span><span class="w">        </span><span class="sd">"""Post-process the output tensors for pose estimation."""</span>
<span></span>        <span class="k">return</span> <span class="n">pp_yolo_pose_ultralytics</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">)</span>
<span></span>
<span></span>
<span></span><span class="n">device</span> <span class="o">=</span> <span class="n">AiCamera</span><span class="p">(</span><span class="n">frame_rate</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>  <span class="c1"># Optimal frame rate for maximum DPS of the YOLO-pose model running on the AI Camera</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOPose</span><span class="p">()</span>
<span></span><span class="n">device</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span></span>
<span></span><span class="n">annotator</span> <span class="o">=</span> <span class="n">Annotator</span><span class="p">()</span>
<span></span>
<span></span><span class="k">with</span> <span class="n">device</span> <span class="k">as</span> <span class="n">stream</span><span class="p">:</span>
<span></span>    <span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
<span></span>        <span class="n">detections</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">detections</span><span class="p">[</span><span class="n">frame</span><span class="o">.</span><span class="n">detections</span><span class="o">.</span><span class="n">confidence</span> <span class="o">&gt;</span> <span class="mf">0.4</span><span class="p">]</span>
<span></span>
<span></span>        <span class="n">annotator</span><span class="o">.</span><span class="n">annotate_keypoints</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">detections</span><span class="p">)</span>
<span></span>        <span class="n">annotator</span><span class="o">.</span><span class="n">annotate_boxes</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">detections</span><span class="p">,</span> <span class="n">corner_length</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span></span>        <span class="n">frame</span><span class="o">.</span><span class="n">display</span><span class="p">()</span>
</code></pre></div></div><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">cv2</span>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.apps</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotator</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.devices</span><span class="w"> </span><span class="kn">import</span> <span class="n">AiCamera</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">COLOR_FORMAT</span><span class="p">,</span> <span class="n">MODEL_TYPE</span><span class="p">,</span> <span class="n">Model</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.models.post_processors</span><span class="w"> </span><span class="kn">import</span> <span class="n">pp_cls</span>
<span></span>
<span></span>
<span></span><span class="k">class</span><span class="w"> </span><span class="nc">YOLOClassification</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""YOLO classification model for IMX500 deployment."""</span>
<span></span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">        </span><span class="sd">"""Initialize the YOLO classification model for IMX500 deployment."""</span>
<span></span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
<span></span>            <span class="n">model_file</span><span class="o">=</span><span class="s2">"yolo11n-cls_imx_model/packerOut.zip"</span><span class="p">,</span>  <span class="c1"># replace with proper directory</span>
<span></span>            <span class="n">model_type</span><span class="o">=</span><span class="n">MODEL_TYPE</span><span class="o">.</span><span class="n">CONVERTED</span><span class="p">,</span>
<span></span>            <span class="n">color_format</span><span class="o">=</span><span class="n">COLOR_FORMAT</span><span class="o">.</span><span class="n">RGB</span><span class="p">,</span>
<span></span>            <span class="n">preserve_aspect_ratio</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">"yolo11n-cls_imx_model/labels.txt"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="nf">post_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_tensors</span><span class="p">):</span>
<span></span><span class="w">        </span><span class="sd">"""Post-process the output tensors for classification."""</span>
<span></span>        <span class="k">return</span> <span class="n">pp_cls</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">)</span>
<span></span>
<span></span>
<span></span><span class="n">device</span> <span class="o">=</span> <span class="n">AiCamera</span><span class="p">()</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOClassification</span><span class="p">()</span>
<span></span><span class="n">device</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span></span>
<span></span><span class="n">annotator</span> <span class="o">=</span> <span class="n">Annotator</span><span class="p">()</span>
<span></span>
<span></span><span class="k">with</span> <span class="n">device</span> <span class="k">as</span> <span class="n">stream</span><span class="p">:</span>
<span></span>    <span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
<span></span>        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">frame</span><span class="o">.</span><span class="n">detections</span><span class="o">.</span><span class="n">class_id</span><span class="p">[:</span><span class="mi">3</span><span class="p">]]):</span>
<span></span>            <span class="n">text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">. </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">frame</span><span class="o">.</span><span class="n">detections</span><span class="o">.</span><span class="n">confidence</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span>
<span></span>            <span class="n">cv2</span><span class="o">.</span><span class="n">putText</span><span class="p">(</span><span class="n">frame</span><span class="o">.</span><span class="n">image</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">30</span> <span class="o">+</span> <span class="mi">40</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">cv2</span><span class="o">.</span><span class="n">FONT_HERSHEY_SIMPLEX</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
<span></span>
<span></span>        <span class="n">frame</span><span class="o">.</span><span class="n">display</span><span class="p">()</span>
</code></pre></div></div><div class="tabbed-block"><div class="highlight"><pre><span></span><code><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span></span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.apps</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotator</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.devices</span><span class="w"> </span><span class="kn">import</span> <span class="n">AiCamera</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">COLOR_FORMAT</span><span class="p">,</span> <span class="n">MODEL_TYPE</span><span class="p">,</span> <span class="n">Model</span>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">modlib.models.post_processors</span><span class="w"> </span><span class="kn">import</span> <span class="n">pp_yolo_segment_ultralytics</span>
<span></span>
<span></span><span class="k">class</span><span class="w"> </span><span class="nc">YOLOSegment</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span>
<span></span><span class="sd">"""YOLO segmentation model for IMX500 deployment."""</span>
<span></span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">        </span><span class="sd">"""Initialize the YOLO segmentation model for IMX500 deployment."""</span>
<span></span>
<span></span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
<span></span>            <span class="n">model_file</span><span class="o">=</span><span class="s2">"yolo11n-seg_imx_model/packerOut.zip"</span><span class="p">,</span>  <span class="c1"># replace with proper directory</span>
<span></span>            <span class="n">model_type</span><span class="o">=</span><span class="n">MODEL_TYPE</span><span class="o">.</span><span class="n">CONVERTED</span><span class="p">,</span>
<span></span>            <span class="n">color_format</span><span class="o">=</span><span class="n">COLOR_FORMAT</span><span class="o">.</span><span class="n">RGB</span><span class="p">,</span>
<span></span>            <span class="n">preserve_aspect_ratio</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span>
<span></span>            <span class="s2">"yolo11n-seg_imx_model/labels.txt"</span><span class="p">,</span>  <span class="c1"># replace with proper directory</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
<span></span>            <span class="n">delimiter</span><span class="o">=</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="nf">post_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_tensors</span><span class="p">):</span>
<span></span><span class="w">        </span><span class="sd">"""Post-process the output tensors for instance segmentation."""</span>
<span></span>        <span class="k">return</span> <span class="n">pp_yolo_segment_ultralytics</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">)</span>
<span></span>
<span></span><span class="n">device</span> <span class="o">=</span> <span class="n">AiCamera</span><span class="p">(</span><span class="n">frame_rate</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span> <span class="c1"># Optimal frame rate for maximum DPS of the YOLO-seg model running on the AI Camera</span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLOSegment</span><span class="p">()</span>
<span></span><span class="n">device</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span></span>
<span></span><span class="n">annotator</span> <span class="o">=</span> <span class="n">Annotator</span><span class="p">()</span>
<span></span>
<span></span><span class="k">with</span> <span class="n">device</span> <span class="k">as</span> <span class="n">stream</span><span class="p">:</span>
<span></span>    <span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
<span></span>        <span class="n">detections</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">detections</span><span class="p">[</span><span class="n">frame</span><span class="o">.</span><span class="n">detections</span><span class="o">.</span><span class="n">confidence</span> <span class="o">&gt;</span> <span class="mf">0.4</span><span class="p">]</span>
<span></span>
<span></span>        <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">detections</span><span class="p">]</span>
<span></span>        <span class="n">annotator</span><span class="o">.</span><span class="n">annotate_instance_segments</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">detections</span><span class="p">)</span>
<span></span>        <span class="n">annotator</span><span class="o">.</span><span class="n">annotate_boxes</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">detections</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span></span>        <span class="n">frame</span><span class="o">.</span><span class="n">display</span><span class="p">()</span>
</code></pre></div></div></div></div></div><h2 id="benchmarks">Benchmarks</h2><p>YOLOv8n, YOLO11n, YOLOv8n-pose, YOLO11n-pose, YOLOv8n-cls and YOLO11n-cls benchmarks below were run by the Ultralytics team on Raspberry Pi AI Camera with <code>imx</code> model format measuring speed and accuracy.</p><table><thead><tr><th>Model</th><th>Format</th><th>Size (pixels)</th><th>Size of <code>packerOut.zip</code> (MB)</th><th>mAP50-95(B)</th><th>Inference time (ms/im)</th></tr></thead><tbody><tr><td>YOLOv8n</td><td>imx</td><td>640</td><td>2.1</td><td>0.470</td><td>58.79</td></tr><tr><td>YOLO11n</td><td>imx</td><td>640</td><td>2.2</td><td>0.517</td><td>58.82</td></tr><tr><td>YOLOv8n-pose</td><td>imx</td><td>640</td><td>2.0</td><td>0.687</td><td>58.79</td></tr><tr><td>YOLO11n-pose</td><td>imx</td><td>640</td><td>2.1</td><td>0.788</td><td>62.50</td></tr></tbody></table><table><thead><tr><th>Model</th><th>Format</th><th>Size (pixels)</th><th>Size of <code>packerOut.zip</code> (MB)</th><th>acc (top1)</th><th>acc (top5)</th><th>Inference time (ms/im)</th></tr></thead><tbody><tr><td>YOLOv8n-cls</td><td>imx</td><td>224</td><td>2.3</td><td>0.25</td><td>0.5</td><td>33.31</td></tr><tr><td>YOLO11n-cls</td><td>imx</td><td>224</td><td>2.3</td><td>0.25</td><td>0.417</td><td>33.31</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Note</p><p>Validation for the above benchmarks were done using COCO128 dataset for detection models, COCO8-Pose dataset for pose estimation models and ImageNet10 for classification models.</p></div><h2 id="whats-under-the-hood">What's Under the Hood?</h2><p align="center"><img alt="IMX500 deployment" src="https://github.com/ultralytics/assets/releases/download/v8.3.0/imx500-deploy.avif" width="640"/></p><h3 id="sony-model-compression-toolkit-mct">Sony Model Compression Toolkit (MCT)</h3><p><a href="https://github.com/SonySemiconductorSolutions/mct-model-optimization">Sony's Model Compression Toolkit (MCT)</a> is a powerful tool for optimizing deep learning models through quantization and pruning. It supports various quantization methods and provides advanced algorithms to reduce model size and computational complexity without significantly sacrificing accuracy. MCT is particularly useful for deploying models on resource-constrained devices, ensuring efficient inference and reduced latency.</p><h3 id="supported-features-of-mct">Supported Features of MCT</h3><p>Sony's MCT offers a range of features designed to optimize neural network models:</p><ol><li><strong>Graph Optimizations</strong>: Transforms models into more efficient versions by folding layers like batch normalization into preceding layers.</li><li><strong>Quantization Parameter Search</strong>: Minimizes quantization noise using metrics like Mean-Square-Error, No-Clipping, and Mean-Average-Error.</li><li><strong>Advanced Quantization Algorithms</strong>:<ul><li><strong>Shift Negative Correction</strong>: Addresses performance issues from symmetric activation quantization.</li><li><strong>Outliers Filtering</strong>: Uses z-score to detect and remove outliers.</li><li><strong>Clustering</strong>: Utilizes non-uniform quantization grids for better distribution matching.</li><li><strong>Mixed-Precision Search</strong>: Assigns different quantization bit-widths per layer based on sensitivity.</li></ul></li><li><strong>Visualization</strong>: Use TensorBoard to observe model performance insights, quantization phases, and bit-width configurations.</li></ol><h4 id="quantization">Quantization</h4><p>MCT supports several quantization methods to reduce model size and improve inference speed:</p><ol><li><strong>Post-Training Quantization (PTQ)</strong>:<ul><li>Available via Keras and PyTorch APIs.</li><li>Complexity: Low</li><li>Computational Cost: Low (CPU minutes)</li></ul></li><li><strong>Gradient-based Post-Training Quantization (GPTQ)</strong>:<ul><li>Available via Keras and PyTorch APIs.</li><li>Complexity: Medium</li><li>Computational Cost: Moderate (2-3 GPU hours)</li></ul></li><li><strong>Quantization-Aware Training (QAT)</strong>:<ul><li>Complexity: High</li><li>Computational Cost: High (12-36 GPU hours)</li></ul></li></ol><p>MCT also supports various quantization schemes for weights and activations:</p><ol><li>Power-of-Two (hardware-friendly)</li><li>Symmetric</li><li>Uniform</li></ol><h4 id="structured-pruning">Structured Pruning</h4><p>MCT introduces structured, hardware-aware model pruning designed for specific hardware architectures. This technique leverages the target platform's Single Instruction, Multiple Data (SIMD) capabilities by pruning SIMD groups. This reduces model size and complexity while optimizing channel utilization, aligned with the SIMD architecture for targeted resource utilization of weights memory footprint. Available via Keras and PyTorch APIs.</p><h3 id="imx500-converter-tool-compiler">IMX500 Converter Tool (Compiler)</h3><p>The IMX500 Converter Tool is integral to the IMX500 toolset, allowing the compilation of models for deployment on Sony's IMX500 sensor (for instance, Raspberry Pi AI Cameras). This tool facilitates the transition of Ultralytics YOLO11 models processed through Ultralytics software, ensuring they are compatible and perform efficiently on the specified hardware. The export procedure following model quantization involves the generation of binary files that encapsulate essential data and device-specific configurations, streamlining the deployment process on the Raspberry Pi AI Camera.</p><h2 id="real-world-use-cases">Real-World Use Cases</h2><p>Export to IMX500 format has wide applicability across industries. Here are some examples:</p><ul><li><strong>Edge AI and IoT</strong>: Enable object detection on drones or security cameras, where real-time processing on low-power devices is essential.</li><li><strong>Wearable Devices</strong>: Deploy models optimized for small-scale AI processing on health-monitoring wearables.</li><li><strong>Smart Cities</strong>: Use IMX500-exported YOLO11 models for traffic monitoring and safety analysis with faster processing and minimal latency.</li><li><strong>Retail Analytics</strong>: Enhance in-store monitoring by deploying optimized models in point-of-sale systems or smart shelves.</li></ul><h2 id="conclusion">Conclusion</h2><p>Exporting Ultralytics YOLO11 models to Sony's IMX500 format allows you to deploy your models for efficient inference on IMX500-based cameras. By leveraging advanced quantization techniques, you can reduce model size and improve inference speed without significantly compromising accuracy.</p><p>For more information and detailed guidelines, refer to Sony's <a href="https://www.aitrios.sony-semicon.com/edge-ai-devices/raspberry-pi-ai-camera">IMX500 website</a>.</p><h2 id="faq">FAQ</h2><h3 id="how-do-i-export-a-yolo11-model-to-imx500-format-for-raspberry-pi-ai-camera">How do I export a YOLO11 model to IMX500 format for Raspberry Pi AI Camera?</h3><p>To export a YOLO11 model to IMX500 format, use either the Python API or CLI command:</p><div class="highlight"><pre><span></span><code><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
<span></span>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolo11n.pt"</span><span class="p">)</span>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">"imx"</span><span class="p">)</span>  <span class="c1"># Exports with PTQ quantization by default</span>
</code></pre></div><p>The export process will create a directory containing the necessary files for deployment, including <code>packerOut.zip</code>.</p><h3 id="what-are-the-key-benefits-of-using-the-imx500-format-for-edge-ai-deployment">What are the key benefits of using the IMX500 format for edge AI deployment?</h3><p>The IMX500 format offers several important advantages for edge deployment:</p><ul><li>On-chip AI processing reduces latency and power consumption</li><li>Outputs both image and metadata (inference result) instead of images only</li><li>Enhanced privacy by processing data locally without cloud dependency</li><li>Real-time processing capabilities ideal for time-sensitive applications</li><li>Optimized quantization for efficient model deployment on resource-constrained devices</li></ul><h3 id="what-hardware-and-software-prerequisites-are-needed-for-imx500-deployment">What hardware and software prerequisites are needed for IMX500 deployment?</h3><p>For deploying IMX500 models, you'll need:</p><p>Hardware:</p><ul><li>Raspberry Pi 5 or Raspberry Pi 4 Model B</li><li>Raspberry Pi AI Camera with IMX500 sensor</li></ul><p>Software:</p><ul><li>Raspberry Pi OS Bookworm</li><li>IMX500 firmware and tools (<code>sudo apt install imx500-all</code>)</li></ul><h3 id="what-performance-can-i-expect-from-yolo11-models-on-the-imx500">What performance can I expect from YOLO11 models on the IMX500?</h3><p>Based on Ultralytics benchmarks on Raspberry Pi AI Camera:</p><ul><li>YOLO11n achieves 62.50ms inference time per image</li><li>mAP50-95 of 0.492 on COCO128 dataset</li><li>Model size of only 3.2MB after quantization</li></ul><p>This demonstrates that IMX500 format provides efficient real-time inference while maintaining good accuracy for edge AI applications.</p><br/><br/><div class="git-info"><div class="dates-container"><span class="date-item" title="This page was first created on November 11, 2024"><span class="hover-item">üìÖ</span> Created 1 year ago </span><span class="date-item" title="This page was last updated on December 02, 2025"><span class="hover-item">‚úèÔ∏è</span> Updated 12 days ago </span></div><div class="authors-container"><a class="author-link" href="https://github.com/ambitious-octopus" title="ambitious-octopus (6 changes)"><img alt="ambitious-octopus" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/3855193?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/glenn-jocher" title="glenn-jocher (6 changes)"><img alt="glenn-jocher" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/26833433?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/lakshanthad" title="lakshanthad (6 changes)"><img alt="lakshanthad" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/20147381?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/Laughing-q" title="Laughing-q (2 changes)"><img alt="Laughing-q" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/61612323?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/onuralpszr" title="onuralpszr (1 change)"><img alt="onuralpszr" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/1688848?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/pderrenger" title="pderrenger (1 change)"><img alt="pderrenger" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/107626595?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/Y-T-G" title="Y-T-G (1 change)"><img alt="Y-T-G" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/32206511?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/ServiAmirPM" title="ServiAmirPM (1 change)"><img alt="ServiAmirPM" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/131249114?v=4&amp;s=96"/></a></div></div><div class="share-buttons"><button class="share-button hover-item" onclick="window.open('https://twitter.com/intent/tweet?url=https%3A%2F%2Fdocs.ultralytics.com%2Fintegrations%2Fsony-imx500%2F', 'TwitterShare', 'width=550,height=680,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-x-twitter"></i> Tweet </button><button class="share-button hover-item linkedin" onclick="window.open('https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fdocs.ultralytics.com%2Fintegrations%2Fsony-imx500%2F', 'LinkedinShare', 'width=550,height=730,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-linkedin-in"></i> Share </button></div><br/><h2 id="__comments">Comments</h2><div id="giscus-container"></div></article></div>
<script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div><button class="md-top md-icon" data-md-component="top" hidden="" type="button"><svg class="lucide lucide-circle-arrow-up" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><path d="m16 12-4-4-4 4M12 16V8"></path></svg> Back to top
</button></main><footer class="md-footer"><nav aria-label="Footer" class="md-footer__inner md-grid"><a aria-label="Previous: Seeed Studio reCamera" class="md-footer__link md-footer__link--prev" href="../seeedstudio-recamera/"><div class="md-footer__button md-icon"><svg class="lucide lucide-arrow-left" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m12 19-7-7 7-7M19 12H5"></path></svg></div><div class="md-footer__title"><span class="md-footer__direction"> Previous </span><div class="md-ellipsis"> Seeed Studio reCamera </div></div></a><a aria-label="Next: TensorBoard" class="md-footer__link md-footer__link--next" href="../tensorboard/"><div class="md-footer__title"><span class="md-footer__direction"> Next </span><div class="md-ellipsis"> TensorBoard </div></div><div class="md-footer__button md-icon"><svg class="lucide lucide-arrow-right" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M5 12h14M12 5l7 7-7 7"></path></svg></div></a></nav><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class="md-copyright"><div class="md-copyright__highlight"><a href="https://www.ultralytics.com/" target="_blank">¬© 2025 Ultralytics Inc.</a> All rights reserved. </div> Made with <a href="https://zensical.org/" rel="noopener" target="_blank"> Zensical </a></div><div class="md-social"><a class="md-social__link" href="https://github.com/ultralytics" rel="noopener" target="_blank" title="github.com"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://www.linkedin.com/company/ultralytics/" rel="noopener" target="_blank" title="www.linkedin.com"><svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://x.com/ultralytics" rel="noopener" target="_blank" title="x.com"><svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://youtube.com/ultralytics?sub_confirmation=1" rel="noopener" target="_blank" title="youtube.com"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M549.7 124.1c-6.2-23.7-24.8-42.3-48.3-48.6C458.9 64 288.1 64 288.1 64S117.3 64 74.7 75.5c-23.5 6.3-42 24.9-48.3 48.6C15 167 15 256.4 15 256.4s0 89.4 11.4 132.3c6.3 23.6 24.8 41.5 48.3 47.8C117.3 448 288.1 448 288.1 448s170.8 0 213.4-11.5c23.5-6.3 42-24.2 48.3-47.8 11.4-42.9 11.4-132.3 11.4-132.3s0-89.4-11.4-132.3zM232.2 337.6V175.2l142.7 81.2z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://hub.docker.com/r/ultralytics/ultralytics/" rel="noopener" target="_blank" title="hub.docker.com"><svg viewbox="0 0 640 512" xmlns="http://www.w3.org/2000/svg"><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://pypi.org/project/ultralytics/" rel="noopener" target="_blank" title="pypi.org"><svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 444.7a20.4 20.4 0 1 1 0-40.7 20.4 20.4 0 1 1 0 40.7M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.6-183.4a20.4 20.4 0 1 1 0 40.8 20.4 20.4 0 1 1 0-40.8" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://discord.com/invite/ultralytics" rel="noopener" target="_blank" title="discord.com"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M492.5 69.8c-.2-.3-.4-.6-.8-.7-38.1-17.5-78.4-30-119.7-37.1-.4-.1-.8 0-1.1.1s-.6.4-.8.8c-5.5 9.9-10.5 20.2-14.9 30.6-44.6-6.8-89.9-6.8-134.4 0-4.5-10.5-9.5-20.7-15.1-30.6-.2-.3-.5-.6-.8-.8s-.7-.2-1.1-.2C162.5 39 122.2 51.5 84.1 69c-.3.1-.6.4-.8.7C7.1 183.5-13.8 294.6-3.6 404.2c0 .3.1.5.2.8s.3.4.5.6c44.4 32.9 94 58 146.8 74.2.4.1.8.1 1.1 0s.7-.4.9-.7c11.3-15.4 21.4-31.8 30-48.8.1-.2.2-.5.2-.8s0-.5-.1-.8-.2-.5-.4-.6-.4-.3-.7-.4c-15.8-6.1-31.2-13.4-45.9-21.9-.3-.2-.5-.4-.7-.6s-.3-.6-.3-.9 0-.6.2-.9.3-.5.6-.7c3.1-2.3 6.2-4.7 9.1-7.1.3-.2.6-.4.9-.4s.7 0 1 .1c96.2 43.9 200.4 43.9 295.5 0 .3-.1.7-.2 1-.2s.7.2.9.4c2.9 2.4 6 4.9 9.1 7.2.2.2.4.4.6.7s.2.6.2.9-.1.6-.3.9-.4.5-.6.6c-14.7 8.6-30 15.9-45.9 21.8-.2.1-.5.2-.7.4s-.3.4-.4.7-.1.5-.1.8.1.5.2.8c8.8 17 18.8 33.3 30 48.8.2.3.6.6.9.7s.8.1 1.1 0c52.9-16.2 102.6-41.3 147.1-74.2.2-.2.4-.4.5-.6s.2-.5.2-.8c12.3-126.8-20.5-236.9-86.9-334.5zm-302 267.7c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.4 59.2-52.8 59.2m195.4 0c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.2 59.2-52.8 59.2" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://reddit.com/r/ultralytics" rel="noopener" target="_blank" title="reddit.com"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M0 256C0 114.6 114.6 0 256 0s256 114.6 256 256-114.6 256-256 256H37.1c-13.7 0-20.5-16.5-10.9-26.2L75 437C28.7 390.7 0 326.7 0 256m349.6-102.4c23.6 0 42.7-19.1 42.7-42.7s-19.1-42.7-42.7-42.7c-20.6 0-37.8 14.6-41.8 34-34.5 3.7-61.4 33-61.4 68.4v.2c-37.5 1.6-71.8 12.3-99 29.1-10.1-7.8-22.8-12.5-36.5-12.5-33 0-59.8 26.8-59.8 59.8 0 24 14.1 44.6 34.4 54.1 2 69.4 77.6 125.2 170.6 125.2s168.7-55.9 170.6-125.3c20.2-9.6 34.1-30.2 34.1-54 0-33-26.8-59.8-59.8-59.8-13.7 0-26.3 4.6-36.4 12.4-27.4-17-62.1-27.7-100-29.1v-.2c0-25.4 18.9-46.5 43.4-49.9 4.4 18.8 21.3 32.8 41.5 32.8zm-172.5 93.3c16.7 0 29.5 17.6 28.5 39.3s-13.5 29.6-30.3 29.6-31.4-8.8-30.4-30.5S160.3 247 177 247zm190.1 38.3c1 21.7-13.7 30.5-30.4 30.5s-29.3-7.9-30.3-29.6 11.8-39.3 28.5-39.3 31.2 16.6 32.1 38.3zm-48.1 56.7c-10.3 24.6-34.6 41.9-63 41.9s-52.7-17.3-63-41.9c-1.2-2.9.8-6.2 3.9-6.5 18.4-1.9 38.3-2.9 59.1-2.9s40.7 1 59.1 2.9c3.1.3 5.1 3.6 3.9 6.5" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://weixin.qq.com/r/mp/LxckPDfEgWr_rXNf90I9" rel="noopener" target="_blank" title="weixin.qq.com"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M385.2 167.6c6.4 0 12.6.3 18.8 1.1C387.4 90.3 303.3 32 207.7 32 100.5 32 13 104.8 13 197.4c0 53.4 29.3 97.5 77.9 131.6l-19.3 58.6 68-34.1c24.4 4.8 43.8 9.7 68.2 9.7 6.2 0 12.1-.3 18.3-.8-4-12.9-6.2-26.6-6.2-40.8-.1-84.9 72.9-154 165.3-154m-104.5-52.9c14.5 0 24.2 9.7 24.2 24.4 0 14.5-9.7 24.2-24.2 24.2-14.8 0-29.3-9.7-29.3-24.2.1-14.7 14.6-24.4 29.3-24.4m-136.4 48.6c-14.5 0-29.3-9.7-29.3-24.2 0-14.8 14.8-24.4 29.3-24.4 14.8 0 24.4 9.7 24.4 24.4 0 14.6-9.6 24.2-24.4 24.2M563 319.4c0-77.9-77.9-141.3-165.4-141.3-92.7 0-165.4 63.4-165.4 141.3S305 460.7 397.6 460.7c19.3 0 38.9-5.1 58.6-9.9l53.4 29.3-14.8-48.6C534 402.1 563 363.2 563 319.4m-219.1-24.5c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.8 0 24.4 9.7 24.4 19.3 0 10-9.7 19.6-24.4 19.6m107.1 0c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.5 0 24.4 9.7 24.4 19.3.1 10-9.9 19.6-24.4 19.6" fill="currentColor"></path></svg></a></div></div></div></footer></div><div class="md-dialog" data-md-component="dialog"><div class="md-dialog__inner md-typeset"></div></div><div class="md-progress" data-md-component="progress" role="progressbar"></div>
<script id="__config" type="application/json">{"annotate":null,"base":"../..","features":["content.action.edit","content.code.annotate","content.code.copy","content.tooltips","toc.follow","navigation.top","navigation.tabs","navigation.tabs.sticky","navigation.prune","navigation.footer","navigation.tracking","navigation.instant","navigation.instant.progress","navigation.indexes","navigation.sections","content.tabs.link"],"search":"../../assets/javascripts/workers/search.5df7522c.min.js","tags":null,"translations":{"clipboard.copied":"Copied to clipboard","clipboard.copy":"Copy to clipboard","search.result.more.one":"1 more on this page","search.result.more.other":"# more on this page","search.result.none":"No matching documents","search.result.one":"1 matching document","search.result.other":"# matching documents","search.result.placeholder":"Type to start searching","search.result.term.missing":"Missing","select.version":"Select version"},"version":null}</script>
<script src="../../assets/javascripts/bundle.21aa498e.min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/ultralytics/llm@v0.1.6/js/chat.min.js"></script>
<script src="https://unpkg.com/tablesort@5.6.0/dist/tablesort.min.js"></script>
<script src="../../javascript/extra.js"></script>
<script src="../../javascript/giscus.js"></script>
<script src="../../javascript/tablesort.js"></script>
<script>
            async function copyMarkdownForLLM(button) {
                const editBtn = document.querySelector('a[title="Edit this page"]');
                if (!editBtn) return;

                const originalHTML = button.innerHTML;
                const checkIcon = '<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 16.17L4.83 12l-1.42 1.41L9 19L21 7l-1.41-1.41L9 16.17z"></path></svg>';

                let rawUrl = editBtn.href.replace('github.com', 'raw.githubusercontent.com');
                rawUrl = rawUrl.replace('/blob/', '/').replace('/tree/', '/');

                try {
                    const response = await fetch(rawUrl);
                    let markdown = await response.text();

                    if (markdown.startsWith('---')) {
                        const frontMatterEnd = markdown.indexOf('\n---\n', 3);
                        if (frontMatterEnd !== -1) {
                            markdown = markdown.substring(frontMatterEnd + 5).trim();
                        }
                    }

                    const title = document.querySelector('h1')?.textContent || document.title;
                    const content = `# ${title}\n\nSource: ${window.location.href}\n\n---\n\n${markdown}`;

                    await navigator.clipboard.writeText(content);
                    button.innerHTML = checkIcon + ' Copied!';
                    setTimeout(() => { button.innerHTML = originalHTML; }, 2000);
                } catch (err) {
                    button.innerHTML = '‚ùå Failed';
                    setTimeout(() => { button.innerHTML = originalHTML; }, 2000);
                }
            }
            </script></body></html>