 <!DOCTYPE html><html class="no-js" lang="en"><head><meta charset="utf-8"/><meta content="width=device-width,initial-scale=1" name="viewport"/><meta content="Explore Ultralytics SAM and SAM 2 Predictor for advanced, real-time image segmentation using the Segment Anything Model (SAM and SAM 2). Complete implementation details and auxiliary utilities." name="description"/><meta content="Ultralytics" name="author"/><link href="https://docs.ultralytics.com/reference/models/sam/predict/" rel="canonical"/><link href="../modules/utils/" rel="prev"/><link href="../sam3/decoder/" rel="next"/><link href="/" hreflang="en" rel="alternate"/><link href="/zh/" hreflang="zh" rel="alternate"/><link href="/ko/" hreflang="ko" rel="alternate"/><link href="/ja/" hreflang="ja" rel="alternate"/><link href="/ru/" hreflang="ru" rel="alternate"/><link href="/de/" hreflang="de" rel="alternate"/><link href="/fr/" hreflang="fr" rel="alternate"/><link href="/es/" hreflang="es" rel="alternate"/><link href="/pt/" hreflang="pt" rel="alternate"/><link href="/it/" hreflang="it" rel="alternate"/><link href="/tr/" hreflang="tr" rel="alternate"/><link href="/vi/" hreflang="vi" rel="alternate"/><link href="/ar/" hreflang="ar" rel="alternate"/><link href="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/logo/favicon-yolo.png" rel="icon"/><meta content="zensical-0.0.15" name="generator"/><title>Reference for ultralytics/models/sam/predict.py</title><link href="../../../../assets/stylesheets/modern/main.f1b6466b.min.css" rel="stylesheet"/><link href="../../../../assets/stylesheets/modern/palette.dfe2e883.min.css" rel="stylesheet"/><style>:root{}</style><link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/><link href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,500,500i,700,700i%7CJetBrains+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/><style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style><link href="../../../../stylesheets/style.css" rel="stylesheet"/>
<script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,t)=>(e<<5)-e+t.charCodeAt(0)),0),__md_get=(e,t=localStorage,a=__md_scope)=>JSON.parse(t.getItem(a.pathname+"."+e)),__md_set=(e,t,a=localStorage,_=__md_scope)=>{try{a.setItem(_.pathname+"."+e,JSON.stringify(t))}catch(e){}},document.documentElement.setAttribute("data-platform",navigator.platform)</script>
<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-2M5EHKC0BH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-2M5EHKC0BH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-2M5EHKC0BH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
<style data-doc-kind="true">.doc-kind{display:inline-flex;align-items:center;gap:0.25em;padding:0.21em 0.59em;border-radius:999px;font-weight:700;font-size:0.81em;letter-spacing:0.06em;text-transform:uppercase;line-height:1;color:var(--doc-kind-color,#f8fafc);background:var(--doc-kind-bg,rgba(255,255,255,0.12));}.doc-kind-class{--doc-kind-color:#039dfc;--doc-kind-bg:rgba(3,157,252,0.22);}.doc-kind-function{--doc-kind-color:#fc9803;--doc-kind-bg:rgba(252,152,3,0.22);}.doc-kind-method{--doc-kind-color:#ef5eff;--doc-kind-bg:rgba(239,94,255,0.22);}.doc-kind-property{--doc-kind-color:#02e835;--doc-kind-bg:rgba(2,232,53,0.22);}</style><meta content="Reference for ultralytics/models/sam/predict.py" name="title"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet"/><meta content="website" property="og:type"/><meta content="https://docs.ultralytics.com/reference/models/sam/predict/" property="og:url"/><meta content="Reference for ultralytics/models/sam/predict.py" property="og:title"/><meta content="" property="og:description"/><meta content="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" property="og:image"/><meta content="summary_large_image" property="twitter:card"/><meta content="https://docs.ultralytics.com/reference/models/sam/predict/" property="twitter:url"/><meta content="Reference for ultralytics/models/sam/predict.py" property="twitter:title"/><meta content="" property="twitter:description"/><meta content="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" property="twitter:image"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": "Article", "headline": "Reference for ultralytics/models/sam/predict.py", "image": ["https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png"], "datePublished": "2023-11-12 02:49:37 +0100", "dateModified": "2025-12-12 21:04:33 +0800", "author": [{"@type": "Organization", "name": "Ultralytics", "url": "https://ultralytics.com/"}], "abstract": ""}</script></head><body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="default" dir="ltr"><input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/><input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/><label class="md-overlay" for="__drawer"></label><div data-md-component="skip"><a class="md-skip" href="#reference-for-ultralyticsmodelssampredictpy"> Skip to content </a></div><div data-md-component="announce"><aside class="md-banner"><div class="md-banner__inner md-grid md-typeset"><a class="banner-wrapper" href="https://www.ultralytics.com/news/ultralytics-raises-30m-series-a" target="_blank"><div class="banner-content-wrapper"><img alt="Ultralytics raises $30M Series A" height="40" src="https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/691dac336b5de2ae8b398bca_writting.svg"/><div class="vc-wrapper"><img alt="Elephant" height="28" src="https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/691dac33c95408144846afc7_image%201.png"/><img alt="SquareOne" height="28" src="https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/691dac3333068d9632cc6df8_image%202.png"/></div></div></a></div></aside></div><header class="md-header md-header--shadow md-header--lifted" data-md-component="header"><nav aria-label="Header" class="md-header__inner md-grid"><a aria-label="Ultralytics YOLO Docs" class="md-header__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs"><img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/></a><label class="md-header__button md-icon" for="__drawer"><svg class="lucide lucide-menu" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 5h16M4 12h16M4 19h16"></path></svg></label><div class="md-header__title" data-md-component="header-title"><div class="md-header__ellipsis"><div class="md-header__topic"><span class="md-ellipsis"> Ultralytics YOLO Docs </span></div><div class="md-header__topic" data-md-component="header-topic"><span class="md-ellipsis"> Reference for ultralytics/models/sam/predict.py </span></div></div></div><form class="md-header__option" data-md-component="palette"><input aria-label="Switch to light mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_0" name="__palette" type="radio"/><label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg></label><input aria-label="Switch to system preference" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="black" data-md-color-scheme="slate" id="__palette_1" name="__palette" type="radio"/><label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to system preference"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label><input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_2" name="__palette" type="radio"/><label class="md-header__button md-icon" for="__palette_0" hidden="" title="Switch to dark mode"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label></form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<div class="md-header__option"><div class="md-select"><button aria-label="Select language" class="md-header__button md-icon"><svg class="lucide lucide-languages" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m5 8 6 6M4 14l6-6 2-3M2 5h12M7 2h1M22 22l-5-10-5 10M14 18h6"></path></svg></button><div class="md-select__inner"><ul class="md-select__list"><li class="md-select__item"><a class="md-select__link" href="/" hreflang="en"> ğŸ‡¬ğŸ‡§ English </a></li><li class="md-select__item"><a class="md-select__link" href="/zh/" hreflang="zh"> ğŸ‡¨ğŸ‡³ ç®€ä½“ä¸­æ–‡ </a></li><li class="md-select__item"><a class="md-select__link" href="/ko/" hreflang="ko"> ğŸ‡°ğŸ‡· í•œêµ­ì–´ </a></li><li class="md-select__item"><a class="md-select__link" href="/ja/" hreflang="ja"> ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª </a></li><li class="md-select__item"><a class="md-select__link" href="/ru/" hreflang="ru"> ğŸ‡·ğŸ‡º Ğ ÑƒÑÑĞºĞ¸Ğ¹ </a></li><li class="md-select__item"><a class="md-select__link" href="/de/" hreflang="de"> ğŸ‡©ğŸ‡ª Deutsch </a></li><li class="md-select__item"><a class="md-select__link" href="/fr/" hreflang="fr"> ğŸ‡«ğŸ‡· FranÃ§ais </a></li><li class="md-select__item"><a class="md-select__link" href="/es/" hreflang="es"> ğŸ‡ªğŸ‡¸ EspaÃ±ol </a></li><li class="md-select__item"><a class="md-select__link" href="/pt/" hreflang="pt"> ğŸ‡µğŸ‡¹ PortuguÃªs </a></li><li class="md-select__item"><a class="md-select__link" href="/it/" hreflang="it"> ğŸ‡®ğŸ‡¹ Italiano </a></li><li class="md-select__item"><a class="md-select__link" href="/tr/" hreflang="tr"> ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e </a></li><li class="md-select__item"><a class="md-select__link" href="/vi/" hreflang="vi"> ğŸ‡»ğŸ‡³ Tiáº¿ng Viá»‡t </a></li><li class="md-select__item"><a class="md-select__link" href="/ar/" hreflang="ar"> ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© </a></li></ul></div></div></div><div class="md-header__source"><a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository"><div class="md-source__icon md-icon"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg></div><div class="md-source__repository"> ultralytics/ultralytics </div></a></div></nav><nav aria-label="Tabs" class="md-tabs" data-md-component="tabs"><div class="md-grid"><ul class="md-tabs__list"><li class="md-tabs__item"><a class="md-tabs__link" href="../../../.."> Home </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../quickstart/"> Quickstart </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../modes/"> Modes </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../tasks/"> Tasks </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../models/"> Models </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../datasets/"> Datasets </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../solutions/"> Solutions </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../guides/"> Guides </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../integrations/"> Integrations </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../hub/"> HUB </a></li><li class="md-tabs__item md-tabs__item--active"><a class="md-tabs__link" href="../../../__init__/"> Reference </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../help/"> Help </a></li></ul></div></nav></header><div class="md-container" data-md-component="container"><main class="md-main" data-md-component="main"><div class="md-main__inner md-grid"><div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0"><label class="md-nav__title" for="__drawer"><a aria-label="Ultralytics YOLO Docs" class="md-nav__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs"><img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/></a> Ultralytics YOLO Docs </label><div class="md-nav__source"><a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository"><div class="md-source__icon md-icon"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg></div><div class="md-source__repository"> ultralytics/ultralytics </div></a></div><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../.."><span class="md-ellipsis"> Home </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../quickstart/"><span class="md-ellipsis"> Quickstart </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../modes/"><span class="md-ellipsis"> Modes </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../tasks/"><span class="md-ellipsis"> Tasks </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../models/"><span class="md-ellipsis"> Models </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../datasets/"><span class="md-ellipsis"> Datasets </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../solutions/"><span class="md-ellipsis"> Solutions </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../guides/"><span class="md-ellipsis"> Guides </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../integrations/"><span class="md-ellipsis"> Integrations </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../hub/"><span class="md-ellipsis"> HUB </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"><input checked="" class="md-nav__toggle md-toggle" id="__nav_11" type="checkbox"/><label class="md-nav__link" for="__nav_11" id="__nav_11_label" tabindex=""><span class="md-ellipsis"> Reference </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="true" aria-labelledby="__nav_11_label" class="md-nav" data-md-level="1"><label class="md-nav__title" for="__nav_11"><span class="md-nav__icon md-icon"></span> Reference </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../__init__/"><span class="md-ellipsis"> __init__ </span></a></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_2" type="checkbox"/><label class="md-nav__link" for="__nav_11_2" id="__nav_11_2_label" tabindex=""><span class="md-ellipsis"> cfg </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_2_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_2"><span class="md-nav__icon md-icon"></span> cfg </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../cfg/__init__/"><span class="md-ellipsis"> __init__ </span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_3" type="checkbox"/><label class="md-nav__link" for="__nav_11_3" id="__nav_11_3_label" tabindex=""><span class="md-ellipsis"> data </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_3_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_3"><span class="md-nav__icon md-icon"></span> data </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../data/annotator/"><span class="md-ellipsis"> annotator </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/augment/"><span class="md-ellipsis"> augment </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/base/"><span class="md-ellipsis"> base </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/build/"><span class="md-ellipsis"> build </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/converter/"><span class="md-ellipsis"> converter </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/dataset/"><span class="md-ellipsis"> dataset </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/loaders/"><span class="md-ellipsis"> loaders </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/split/"><span class="md-ellipsis"> split </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/split_dota/"><span class="md-ellipsis"> split_dota </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/utils/"><span class="md-ellipsis"> utils </span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_4" type="checkbox"/><label class="md-nav__link" for="__nav_11_4" id="__nav_11_4_label" tabindex=""><span class="md-ellipsis"> engine </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_4_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_4"><span class="md-nav__icon md-icon"></span> engine </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../engine/exporter/"><span class="md-ellipsis"> exporter </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../engine/model/"><span class="md-ellipsis"> model </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../engine/predictor/"><span class="md-ellipsis"> predictor </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../engine/results/"><span class="md-ellipsis"> results </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../engine/trainer/"><span class="md-ellipsis"> trainer </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../engine/tuner/"><span class="md-ellipsis"> tuner </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../engine/validator/"><span class="md-ellipsis"> validator </span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_5" type="checkbox"/><label class="md-nav__link" for="__nav_11_5" id="__nav_11_5_label" tabindex=""><span class="md-ellipsis"> hub </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_5_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_5"><span class="md-nav__icon md-icon"></span> hub </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../hub/__init__/"><span class="md-ellipsis"> __init__ </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../hub/auth/"><span class="md-ellipsis"> auth </span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../hub/google/__init__/"><span class="md-ellipsis"> google </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../hub/session/"><span class="md-ellipsis"> session </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../hub/utils/"><span class="md-ellipsis"> utils </span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"><input checked="" class="md-nav__toggle md-toggle" id="__nav_11_6" type="checkbox"/><label class="md-nav__link" for="__nav_11_6" id="__nav_11_6_label" tabindex=""><span class="md-ellipsis"> models </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="true" aria-labelledby="__nav_11_6_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_6"><span class="md-nav__icon md-icon"></span> models </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../fastsam/model/"><span class="md-ellipsis"> fastsam </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../nas/model/"><span class="md-ellipsis"> nas </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../rtdetr/model/"><span class="md-ellipsis"> rtdetr </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input checked="" class="md-nav__toggle md-toggle" id="__nav_11_6_4" type="checkbox"/><label class="md-nav__link" for="__nav_11_6_4" id="__nav_11_6_4_label" tabindex="0"><span class="md-ellipsis"> sam </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="true" aria-labelledby="__nav_11_6_4_label" class="md-nav" data-md-level="3"><label class="md-nav__title" for="__nav_11_6_4"><span class="md-nav__icon md-icon"></span> sam </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../amg/"><span class="md-ellipsis"> amg </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../build/"><span class="md-ellipsis"> build </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../build_sam3/"><span class="md-ellipsis"> build_sam3 </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../model/"><span class="md-ellipsis"> model </span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../modules/blocks/"><span class="md-ellipsis"> modules </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--active"><input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/><label class="md-nav__link md-nav__link--active" for="__toc"><span class="md-ellipsis"> predict </span><span class="md-nav__icon md-icon"></span></label><a class="md-nav__link md-nav__link--active" href="./"><span class="md-ellipsis"> predict </span></a><nav aria-label="On this page" class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc"><span class="md-nav__icon md-icon"></span> On this page </label><ul class="md-nav__list" data-md-component="toc" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> Predictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.Predictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor._inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor._prepare_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prepare_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.generate"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> generate</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.postprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> postprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.pre_transform"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> pre_transform</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.preprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> preprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.prompt_inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> prompt_inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.remove_small_regions"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> remove_small_regions</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.reset_image"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> reset_image</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.set_image"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> set_image</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.set_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> set_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.setup_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.setup_source"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_source</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM2Predictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM2Predictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor._inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor._prepare_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prepare_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor.setup_source"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_source</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM2VideoPredictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM2VideoPredictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._add_output_per_object"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _add_output_per_object</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._clear_non_cond_mem_around_input"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _clear_non_cond_mem_around_input</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._consolidate_temp_output_across_obj"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _consolidate_temp_output_across_obj</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_empty_mask_ptr"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _get_empty_mask_ptr</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_maskmem_pos_enc"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _get_maskmem_pos_enc</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._init_state"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _init_state</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._obj_id_to_idx"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _obj_id_to_idx</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._prune_non_cond_memory"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prune_non_cond_memory</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._reset_tracking_results"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _reset_tracking_results</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_memory_encoder"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _run_memory_encoder</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_single_frame_inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _run_single_frame_inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.add_new_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> add_new_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.clear_all_points_in_frame"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> clear_all_points_in_frame</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.clear_all_points_in_video"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> clear_all_points_in_video</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.init_state"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> init_state</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.postprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> postprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.propagate_in_video_preflight"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> propagate_in_video_preflight</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.remove_object"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> remove_object</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM2DynamicInteractivePredictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._obj_id_to_idx"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _obj_id_to_idx</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._prepare_memory_conditioned_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prepare_memory_conditioned_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_maskmem_enc"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_maskmem_enc</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.track_step"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> track_step</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.update_memory"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> update_memory</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3Predictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM3Predictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM3Predictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3Predictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3Predictor.setup_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_model</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM3SemanticPredictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM3SemanticPredictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor._get_dummy_prompt"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _get_dummy_prompt</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor._inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor._prepare_geometric_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prepare_geometric_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.postprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> postprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.pre_transform"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> pre_transform</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.reset_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> reset_prompts</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoPredictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM3VideoPredictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM3VideoPredictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoPredictor.propagate_in_video"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> propagate_in_video</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM3VideoSemanticPredictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM3VideoSemanticPredictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._apply_object_wise_non_overlapping_constraints"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _apply_object_wise_non_overlapping_constraints</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._associate_det_trk"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _associate_det_trk</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._cache_backbone_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _cache_backbone_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._det_track_one_frame"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _det_track_one_frame</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._drop_new_det_with_obj_limit"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _drop_new_det_with_obj_limit</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._extract_detection_outputs"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _extract_detection_outputs</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._initialize_metadata"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _initialize_metadata</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._process_hotstart"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _process_hotstart</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._propogate_tracker_one_frame_local_gpu"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _propogate_tracker_one_frame_local_gpu</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._recondition_masklets"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _recondition_masklets</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._run_single_frame_inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _run_single_frame_inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._suppress_detections_close_to_boundary"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _suppress_detections_close_to_boundary</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._suppress_overlapping_based_on_recent_occlusion"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _suppress_overlapping_based_on_recent_occlusion</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_add_new_objects"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _tracker_add_new_objects</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_remove_objects"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _tracker_remove_objects</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_update_memories"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _tracker_update_memories</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.add_prompt"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> add_prompt</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.build_outputs"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> build_outputs</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.init_state"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> init_state</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.postprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> postprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_backbone_and_detection"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> run_backbone_and_detection</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_propagation"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> run_tracker_propagation</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_update_execution_phase"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> run_tracker_update_execution_phase</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_update_planning_phase"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> run_tracker_update_planning_phase</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.setup_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.setup_source"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_source</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.update_masklet_confirmation_status"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> update_masklet_confirmation_status</span></a></li></ul></nav></li></ul></nav></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../sam3/decoder/"><span class="md-ellipsis"> sam3 </span><span class="md-nav__icon md-icon"></span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../utils/loss/"><span class="md-ellipsis"> utils </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../yolo/classify/predict/"><span class="md-ellipsis"> yolo </span><span class="md-nav__icon md-icon"></span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_7" type="checkbox"/><label class="md-nav__link" for="__nav_11_7" id="__nav_11_7_label" tabindex=""><span class="md-ellipsis"> nn </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_7_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_7"><span class="md-nav__icon md-icon"></span> nn </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../nn/autobackend/"><span class="md-ellipsis"> autobackend </span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../nn/modules/activation/"><span class="md-ellipsis"> modules </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../nn/tasks/"><span class="md-ellipsis"> tasks </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../nn/text_model/"><span class="md-ellipsis"> text_model </span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_8" type="checkbox"/><label class="md-nav__link" for="__nav_11_8" id="__nav_11_8_label" tabindex=""><span class="md-ellipsis"> solutions </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_8_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_8"><span class="md-nav__icon md-icon"></span> solutions </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/ai_gym/"><span class="md-ellipsis"> ai_gym </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/analytics/"><span class="md-ellipsis"> analytics </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/config/"><span class="md-ellipsis"> config </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/distance_calculation/"><span class="md-ellipsis"> distance_calculation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/heatmap/"><span class="md-ellipsis"> heatmap </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/instance_segmentation/"><span class="md-ellipsis"> instance_segmentation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/object_blurrer/"><span class="md-ellipsis"> object_blurrer </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/object_counter/"><span class="md-ellipsis"> object_counter </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/object_cropper/"><span class="md-ellipsis"> object_cropper </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/parking_management/"><span class="md-ellipsis"> parking_management </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/queue_management/"><span class="md-ellipsis"> queue_management </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/region_counter/"><span class="md-ellipsis"> region_counter </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/security_alarm/"><span class="md-ellipsis"> security_alarm </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/similarity_search/"><span class="md-ellipsis"> similarity_search </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/solutions/"><span class="md-ellipsis"> solutions </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/speed_estimation/"><span class="md-ellipsis"> speed_estimation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/streamlit_inference/"><span class="md-ellipsis"> streamlit_inference </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/trackzone/"><span class="md-ellipsis"> trackzone </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/vision_eye/"><span class="md-ellipsis"> vision_eye </span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_9" type="checkbox"/><label class="md-nav__link" for="__nav_11_9" id="__nav_11_9_label" tabindex=""><span class="md-ellipsis"> trackers </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_9_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_9"><span class="md-nav__icon md-icon"></span> trackers </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../trackers/basetrack/"><span class="md-ellipsis"> basetrack </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../trackers/bot_sort/"><span class="md-ellipsis"> bot_sort </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../trackers/byte_tracker/"><span class="md-ellipsis"> byte_tracker </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../trackers/track/"><span class="md-ellipsis"> track </span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../trackers/utils/gmc/"><span class="md-ellipsis"> utils </span><span class="md-nav__icon md-icon"></span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_10" type="checkbox"/><label class="md-nav__link" for="__nav_11_10" id="__nav_11_10_label" tabindex=""><span class="md-ellipsis"> utils </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_10_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_10"><span class="md-nav__icon md-icon"></span> utils </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/__init__/"><span class="md-ellipsis"> __init__ </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/autobatch/"><span class="md-ellipsis"> autobatch </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/autodevice/"><span class="md-ellipsis"> autodevice </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/benchmarks/"><span class="md-ellipsis"> benchmarks </span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../utils/callbacks/base/"><span class="md-ellipsis"> callbacks </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/checks/"><span class="md-ellipsis"> checks </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/cpu/"><span class="md-ellipsis"> cpu </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/dist/"><span class="md-ellipsis"> dist </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/downloads/"><span class="md-ellipsis"> downloads </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/errors/"><span class="md-ellipsis"> errors </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/events/"><span class="md-ellipsis"> events </span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../utils/export/engine/"><span class="md-ellipsis"> export </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/files/"><span class="md-ellipsis"> files </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/git/"><span class="md-ellipsis"> git </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/instance/"><span class="md-ellipsis"> instance </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/logger/"><span class="md-ellipsis"> logger </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/loss/"><span class="md-ellipsis"> loss </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/metrics/"><span class="md-ellipsis"> metrics </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/nms/"><span class="md-ellipsis"> nms </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/ops/"><span class="md-ellipsis"> ops </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/patches/"><span class="md-ellipsis"> patches </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/plotting/"><span class="md-ellipsis"> plotting </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/tal/"><span class="md-ellipsis"> tal </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/torch_utils/"><span class="md-ellipsis"> torch_utils </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/tqdm/"><span class="md-ellipsis"> tqdm </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/triton/"><span class="md-ellipsis"> triton </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/tuner/"><span class="md-ellipsis"> tuner </span></a></li></ul></nav></li></ul></nav></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../help/"><span class="md-ellipsis"> Help </span><span class="md-nav__icon md-icon"></span></a></li></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav aria-label="On this page" class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc"><span class="md-nav__icon md-icon"></span> On this page </label><ul class="md-nav__list" data-md-component="toc" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> Predictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.Predictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor._inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor._prepare_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prepare_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.generate"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> generate</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.postprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> postprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.pre_transform"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> pre_transform</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.preprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> preprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.prompt_inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> prompt_inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.remove_small_regions"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> remove_small_regions</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.reset_image"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> reset_image</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.set_image"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> set_image</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.set_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> set_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.setup_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.setup_source"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_source</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM2Predictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM2Predictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor._inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor._prepare_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prepare_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor.setup_source"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_source</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM2VideoPredictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM2VideoPredictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._add_output_per_object"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _add_output_per_object</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._clear_non_cond_mem_around_input"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _clear_non_cond_mem_around_input</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._consolidate_temp_output_across_obj"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _consolidate_temp_output_across_obj</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_empty_mask_ptr"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _get_empty_mask_ptr</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_maskmem_pos_enc"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _get_maskmem_pos_enc</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._init_state"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _init_state</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._obj_id_to_idx"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _obj_id_to_idx</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._prune_non_cond_memory"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prune_non_cond_memory</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._reset_tracking_results"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _reset_tracking_results</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_memory_encoder"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _run_memory_encoder</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_single_frame_inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _run_single_frame_inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.add_new_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> add_new_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.clear_all_points_in_frame"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> clear_all_points_in_frame</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.clear_all_points_in_video"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> clear_all_points_in_video</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.init_state"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> init_state</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.postprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> postprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.propagate_in_video_preflight"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> propagate_in_video_preflight</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.remove_object"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> remove_object</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM2DynamicInteractivePredictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._obj_id_to_idx"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _obj_id_to_idx</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._prepare_memory_conditioned_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prepare_memory_conditioned_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_maskmem_enc"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_maskmem_enc</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.track_step"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> track_step</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.update_memory"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> update_memory</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3Predictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM3Predictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM3Predictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3Predictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3Predictor.setup_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_model</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM3SemanticPredictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM3SemanticPredictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor._get_dummy_prompt"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _get_dummy_prompt</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor._inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor._prepare_geometric_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prepare_geometric_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.postprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> postprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.pre_transform"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> pre_transform</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.reset_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> reset_prompts</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoPredictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM3VideoPredictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM3VideoPredictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoPredictor.propagate_in_video"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> propagate_in_video</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM3VideoSemanticPredictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM3VideoSemanticPredictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._apply_object_wise_non_overlapping_constraints"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _apply_object_wise_non_overlapping_constraints</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._associate_det_trk"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _associate_det_trk</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._cache_backbone_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _cache_backbone_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._det_track_one_frame"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _det_track_one_frame</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._drop_new_det_with_obj_limit"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _drop_new_det_with_obj_limit</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._extract_detection_outputs"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _extract_detection_outputs</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._initialize_metadata"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _initialize_metadata</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._process_hotstart"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _process_hotstart</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._propogate_tracker_one_frame_local_gpu"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _propogate_tracker_one_frame_local_gpu</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._recondition_masklets"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _recondition_masklets</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._run_single_frame_inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _run_single_frame_inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._suppress_detections_close_to_boundary"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _suppress_detections_close_to_boundary</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._suppress_overlapping_based_on_recent_occlusion"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _suppress_overlapping_based_on_recent_occlusion</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_add_new_objects"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _tracker_add_new_objects</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_remove_objects"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _tracker_remove_objects</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_update_memories"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _tracker_update_memories</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.add_prompt"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> add_prompt</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.build_outputs"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> build_outputs</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.init_state"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> init_state</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.postprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> postprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_backbone_and_detection"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> run_backbone_and_detection</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_propagation"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> run_tracker_propagation</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_update_execution_phase"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> run_tracker_update_execution_phase</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_update_planning_phase"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> run_tracker_update_planning_phase</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.setup_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.setup_source"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_source</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.update_masklet_confirmation_status"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> update_masklet_confirmation_status</span></a></li></ul></nav></li></ul></nav></div></div></div><div class="md-content" data-md-component="content"><article class="md-content__inner md-typeset"><a class="md-content__button md-icon" href="https://github.com/ultralytics/ultralytics/tree/main/docs/en/reference/models/sam/predict.md" rel="edit" title="Edit this page"><svg class="lucide lucide-file-pen" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12.659 22H18a2 2 0 0 0 2-2V8a2.4 2.4 0 0 0-.706-1.706l-3.588-3.588A2.4 2.4 0 0 0 14 2H6a2 2 0 0 0-2 2v9.34"></path><path d="M14 2v5a1 1 0 0 0 1 1h5M10.378 12.622a1 1 0 0 1 3 3.003L8.36 20.637a2 2 0 0 1-.854.506l-2.867.837a.5.5 0 0 1-.62-.62l.836-2.869a2 2 0 0 1 .506-.853z"></path></svg></a><h1 id="reference-for-ultralyticsmodelssampredictpy">Reference for <code>ultralytics/models/sam/predict.py</code></h1><div class="admonition success"><p class="admonition-title">Improvements</p><p>This page is sourced from <a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py">https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py</a>. Have an improvement or example to add? Open a <a href="https://docs.ultralytics.com/help/contributing/">Pull Request</a> â€” thank you! ğŸ™</p></div><p><br/></p><div class="admonition abstract"><p class="admonition-title">Summary</p><div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="classes" name="__tabbed_1" type="radio"/><input id="methods" name="__tabbed_1" type="radio"/><div class="tabbed-labels"><label for="classes"><span class="doc-kind doc-kind-class">Classes</span></label><label for="methods"><span class="doc-kind doc-kind-method">Methods</span></label></div><div class="tabbed-content"><div class="tabbed-block"><ul><li><a href="#ultralytics.models.sam.predict.Predictor"><code>Predictor</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2Predictor"><code>SAM2Predictor</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor"><code>SAM2VideoPredictor</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor"><code>SAM2DynamicInteractivePredictor</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3Predictor"><code>SAM3Predictor</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor"><code>SAM3SemanticPredictor</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoPredictor"><code>SAM3VideoPredictor</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor"><code>SAM3VideoSemanticPredictor</code></a></li></ul></div><div class="tabbed-block"><ul><li><a href="#ultralytics.models.sam.predict.Predictor.preprocess"><code>Predictor.preprocess</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.pre_transform"><code>Predictor.pre_transform</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.inference"><code>Predictor.inference</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.prompt_inference"><code>Predictor.prompt_inference</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor._inference_features"><code>Predictor._inference_features</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor._prepare_prompts"><code>Predictor._prepare_prompts</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.generate"><code>Predictor.generate</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.setup_model"><code>Predictor.setup_model</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.get_model"><code>Predictor.get_model</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.postprocess"><code>Predictor.postprocess</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.set_image"><code>Predictor.set_image</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.setup_source"><code>Predictor.setup_source</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.get_im_features"><code>Predictor.get_im_features</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.set_prompts"><code>Predictor.set_prompts</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.reset_image"><code>Predictor.reset_image</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.remove_small_regions"><code>Predictor.remove_small_regions</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.inference_features"><code>Predictor.inference_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2Predictor.get_model"><code>SAM2Predictor.get_model</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2Predictor._prepare_prompts"><code>SAM2Predictor._prepare_prompts</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2Predictor.setup_source"><code>SAM2Predictor.setup_source</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2Predictor.get_im_features"><code>SAM2Predictor.get_im_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2Predictor._inference_features"><code>SAM2Predictor._inference_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_model"><code>SAM2VideoPredictor.get_model</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.inference"><code>SAM2VideoPredictor.inference</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.postprocess"><code>SAM2VideoPredictor.postprocess</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.add_new_prompts"><code>SAM2VideoPredictor.add_new_prompts</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.propagate_in_video_preflight"><code>SAM2VideoPredictor.propagate_in_video_preflight</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.init_state"><code>SAM2VideoPredictor.init_state</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._init_state"><code>SAM2VideoPredictor._init_state</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_im_features"><code>SAM2VideoPredictor.get_im_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._obj_id_to_idx"><code>SAM2VideoPredictor._obj_id_to_idx</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_single_frame_inference"><code>SAM2VideoPredictor._run_single_frame_inference</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_maskmem_pos_enc"><code>SAM2VideoPredictor._get_maskmem_pos_enc</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._consolidate_temp_output_across_obj"><code>SAM2VideoPredictor._consolidate_temp_output_across_obj</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_empty_mask_ptr"><code>SAM2VideoPredictor._get_empty_mask_ptr</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_memory_encoder"><code>SAM2VideoPredictor._run_memory_encoder</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._add_output_per_object"><code>SAM2VideoPredictor._add_output_per_object</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._clear_non_cond_mem_around_input"><code>SAM2VideoPredictor._clear_non_cond_mem_around_input</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.remove_object"><code>SAM2VideoPredictor.remove_object</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.clear_all_points_in_frame"><code>SAM2VideoPredictor.clear_all_points_in_frame</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.clear_all_points_in_video"><code>SAM2VideoPredictor.clear_all_points_in_video</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._reset_tracking_results"><code>SAM2VideoPredictor._reset_tracking_results</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._prune_non_cond_memory"><code>SAM2VideoPredictor._prune_non_cond_memory</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.inference"><code>SAM2DynamicInteractivePredictor.inference</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_im_features"><code>SAM2DynamicInteractivePredictor.get_im_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.update_memory"><code>SAM2DynamicInteractivePredictor.update_memory</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._prepare_memory_conditioned_features"><code>SAM2DynamicInteractivePredictor._prepare_memory_conditioned_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_maskmem_enc"><code>SAM2DynamicInteractivePredictor.get_maskmem_enc</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._obj_id_to_idx"><code>SAM2DynamicInteractivePredictor._obj_id_to_idx</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.track_step"><code>SAM2DynamicInteractivePredictor.track_step</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3Predictor.setup_model"><code>SAM3Predictor.setup_model</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3Predictor.get_model"><code>SAM3Predictor.get_model</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.get_model"><code>SAM3SemanticPredictor.get_model</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.get_im_features"><code>SAM3SemanticPredictor.get_im_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.pre_transform"><code>SAM3SemanticPredictor.pre_transform</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor._prepare_geometric_prompts"><code>SAM3SemanticPredictor._prepare_geometric_prompts</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor._inference_features"><code>SAM3SemanticPredictor._inference_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.postprocess"><code>SAM3SemanticPredictor.postprocess</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.inference"><code>SAM3SemanticPredictor.inference</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.inference_features"><code>SAM3SemanticPredictor.inference_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.reset_prompts"><code>SAM3SemanticPredictor.reset_prompts</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor._get_dummy_prompt"><code>SAM3SemanticPredictor._get_dummy_prompt</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoPredictor.propagate_in_video"><code>SAM3VideoPredictor.propagate_in_video</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.setup_model"><code>SAM3VideoSemanticPredictor.setup_model</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.setup_source"><code>SAM3VideoSemanticPredictor.setup_source</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.init_state"><code>SAM3VideoSemanticPredictor.init_state</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.inference"><code>SAM3VideoSemanticPredictor.inference</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.postprocess"><code>SAM3VideoSemanticPredictor.postprocess</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._run_single_frame_inference"><code>SAM3VideoSemanticPredictor._run_single_frame_inference</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.add_prompt"><code>SAM3VideoSemanticPredictor.add_prompt</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._apply_object_wise_non_overlapping_constraints"><code>SAM3VideoSemanticPredictor._apply_object_wise_non_overlapping_constraints</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._det_track_one_frame"><code>SAM3VideoSemanticPredictor._det_track_one_frame</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._suppress_detections_close_to_boundary"><code>SAM3VideoSemanticPredictor._suppress_detections_close_to_boundary</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_backbone_and_detection"><code>SAM3VideoSemanticPredictor.run_backbone_and_detection</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._extract_detection_outputs"><code>SAM3VideoSemanticPredictor._extract_detection_outputs</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._cache_backbone_features"><code>SAM3VideoSemanticPredictor._cache_backbone_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_propagation"><code>SAM3VideoSemanticPredictor.run_tracker_propagation</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._recondition_masklets"><code>SAM3VideoSemanticPredictor._recondition_masklets</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_update_planning_phase"><code>SAM3VideoSemanticPredictor.run_tracker_update_planning_phase</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._suppress_overlapping_based_on_recent_occlusion"><code>SAM3VideoSemanticPredictor._suppress_overlapping_based_on_recent_occlusion</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_update_execution_phase"><code>SAM3VideoSemanticPredictor.run_tracker_update_execution_phase</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.build_outputs"><code>SAM3VideoSemanticPredictor.build_outputs</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._propogate_tracker_one_frame_local_gpu"><code>SAM3VideoSemanticPredictor._propogate_tracker_one_frame_local_gpu</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._associate_det_trk"><code>SAM3VideoSemanticPredictor._associate_det_trk</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._process_hotstart"><code>SAM3VideoSemanticPredictor._process_hotstart</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_update_memories"><code>SAM3VideoSemanticPredictor._tracker_update_memories</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_add_new_objects"><code>SAM3VideoSemanticPredictor._tracker_add_new_objects</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_remove_objects"><code>SAM3VideoSemanticPredictor._tracker_remove_objects</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._initialize_metadata"><code>SAM3VideoSemanticPredictor._initialize_metadata</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.update_masklet_confirmation_status"><code>SAM3VideoSemanticPredictor.update_masklet_confirmation_status</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._drop_new_det_with_obj_limit"><code>SAM3VideoSemanticPredictor._drop_new_det_with_obj_limit</code></a></li></ul></div></div></div></div><h2 id="ultralytics.models.sam.predict.Predictor"><span class="doc-kind doc-kind-class">class</span> <code>ultralytics.models.sam.predict.Predictor</code></h2><div class="highlight"><pre><span></span><code><span></span><span class="n">Predictor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span> <span class="o">=</span> <span class="n">DEFAULT_CFG</span><span class="p">,</span> <span class="n">overrides</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">_callbacks</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p><strong>Bases:</strong> <code>BasePredictor</code></p><p>Predictor class for SAM, enabling real-time image segmentation with promptable capabilities.</p><p>This class extends BasePredictor and implements the Segment Anything Model (SAM) for advanced image segmentation tasks. It supports various input prompts like points, bounding boxes, and masks for fine-grained control over segmentation results.</p><p>Sets up the Predictor object for SAM (Segment Anything Model) and applies any configuration overrides or callbacks provided. Initializes task-specific settings for SAM, such as retina_masks being set to True for optimal results.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>cfg</code></td><td><code>dict</code></td><td>Configuration dictionary containing default settings.</td><td><code>DEFAULT_CFG</code></td></tr><tr><td><code>overrides</code></td><td><code>dict | None</code></td><td>Dictionary of values to override default configuration.</td><td><code>None</code></td></tr><tr><td><code>_callbacks</code></td><td><code>dict | None</code></td><td>Dictionary of callback functions to customize behavior.</td><td><code>None</code></td></tr></tbody></table><p><strong>Attributes</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>args</code></td><td><code>SimpleNamespace</code></td><td>Configuration arguments for the predictor.</td></tr><tr><td><code>model</code></td><td><code>torch.nn.Module</code></td><td>The loaded SAM model.</td></tr><tr><td><code>device</code></td><td><code>torch.device</code></td><td>The device (CPU or GPU) on which the model is loaded.</td></tr><tr><td><code>im</code></td><td><code>torch.Tensor</code></td><td>The preprocessed input image.</td></tr><tr><td><code>features</code></td><td><code>torch.Tensor</code></td><td>Extracted image features.</td></tr><tr><td><code>prompts</code></td><td><code>dict[str, Any]</code></td><td>Dictionary to store various types of prompts (e.g., bboxes, points, masks).</td></tr><tr><td><code>segment_all</code></td><td><code>bool</code></td><td>Flag to indicate if full image segmentation should be performed.</td></tr><tr><td><code>mean</code></td><td><code>torch.Tensor</code></td><td>Mean values for image normalization.</td></tr><tr><td><code>std</code></td><td><code>torch.Tensor</code></td><td>Standard deviation values for image normalization.</td></tr></tbody></table><p><strong>Methods</strong></p><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><a href="#ultralytics.models.sam.predict.Predictor._inference_features"><code>_inference_features</code></a></td><td>Perform inference on image features using the SAM model.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor._prepare_prompts"><code>_prepare_prompts</code></a></td><td>Prepare and transform the input prompts for processing based on the destination shape.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.generate"><code>generate</code></a></td><td>Perform image segmentation using the Segment Anything Model (SAM).</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.get_im_features"><code>get_im_features</code></a></td><td>Extract image features using the SAM model's image encoder for subsequent mask prediction.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.get_model"><code>get_model</code></a></td><td>Retrieve or build the Segment Anything Model (SAM) for image segmentation tasks.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.inference"><code>inference</code></a></td><td>Perform image segmentation inference based on the given input cues, using the currently loaded image.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.inference_features"><code>inference_features</code></a></td><td>Perform prompts preprocessing and inference on provided image features using the SAM model.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.postprocess"><code>postprocess</code></a></td><td>Post-process SAM's inference outputs to generate object detection masks and bounding boxes.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.pre_transform"><code>pre_transform</code></a></td><td>Perform initial transformations on the input image for preprocessing.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.preprocess"><code>preprocess</code></a></td><td>Preprocess the input image for model inference.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.prompt_inference"><code>prompt_inference</code></a></td><td>Perform image segmentation inference based on input cues using SAM's specialized architecture.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.remove_small_regions"><code>remove_small_regions</code></a></td><td>Remove small disconnected regions and holes from segmentation masks.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.reset_image"><code>reset_image</code></a></td><td>Reset the current image and its features, clearing them for subsequent inference.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.set_image"><code>set_image</code></a></td><td>Preprocess and set a single image for inference.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.set_prompts"><code>set_prompts</code></a></td><td>Set prompts for subsequent inference operations.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.setup_model"><code>setup_model</code></a></td><td>Initialize the Segment Anything Model (SAM) for inference.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.setup_source"><code>setup_source</code></a></td><td>Set up the data source for SAM inference.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">setup_model</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s2">"sam_model.pt"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="s2">"image.jpg"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">]]</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">results</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L43-L682"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Predictor</span><span class="p">(</span><span class="n">BasePredictor</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Predictor class for SAM, enabling real-time image segmentation with promptable capabilities.</span>
<span></span>
<span></span><span class="sd">    This class extends BasePredictor and implements the Segment Anything Model (SAM) for advanced image segmentation</span>
<span></span><span class="sd">    tasks. It supports various input prompts like points, bounding boxes, and masks for fine-grained control over</span>
<span></span><span class="sd">    segmentation results.</span>
<span></span>
<span></span><span class="sd">    Attributes:</span>
<span></span><span class="sd">        args (SimpleNamespace): Configuration arguments for the predictor.</span>
<span></span><span class="sd">        model (torch.nn.Module): The loaded SAM model.</span>
<span></span><span class="sd">        device (torch.device): The device (CPU or GPU) on which the model is loaded.</span>
<span></span><span class="sd">        im (torch.Tensor): The preprocessed input image.</span>
<span></span><span class="sd">        features (torch.Tensor): Extracted image features.</span>
<span></span><span class="sd">        prompts (dict[str, Any]): Dictionary to store various types of prompts (e.g., bboxes, points, masks).</span>
<span></span><span class="sd">        segment_all (bool): Flag to indicate if full image segmentation should be performed.</span>
<span></span><span class="sd">        mean (torch.Tensor): Mean values for image normalization.</span>
<span></span><span class="sd">        std (torch.Tensor): Standard deviation values for image normalization.</span>
<span></span>
<span></span><span class="sd">    Methods:</span>
<span></span><span class="sd">        preprocess: Prepare input images for model inference.</span>
<span></span><span class="sd">        pre_transform: Perform initial transformations on the input image.</span>
<span></span><span class="sd">        inference: Perform segmentation inference based on input prompts.</span>
<span></span><span class="sd">        prompt_inference: Internal function for prompt-based segmentation inference.</span>
<span></span><span class="sd">        generate: Generate segmentation masks for an entire image.</span>
<span></span><span class="sd">        setup_model: Initialize the SAM model for inference.</span>
<span></span><span class="sd">        get_model: Build and return a SAM model.</span>
<span></span><span class="sd">        postprocess: Post-process model outputs to generate final results.</span>
<span></span><span class="sd">        setup_source: Set up the data source for inference.</span>
<span></span><span class="sd">        set_image: Set and preprocess a single image for inference.</span>
<span></span><span class="sd">        get_im_features: Extract image features using the SAM image encoder.</span>
<span></span><span class="sd">        set_prompts: Set prompts for subsequent inference.</span>
<span></span><span class="sd">        reset_image: Reset the current image and its features.</span>
<span></span><span class="sd">        remove_small_regions: Remove small disconnected regions and holes from masks.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.setup_model(model_path="sam_model.pt")</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.set_image("image.jpg")</span>
<span></span><span class="sd">        &gt;&gt;&gt; bboxes = [[100, 100, 200, 200]]</span>
<span></span><span class="sd">        &gt;&gt;&gt; results = predictor(bboxes=bboxes)</span>
<span></span><span class="sd">    """</span>
<span></span>
<span></span>    <span class="n">stride</span> <span class="o">=</span> <span class="mi">16</span>
<span></span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="o">=</span><span class="n">DEFAULT_CFG</span><span class="p">,</span> <span class="n">overrides</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">        </span><span class="sd">"""Initialize the Predictor with configuration, overrides, and callbacks.</span>
<span></span>
<span></span><span class="sd">        Sets up the Predictor object for SAM (Segment Anything Model) and applies any configuration overrides or</span>
<span></span><span class="sd">        callbacks provided. Initializes task-specific settings for SAM, such as retina_masks being set to True for</span>
<span></span><span class="sd">        optimal results.</span>
<span></span>
<span></span><span class="sd">        Args:</span>
<span></span><span class="sd">            cfg (dict): Configuration dictionary containing default settings.</span>
<span></span><span class="sd">            overrides (dict | None): Dictionary of values to override default configuration.</span>
<span></span><span class="sd">            _callbacks (dict | None): Dictionary of callback functions to customize behavior.</span>
<span></span><span class="sd">        """</span>
<span></span>        <span class="k">if</span> <span class="n">overrides</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">overrides</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>        <span class="n">overrides</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"segment"</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"predict"</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span></span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">overrides</span><span class="p">,</span> <span class="n">_callbacks</span><span class="p">)</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">retina_masks</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">im</span> <span class="o">=</span> <span class="kc">None</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="kc">None</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">segment_all</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor._inference_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor._inference_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_inference_features</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">features</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Perform inference on image features using the SAM model.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>features</code></td><td><code>torch.Tensor</code></td><td>Extracted image features with shape (B, C, H, W) from the SAM model image encoder.</td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td><code>np.ndarray | list[list[float]] | None</code></td><td>Bounding boxes in XYXY format with shape (N, 4).</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list[list[float]] | None</code></td><td>Object location points with shape (N, 2), in pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list[int] | None</code></td><td>Point prompt labels with shape (N,). 1 = foreground, 0 = background.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>list[np.ndarray] | np.ndarray | None</code></td><td>Masks for the objects, where each mask is a 2D array.</td><td><code>None</code></td></tr><tr><td><code>multimask_output</code></td><td><code>bool</code></td><td>Flag to return multiple masks for ambiguous prompts.</td><td><code>False</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>Output masks with shape (C, H, W), where C is the number of generated masks.</td></tr><tr><td><code>pred_scores (torch.Tensor)</code></td><td>Quality scores for each mask, with length C.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L238-L276"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_inference_features</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">features</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform inference on image features using the SAM model.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        features (torch.Tensor): Extracted image features with shape (B, C, H, W) from the SAM model image encoder.</span>
<span></span><span class="sd">        bboxes (np.ndarray | list[list[float]] | None): Bounding boxes in XYXY format with shape (N, 4).</span>
<span></span><span class="sd">        points (np.ndarray | list[list[float]] | None): Object location points with shape (N, 2), in pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list[int] | None): Point prompt labels with shape (N,). 1 = foreground, 0 = background.</span>
<span></span><span class="sd">        masks (list[np.ndarray] | np.ndarray | None): Masks for the objects, where each mask is a 2D array.</span>
<span></span><span class="sd">        multimask_output (bool): Flag to return multiple masks for ambiguous prompts.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): Output masks with shape (C, H, W), where C is the number of generated masks.</span>
<span></span><span class="sd">        pred_scores (torch.Tensor): Quality scores for each mask, with length C.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">points</span> <span class="o">=</span> <span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
<span></span>    <span class="c1"># Embed prompts</span>
<span></span>    <span class="n">sparse_embeddings</span><span class="p">,</span> <span class="n">dense_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">prompt_encoder</span><span class="p">(</span><span class="n">points</span><span class="o">=</span><span class="n">points</span><span class="p">,</span> <span class="n">boxes</span><span class="o">=</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="n">masks</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Predict masks</span>
<span></span>    <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">mask_decoder</span><span class="p">(</span>
<span></span>        <span class="n">image_embeddings</span><span class="o">=</span><span class="n">features</span><span class="p">,</span>
<span></span>        <span class="n">image_pe</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">prompt_encoder</span><span class="o">.</span><span class="n">get_dense_pe</span><span class="p">(),</span>
<span></span>        <span class="n">sparse_prompt_embeddings</span><span class="o">=</span><span class="n">sparse_embeddings</span><span class="p">,</span>
<span></span>        <span class="n">dense_prompt_embeddings</span><span class="o">=</span><span class="n">dense_embeddings</span><span class="p">,</span>
<span></span>        <span class="n">multimask_output</span><span class="o">=</span><span class="n">multimask_output</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># (N, d, H, W) --&gt; (N*d, H, W), (N, d) --&gt; (N*d, )</span>
<span></span>    <span class="c1"># `d` could be 1 or 3 depends on `multimask_output`.</span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">pred_scores</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor._prepare_prompts"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor._prepare_prompts</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prepare_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dst_shape</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">points</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Prepare and transform the input prompts for processing based on the destination shape.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>dst_shape</code></td><td><code>tuple[int, int]</code></td><td>The target shape (height, width) for the prompts.</td><td><em>required</em></td></tr><tr><td><code>src_shape</code></td><td><code>tuple[int, int]</code></td><td>The source shape (height, width) of the input image.</td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td><code>np.ndarray | list | None</code></td><td>Bounding boxes in XYXY format with shape (N, 4).</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list | None</code></td><td>Points indicating object locations with shape (N, 2) or (N, num_points,<br/> 2), in pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list | None</code></td><td>Point prompt labels with shape (N) or (N, num_points). 1 for foreground,<br/> 0 for background.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>list[np.ndarray] | np.ndarray | None</code></td><td>Masks for the objects, where each mask is a 2D array with<br/> shape (H, W).</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>bboxes (torch.Tensor | None)</code></td><td>Transformed bounding boxes.</td></tr><tr><td><code>points (torch.Tensor | None)</code></td><td>Transformed points.</td></tr><tr><td><code>labels (torch.Tensor | None)</code></td><td>Transformed labels.</td></tr><tr><td><code>masks (torch.Tensor | None)</code></td><td>Transformed masks.</td></tr></tbody></table><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>AssertionError</code></td><td>If the number of points don't match the number of labels, in case labels were passed.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L278-L327"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prepare_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dst_shape</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Prepare and transform the input prompts for processing based on the destination shape.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        dst_shape (tuple[int, int]): The target shape (height, width) for the prompts.</span>
<span></span><span class="sd">        src_shape (tuple[int, int]): The source shape (height, width) of the input image.</span>
<span></span><span class="sd">        bboxes (np.ndarray | list | None): Bounding boxes in XYXY format with shape (N, 4).</span>
<span></span><span class="sd">        points (np.ndarray | list | None): Points indicating object locations with shape (N, 2) or (N, num_points,</span>
<span></span><span class="sd">            2), in pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list | None): Point prompt labels with shape (N) or (N, num_points). 1 for foreground,</span>
<span></span><span class="sd">            0 for background.</span>
<span></span><span class="sd">        masks (list[np.ndarray] | np.ndarray | None): Masks for the objects, where each mask is a 2D array with</span>
<span></span><span class="sd">            shape (H, W).</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        bboxes (torch.Tensor | None): Transformed bounding boxes.</span>
<span></span><span class="sd">        points (torch.Tensor | None): Transformed points.</span>
<span></span><span class="sd">        labels (torch.Tensor | None): Transformed labels.</span>
<span></span><span class="sd">        masks (torch.Tensor | None): Transformed masks.</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        AssertionError: If the number of points don't match the number of labels, in case labels were passed.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">r</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">segment_all</span> <span class="k">else</span> <span class="nb">min</span><span class="p">(</span><span class="n">dst_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">src_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dst_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">src_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span></span>    <span class="c1"># Transform input prompts</span>
<span></span>    <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="n">points</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="k">if</span> <span class="n">points</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">points</span>
<span></span>        <span class="c1"># Assuming labels are all positive if users don't pass labels.</span>
<span></span>        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">points</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span></span>        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="k">assert</span> <span class="n">points</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span>
<span></span>            <span class="sa">f</span><span class="s2">"Number of points </span><span class="si">{</span><span class="n">points</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> should match number of labels </span><span class="si">{</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">."</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="n">points</span> <span class="o">*=</span> <span class="n">r</span>
<span></span>        <span class="k">if</span> <span class="n">points</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
<span></span>            <span class="c1"># (N, 2) --&gt; (N, 1, 2), (N, ) --&gt; (N, 1)</span>
<span></span>            <span class="n">points</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">points</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="n">labels</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
<span></span>    <span class="k">if</span> <span class="n">bboxes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="n">bboxes</span> <span class="o">=</span> <span class="n">bboxes</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="k">if</span> <span class="n">bboxes</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">bboxes</span>
<span></span>        <span class="n">bboxes</span> <span class="o">*=</span> <span class="n">r</span>
<span></span>    <span class="k">if</span> <span class="n">masks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">masks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span></span>        <span class="n">masks</span> <span class="o">=</span> <span class="n">masks</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="k">if</span> <span class="n">masks</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="n">masks</span>
<span></span>        <span class="n">letterbox</span> <span class="o">=</span> <span class="n">LetterBox</span><span class="p">(</span><span class="n">dst_shape</span><span class="p">,</span> <span class="n">auto</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">INTER_NEAREST</span><span class="p">)</span>
<span></span>        <span class="n">masks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">letterbox</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">masks</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span></span>        <span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.generate"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.generate</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">im</span><span class="p">,</span>
<span></span>    <span class="n">crop_n_layers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span></span>    <span class="n">crop_overlap_ratio</span><span class="o">=</span><span class="mi">512</span> <span class="o">/</span> <span class="mi">1500</span><span class="p">,</span>
<span></span>    <span class="n">crop_downscale_factor</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span></span>    <span class="n">point_grids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points_stride</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
<span></span>    <span class="n">points_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span></span>    <span class="n">conf_thres</span><span class="o">=</span><span class="mf">0.88</span><span class="p">,</span>
<span></span>    <span class="n">stability_score_thresh</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
<span></span>    <span class="n">stability_score_offset</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
<span></span>    <span class="n">crop_nms_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Perform image segmentation using the Segment Anything Model (SAM).</p><p>This method segments an entire image into constituent parts by leveraging SAM's advanced architecture and real-time performance capabilities. It can optionally work on image crops for finer segmentation.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor</code></td><td>Input tensor representing the preprocessed image with shape (N, C, H, W).</td><td><em>required</em></td></tr><tr><td><code>crop_n_layers</code></td><td><code>int</code></td><td>Number of layers for additional mask predictions on image crops.</td><td><code>0</code></td></tr><tr><td><code>crop_overlap_ratio</code></td><td><code>float</code></td><td>Overlap between crops, scaled down in subsequent layers.</td><td><code>512 / 1500</code></td></tr><tr><td><code>crop_downscale_factor</code></td><td><code>int</code></td><td>Scaling factor for sampled points-per-side in each layer.</td><td><code>1</code></td></tr><tr><td><code>point_grids</code></td><td><code>list[np.ndarray] | None</code></td><td>Custom grids for point sampling normalized to [0,1].</td><td><code>None</code></td></tr><tr><td><code>points_stride</code></td><td><code>int</code></td><td>Number of points to sample along each side of the image.</td><td><code>32</code></td></tr><tr><td><code>points_batch_size</code></td><td><code>int</code></td><td>Batch size for the number of points processed simultaneously.</td><td><code>64</code></td></tr><tr><td><code>conf_thres</code></td><td><code>float</code></td><td>Confidence threshold [0,1] for filtering based on mask quality prediction.</td><td><code>0.88</code></td></tr><tr><td><code>stability_score_thresh</code></td><td><code>float</code></td><td>Stability threshold [0,1] for mask filtering based on stability.</td><td><code>0.95</code></td></tr><tr><td><code>stability_score_offset</code></td><td><code>float</code></td><td>Offset value for calculating stability score.</td><td><code>0.95</code></td></tr><tr><td><code>crop_nms_thresh</code></td><td><code>float</code></td><td>IoU cutoff for NMS to remove duplicate masks between crops.</td><td><code>0.7</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>Segmented masks with shape (N, H, W).</td></tr><tr><td><code>pred_scores (torch.Tensor)</code></td><td>Confidence scores for each mask with shape (N,).</td></tr><tr><td><code>pred_bboxes (torch.Tensor)</code></td><td>Bounding boxes for each mask with shape (N, 4).</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">im</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>  <span class="c1"># Example input image</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">masks</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">boxes</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L329-L438"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">im</span><span class="p">,</span>
<span></span>    <span class="n">crop_n_layers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span></span>    <span class="n">crop_overlap_ratio</span><span class="o">=</span><span class="mi">512</span> <span class="o">/</span> <span class="mi">1500</span><span class="p">,</span>
<span></span>    <span class="n">crop_downscale_factor</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span></span>    <span class="n">point_grids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points_stride</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
<span></span>    <span class="n">points_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span></span>    <span class="n">conf_thres</span><span class="o">=</span><span class="mf">0.88</span><span class="p">,</span>
<span></span>    <span class="n">stability_score_thresh</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
<span></span>    <span class="n">stability_score_offset</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
<span></span>    <span class="n">crop_nms_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform image segmentation using the Segment Anything Model (SAM).</span>
<span></span>
<span></span><span class="sd">    This method segments an entire image into constituent parts by leveraging SAM's advanced architecture and</span>
<span></span><span class="sd">    real-time performance capabilities. It can optionally work on image crops for finer segmentation.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (torch.Tensor): Input tensor representing the preprocessed image with shape (N, C, H, W).</span>
<span></span><span class="sd">        crop_n_layers (int): Number of layers for additional mask predictions on image crops.</span>
<span></span><span class="sd">        crop_overlap_ratio (float): Overlap between crops, scaled down in subsequent layers.</span>
<span></span><span class="sd">        crop_downscale_factor (int): Scaling factor for sampled points-per-side in each layer.</span>
<span></span><span class="sd">        point_grids (list[np.ndarray] | None): Custom grids for point sampling normalized to [0,1].</span>
<span></span><span class="sd">        points_stride (int): Number of points to sample along each side of the image.</span>
<span></span><span class="sd">        points_batch_size (int): Batch size for the number of points processed simultaneously.</span>
<span></span><span class="sd">        conf_thres (float): Confidence threshold [0,1] for filtering based on mask quality prediction.</span>
<span></span><span class="sd">        stability_score_thresh (float): Stability threshold [0,1] for mask filtering based on stability.</span>
<span></span><span class="sd">        stability_score_offset (float): Offset value for calculating stability score.</span>
<span></span><span class="sd">        crop_nms_thresh (float): IoU cutoff for NMS to remove duplicate masks between crops.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): Segmented masks with shape (N, H, W).</span>
<span></span><span class="sd">        pred_scores (torch.Tensor): Confidence scores for each mask with shape (N,).</span>
<span></span><span class="sd">        pred_bboxes (torch.Tensor): Bounding boxes for each mask with shape (N, 4).</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; im = torch.rand(1, 3, 1024, 1024)  # Example input image</span>
<span></span><span class="sd">        &gt;&gt;&gt; masks, scores, boxes = predictor.generate(im)</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>  <span class="c1"># scope for faster 'import ultralytics'</span>
<span></span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">segment_all</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>    <span class="n">ih</span><span class="p">,</span> <span class="n">iw</span> <span class="o">=</span> <span class="n">im</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
<span></span>    <span class="n">crop_regions</span><span class="p">,</span> <span class="n">layer_idxs</span> <span class="o">=</span> <span class="n">generate_crop_boxes</span><span class="p">((</span><span class="n">ih</span><span class="p">,</span> <span class="n">iw</span><span class="p">),</span> <span class="n">crop_n_layers</span><span class="p">,</span> <span class="n">crop_overlap_ratio</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">point_grids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">point_grids</span> <span class="o">=</span> <span class="n">build_all_layer_point_grids</span><span class="p">(</span><span class="n">points_stride</span><span class="p">,</span> <span class="n">crop_n_layers</span><span class="p">,</span> <span class="n">crop_downscale_factor</span><span class="p">)</span>
<span></span>    <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">pred_bboxes</span><span class="p">,</span> <span class="n">region_areas</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
<span></span>    <span class="k">for</span> <span class="n">crop_region</span><span class="p">,</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">crop_regions</span><span class="p">,</span> <span class="n">layer_idxs</span><span class="p">):</span>
<span></span>        <span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">crop_region</span>
<span></span>        <span class="n">w</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">-</span> <span class="n">x1</span><span class="p">,</span> <span class="n">y2</span> <span class="o">-</span> <span class="n">y1</span>
<span></span>        <span class="n">area</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">h</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">im</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="n">points_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">]])</span>  <span class="c1"># w, h</span>
<span></span>        <span class="c1"># Crop image and interpolate to input size</span>
<span></span>        <span class="n">crop_im</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">im</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">y1</span><span class="p">:</span><span class="n">y2</span><span class="p">,</span> <span class="n">x1</span><span class="p">:</span><span class="n">x2</span><span class="p">],</span> <span class="p">(</span><span class="n">ih</span><span class="p">,</span> <span class="n">iw</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span></span>        <span class="c1"># (num_points, 2)</span>
<span></span>        <span class="n">points_for_image</span> <span class="o">=</span> <span class="n">point_grids</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">points_scale</span>
<span></span>        <span class="n">crop_masks</span><span class="p">,</span> <span class="n">crop_scores</span><span class="p">,</span> <span class="n">crop_bboxes</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
<span></span>        <span class="k">for</span> <span class="p">(</span><span class="n">points</span><span class="p">,)</span> <span class="ow">in</span> <span class="n">batch_iterator</span><span class="p">(</span><span class="n">points_batch_size</span><span class="p">,</span> <span class="n">points_for_image</span><span class="p">):</span>
<span></span>            <span class="n">pred_mask</span><span class="p">,</span> <span class="n">pred_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompt_inference</span><span class="p">(</span><span class="n">crop_im</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="n">points</span><span class="p">,</span> <span class="n">multimask_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span></span>            <span class="c1"># Interpolate predicted masks to input size</span>
<span></span>            <span class="n">pred_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">pred_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">],</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span></span>            <span class="n">idx</span> <span class="o">=</span> <span class="n">pred_score</span> <span class="o">&gt;</span> <span class="n">conf_thres</span>
<span></span>            <span class="n">pred_mask</span><span class="p">,</span> <span class="n">pred_score</span> <span class="o">=</span> <span class="n">pred_mask</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">pred_score</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span></span>
<span></span>            <span class="n">stability_score</span> <span class="o">=</span> <span class="n">calculate_stability_score</span><span class="p">(</span>
<span></span>                <span class="n">pred_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">mask_threshold</span><span class="p">,</span> <span class="n">stability_score_offset</span>
<span></span>            <span class="p">)</span>
<span></span>            <span class="n">idx</span> <span class="o">=</span> <span class="n">stability_score</span> <span class="o">&gt;</span> <span class="n">stability_score_thresh</span>
<span></span>            <span class="n">pred_mask</span><span class="p">,</span> <span class="n">pred_score</span> <span class="o">=</span> <span class="n">pred_mask</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">pred_score</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span></span>            <span class="c1"># Bool type is much more memory-efficient.</span>
<span></span>            <span class="n">pred_mask</span> <span class="o">=</span> <span class="n">pred_mask</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">mask_threshold</span>
<span></span>            <span class="c1"># (N, 4)</span>
<span></span>            <span class="n">pred_bbox</span> <span class="o">=</span> <span class="n">batched_mask_to_box</span><span class="p">(</span><span class="n">pred_mask</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span></span>            <span class="n">keep_mask</span> <span class="o">=</span> <span class="o">~</span><span class="n">is_box_near_crop_edge</span><span class="p">(</span><span class="n">pred_bbox</span><span class="p">,</span> <span class="n">crop_region</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">iw</span><span class="p">,</span> <span class="n">ih</span><span class="p">])</span>
<span></span>            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">keep_mask</span><span class="p">):</span>
<span></span>                <span class="n">pred_bbox</span><span class="p">,</span> <span class="n">pred_mask</span><span class="p">,</span> <span class="n">pred_score</span> <span class="o">=</span> <span class="n">pred_bbox</span><span class="p">[</span><span class="n">keep_mask</span><span class="p">],</span> <span class="n">pred_mask</span><span class="p">[</span><span class="n">keep_mask</span><span class="p">],</span> <span class="n">pred_score</span><span class="p">[</span><span class="n">keep_mask</span><span class="p">]</span>
<span></span>
<span></span>            <span class="n">crop_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_mask</span><span class="p">)</span>
<span></span>            <span class="n">crop_bboxes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_bbox</span><span class="p">)</span>
<span></span>            <span class="n">crop_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_score</span><span class="p">)</span>
<span></span>
<span></span>        <span class="c1"># Do nms within this crop</span>
<span></span>        <span class="n">crop_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">crop_masks</span><span class="p">)</span>
<span></span>        <span class="n">crop_bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">crop_bboxes</span><span class="p">)</span>
<span></span>        <span class="n">crop_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">crop_scores</span><span class="p">)</span>
<span></span>        <span class="n">keep</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">nms</span><span class="p">(</span><span class="n">crop_bboxes</span><span class="p">,</span> <span class="n">crop_scores</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">iou</span><span class="p">)</span>  <span class="c1"># NMS</span>
<span></span>        <span class="n">crop_bboxes</span> <span class="o">=</span> <span class="n">uncrop_boxes_xyxy</span><span class="p">(</span><span class="n">crop_bboxes</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span> <span class="n">crop_region</span><span class="p">)</span>
<span></span>        <span class="n">crop_masks</span> <span class="o">=</span> <span class="n">uncrop_masks</span><span class="p">(</span><span class="n">crop_masks</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span> <span class="n">crop_region</span><span class="p">,</span> <span class="n">ih</span><span class="p">,</span> <span class="n">iw</span><span class="p">)</span>
<span></span>        <span class="n">crop_scores</span> <span class="o">=</span> <span class="n">crop_scores</span><span class="p">[</span><span class="n">keep</span><span class="p">]</span>
<span></span>
<span></span>        <span class="n">pred_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">crop_masks</span><span class="p">)</span>
<span></span>        <span class="n">pred_bboxes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">crop_bboxes</span><span class="p">)</span>
<span></span>        <span class="n">pred_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">crop_scores</span><span class="p">)</span>
<span></span>        <span class="n">region_areas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">area</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">crop_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span></span>
<span></span>    <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">pred_masks</span><span class="p">)</span>
<span></span>    <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">pred_bboxes</span><span class="p">)</span>
<span></span>    <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">pred_scores</span><span class="p">)</span>
<span></span>    <span class="n">region_areas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">region_areas</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Remove duplicate masks between crops</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">crop_regions</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<span></span>        <span class="n">scores</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">region_areas</span>
<span></span>        <span class="n">keep</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">nms</span><span class="p">(</span><span class="n">pred_bboxes</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">crop_nms_thresh</span><span class="p">)</span>
<span></span>        <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_bboxes</span><span class="p">,</span> <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">pred_masks</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span> <span class="n">pred_bboxes</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span> <span class="n">pred_scores</span><span class="p">[</span><span class="n">keep</span><span class="p">]</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">pred_bboxes</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.get_im_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.get_im_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">)</span>
</code></pre></div><p>Extract image features using the SAM model's image encoder for subsequent mask prediction.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L572-L574"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Extract image features using the SAM model's image encoder for subsequent mask prediction."""</span>
<span></span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">image_encoder</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.get_model"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.get_model</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div><p>Retrieve or build the Segment Anything Model (SAM) for image segmentation tasks.</p><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L472-L476"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Retrieve or build the Segment Anything Model (SAM) for image segmentation tasks."""</span>
<span></span>    <span class="kn">from</span><span class="w"> </span><span class="nn">.build</span><span class="w"> </span><span class="kn">import</span> <span class="n">build_sam</span>  <span class="c1"># slow import</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">build_sam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.inference"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.inference</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div><p>Perform image segmentation inference based on the given input cues, using the currently loaded image.</p><p>This method leverages SAM's (Segment Anything Model) architecture consisting of image encoder, prompt encoder, and mask decoder for real-time and promptable segmentation tasks.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor</code></td><td>The preprocessed input image in tensor format, with shape (N, C, H, W).</td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td><code>np.ndarray | list | None</code></td><td>Bounding boxes with shape (N, 4), in XYXY format.</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list | None</code></td><td>Points indicating object locations with shape (N, 2), in pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list | None</code></td><td>Labels for point prompts, shape (N,). 1 = foreground, 0 = background.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>np.ndarray | None</code></td><td>Low-resolution masks from previous predictions, shape (N, H, W). For SAM H=W=256.</td><td><code>None</code></td></tr><tr><td><code>multimask_output</code></td><td><code>bool</code></td><td>Flag to return multiple masks. Helpful for ambiguous prompts.</td><td><code>False</code></td></tr><tr><td><code>*args</code></td><td><code>Any</code></td><td>Additional positional arguments.</td><td><em>required</em></td></tr><tr><td><code>**kwargs</code></td><td><code>Any</code></td><td>Additional keyword arguments.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>The output masks in shape (C, H, W), where C is the number of generated masks.</td></tr><tr><td><code>pred_scores (torch.Tensor)</code></td><td>An array of length C containing quality scores predicted by the model for each</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">setup_model</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s2">"sam_model.pt"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="s2">"image.jpg"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">results</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">bboxes</span><span class="o">=</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">]])</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L169-L205"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform image segmentation inference based on the given input cues, using the currently loaded image.</span>
<span></span>
<span></span><span class="sd">    This method leverages SAM's (Segment Anything Model) architecture consisting of image encoder, prompt encoder,</span>
<span></span><span class="sd">    and mask decoder for real-time and promptable segmentation tasks.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (torch.Tensor): The preprocessed input image in tensor format, with shape (N, C, H, W).</span>
<span></span><span class="sd">        bboxes (np.ndarray | list | None): Bounding boxes with shape (N, 4), in XYXY format.</span>
<span></span><span class="sd">        points (np.ndarray | list | None): Points indicating object locations with shape (N, 2), in pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list | None): Labels for point prompts, shape (N,). 1 = foreground, 0 = background.</span>
<span></span><span class="sd">        masks (np.ndarray | None): Low-resolution masks from previous predictions, shape (N, H, W). For SAM H=W=256.</span>
<span></span><span class="sd">        multimask_output (bool): Flag to return multiple masks. Helpful for ambiguous prompts.</span>
<span></span><span class="sd">        *args (Any): Additional positional arguments.</span>
<span></span><span class="sd">        **kwargs (Any): Additional keyword arguments.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): The output masks in shape (C, H, W), where C is the number of generated masks.</span>
<span></span><span class="sd">        pred_scores (torch.Tensor): An array of length C containing quality scores predicted by the model for each</span>
<span></span><span class="sd">            mask.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.setup_model(model_path="sam_model.pt")</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.set_image("image.jpg")</span>
<span></span><span class="sd">        &gt;&gt;&gt; results = predictor(bboxes=[[0, 0, 100, 100]])</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="c1"># Override prompts if any stored in self.prompts</span>
<span></span>    <span class="n">bboxes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"bboxes"</span><span class="p">,</span> <span class="n">bboxes</span><span class="p">)</span>
<span></span>    <span class="n">points</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"points"</span><span class="p">,</span> <span class="n">points</span><span class="p">)</span>
<span></span>    <span class="n">masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"masks"</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span>    <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"labels"</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">i</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">masks</span><span class="p">]):</span>
<span></span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompt_inference</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span><span class="p">,</span> <span class="n">multimask_output</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.inference_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.inference_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference_features</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">features</span><span class="p">,</span>
<span></span>    <span class="n">src_shape</span><span class="p">,</span>
<span></span>    <span class="n">dst_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Perform prompts preprocessing and inference on provided image features using the SAM model.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>features</code></td><td><code>torch.Tensor | dict[str, Any]</code></td><td>Extracted image features from the SAM/SAM2 model image encoder.</td><td><em>required</em></td></tr><tr><td><code>src_shape</code></td><td><code>tuple[int, int]</code></td><td>The source shape (height, width) of the input image.</td><td><em>required</em></td></tr><tr><td><code>dst_shape</code></td><td><code>tuple[int, int] | None</code></td><td>The target shape (height, width) for the prompts. If None, defaults to<br/> (imgsz, imgsz).</td><td><code>None</code></td></tr><tr><td><code>bboxes</code></td><td><code>np.ndarray | list[list[float]] | None</code></td><td>Bounding boxes in xyxy format with shape (N, 4).</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list[list[float]] | None</code></td><td>Points indicating object locations with shape (N, 2), in<br/> pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list[int] | None</code></td><td>Point prompt labels with shape (N, ).</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>list[np.ndarray] | np.ndarray | None</code></td><td>Masks for the objects, where each mask is a 2D array.</td><td><code>None</code></td></tr><tr><td><code>multimask_output</code></td><td><code>bool</code></td><td>Flag to return multiple masks for ambiguous prompts.</td><td><code>False</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>The output masks in shape (C, H, W), where C is the number of generated masks.</td></tr><tr><td><code>pred_bboxes (torch.Tensor)</code></td><td>Bounding boxes for each mask with shape (N, 6), where N is the number of boxes.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>The input features is a torch.Tensor of shape (B, C, H, W) if performing on SAM, or a dict[str, Any] if performing on SAM2.</li></ul></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L637-L682"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference_features</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">features</span><span class="p">,</span>
<span></span>    <span class="n">src_shape</span><span class="p">,</span>
<span></span>    <span class="n">dst_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform prompts preprocessing and inference on provided image features using the SAM model.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        features (torch.Tensor | dict[str, Any]): Extracted image features from the SAM/SAM2 model image encoder.</span>
<span></span><span class="sd">        src_shape (tuple[int, int]): The source shape (height, width) of the input image.</span>
<span></span><span class="sd">        dst_shape (tuple[int, int] | None): The target shape (height, width) for the prompts. If None, defaults to</span>
<span></span><span class="sd">            (imgsz, imgsz).</span>
<span></span><span class="sd">        bboxes (np.ndarray | list[list[float]] | None): Bounding boxes in xyxy format with shape (N, 4).</span>
<span></span><span class="sd">        points (np.ndarray | list[list[float]] | None): Points indicating object locations with shape (N, 2), in</span>
<span></span><span class="sd">            pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list[int] | None): Point prompt labels with shape (N, ).</span>
<span></span><span class="sd">        masks (list[np.ndarray] | np.ndarray | None): Masks for the objects, where each mask is a 2D array.</span>
<span></span><span class="sd">        multimask_output (bool): Flag to return multiple masks for ambiguous prompts.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): The output masks in shape (C, H, W), where C is the number of generated masks.</span>
<span></span><span class="sd">        pred_bboxes (torch.Tensor): Bounding boxes for each mask with shape (N, 6), where N is the number of boxes.</span>
<span></span><span class="sd">            Each box is in xyxy format with additional columns for score and class.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - The input features is a torch.Tensor of shape (B, C, H, W) if performing on SAM, or a dict[str, Any] if performing on SAM2.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">dst_shape</span> <span class="o">=</span> <span class="n">dst_shape</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">imgsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">imgsz</span><span class="p">)</span>
<span></span>    <span class="n">prompts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_prompts</span><span class="p">(</span><span class="n">dst_shape</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span>    <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inference_features</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="o">*</span><span class="n">prompts</span><span class="p">,</span> <span class="n">multimask_output</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">pred_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">scale_masks</span><span class="p">(</span><span class="n">pred_masks</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span></span>        <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">pred_masks</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">mask_threshold</span>  <span class="c1"># to bool</span>
<span></span>        <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">batched_mask_to_box</span><span class="p">(</span><span class="n">pred_masks</span><span class="p">)</span>
<span></span>        <span class="c1"># NOTE: SAM models do not return cls info. This `cls` here is just a placeholder for consistency.</span>
<span></span>        <span class="bp">cls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pred_bboxes</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="bp">cls</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_bboxes</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.postprocess"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.postprocess</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">)</span>
</code></pre></div><p>Post-process SAM's inference outputs to generate object detection masks and bounding boxes.</p><p>This method scales masks and boxes to the original image size and applies a threshold to the mask predictions. It leverages SAM's advanced architecture for real-time, promptable segmentation tasks.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>preds</code></td><td><code>tuple</code></td><td>The output from SAM model inference, containing:<br/> - pred_masks (torch.Tensor): Predicted masks with shape (N, 1, H, W).<br/> - pred_scores (torch.Tensor): Confidence scores for each mask with shape (N, 1).<br/> - pred_bboxes (torch.Tensor, optional): Predicted bounding boxes if segment_all is True.</td><td><em>required</em></td></tr><tr><td><code>img</code></td><td><code>torch.Tensor</code></td><td>The processed input image tensor with shape (C, H, W).</td><td><em>required</em></td></tr><tr><td><code>orig_imgs</code></td><td><code>list[np.ndarray] | torch.Tensor</code></td><td>The original, unprocessed images.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>list[Results]</code></td><td>List of Results objects containing detection masks, bounding boxes, and other metadata for</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">results</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">postprocess</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L478-L528"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Post-process SAM's inference outputs to generate object detection masks and bounding boxes.</span>
<span></span>
<span></span><span class="sd">    This method scales masks and boxes to the original image size and applies a threshold to the mask</span>
<span></span><span class="sd">    predictions. It leverages SAM's advanced architecture for real-time, promptable segmentation tasks.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        preds (tuple): The output from SAM model inference, containing:</span>
<span></span><span class="sd">            - pred_masks (torch.Tensor): Predicted masks with shape (N, 1, H, W).</span>
<span></span><span class="sd">            - pred_scores (torch.Tensor): Confidence scores for each mask with shape (N, 1).</span>
<span></span><span class="sd">            - pred_bboxes (torch.Tensor, optional): Predicted bounding boxes if segment_all is True.</span>
<span></span><span class="sd">        img (torch.Tensor): The processed input image tensor with shape (C, H, W).</span>
<span></span><span class="sd">        orig_imgs (list[np.ndarray] | torch.Tensor): The original, unprocessed images.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (list[Results]): List of Results objects containing detection masks, bounding boxes, and other metadata for</span>
<span></span><span class="sd">            each processed image.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; preds = predictor.inference(img)</span>
<span></span><span class="sd">        &gt;&gt;&gt; results = predictor.postprocess(preds, img, orig_imgs)</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="c1"># (N, 1, H, W), (N, 1)</span>
<span></span>    <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
<span></span>    <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">segment_all</span> <span class="k">else</span> <span class="kc">None</span>
<span></span>    <span class="n">names</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">orig_imgs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>  <span class="c1"># input images are a torch.Tensor, not a list</span>
<span></span>        <span class="n">orig_imgs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_torch2numpy_batch</span><span class="p">(</span><span class="n">orig_imgs</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span></span>
<span></span>    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>    <span class="k">for</span> <span class="n">masks</span><span class="p">,</span> <span class="n">orig_img</span><span class="p">,</span> <span class="n">img_path</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">pred_masks</span><span class="p">],</span> <span class="n">orig_imgs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
<span></span>        <span class="k">if</span> <span class="n">masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>            <span class="n">masks</span><span class="p">,</span> <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="k">else</span><span class="p">:</span>
<span></span>            <span class="n">masks</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">scale_masks</span><span class="p">(</span><span class="n">masks</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">orig_img</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span></span>            <span class="n">masks</span> <span class="o">=</span> <span class="n">masks</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">mask_threshold</span>  <span class="c1"># to bool</span>
<span></span>            <span class="k">if</span> <span class="n">pred_bboxes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>                <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">scale_boxes</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="n">pred_bboxes</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">orig_img</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span></span>            <span class="k">else</span><span class="p">:</span>
<span></span>                <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">batched_mask_to_box</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span>
<span></span>            <span class="c1"># NOTE: SAM models do not return cls info. This `cls` here is just a placeholder for consistency.</span>
<span></span>            <span class="bp">cls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>            <span class="n">idx</span> <span class="o">=</span> <span class="n">pred_scores</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">conf</span>
<span></span>            <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pred_bboxes</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="bp">cls</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="n">idx</span><span class="p">]</span>
<span></span>            <span class="n">masks</span> <span class="o">=</span> <span class="n">masks</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span></span>        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Results</span><span class="p">(</span><span class="n">orig_img</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="n">img_path</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">names</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="n">masks</span><span class="p">,</span> <span class="n">boxes</span><span class="o">=</span><span class="n">pred_bboxes</span><span class="p">))</span>
<span></span>    <span class="c1"># Reset segment-all mode.</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">segment_all</span> <span class="o">=</span> <span class="kc">False</span>
<span></span>    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.pre_transform"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.pre_transform</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">pre_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">)</span>
</code></pre></div><p>Perform initial transformations on the input image for preprocessing.</p><p>This method applies transformations such as resizing to prepare the image for further preprocessing. Currently, batched inference is not supported; hence the list length should be 1.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>list[np.ndarray]</code></td><td>List containing a single image in HWC numpy array format.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>list[np.ndarray]</code></td><td>List containing the transformed image.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">480</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># Single HWC image</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">transformed</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">pre_transform</span><span class="p">([</span><span class="n">image</span><span class="p">])</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">transformed</span><span class="p">))</span>
<span></span><span class="mi">1</span>
</code></pre></div><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>AssertionError</code></td><td>If the input list contains more than one image.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L143-L167"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">pre_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform initial transformations on the input image for preprocessing.</span>
<span></span>
<span></span><span class="sd">    This method applies transformations such as resizing to prepare the image for further preprocessing. Currently,</span>
<span></span><span class="sd">    batched inference is not supported; hence the list length should be 1.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (list[np.ndarray]): List containing a single image in HWC numpy array format.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (list[np.ndarray]): List containing the transformed image.</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        AssertionError: If the input list contains more than one image.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; image = np.random.rand(480, 640, 3)  # Single HWC image</span>
<span></span><span class="sd">        &gt;&gt;&gt; transformed = predictor.pre_transform([image])</span>
<span></span><span class="sd">        &gt;&gt;&gt; print(len(transformed))</span>
<span></span><span class="sd">        1</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">im</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"SAM model does not currently support batched inference"</span>
<span></span>    <span class="n">letterbox</span> <span class="o">=</span> <span class="n">LetterBox</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">,</span> <span class="n">auto</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="p">[</span><span class="n">letterbox</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">im</span><span class="p">]</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.preprocess"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.preprocess</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">)</span>
</code></pre></div><p>Preprocess the input image for model inference.</p><p>This method prepares the input image by applying transformations and normalization. It supports both torch.Tensor and list of np.ndarray as input formats. For OpenCV-loaded images, the input is typically BGR and is converted to RGB during preprocessing.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor | list[np.ndarray]</code></td><td>Input image(s) in BCHW tensor format or a list of HWC NumPy arrays.<br/> NumPy arrays are expected to be in BGR order (as returned by OpenCV) and will be converted to RGB.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>torch.Tensor</code></td><td>The preprocessed image tensor, normalized and converted to the appropriate dtype.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">640</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">preprocessed_image</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L109-L141"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Preprocess the input image for model inference.</span>
<span></span>
<span></span><span class="sd">    This method prepares the input image by applying transformations and normalization. It supports both</span>
<span></span><span class="sd">    torch.Tensor and list of np.ndarray as input formats. For OpenCV-loaded images, the input is typically BGR and</span>
<span></span><span class="sd">    is converted to RGB during preprocessing.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (torch.Tensor | list[np.ndarray]): Input image(s) in BCHW tensor format or a list of HWC NumPy arrays.</span>
<span></span><span class="sd">            NumPy arrays are expected to be in BGR order (as returned by OpenCV) and will be converted to RGB.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (torch.Tensor): The preprocessed image tensor, normalized and converted to the appropriate dtype.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; image = torch.rand(1, 3, 640, 640)</span>
<span></span><span class="sd">        &gt;&gt;&gt; preprocessed_image = predictor.preprocess(image)</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">im</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">im</span>
<span></span>    <span class="n">not_tensor</span> <span class="o">=</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">not_tensor</span><span class="p">:</span>
<span></span>        <span class="n">im</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pre_transform</span><span class="p">(</span><span class="n">im</span><span class="p">))</span>
<span></span>        <span class="n">im</span> <span class="o">=</span> <span class="n">im</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span></span>        <span class="n">im</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
<span></span>        <span class="n">im</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">im</span> <span class="o">=</span> <span class="n">im</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">not_tensor</span><span class="p">:</span>
<span></span>        <span class="n">im</span> <span class="o">=</span> <span class="p">(</span><span class="n">im</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">std</span>
<span></span>    <span class="n">im</span> <span class="o">=</span> <span class="n">im</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fp16</span> <span class="k">else</span> <span class="n">im</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span></span>    <span class="k">return</span> <span class="n">im</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.prompt_inference"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.prompt_inference</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">prompt_inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">points</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">multimask_output</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</code></pre></div><p>Perform image segmentation inference based on input cues using SAM's specialized architecture.</p><p>This internal function leverages the Segment Anything Model (SAM) for prompt-based, real-time segmentation. It processes various input prompts such as bounding boxes, points, and masks to generate segmentation masks.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor</code></td><td>Preprocessed input image tensor with shape (N, C, H, W).</td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td><code>np.ndarray | list | None</code></td><td>Bounding boxes in XYXY format with shape (N, 4).</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list | None</code></td><td>Points indicating object locations with shape (N, 2) or (N, num_points,<br/> 2), in pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list | None</code></td><td>Point prompt labels with shape (N) or (N, num_points). 1 for foreground,<br/> 0 for background.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>np.ndarray | None</code></td><td>Low-res masks from previous predictions with shape (N, H, W). For SAM, H=W=256.</td><td><code>None</code></td></tr><tr><td><code>multimask_output</code></td><td><code>bool</code></td><td>Flag to return multiple masks for ambiguous prompts.</td><td><code>False</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>Output masks with shape (C, H, W), where C is the number of generated masks.</td></tr><tr><td><code>pred_scores (torch.Tensor)</code></td><td>Quality scores predicted by the model for each mask, with length C.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">im</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">]]</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">masks</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">prompt_inference</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L207-L236"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">prompt_inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform image segmentation inference based on input cues using SAM's specialized architecture.</span>
<span></span>
<span></span><span class="sd">    This internal function leverages the Segment Anything Model (SAM) for prompt-based, real-time segmentation. It</span>
<span></span><span class="sd">    processes various input prompts such as bounding boxes, points, and masks to generate segmentation masks.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (torch.Tensor): Preprocessed input image tensor with shape (N, C, H, W).</span>
<span></span><span class="sd">        bboxes (np.ndarray | list | None): Bounding boxes in XYXY format with shape (N, 4).</span>
<span></span><span class="sd">        points (np.ndarray | list | None): Points indicating object locations with shape (N, 2) or (N, num_points,</span>
<span></span><span class="sd">            2), in pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list | None): Point prompt labels with shape (N) or (N, num_points). 1 for foreground,</span>
<span></span><span class="sd">            0 for background.</span>
<span></span><span class="sd">        masks (np.ndarray | None): Low-res masks from previous predictions with shape (N, H, W). For SAM, H=W=256.</span>
<span></span><span class="sd">        multimask_output (bool): Flag to return multiple masks for ambiguous prompts.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): Output masks with shape (C, H, W), where C is the number of generated masks.</span>
<span></span><span class="sd">        pred_scores (torch.Tensor): Quality scores predicted by the model for each mask, with length C.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; im = torch.rand(1, 3, 1024, 1024)</span>
<span></span><span class="sd">        &gt;&gt;&gt; bboxes = [[100, 100, 200, 200]]</span>
<span></span><span class="sd">        &gt;&gt;&gt; masks, scores, logits = predictor.prompt_inference(im, bboxes=bboxes)</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span><span class="n">im</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span>
<span></span>
<span></span>    <span class="n">prompts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_prompts</span><span class="p">(</span><span class="n">im</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inference_features</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="o">*</span><span class="n">prompts</span><span class="p">,</span> <span class="n">multimask_output</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.remove_small_regions"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.remove_small_regions</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">remove_small_regions</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="n">min_area</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">nms_thresh</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">)</span>
</code></pre></div><p>Remove small disconnected regions and holes from segmentation masks.</p><p>This function performs post-processing on segmentation masks generated by the Segment Anything Model (SAM). It removes small disconnected regions and holes from the input masks, and then performs Non-Maximum Suppression (NMS) to eliminate any newly created duplicate boxes.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>masks</code></td><td><code>torch.Tensor</code></td><td>Segmentation masks to be processed, with shape (N, H, W) where N is the number of<br/> masks, H is height, and W is width.</td><td><em>required</em></td></tr><tr><td><code>min_area</code></td><td><code>int</code></td><td>Minimum area threshold for removing disconnected regions and holes. Regions smaller than<br/> this will be removed.</td><td><code>0</code></td></tr><tr><td><code>nms_thresh</code></td><td><code>float</code></td><td>IoU threshold for the NMS algorithm to remove duplicate boxes.</td><td><code>0.7</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>new_masks (torch.Tensor)</code></td><td>Processed masks with small regions removed, shape (N, H, W).</td></tr><tr><td><code>keep (list[int])</code></td><td>Indices of remaining masks after NMS, for filtering corresponding boxes.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">640</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span>  <span class="c1"># 5 random binary masks</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">new_masks</span><span class="p">,</span> <span class="n">keep</span> <span class="o">=</span> <span class="n">remove_small_regions</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="n">min_area</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">nms_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Original masks: </span><span class="si">{</span><span class="n">masks</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, Processed masks: </span><span class="si">{</span><span class="n">new_masks</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Indices of kept masks: </span><span class="si">{</span><span class="n">keep</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L586-L634"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@staticmethod</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">remove_small_regions</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="n">min_area</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">nms_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Remove small disconnected regions and holes from segmentation masks.</span>
<span></span>
<span></span><span class="sd">    This function performs post-processing on segmentation masks generated by the Segment Anything Model (SAM). It</span>
<span></span><span class="sd">    removes small disconnected regions and holes from the input masks, and then performs Non-Maximum Suppression</span>
<span></span><span class="sd">    (NMS) to eliminate any newly created duplicate boxes.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        masks (torch.Tensor): Segmentation masks to be processed, with shape (N, H, W) where N is the number of</span>
<span></span><span class="sd">            masks, H is height, and W is width.</span>
<span></span><span class="sd">        min_area (int): Minimum area threshold for removing disconnected regions and holes. Regions smaller than</span>
<span></span><span class="sd">            this will be removed.</span>
<span></span><span class="sd">        nms_thresh (float): IoU threshold for the NMS algorithm to remove duplicate boxes.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        new_masks (torch.Tensor): Processed masks with small regions removed, shape (N, H, W).</span>
<span></span><span class="sd">        keep (list[int]): Indices of remaining masks after NMS, for filtering corresponding boxes.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; masks = torch.rand(5, 640, 640) &gt; 0.5  # 5 random binary masks</span>
<span></span><span class="sd">        &gt;&gt;&gt; new_masks, keep = remove_small_regions(masks, min_area=100, nms_thresh=0.7)</span>
<span></span><span class="sd">        &gt;&gt;&gt; print(f"Original masks: {masks.shape}, Processed masks: {new_masks.shape}")</span>
<span></span><span class="sd">        &gt;&gt;&gt; print(f"Indices of kept masks: {keep}")</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>  <span class="c1"># scope for faster 'import ultralytics'</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="n">masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="k">return</span> <span class="n">masks</span>
<span></span>
<span></span>    <span class="c1"># Filter small disconnected regions and holes</span>
<span></span>    <span class="n">new_masks</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>    <span class="k">for</span> <span class="n">mask</span> <span class="ow">in</span> <span class="n">masks</span><span class="p">:</span>
<span></span>        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span></span>        <span class="n">mask</span><span class="p">,</span> <span class="n">changed</span> <span class="o">=</span> <span class="n">remove_small_regions</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">min_area</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"holes"</span><span class="p">)</span>
<span></span>        <span class="n">unchanged</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">changed</span>
<span></span>        <span class="n">mask</span><span class="p">,</span> <span class="n">changed</span> <span class="o">=</span> <span class="n">remove_small_regions</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">min_area</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"islands"</span><span class="p">)</span>
<span></span>        <span class="n">unchanged</span> <span class="o">=</span> <span class="n">unchanged</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">changed</span>
<span></span>
<span></span>        <span class="n">new_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span></span>        <span class="c1"># Give score=0 to changed masks and 1 to unchanged masks so NMS prefers masks not needing postprocessing</span>
<span></span>        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">unchanged</span><span class="p">))</span>
<span></span>
<span></span>    <span class="c1"># Recalculate boxes and remove any new duplicates</span>
<span></span>    <span class="n">new_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_masks</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span></span>    <span class="n">boxes</span> <span class="o">=</span> <span class="n">batched_mask_to_box</span><span class="p">(</span><span class="n">new_masks</span><span class="p">)</span>
<span></span>    <span class="n">keep</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">nms</span><span class="p">(</span><span class="n">boxes</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">nms_thresh</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">new_masks</span><span class="p">[</span><span class="n">keep</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">masks</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">masks</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">keep</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.reset_image"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.reset_image</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">reset_image</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div><p>Reset the current image and its features, clearing them for subsequent inference.</p><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L580-L583"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">reset_image</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Reset the current image and its features, clearing them for subsequent inference."""</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">im</span> <span class="o">=</span> <span class="kc">None</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.set_image"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.set_image</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">set_image</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>
</code></pre></div><p>Preprocess and set a single image for inference.</p><p>This method prepares the model for inference on a single image by setting up the model if not already initialized, configuring the data source, and preprocessing the image for feature extraction. It ensures that only one image is set at a time and extracts image features for subsequent use.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>image</code></td><td><code>str | np.ndarray</code></td><td>Path to the image file as a string, or a numpy array representing an image read by<br/> cv2 (BGR channel order).</td><td><em>required</em></td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">))</span>
</code></pre></div><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>This method should be called before performing inference on a new image.</li><li>The extracted features are stored in the <code>self.features</code> attribute for later use.</li></ul></div><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>AssertionError</code></td><td>If more than one image is attempted to be set.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L530-L560"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">set_image</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Preprocess and set a single image for inference.</span>
<span></span>
<span></span><span class="sd">    This method prepares the model for inference on a single image by setting up the model if not already</span>
<span></span><span class="sd">    initialized, configuring the data source, and preprocessing the image for feature extraction. It ensures that</span>
<span></span><span class="sd">    only one image is set at a time and extracts image features for subsequent use.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        image (str | np.ndarray): Path to the image file as a string, or a numpy array representing an image read by</span>
<span></span><span class="sd">            cv2 (BGR channel order).</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        AssertionError: If more than one image is attempted to be set.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.set_image("path/to/image.jpg")</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.set_image(cv2.imread("path/to/image.jpg"))</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - This method should be called before performing inference on a new image.</span>
<span></span><span class="sd">        - The extracted features are stored in the `self.features` attribute for later use.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">setup_model</span><span class="p">()</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">setup_source</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span></span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"`set_image` only supports setting one image!"</span>
<span></span>    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">:</span>
<span></span>        <span class="n">im</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
<span></span>        <span class="k">break</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.set_prompts"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.set_prompts</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">set_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompts</span><span class="p">)</span>
</code></pre></div><p>Set prompts for subsequent inference operations.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>prompts</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L576-L578"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">set_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompts</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Set prompts for subsequent inference operations."""</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span> <span class="o">=</span> <span class="n">prompts</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.setup_model"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.setup_model</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</code></pre></div><p>Initialize the Segment Anything Model (SAM) for inference.</p><p>This method sets up the SAM model by allocating it to the appropriate device and initializing the necessary parameters for image normalization and other Ultralytics compatibility settings.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>model</code></td><td><code>torch.nn.Module | None</code></td><td>A pretrained SAM model. If None, a new model is built based on config.</td><td><code>None</code></td></tr><tr><td><code>verbose</code></td><td><code>bool</code></td><td>If True, prints selected device information.</td><td><code>True</code></td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">setup_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">sam_model</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L440-L470"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Initialize the Segment Anything Model (SAM) for inference.</span>
<span></span>
<span></span><span class="sd">    This method sets up the SAM model by allocating it to the appropriate device and initializing the necessary</span>
<span></span><span class="sd">    parameters for image normalization and other Ultralytics compatibility settings.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        model (torch.nn.Module | None): A pretrained SAM model. If None, a new model is built based on config.</span>
<span></span><span class="sd">        verbose (bool): If True, prints selected device information.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.setup_model(model=sam_model, verbose=True)</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">device</span> <span class="o">=</span> <span class="n">select_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_model</span><span class="p">()</span>
<span></span>    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span></span>    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">half</span> <span class="k">else</span> <span class="n">model</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">123.675</span><span class="p">,</span> <span class="mf">116.28</span><span class="p">,</span> <span class="mf">103.53</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">58.395</span><span class="p">,</span> <span class="mf">57.12</span><span class="p">,</span> <span class="mf">57.375</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Ultralytics compatibility settings</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">pt</span> <span class="o">=</span> <span class="kc">False</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">triton</span> <span class="o">=</span> <span class="kc">False</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="mi">32</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fp16</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">half</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">done_warmup</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fp16</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.setup_source"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.setup_source</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_source</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">)</span>
</code></pre></div><p>Set up the data source for SAM inference.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>source</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L562-L570"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_source</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Set up the data source for SAM inference."""</span>
<span></span>    <span class="k">if</span> <span class="n">source</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># handle the situation when set_imgsz in advance</span>
<span></span>        <span class="k">return</span>
<span></span>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">setup_source</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
<span></span>    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span>
<span></span>        <span class="sa">f</span><span class="s2">"SAM models only support square image size, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="si">}</span><span class="s2">."</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">set_imgsz</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">)</span>
</code></pre></div></details><p><br/><br/><hr/><br/></p><h2 id="ultralytics.models.sam.predict.SAM2Predictor"><span class="doc-kind doc-kind-class">class</span> <code>ultralytics.models.sam.predict.SAM2Predictor</code></h2><div class="highlight"><pre><span></span><code><span></span><span class="n">SAM2Predictor</span><span class="p">()</span>
</code></pre></div><p><strong>Bases:</strong> <code>Predictor</code></p><p>SAM2Predictor class for advanced image segmentation using Segment Anything Model 2 architecture.</p><p>This class extends the base Predictor class to implement SAM2-specific functionality for image segmentation tasks. It provides methods for model initialization, feature extraction, and prompt-based inference.</p><p><strong>Attributes</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>_bb_feat_sizes</code></td><td><code>list[tuple]</code></td><td>Feature sizes for different backbone levels.</td></tr><tr><td><code>model</code></td><td><code>torch.nn.Module</code></td><td>The loaded SAM2 model.</td></tr><tr><td><code>device</code></td><td><code>torch.device</code></td><td>The device (CPU or GPU) on which the model is loaded.</td></tr><tr><td><code>features</code></td><td><code>dict</code></td><td>Cached image features for efficient inference.</td></tr><tr><td><code>segment_all</code></td><td><code>bool</code></td><td>Flag to indicate if all segments should be predicted.</td></tr><tr><td><code>prompts</code></td><td><code>dict[str, Any]</code></td><td>Dictionary to store various types of prompts for inference.</td></tr></tbody></table><p><strong>Methods</strong></p><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><a href="#ultralytics.models.sam.predict.SAM2Predictor._inference_features"><code>_inference_features</code></a></td><td>Perform inference on image features using the SAM2 model.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2Predictor._prepare_prompts"><code>_prepare_prompts</code></a></td><td>Prepare and transform the input prompts for processing based on the destination shape.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2Predictor.get_im_features"><code>get_im_features</code></a></td><td>Extract image features from the SAM image encoder for subsequent processing.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2Predictor.get_model"><code>get_model</code></a></td><td>Retrieve and initialize the Segment Anything Model 2 (SAM2) for image segmentation tasks.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2Predictor.setup_source"><code>setup_source</code></a></td><td>Set up the data source and image size for SAM2 inference.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">SAM2Predictor</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">]]</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Predicted </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">masks</span><span class="p">)</span><span class="si">}</span><span class="s2"> masks with average score </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">boxes</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L685-L825"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SAM2Predictor</span><span class="p">(</span><span class="n">Predictor</span><span class="p">):</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2Predictor._inference_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2Predictor._inference_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_inference_features</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">features</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="n">img_idx</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Perform inference on image features using the SAM2 model.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>features</code></td><td><code>torch.Tensor | dict[str, Any]</code></td><td>Extracted image features with shape (B, C, H, W) from the SAM2</td><td><em>required</em></td></tr><tr><td><code>model image encoder, it could also be a dictionary including:&lt;br&gt;    - image_embed (torch.Tensor): Image embedding with shape (B, C, H, W).&lt;br&gt;    - high_res_feats (list[torch.Tensor]): List of high-resolution feature maps from the backbone, each with shape (B, C, H, W).</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list[list[float]] | None</code></td><td>Object location points with shape (N, 2), in pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list[int] | None</code></td><td>Point prompt labels with shape (N,). 1 = foreground, 0 = background.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>list[np.ndarray] | np.ndarray | None</code></td><td>Masks for the objects, where each mask is a 2D array.</td><td><code>None</code></td></tr><tr><td><code>multimask_output</code></td><td><code>bool</code></td><td>Flag to return multiple masks for ambiguous prompts.</td><td><code>False</code></td></tr><tr><td><code>img_idx</code></td><td><code>int</code></td><td>Index of the image in the batch to process.</td><td><code>-1</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>Output masks with shape (C, H, W), where C is the number of generated masks.</td></tr><tr><td><code>pred_scores (torch.Tensor)</code></td><td>Quality scores for each mask, with length C.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L776-L825"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_inference_features</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">features</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="n">img_idx</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform inference on image features using the SAM2 model.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        features (torch.Tensor | dict[str, Any]): Extracted image features with shape (B, C, H, W) from the SAM2</span>
<span></span><span class="sd">        model image encoder, it could also be a dictionary including:</span>
<span></span><span class="sd">            - image_embed (torch.Tensor): Image embedding with shape (B, C, H, W).</span>
<span></span><span class="sd">            - high_res_feats (list[torch.Tensor]): List of high-resolution feature maps from the backbone, each with shape (B, C, H, W).</span>
<span></span><span class="sd">        points (np.ndarray | list[list[float]] | None): Object location points with shape (N, 2), in pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list[int] | None): Point prompt labels with shape (N,). 1 = foreground, 0 = background.</span>
<span></span><span class="sd">        masks (list[np.ndarray] | np.ndarray | None): Masks for the objects, where each mask is a 2D array.</span>
<span></span><span class="sd">        multimask_output (bool): Flag to return multiple masks for ambiguous prompts.</span>
<span></span><span class="sd">        img_idx (int): Index of the image in the batch to process.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): Output masks with shape (C, H, W), where C is the number of generated masks.</span>
<span></span><span class="sd">        pred_scores (torch.Tensor): Quality scores for each mask, with length C.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">points</span> <span class="o">=</span> <span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
<span></span>    <span class="n">sparse_embeddings</span><span class="p">,</span> <span class="n">dense_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">sam_prompt_encoder</span><span class="p">(</span>
<span></span>        <span class="n">points</span><span class="o">=</span><span class="n">points</span><span class="p">,</span>
<span></span>        <span class="n">boxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="n">masks</span><span class="o">=</span><span class="n">masks</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="c1"># Predict masks</span>
<span></span>    <span class="n">batched_mode</span> <span class="o">=</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">points</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span>  <span class="c1"># multi object prediction</span>
<span></span>    <span class="n">high_res_features</span> <span class="o">=</span> <span class="kc">None</span>
<span></span>    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
<span></span>        <span class="n">high_res_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">feat_level</span><span class="p">[</span><span class="n">img_idx</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">feat_level</span> <span class="ow">in</span> <span class="n">features</span><span class="p">[</span><span class="s2">"high_res_feats"</span><span class="p">]]</span>
<span></span>        <span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="s2">"image_embed"</span><span class="p">][[</span><span class="n">img_idx</span><span class="p">]]</span>
<span></span>    <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">sam_mask_decoder</span><span class="p">(</span>
<span></span>        <span class="n">image_embeddings</span><span class="o">=</span><span class="n">features</span><span class="p">,</span>
<span></span>        <span class="n">image_pe</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">sam_prompt_encoder</span><span class="o">.</span><span class="n">get_dense_pe</span><span class="p">(),</span>
<span></span>        <span class="n">sparse_prompt_embeddings</span><span class="o">=</span><span class="n">sparse_embeddings</span><span class="p">,</span>
<span></span>        <span class="n">dense_prompt_embeddings</span><span class="o">=</span><span class="n">dense_embeddings</span><span class="p">,</span>
<span></span>        <span class="n">multimask_output</span><span class="o">=</span><span class="n">multimask_output</span><span class="p">,</span>
<span></span>        <span class="n">repeat_image</span><span class="o">=</span><span class="n">batched_mode</span><span class="p">,</span>
<span></span>        <span class="n">high_res_features</span><span class="o">=</span><span class="n">high_res_features</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="c1"># (N, d, H, W) --&gt; (N*d, H, W), (N, d) --&gt; (N*d, )</span>
<span></span>    <span class="c1"># `d` could be 1 or 3 depends on `multimask_output`.</span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">pred_scores</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2Predictor._prepare_prompts"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2Predictor._prepare_prompts</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prepare_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dst_shape</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">points</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Prepare and transform the input prompts for processing based on the destination shape.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>dst_shape</code></td><td><code>tuple[int, int]</code></td><td>The target shape (height, width) for the prompts.</td><td><em>required</em></td></tr><tr><td><code>src_shape</code></td><td><code>tuple[int, int]</code></td><td>The source shape (height, width) of the input image.</td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td><code>np.ndarray | list | None</code></td><td>Bounding boxes in XYXY format with shape (N, 4).</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list | None</code></td><td>Points indicating object locations with shape (N, 2) or (N, num_points,<br/> 2), in pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list | None</code></td><td>Point prompt labels with shape (N,) or (N, num_points). 1 for foreground,<br/> 0 for background.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>list | np.ndarray | None</code></td><td>Masks for the objects, where each mask is a 2D array.</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>points (torch.Tensor | None)</code></td><td>Transformed points.</td></tr><tr><td><code>labels (torch.Tensor | None)</code></td><td>Transformed labels.</td></tr><tr><td><code>masks (torch.Tensor | None)</code></td><td>Transformed masks.</td></tr></tbody></table><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>AssertionError</code></td><td>If the number of points don't match the number of labels, in case labels were passed.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L726-L758"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prepare_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dst_shape</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Prepare and transform the input prompts for processing based on the destination shape.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        dst_shape (tuple[int, int]): The target shape (height, width) for the prompts.</span>
<span></span><span class="sd">        src_shape (tuple[int, int]): The source shape (height, width) of the input image.</span>
<span></span><span class="sd">        bboxes (np.ndarray | list | None): Bounding boxes in XYXY format with shape (N, 4).</span>
<span></span><span class="sd">        points (np.ndarray | list | None): Points indicating object locations with shape (N, 2) or (N, num_points,</span>
<span></span><span class="sd">            2), in pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list | None): Point prompt labels with shape (N,) or (N, num_points). 1 for foreground,</span>
<span></span><span class="sd">            0 for background.</span>
<span></span><span class="sd">        masks (list | np.ndarray | None): Masks for the objects, where each mask is a 2D array.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        points (torch.Tensor | None): Transformed points.</span>
<span></span><span class="sd">        labels (torch.Tensor | None): Transformed labels.</span>
<span></span><span class="sd">        masks (torch.Tensor | None): Transformed masks.</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        AssertionError: If the number of points don't match the number of labels, in case labels were passed.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_prepare_prompts</span><span class="p">(</span><span class="n">dst_shape</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">bboxes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">bboxes</span> <span class="o">=</span> <span class="n">bboxes</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span></span>        <span class="n">bbox_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">bboxes</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">bboxes</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span></span>        <span class="c1"># NOTE: merge "boxes" and "points" into a single "points" input</span>
<span></span>        <span class="c1"># (where boxes are added at the beginning) to model.sam_prompt_encoder</span>
<span></span>        <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span></span>            <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">bbox_labels</span><span class="p">,</span> <span class="n">labels</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span></span>        <span class="k">else</span><span class="p">:</span>
<span></span>            <span class="n">points</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">bbox_labels</span>
<span></span>    <span class="k">return</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2Predictor.get_im_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2Predictor.get_im_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">)</span>
</code></pre></div><p>Extract image features from the SAM image encoder for subsequent processing.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L765-L774"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Extract image features from the SAM image encoder for subsequent processing."""</span>
<span></span>    <span class="n">backbone_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward_image</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
<span></span>    <span class="n">_</span><span class="p">,</span> <span class="n">vision_feats</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_prepare_backbone_features</span><span class="p">(</span><span class="n">backbone_out</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">directly_add_no_mem_embed</span><span class="p">:</span>
<span></span>        <span class="n">vision_feats</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">vision_feats</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">no_mem_embed</span>
<span></span>    <span class="n">feats</span> <span class="o">=</span> <span class="p">[</span>
<span></span>        <span class="n">feat</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">feat_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">feat</span><span class="p">,</span> <span class="n">feat_size</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">vision_feats</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bb_feat_sizes</span><span class="p">)</span>
<span></span>    <span class="p">]</span>
<span></span>    <span class="k">return</span> <span class="p">{</span><span class="s2">"image_embed"</span><span class="p">:</span> <span class="n">feats</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">"high_res_feats"</span><span class="p">:</span> <span class="n">feats</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]}</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2Predictor.get_model"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2Predictor.get_model</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div><p>Retrieve and initialize the Segment Anything Model 2 (SAM2) for image segmentation tasks.</p><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L720-L724"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Retrieve and initialize the Segment Anything Model 2 (SAM2) for image segmentation tasks."""</span>
<span></span>    <span class="kn">from</span><span class="w"> </span><span class="nn">.build</span><span class="w"> </span><span class="kn">import</span> <span class="n">build_sam</span>  <span class="c1"># slow import</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">build_sam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2Predictor.setup_source"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2Predictor.setup_source</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_source</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">)</span>
</code></pre></div><p>Set up the data source and image size for SAM2 inference.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>source</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L760-L763"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_source</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Set up the data source and image size for SAM2 inference."""</span>
<span></span>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">setup_source</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">_bb_feat_sizes</span> <span class="o">=</span> <span class="p">[[</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">*</span> <span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
</code></pre></div></details><p><br/><br/><hr/><br/></p><h2 id="ultralytics.models.sam.predict.SAM2VideoPredictor"><span class="doc-kind doc-kind-class">class</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor</code></h2><div class="highlight"><pre><span></span><code><span></span><span class="n">SAM2VideoPredictor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span> <span class="o">=</span> <span class="n">DEFAULT_CFG</span><span class="p">,</span> <span class="n">overrides</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">_callbacks</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p><strong>Bases:</strong> <code>SAM2Predictor</code></p><p>SAM2VideoPredictor to handle user interactions with videos and manage inference states.</p><p>This class extends the functionality of SAM2Predictor to support video processing and maintains the state of inference operations. It includes configurations for managing non-overlapping masks, clearing memory for non-conditional inputs, and setting up callbacks for prediction events.</p><p>This constructor initializes the SAM2VideoPredictor with a given configuration, applies any specified overrides, and sets up the inference state along with certain flags that control the behavior of the predictor.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>cfg</code></td><td><code>dict</code></td><td>Configuration dictionary containing default settings.</td><td><code>DEFAULT_CFG</code></td></tr><tr><td><code>overrides</code></td><td><code>dict | None</code></td><td>Dictionary of values to override default configuration.</td><td><code>None</code></td></tr><tr><td><code>_callbacks</code></td><td><code>dict | None</code></td><td>Dictionary of callback functions to customize behavior.</td><td><code>None</code></td></tr></tbody></table><p><strong>Attributes</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>inference_state</code></td><td><code>dict</code></td><td>A dictionary to store the current state of inference operations.</td></tr><tr><td><code>non_overlap_masks</code></td><td><code>bool</code></td><td>A flag indicating whether masks should be non-overlapping.</td></tr><tr><td><code>clear_non_cond_mem_around_input</code></td><td><code>bool</code></td><td>A flag to control clearing non-conditional memory around inputs.</td></tr><tr><td><code>clear_non_cond_mem_for_multi_obj</code></td><td><code>bool</code></td><td>A flag to control clearing non-conditional memory for multi-object<br/> scenarios.</td></tr><tr><td><code>callbacks</code></td><td><code>dict</code></td><td>A dictionary of callbacks for various prediction lifecycle events.</td></tr></tbody></table><p><strong>Methods</strong></p><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._add_output_per_object"><code>_add_output_per_object</code></a></td><td>Split a multi-object output into per-object output slices and add them into Output_Dict_Per_Obj.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._clear_non_cond_mem_around_input"><code>_clear_non_cond_mem_around_input</code></a></td><td>Remove the non-conditioning memory around the input frame.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._consolidate_temp_output_across_obj"><code>_consolidate_temp_output_across_obj</code></a></td><td>Consolidate per-object temporary outputs into a single output for all objects.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_empty_mask_ptr"><code>_get_empty_mask_ptr</code></a></td><td>Get a dummy object pointer based on an empty mask on the current frame.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_maskmem_pos_enc"><code>_get_maskmem_pos_enc</code></a></td><td>Cache and manage the positional encoding for mask memory across frames and objects.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._init_state"><code>_init_state</code></a></td><td>Initialize an inference state.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._obj_id_to_idx"><code>_obj_id_to_idx</code></a></td><td>Map client-side object id to model-side object index.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._prune_non_cond_memory"><code>_prune_non_cond_memory</code></a></td><td>Prune old non-conditioning frames to bound memory usage.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._reset_tracking_results"><code>_reset_tracking_results</code></a></td><td>Reset all tracking inputs and results across the videos.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_memory_encoder"><code>_run_memory_encoder</code></a></td><td>Run the memory encoder on masks.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_single_frame_inference"><code>_run_single_frame_inference</code></a></td><td>Run tracking on a single frame based on current inputs and previous memory.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.add_new_prompts"><code>add_new_prompts</code></a></td><td>Add new points or masks to a specific frame for a given object ID.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.clear_all_points_in_frame"><code>clear_all_points_in_frame</code></a></td><td>Remove all input points or mask in a specific frame for a given object.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.clear_all_points_in_video"><code>clear_all_points_in_video</code></a></td><td>Remove all input points or mask in all frames throughout the video.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_im_features"><code>get_im_features</code></a></td><td>Extract and process image features using SAM2's image encoder for subsequent segmentation tasks.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_model"><code>get_model</code></a></td><td>Retrieve and configure the model with binarization enabled.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.inference"><code>inference</code></a></td><td>Perform image segmentation inference based on the given input cues, using the currently loaded image. This</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.init_state"><code>init_state</code></a></td><td>Initialize an inference state for the predictor.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.postprocess"><code>postprocess</code></a></td><td>Post-process the predictions to apply non-overlapping constraints if required.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.propagate_in_video_preflight"><code>propagate_in_video_preflight</code></a></td><td>Prepare inference_state and consolidate temporary outputs before tracking.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.remove_object"><code>remove_object</code></a></td><td>Remove an object id from the tracking state. If strict is True, we check whether the object id actually</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">SAM2VideoPredictor</span><span class="p">(</span><span class="n">cfg</span><span class="o">=</span><span class="n">DEFAULT_CFG</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="s2">"path/to/video_frame.jpg"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">]]</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">results</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes</span><span class="p">)</span>
</code></pre></div><div class="admonition note"><p class="admonition-title">Notes</p><p>The <code>fill_hole_area</code> attribute is defined but not used in the current implementation.</p></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L828-L1853"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SAM2VideoPredictor</span><span class="p">(</span><span class="n">SAM2Predictor</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""SAM2VideoPredictor to handle user interactions with videos and manage inference states.</span>
<span></span>
<span></span><span class="sd">    This class extends the functionality of SAM2Predictor to support video processing and maintains the state of</span>
<span></span><span class="sd">    inference operations. It includes configurations for managing non-overlapping masks, clearing memory for</span>
<span></span><span class="sd">    non-conditional inputs, and setting up callbacks for prediction events.</span>
<span></span>
<span></span><span class="sd">    Attributes:</span>
<span></span><span class="sd">        inference_state (dict): A dictionary to store the current state of inference operations.</span>
<span></span><span class="sd">        non_overlap_masks (bool): A flag indicating whether masks should be non-overlapping.</span>
<span></span><span class="sd">        clear_non_cond_mem_around_input (bool): A flag to control clearing non-conditional memory around inputs.</span>
<span></span><span class="sd">        clear_non_cond_mem_for_multi_obj (bool): A flag to control clearing non-conditional memory for multi-object</span>
<span></span><span class="sd">            scenarios.</span>
<span></span><span class="sd">        callbacks (dict): A dictionary of callbacks for various prediction lifecycle events.</span>
<span></span>
<span></span><span class="sd">    Methods:</span>
<span></span><span class="sd">        get_model: Retrieve and configure the model with binarization enabled.</span>
<span></span><span class="sd">        inference: Perform image segmentation inference based on the given input cues.</span>
<span></span><span class="sd">        postprocess: Post-process the predictions to apply non-overlapping constraints if required.</span>
<span></span><span class="sd">        add_new_prompts: Add new points or masks to a specific frame for a given object ID.</span>
<span></span><span class="sd">        propagate_in_video_preflight: Prepare inference_state and consolidate temporary outputs before tracking.</span>
<span></span><span class="sd">        init_state: Initialize an inference state for the predictor.</span>
<span></span><span class="sd">        get_im_features: Extract image features using SAM2's image encoder for subsequent segmentation tasks.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = SAM2VideoPredictor(cfg=DEFAULT_CFG)</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.set_image("path/to/video_frame.jpg")</span>
<span></span><span class="sd">        &gt;&gt;&gt; bboxes = [[100, 100, 200, 200]]</span>
<span></span><span class="sd">        &gt;&gt;&gt; results = predictor(bboxes=bboxes)</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        The `fill_hole_area` attribute is defined but not used in the current implementation.</span>
<span></span><span class="sd">    """</span>
<span></span>
<span></span>    <span class="c1"># fill_hole_area = 8  # not used</span>
<span></span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="o">=</span><span class="n">DEFAULT_CFG</span><span class="p">,</span> <span class="n">overrides</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">        </span><span class="sd">"""Initialize the predictor with configuration and optional overrides.</span>
<span></span>
<span></span><span class="sd">        This constructor initializes the SAM2VideoPredictor with a given configuration, applies any specified overrides,</span>
<span></span><span class="sd">        and sets up the inference state along with certain flags that control the behavior of the predictor.</span>
<span></span>
<span></span><span class="sd">        Args:</span>
<span></span><span class="sd">            cfg (dict): Configuration dictionary containing default settings.</span>
<span></span><span class="sd">            overrides (dict | None): Dictionary of values to override default configuration.</span>
<span></span><span class="sd">            _callbacks (dict | None): Dictionary of callback functions to customize behavior.</span>
<span></span><span class="sd">        """</span>
<span></span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">overrides</span><span class="p">,</span> <span class="n">_callbacks</span><span class="p">)</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">non_overlap_masks</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem_around_input</span> <span class="o">=</span> <span class="kc">False</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem_for_multi_obj</span> <span class="o">=</span> <span class="kc">False</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">[</span><span class="s2">"on_predict_start"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_state</span><span class="p">)</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Whether to clear non-conditioning memory periodically</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._add_output_per_object"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._add_output_per_object</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_add_output_per_object</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">current_out</span><span class="p">,</span> <span class="n">storage_key</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Split a multi-object output into per-object output slices and add them into Output_Dict_Per_Obj.</p><p>The resulting slices share the same tensor storage.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td><code>int</code></td><td>The index of the current frame.</td><td><em>required</em></td></tr><tr><td><code>current_out</code></td><td><code>dict</code></td><td>The current output dictionary containing multi-object outputs.</td><td><em>required</em></td></tr><tr><td><code>storage_key</code></td><td><code>str</code></td><td>The key used to store the output in the per-object output dictionary.</td><td><em>required</em></td></tr><tr><td><code>inference_state</code></td><td><code>dict[str, Any], optional</code></td><td>The current inference state. If None, uses the instance's<br/> inference state.</td><td><code>None</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1617-L1650"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_add_output_per_object</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">current_out</span><span class="p">,</span> <span class="n">storage_key</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Split a multi-object output into per-object output slices and add them into Output_Dict_Per_Obj.</span>
<span></span>
<span></span><span class="sd">    The resulting slices share the same tensor storage.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        frame_idx (int): The index of the current frame.</span>
<span></span><span class="sd">        current_out (dict): The current output dictionary containing multi-object outputs.</span>
<span></span><span class="sd">        storage_key (str): The key used to store the output in the per-object output dictionary.</span>
<span></span><span class="sd">        inference_state (dict[str, Any], optional): The current inference state. If None, uses the instance's</span>
<span></span><span class="sd">            inference state.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="n">inference_state</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span>
<span></span>    <span class="n">maskmem_features</span> <span class="o">=</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">]</span>
<span></span>    <span class="k">assert</span> <span class="n">maskmem_features</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">maskmem_features</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">maskmem_pos_enc</span> <span class="o">=</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span>
<span></span>    <span class="k">assert</span> <span class="n">maskmem_pos_enc</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">maskmem_pos_enc</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">for</span> <span class="n">obj_idx</span><span class="p">,</span> <span class="n">obj_output_dict</span> <span class="ow">in</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<span></span>        <span class="n">obj_slice</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">obj_idx</span><span class="p">,</span> <span class="n">obj_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span></span>        <span class="n">obj_out</span> <span class="o">=</span> <span class="p">{</span>
<span></span>            <span class="s2">"maskmem_features"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span></span>            <span class="s2">"maskmem_pos_enc"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span></span>            <span class="s2">"pred_masks"</span><span class="p">:</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">][</span><span class="n">obj_slice</span><span class="p">],</span>
<span></span>            <span class="s2">"obj_ptr"</span><span class="p">:</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">][</span><span class="n">obj_slice</span><span class="p">],</span>
<span></span>        <span class="p">}</span>
<span></span>        <span class="k">if</span> <span class="n">maskmem_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">obj_out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">]</span> <span class="o">=</span> <span class="n">maskmem_features</span><span class="p">[</span><span class="n">obj_slice</span><span class="p">]</span>
<span></span>        <span class="k">if</span> <span class="n">maskmem_pos_enc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">obj_out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="n">obj_slice</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">maskmem_pos_enc</span><span class="p">]</span>
<span></span>        <span class="n">obj_output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj_out</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._clear_non_cond_mem_around_input"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._clear_non_cond_mem_around_input</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_clear_non_cond_mem_around_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Remove the non-conditioning memory around the input frame.</p><p>When users provide correction clicks, the surrounding frames' non-conditioning memories can still contain outdated object appearance information and could confuse the model. This method clears those non-conditioning memories surrounding the interacted frame to avoid giving the model both old and new information about the object.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td><code>int</code></td><td>The index of the current frame where user interaction occurred.</td><td><em>required</em></td></tr><tr><td><code>inference_state</code></td><td><code>dict[str, Any], optional</code></td><td>The current inference state. If None, uses the instance's<br/> inference state.</td><td><code>None</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1652-L1672"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_clear_non_cond_mem_around_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Remove the non-conditioning memory around the input frame.</span>
<span></span>
<span></span><span class="sd">    When users provide correction clicks, the surrounding frames' non-conditioning memories can still contain</span>
<span></span><span class="sd">    outdated object appearance information and could confuse the model. This method clears those non-conditioning</span>
<span></span><span class="sd">    memories surrounding the interacted frame to avoid giving the model both old and new information about the</span>
<span></span><span class="sd">    object.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        frame_idx (int): The index of the current frame where user interaction occurred.</span>
<span></span><span class="sd">        inference_state (dict[str, Any], optional): The current inference state. If None, uses the instance's</span>
<span></span><span class="sd">            inference state.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="n">inference_state</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span>
<span></span>    <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">memory_temporal_stride_for_eval</span>
<span></span>    <span class="n">frame_idx_begin</span> <span class="o">=</span> <span class="n">frame_idx</span> <span class="o">-</span> <span class="n">r</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_maskmem</span>
<span></span>    <span class="n">frame_idx_end</span> <span class="o">=</span> <span class="n">frame_idx</span> <span class="o">+</span> <span class="n">r</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_maskmem</span>
<span></span>    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">frame_idx_begin</span><span class="p">,</span> <span class="n">frame_idx_end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
<span></span>        <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict"</span><span class="p">][</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>        <span class="k">for</span> <span class="n">obj_output_dict</span> <span class="ow">in</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>            <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._consolidate_temp_output_across_obj"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._consolidate_temp_output_across_obj</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_consolidate_temp_output_across_obj</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">,</span>
<span></span>    <span class="n">is_cond</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="n">run_mem_encoder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Consolidate per-object temporary outputs into a single output for all objects.</p><p>This method combines the temporary outputs for each object on a given frame into a unified output. It fills in any missing objects either from the main output dictionary or leaves placeholders if they do not exist in the main output. Optionally, it can re-run the memory encoder after applying non-overlapping constraints to the object scores.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td><code>int</code></td><td>The index of the frame for which to consolidate outputs.</td><td><em>required</em></td></tr><tr><td><code>is_cond</code></td><td><code>bool, optional</code></td><td>Indicates if the frame is considered a conditioning frame.</td><td><code>False</code></td></tr><tr><td><code>run_mem_encoder</code></td><td><code>bool, optional</code></td><td>Specifies whether to run the memory encoder after consolidating the<br/> outputs.</td><td><code>False</code></td></tr><tr><td><code>inference_state</code></td><td><code>dict[str, Any], optional</code></td><td>The current inference state. If None, uses the instance's<br/> inference state.</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>dict</code></td><td>A consolidated output dictionary containing the combined results for all objects.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>The method initializes the consolidated output with placeholder values for missing objects.</li><li>It searches for outputs in both the temporary and main output dictionaries.</li><li>If <code>run_mem_encoder</code> is True, it applies non-overlapping constraints and re-runs the memory encoder.</li><li>The <code>maskmem_features</code> and <code>maskmem_pos_enc</code> are only populated when <code>run_mem_encoder</code> is True.</li></ul></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1431-L1540"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_consolidate_temp_output_across_obj</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">,</span>
<span></span>    <span class="n">is_cond</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="n">run_mem_encoder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Consolidate per-object temporary outputs into a single output for all objects.</span>
<span></span>
<span></span><span class="sd">    This method combines the temporary outputs for each object on a given frame into a unified</span>
<span></span><span class="sd">    output. It fills in any missing objects either from the main output dictionary or leaves</span>
<span></span><span class="sd">    placeholders if they do not exist in the main output. Optionally, it can re-run the memory encoder after</span>
<span></span><span class="sd">    applying non-overlapping constraints to the object scores.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        frame_idx (int): The index of the frame for which to consolidate outputs.</span>
<span></span><span class="sd">        is_cond (bool, optional): Indicates if the frame is considered a conditioning frame.</span>
<span></span><span class="sd">        run_mem_encoder (bool, optional): Specifies whether to run the memory encoder after consolidating the</span>
<span></span><span class="sd">            outputs.</span>
<span></span><span class="sd">        inference_state (dict[str, Any], optional): The current inference state. If None, uses the instance's</span>
<span></span><span class="sd">            inference state.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (dict): A consolidated output dictionary containing the combined results for all objects.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - The method initializes the consolidated output with placeholder values for missing objects.</span>
<span></span><span class="sd">        - It searches for outputs in both the temporary and main output dictionaries.</span>
<span></span><span class="sd">        - If `run_mem_encoder` is True, it applies non-overlapping constraints and re-runs the memory encoder.</span>
<span></span><span class="sd">        - The `maskmem_features` and `maskmem_pos_enc` are only populated when `run_mem_encoder` is True.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="n">inference_state</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span>
<span></span>    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_idx_to_id"</span><span class="p">])</span>
<span></span>    <span class="n">storage_key</span> <span class="o">=</span> <span class="s2">"cond_frame_outputs"</span> <span class="k">if</span> <span class="n">is_cond</span> <span class="k">else</span> <span class="s2">"non_cond_frame_outputs"</span>
<span></span>
<span></span>    <span class="c1"># Initialize `consolidated_out`. Its "maskmem_features" and "maskmem_pos_enc"</span>
<span></span>    <span class="c1"># will be added when rerunning the memory encoder after applying non-overlapping</span>
<span></span>    <span class="c1"># constraints to object scores. Its "pred_masks" are prefilled with a large</span>
<span></span>    <span class="c1"># negative value (NO_OBJ_SCORE) to represent missing objects.</span>
<span></span>    <span class="n">consolidated_out</span> <span class="o">=</span> <span class="p">{</span>
<span></span>        <span class="s2">"maskmem_features"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="s2">"maskmem_pos_enc"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="s2">"pred_masks"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
<span></span>            <span class="c1"># size=(batch_size, 1, self.imgsz[0] // 4, self.imgsz[1] // 4),</span>
<span></span>            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_bb_feat_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
<span></span>            <span class="n">fill_value</span><span class="o">=-</span><span class="mf">1024.0</span><span class="p">,</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
<span></span>            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>        <span class="p">),</span>
<span></span>        <span class="s2">"obj_ptr"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
<span></span>            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">),</span>
<span></span>            <span class="n">fill_value</span><span class="o">=-</span><span class="mf">1024.0</span><span class="p">,</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
<span></span>            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>        <span class="p">),</span>
<span></span>        <span class="s2">"object_score_logits"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
<span></span>            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span></span>            <span class="c1"># default to 10.0 for object_score_logits, i.e. assuming the object is</span>
<span></span>            <span class="c1"># present as sigmoid(10)=1, same as in `predict_masks` of `MaskDecoder`</span>
<span></span>            <span class="n">fill_value</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
<span></span>            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>        <span class="p">),</span>
<span></span>    <span class="p">}</span>
<span></span>    <span class="k">for</span> <span class="n">obj_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
<span></span>        <span class="n">obj_temp_output_dict</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"temp_output_dict_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span>
<span></span>        <span class="n">obj_output_dict</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span>
<span></span>        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span>
<span></span>            <span class="n">obj_temp_output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>            <span class="c1"># If the object doesn't appear in "temp_output_dict_per_obj" on this frame,</span>
<span></span>            <span class="c1"># we fall back and look up its previous output in "output_dict_per_obj".</span>
<span></span>            <span class="c1"># We look up both "cond_frame_outputs" and "non_cond_frame_outputs" in</span>
<span></span>            <span class="c1"># "output_dict_per_obj" to find a previous output for this object.</span>
<span></span>            <span class="ow">or</span> <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>            <span class="ow">or</span> <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="c1"># If the object doesn't appear in "output_dict_per_obj" either, we skip it</span>
<span></span>        <span class="c1"># and leave its mask scores to the default scores (i.e. the NO_OBJ_SCORE</span>
<span></span>        <span class="c1"># placeholder above) and set its object pointer to be a dummy pointer.</span>
<span></span>        <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="c1"># Fill in dummy object pointers for those objects without any inputs or</span>
<span></span>            <span class="c1"># tracking outcomes on this frame (only do it under `run_mem_encoder=True`,</span>
<span></span>            <span class="c1"># i.e. when we need to build the memory for tracking).</span>
<span></span>            <span class="k">if</span> <span class="n">run_mem_encoder</span><span class="p">:</span>
<span></span>                <span class="c1"># fill object pointer with a dummy pointer (based on an empty mask)</span>
<span></span>                <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">][</span><span class="n">obj_idx</span> <span class="p">:</span> <span class="n">obj_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_empty_mask_ptr</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>            <span class="k">continue</span>
<span></span>        <span class="c1"># Add the temporary object output mask to consolidated output mask</span>
<span></span>        <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">][</span><span class="n">obj_idx</span> <span class="p">:</span> <span class="n">obj_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span>
<span></span>        <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">][</span><span class="n">obj_idx</span> <span class="p">:</span> <span class="n">obj_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">]</span>
<span></span>
<span></span>    <span class="c1"># Optionally, apply non-overlapping constraints on the consolidated scores and rerun the memory encoder</span>
<span></span>    <span class="k">if</span> <span class="n">run_mem_encoder</span><span class="p">:</span>
<span></span>        <span class="n">high_res_masks</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
<span></span>            <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">],</span>
<span></span>            <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">,</span>
<span></span>            <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">,</span>
<span></span>            <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">non_overlap_masks_for_mem_enc</span><span class="p">:</span>
<span></span>            <span class="n">high_res_masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_apply_non_overlapping_constraints</span><span class="p">(</span><span class="n">high_res_masks</span><span class="p">)</span>
<span></span>        <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">],</span> <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_memory_encoder</span><span class="p">(</span>
<span></span>            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span></span>            <span class="n">high_res_masks</span><span class="o">=</span><span class="n">high_res_masks</span><span class="p">,</span>
<span></span>            <span class="n">is_mask_from_pts</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># these frames are what the user interacted with</span>
<span></span>            <span class="n">object_score_logits</span><span class="o">=</span><span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"object_score_logits"</span><span class="p">],</span>
<span></span>            <span class="n">inference_state</span><span class="o">=</span><span class="n">inference_state</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">consolidated_out</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._get_empty_mask_ptr"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._get_empty_mask_ptr</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_get_empty_mask_ptr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Get a dummy object pointer based on an empty mask on the current frame.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td><code>int</code></td><td>The index of the current frame for which to generate the dummy object pointer.</td><td><em>required</em></td></tr><tr><td><code>inference_state</code></td><td><code>dict[str, Any], optional</code></td><td>The current inference state. If None, uses the instance's<br/> inference state.</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>torch.Tensor</code></td><td>A tensor representing the dummy object pointer generated from the empty mask.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1542-L1573"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_get_empty_mask_ptr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Get a dummy object pointer based on an empty mask on the current frame.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        frame_idx (int): The index of the current frame for which to generate the dummy object pointer.</span>
<span></span><span class="sd">        inference_state (dict[str, Any], optional): The current inference state. If None, uses the instance's</span>
<span></span><span class="sd">            inference state.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (torch.Tensor): A tensor representing the dummy object pointer generated from the empty mask.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="n">inference_state</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span>
<span></span>    <span class="c1"># Retrieve correct image features</span>
<span></span>    <span class="n">current_vision_feats</span><span class="p">,</span> <span class="n">current_vision_pos_embeds</span><span class="p">,</span> <span class="n">feat_sizes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"im"</span><span class="p">])</span>
<span></span>
<span></span>    <span class="c1"># Feed the empty mask and image feature above to get a dummy object pointer</span>
<span></span>    <span class="n">current_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">track_step</span><span class="p">(</span>
<span></span>        <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span><span class="p">,</span>
<span></span>        <span class="n">is_init_cond_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span></span>        <span class="n">current_vision_feats</span><span class="o">=</span><span class="n">current_vision_feats</span><span class="p">,</span>
<span></span>        <span class="n">current_vision_pos_embeds</span><span class="o">=</span><span class="n">current_vision_pos_embeds</span><span class="p">,</span>
<span></span>        <span class="n">feat_sizes</span><span class="o">=</span><span class="n">feat_sizes</span><span class="p">,</span>
<span></span>        <span class="n">point_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="c1"># A dummy (empty) mask with a single object</span>
<span></span>        <span class="n">mask_inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
<span></span>        <span class="n">output_dict</span><span class="o">=</span><span class="p">{},</span>
<span></span>        <span class="n">num_frames</span><span class="o">=</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"num_frames"</span><span class="p">],</span>
<span></span>        <span class="n">track_in_reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="n">run_mem_encoder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="n">prev_sam_mask_logits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">]</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._get_maskmem_pos_enc"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._get_maskmem_pos_enc</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_get_maskmem_pos_enc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out_maskmem_pos_enc</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Cache and manage the positional encoding for mask memory across frames and objects.</p><p>This method optimizes storage by caching the positional encoding (<code>maskmem_pos_enc</code>) for mask memory, which is constant across frames and objects, thus reducing the amount of redundant information stored during an inference session. It checks if the positional encoding has already been cached; if not, it caches a slice of the provided encoding. If the batch size is greater than one, it expands the cached positional encoding to match the current batch size.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>out_maskmem_pos_enc</code></td><td><code>list[torch.Tensor] | None</code></td><td>The positional encoding for mask memory. Should be a list<br/> of tensors or None.</td><td><em>required</em></td></tr><tr><td><code>inference_state</code></td><td><code>dict[str, Any], optional</code></td><td>The current inference state. If None, uses the instance's<br/> inference state.</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>list[torch.Tensor]</code></td><td>The positional encoding for mask memory, either cached or expanded.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>The method assumes that <code>out_maskmem_pos_enc</code> is a list of tensors or None.</li><li>Only a single object's slice is cached since the encoding is the same across objects.</li><li>The method checks if the positional encoding has already been cached in the session's constants.</li><li>If the batch size is greater than one, the cached encoding is expanded to fit the batch size.</li></ul></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1390-L1429"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_get_maskmem_pos_enc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out_maskmem_pos_enc</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Cache and manage the positional encoding for mask memory across frames and objects.</span>
<span></span>
<span></span><span class="sd">    This method optimizes storage by caching the positional encoding (`maskmem_pos_enc`) for mask memory, which is</span>
<span></span><span class="sd">    constant across frames and objects, thus reducing the amount of redundant information stored during an inference</span>
<span></span><span class="sd">    session. It checks if the positional encoding has already been cached; if not, it caches a slice of the provided</span>
<span></span><span class="sd">    encoding. If the batch size is greater than one, it expands the cached positional encoding to match the current</span>
<span></span><span class="sd">    batch size.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        out_maskmem_pos_enc (list[torch.Tensor] | None): The positional encoding for mask memory. Should be a list</span>
<span></span><span class="sd">            of tensors or None.</span>
<span></span><span class="sd">        inference_state (dict[str, Any], optional): The current inference state. If None, uses the instance's</span>
<span></span><span class="sd">            inference state.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (list[torch.Tensor]): The positional encoding for mask memory, either cached or expanded.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - The method assumes that `out_maskmem_pos_enc` is a list of tensors or None.</span>
<span></span><span class="sd">        - Only a single object's slice is cached since the encoding is the same across objects.</span>
<span></span><span class="sd">        - The method checks if the positional encoding has already been cached in the session's constants.</span>
<span></span><span class="sd">        - If the batch size is greater than one, the cached encoding is expanded to fit the batch size.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="n">inference_state</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span>
<span></span>    <span class="n">model_constants</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"constants"</span><span class="p">]</span>
<span></span>    <span class="c1"># "out_maskmem_pos_enc" should be either a list of tensors or None</span>
<span></span>    <span class="k">if</span> <span class="n">out_maskmem_pos_enc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="k">if</span> <span class="s2">"maskmem_pos_enc"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_constants</span><span class="p">:</span>
<span></span>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out_maskmem_pos_enc</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
<span></span>            <span class="c1"># only take the slice for one object, since it's same across objects</span>
<span></span>            <span class="n">maskmem_pos_enc</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">out_maskmem_pos_enc</span><span class="p">]</span>
<span></span>            <span class="n">model_constants</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span> <span class="o">=</span> <span class="n">maskmem_pos_enc</span>
<span></span>        <span class="k">else</span><span class="p">:</span>
<span></span>            <span class="n">maskmem_pos_enc</span> <span class="o">=</span> <span class="n">model_constants</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span>
<span></span>        <span class="c1"># expand the cached maskmem_pos_enc to the actual batch size</span>
<span></span>        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">out_maskmem_pos_enc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span></span>        <span class="k">if</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<span></span>            <span class="n">out_maskmem_pos_enc</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">maskmem_pos_enc</span><span class="p">]</span>
<span></span>    <span class="k">return</span> <span class="n">out_maskmem_pos_enc</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._init_state"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._init_state</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_init_state</span><span class="p">(</span><span class="n">num_frames</span><span class="p">)</span>
</code></pre></div><p>Initialize an inference state.</p><p>This function sets up the initial state required for performing inference on video data. It includes initializing various dictionaries and ordered dictionaries that will store inputs, outputs, and other metadata relevant to the tracking process.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>num_frames</code></td><td><code>int</code></td><td>The number of frames in the video.</td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1194-L1233"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@staticmethod</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">_init_state</span><span class="p">(</span><span class="n">num_frames</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Initialize an inference state.</span>
<span></span>
<span></span><span class="sd">    This function sets up the initial state required for performing inference on video data. It includes</span>
<span></span><span class="sd">    initializing various dictionaries and ordered dictionaries that will store inputs, outputs, and other metadata</span>
<span></span><span class="sd">    relevant to the tracking process.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        num_frames (int): The number of frames in the video.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="p">{</span>
<span></span>        <span class="s2">"num_frames"</span><span class="p">:</span> <span class="n">num_frames</span><span class="p">,</span>  <span class="c1"># TODO: see if there's any chance to remove it</span>
<span></span>        <span class="s2">"point_inputs_per_obj"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># inputs points on each frame</span>
<span></span>        <span class="s2">"mask_inputs_per_obj"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># inputs mask on each frame</span>
<span></span>        <span class="s2">"constants"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># values that don't change across frames (so we only need to hold one copy of them)</span>
<span></span>        <span class="c1"># mapping between client-side object id and model-side object index</span>
<span></span>        <span class="s2">"obj_id_to_idx"</span><span class="p">:</span> <span class="n">OrderedDict</span><span class="p">(),</span>
<span></span>        <span class="s2">"obj_idx_to_id"</span><span class="p">:</span> <span class="n">OrderedDict</span><span class="p">(),</span>
<span></span>        <span class="s2">"obj_ids"</span><span class="p">:</span> <span class="p">[],</span>
<span></span>        <span class="c1"># A storage to hold the model's tracking results and states on each frame</span>
<span></span>        <span class="s2">"output_dict"</span><span class="p">:</span> <span class="p">{</span>
<span></span>            <span class="s2">"cond_frame_outputs"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># dict containing {frame_idx: &lt;out&gt;}</span>
<span></span>            <span class="s2">"non_cond_frame_outputs"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># dict containing {frame_idx: &lt;out&gt;}</span>
<span></span>        <span class="p">},</span>
<span></span>        <span class="c1"># Slice (view) of each object tracking results, sharing the same memory with "output_dict"</span>
<span></span>        <span class="s2">"output_dict_per_obj"</span><span class="p">:</span> <span class="p">{},</span>
<span></span>        <span class="c1"># A temporary storage to hold new outputs when user interact with a frame</span>
<span></span>        <span class="c1"># to add clicks or mask (it's merged into "output_dict" before propagation starts)</span>
<span></span>        <span class="s2">"temp_output_dict_per_obj"</span><span class="p">:</span> <span class="p">{},</span>
<span></span>        <span class="c1"># Frames that already holds consolidated outputs from click or mask inputs</span>
<span></span>        <span class="c1"># (we directly use their consolidated outputs during tracking)</span>
<span></span>        <span class="s2">"consolidated_frame_inds"</span><span class="p">:</span> <span class="p">{</span>
<span></span>            <span class="s2">"cond_frame_outputs"</span><span class="p">:</span> <span class="nb">set</span><span class="p">(),</span>  <span class="c1"># set containing frame indices</span>
<span></span>            <span class="s2">"non_cond_frame_outputs"</span><span class="p">:</span> <span class="nb">set</span><span class="p">(),</span>  <span class="c1"># set containing frame indices</span>
<span></span>        <span class="p">},</span>
<span></span>        <span class="c1"># metadata for each tracking frame (e.g. which direction it's tracked)</span>
<span></span>        <span class="s2">"tracking_has_started"</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="s2">"frames_already_tracked"</span><span class="p">:</span> <span class="p">[],</span>
<span></span>    <span class="p">}</span>
<span></span>    <span class="k">return</span> <span class="n">inference_state</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._obj_id_to_idx"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._obj_id_to_idx</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_obj_id_to_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Map client-side object id to model-side object index.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>obj_id</code></td><td><code>int</code></td><td>The unique identifier of the object provided by the client side.</td><td><em>required</em></td></tr><tr><td><code>inference_state</code></td><td><code>dict[str, Any], optional</code></td><td>The current inference state. If None, uses the instance's<br/> inference state.</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>int</code></td><td>The index of the object on the model side.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>The method updates or retrieves mappings between object IDs and indices stored in <code>inference_state</code>.</li><li>It ensures that new objects can only be added before tracking commences.</li><li>It maintains two-way mappings between IDs and indices (<code>obj_id_to_idx</code> and <code>obj_idx_to_id</code>).</li><li>Additional data structures are initialized for the new object to store inputs and outputs.</li></ul></div><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>RuntimeError</code></td><td>If an attempt is made to add a new object after tracking has started.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1258-L1310"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_obj_id_to_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Map client-side object id to model-side object index.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        obj_id (int): The unique identifier of the object provided by the client side.</span>
<span></span><span class="sd">        inference_state (dict[str, Any], optional): The current inference state. If None, uses the instance's</span>
<span></span><span class="sd">            inference state.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (int): The index of the object on the model side.</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        RuntimeError: If an attempt is made to add a new object after tracking has started.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - The method updates or retrieves mappings between object IDs and indices stored in</span>
<span></span><span class="sd">          `inference_state`.</span>
<span></span><span class="sd">        - It ensures that new objects can only be added before tracking commences.</span>
<span></span><span class="sd">        - It maintains two-way mappings between IDs and indices (`obj_id_to_idx` and `obj_idx_to_id`).</span>
<span></span><span class="sd">        - Additional data structures are initialized for the new object to store inputs and outputs.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="n">inference_state</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span>
<span></span>    <span class="n">obj_idx</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_id_to_idx"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">obj_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">obj_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="k">return</span> <span class="n">obj_idx</span>
<span></span>
<span></span>    <span class="c1"># This is a new object id not sent to the server before. We only allow adding</span>
<span></span>    <span class="c1"># new objects *before* the tracking starts.</span>
<span></span>    <span class="n">allow_new_object</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"tracking_has_started"</span><span class="p">]</span>
<span></span>    <span class="k">if</span> <span class="n">allow_new_object</span><span class="p">:</span>
<span></span>        <span class="c1"># get the next object slot</span>
<span></span>        <span class="n">obj_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_id_to_idx"</span><span class="p">])</span>
<span></span>        <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_id_to_idx"</span><span class="p">][</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj_idx</span>
<span></span>        <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_idx_to_id"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj_id</span>
<span></span>        <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_id_to_idx"</span><span class="p">])</span>
<span></span>        <span class="c1"># set up input and output structures for this object</span>
<span></span>        <span class="n">inference_state</span><span class="p">[</span><span class="s2">"point_inputs_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>        <span class="n">inference_state</span><span class="p">[</span><span class="s2">"mask_inputs_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>        <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
<span></span>            <span class="s2">"cond_frame_outputs"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># dict containing {frame_idx: &lt;out&gt;}</span>
<span></span>            <span class="s2">"non_cond_frame_outputs"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># dict containing {frame_idx: &lt;out&gt;}</span>
<span></span>        <span class="p">}</span>
<span></span>        <span class="n">inference_state</span><span class="p">[</span><span class="s2">"temp_output_dict_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
<span></span>            <span class="s2">"cond_frame_outputs"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># dict containing {frame_idx: &lt;out&gt;}</span>
<span></span>            <span class="s2">"non_cond_frame_outputs"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># dict containing {frame_idx: &lt;out&gt;}</span>
<span></span>        <span class="p">}</span>
<span></span>        <span class="k">return</span> <span class="n">obj_idx</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
<span></span>            <span class="sa">f</span><span class="s2">"Cannot add new object id </span><span class="si">{</span><span class="n">obj_id</span><span class="si">}</span><span class="s2"> after tracking starts. "</span>
<span></span>            <span class="sa">f</span><span class="s2">"All existing object ids: </span><span class="si">{</span><span class="n">inference_state</span><span class="p">[</span><span class="s1">'obj_ids'</span><span class="p">]</span><span class="si">}</span><span class="s2">. "</span>
<span></span>            <span class="sa">f</span><span class="s2">"Please call 'reset_state' to restart from scratch."</span>
<span></span>        <span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._prune_non_cond_memory"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._prune_non_cond_memory</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prune_non_cond_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">inference_state</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Prune old non-conditioning frames to bound memory usage.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>inference_state</code></td><td></td><td></td><td><code>None</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1836-L1853"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prune_non_cond_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">inference_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Prune old non-conditioning frames to bound memory usage."""</span>
<span></span>    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem</span><span class="p">:</span>
<span></span>        <span class="k">return</span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="n">inference_state</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span>
<span></span>
<span></span>    <span class="c1"># Determine window size</span>
<span></span>    <span class="n">min_frame</span> <span class="o">=</span> <span class="n">frame_idx</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_maskmem</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">memory_temporal_stride_for_eval</span>
<span></span>    <span class="n">output_dict</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict"</span><span class="p">]</span>
<span></span>
<span></span>    <span class="c1"># Prune global non_cond_frame_outputs</span>
<span></span>    <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span> <span class="k">if</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">min_frame</span><span class="p">]:</span>
<span></span>        <span class="n">output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Prune per-object non_cond_frame_outputs</span>
<span></span>    <span class="k">for</span> <span class="n">obj_output_dict</span> <span class="ow">in</span> <span class="n">inference_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"output_dict_per_obj"</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>        <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span> <span class="k">if</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">min_frame</span><span class="p">]:</span>
<span></span>            <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._reset_tracking_results"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._reset_tracking_results</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_reset_tracking_results</span><span class="p">(</span><span class="n">inference_state</span><span class="p">)</span>
</code></pre></div><p>Reset all tracking inputs and results across the videos.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>inference_state</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1816-L1834"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@staticmethod</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">_reset_tracking_results</span><span class="p">(</span><span class="n">inference_state</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Reset all tracking inputs and results across the videos."""</span>
<span></span>    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"point_inputs_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>        <span class="n">v</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"mask_inputs_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>        <span class="n">v</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>        <span class="n">v</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>        <span class="n">v</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"temp_output_dict_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>        <span class="n">v</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>        <span class="n">v</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict"</span><span class="p">][</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict"</span><span class="p">][</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"consolidated_frame_inds"</span><span class="p">][</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"consolidated_frame_inds"</span><span class="p">][</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"tracking_has_started"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"frames_already_tracked"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"first_ann_frame_idx"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._run_memory_encoder"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._run_memory_encoder</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_run_memory_encoder</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">batch_size</span><span class="p">,</span>
<span></span>    <span class="n">high_res_masks</span><span class="p">,</span>
<span></span>    <span class="n">object_score_logits</span><span class="p">,</span>
<span></span>    <span class="n">is_mask_from_pts</span><span class="p">,</span>
<span></span>    <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Run the memory encoder on masks.</p><p>This is usually after applying non-overlapping constraints to object scores. Since their scores changed, their memory also needs to be computed again with the memory encoder.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>batch_size</code></td><td><code>int</code></td><td>The batch size for processing the frame.</td><td><em>required</em></td></tr><tr><td><code>high_res_masks</code></td><td><code>torch.Tensor</code></td><td>High-resolution masks for which to compute the memory.</td><td><em>required</em></td></tr><tr><td><code>object_score_logits</code></td><td><code>torch.Tensor</code></td><td>Logits representing the object scores.</td><td><em>required</em></td></tr><tr><td><code>is_mask_from_pts</code></td><td><code>bool</code></td><td>Indicates if the mask is derived from point interactions.</td><td><em>required</em></td></tr><tr><td><code>inference_state</code></td><td><code>dict[str, Any], optional</code></td><td>The current inference state. If None, uses the instance's<br/> inference state.</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>maskmem_features (torch.Tensor)</code></td><td>The encoded mask features.</td></tr><tr><td><code>maskmem_pos_enc (torch.Tensor)</code></td><td>The positional encoding.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1575-L1615"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_run_memory_encoder</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">batch_size</span><span class="p">,</span>
<span></span>    <span class="n">high_res_masks</span><span class="p">,</span>
<span></span>    <span class="n">object_score_logits</span><span class="p">,</span>
<span></span>    <span class="n">is_mask_from_pts</span><span class="p">,</span>
<span></span>    <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Run the memory encoder on masks.</span>
<span></span>
<span></span><span class="sd">    This is usually after applying non-overlapping constraints to object scores. Since their scores changed, their</span>
<span></span><span class="sd">    memory also needs to be computed again with the memory encoder.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        batch_size (int): The batch size for processing the frame.</span>
<span></span><span class="sd">        high_res_masks (torch.Tensor): High-resolution masks for which to compute the memory.</span>
<span></span><span class="sd">        object_score_logits (torch.Tensor): Logits representing the object scores.</span>
<span></span><span class="sd">        is_mask_from_pts (bool): Indicates if the mask is derived from point interactions.</span>
<span></span><span class="sd">        inference_state (dict[str, Any], optional): The current inference state. If None, uses the instance's</span>
<span></span><span class="sd">            inference state.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        maskmem_features (torch.Tensor): The encoded mask features.</span>
<span></span><span class="sd">        maskmem_pos_enc (torch.Tensor): The positional encoding.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="n">inference_state</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span>
<span></span>    <span class="c1"># Retrieve correct image features</span>
<span></span>    <span class="n">current_vision_feats</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">feat_sizes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"im"</span><span class="p">],</span> <span class="n">batch_size</span><span class="p">)</span>
<span></span>    <span class="n">maskmem_features</span><span class="p">,</span> <span class="n">maskmem_pos_enc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_encode_new_memory</span><span class="p">(</span>
<span></span>        <span class="n">current_vision_feats</span><span class="o">=</span><span class="n">current_vision_feats</span><span class="p">,</span>
<span></span>        <span class="n">feat_sizes</span><span class="o">=</span><span class="n">feat_sizes</span><span class="p">,</span>
<span></span>        <span class="n">pred_masks_high_res</span><span class="o">=</span><span class="n">high_res_masks</span><span class="p">,</span>
<span></span>        <span class="n">is_mask_from_pts</span><span class="o">=</span><span class="n">is_mask_from_pts</span><span class="p">,</span>
<span></span>        <span class="n">object_score_logits</span><span class="o">=</span><span class="n">object_score_logits</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># "maskmem_pos_enc" is the same across frames, so we only need to store one copy of it</span>
<span></span>    <span class="n">maskmem_pos_enc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_maskmem_pos_enc</span><span class="p">(</span><span class="n">maskmem_pos_enc</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">maskmem_features</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
<span></span>        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"cuda"</span>
<span></span>    <span class="p">),</span> <span class="n">maskmem_pos_enc</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._run_single_frame_inference"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._run_single_frame_inference</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_run_single_frame_inference</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">output_dict</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">,</span>
<span></span>    <span class="n">batch_size</span><span class="p">,</span>
<span></span>    <span class="n">is_init_cond_frame</span><span class="p">,</span>
<span></span>    <span class="n">point_inputs</span><span class="p">,</span>
<span></span>    <span class="n">mask_inputs</span><span class="p">,</span>
<span></span>    <span class="n">reverse</span><span class="p">,</span>
<span></span>    <span class="n">run_mem_encoder</span><span class="p">,</span>
<span></span>    <span class="n">prev_sam_mask_logits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Run tracking on a single frame based on current inputs and previous memory.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>output_dict</code></td><td><code>dict</code></td><td>The dictionary containing the output states of the tracking process.</td><td><em>required</em></td></tr><tr><td><code>frame_idx</code></td><td><code>int</code></td><td>The index of the current frame.</td><td><em>required</em></td></tr><tr><td><code>batch_size</code></td><td><code>int</code></td><td>The batch size for processing the frame.</td><td><em>required</em></td></tr><tr><td><code>is_init_cond_frame</code></td><td><code>bool</code></td><td>Indicates if the current frame is an initial conditioning frame.</td><td><em>required</em></td></tr><tr><td><code>point_inputs</code></td><td><code>dict | None</code></td><td>Input points and their labels.</td><td><em>required</em></td></tr><tr><td><code>mask_inputs</code></td><td><code>torch.Tensor | None</code></td><td>Input binary masks.</td><td><em>required</em></td></tr><tr><td><code>reverse</code></td><td><code>bool</code></td><td>Indicates if the tracking should be performed in reverse order.</td><td><em>required</em></td></tr><tr><td><code>run_mem_encoder</code></td><td><code>bool</code></td><td>Indicates if the memory encoder should be executed.</td><td><em>required</em></td></tr><tr><td><code>prev_sam_mask_logits</code></td><td><code>torch.Tensor | None</code></td><td>Previous mask logits for the current object.</td><td><code>None</code></td></tr><tr><td><code>inference_state</code></td><td><code>dict[str, Any], optional</code></td><td>The current inference state. If None, uses the instance's<br/> inference state.</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>dict</code></td><td>A dictionary containing the output of the tracking step, including updated features and predictions.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>The method assumes that <code>point_inputs</code> and <code>mask_inputs</code> are mutually exclusive.</li><li>The method retrieves image features using the <code>get_im_features</code> method.</li><li>The <code>maskmem_pos_enc</code> is assumed to be constant across frames, hence only one copy is stored.</li><li>The <code>fill_holes_in_mask_scores</code> function is commented out and currently unsupported due to CUDA extension requirements.</li></ul></div><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>AssertionError</code></td><td>If both <code>point_inputs</code> and <code>mask_inputs</code> are provided, or neither is provided.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1312-L1388"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_run_single_frame_inference</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">output_dict</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">,</span>
<span></span>    <span class="n">batch_size</span><span class="p">,</span>
<span></span>    <span class="n">is_init_cond_frame</span><span class="p">,</span>
<span></span>    <span class="n">point_inputs</span><span class="p">,</span>
<span></span>    <span class="n">mask_inputs</span><span class="p">,</span>
<span></span>    <span class="n">reverse</span><span class="p">,</span>
<span></span>    <span class="n">run_mem_encoder</span><span class="p">,</span>
<span></span>    <span class="n">prev_sam_mask_logits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Run tracking on a single frame based on current inputs and previous memory.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        output_dict (dict): The dictionary containing the output states of the tracking process.</span>
<span></span><span class="sd">        frame_idx (int): The index of the current frame.</span>
<span></span><span class="sd">        batch_size (int): The batch size for processing the frame.</span>
<span></span><span class="sd">        is_init_cond_frame (bool): Indicates if the current frame is an initial conditioning frame.</span>
<span></span><span class="sd">        point_inputs (dict | None): Input points and their labels.</span>
<span></span><span class="sd">        mask_inputs (torch.Tensor | None): Input binary masks.</span>
<span></span><span class="sd">        reverse (bool): Indicates if the tracking should be performed in reverse order.</span>
<span></span><span class="sd">        run_mem_encoder (bool): Indicates if the memory encoder should be executed.</span>
<span></span><span class="sd">        prev_sam_mask_logits (torch.Tensor | None): Previous mask logits for the current object.</span>
<span></span><span class="sd">        inference_state (dict[str, Any], optional): The current inference state. If None, uses the instance's</span>
<span></span><span class="sd">            inference state.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (dict): A dictionary containing the output of the tracking step, including updated features and predictions.</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        AssertionError: If both `point_inputs` and `mask_inputs` are provided, or neither is provided.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - The method assumes that `point_inputs` and `mask_inputs` are mutually exclusive.</span>
<span></span><span class="sd">        - The method retrieves image features using the `get_im_features` method.</span>
<span></span><span class="sd">        - The `maskmem_pos_enc` is assumed to be constant across frames, hence only one copy is stored.</span>
<span></span><span class="sd">        - The `fill_holes_in_mask_scores` function is commented out and currently unsupported due to CUDA extension requirements.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="n">inference_state</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span>
<span></span>    <span class="c1"># Retrieve correct image features</span>
<span></span>    <span class="n">current_vision_feats</span><span class="p">,</span> <span class="n">current_vision_pos_embeds</span><span class="p">,</span> <span class="n">feat_sizes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span>
<span></span>        <span class="n">inference_state</span><span class="p">[</span><span class="s2">"im"</span><span class="p">],</span> <span class="n">batch_size</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># point and mask should not appear as input simultaneously on the same frame</span>
<span></span>    <span class="k">assert</span> <span class="n">point_inputs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">mask_inputs</span> <span class="ow">is</span> <span class="kc">None</span>
<span></span>    <span class="n">current_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">track_step</span><span class="p">(</span>
<span></span>        <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span><span class="p">,</span>
<span></span>        <span class="n">is_init_cond_frame</span><span class="o">=</span><span class="n">is_init_cond_frame</span><span class="p">,</span>
<span></span>        <span class="n">current_vision_feats</span><span class="o">=</span><span class="n">current_vision_feats</span><span class="p">,</span>
<span></span>        <span class="n">current_vision_pos_embeds</span><span class="o">=</span><span class="n">current_vision_pos_embeds</span><span class="p">,</span>
<span></span>        <span class="n">feat_sizes</span><span class="o">=</span><span class="n">feat_sizes</span><span class="p">,</span>
<span></span>        <span class="n">point_inputs</span><span class="o">=</span><span class="n">point_inputs</span><span class="p">,</span>
<span></span>        <span class="n">mask_inputs</span><span class="o">=</span><span class="n">mask_inputs</span><span class="p">,</span>
<span></span>        <span class="n">output_dict</span><span class="o">=</span><span class="n">output_dict</span><span class="p">,</span>
<span></span>        <span class="n">num_frames</span><span class="o">=</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"num_frames"</span><span class="p">],</span>
<span></span>        <span class="n">track_in_reverse</span><span class="o">=</span><span class="n">reverse</span><span class="p">,</span>
<span></span>        <span class="n">run_mem_encoder</span><span class="o">=</span><span class="n">run_mem_encoder</span><span class="p">,</span>
<span></span>        <span class="n">prev_sam_mask_logits</span><span class="o">=</span><span class="n">prev_sam_mask_logits</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="n">maskmem_features</span> <span class="o">=</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">]</span>
<span></span>    <span class="k">if</span> <span class="n">maskmem_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">current_out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">]</span> <span class="o">=</span> <span class="n">maskmem_features</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"cuda"</span>
<span></span>        <span class="p">)</span>
<span></span>    <span class="c1"># NOTE: Do not support the `fill_holes_in_mask_scores` function since it needs cuda extensions</span>
<span></span>    <span class="c1"># potentially fill holes in the predicted masks</span>
<span></span>    <span class="c1"># if self.fill_hole_area &gt; 0:</span>
<span></span>    <span class="c1">#     pred_masks = current_out["pred_masks"].to(self.device, non_blocking=self.device.type == "cuda")</span>
<span></span>    <span class="c1">#     pred_masks = fill_holes_in_mask_scores(pred_masks, self.fill_hole_area)</span>
<span></span>
<span></span>    <span class="c1"># "maskmem_pos_enc" is the same across frames, so we only need to store one copy of it</span>
<span></span>    <span class="n">current_out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_maskmem_pos_enc</span><span class="p">(</span><span class="n">current_out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">],</span> <span class="n">inference_state</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">current_out</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.add_new_prompts"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.add_new_prompts</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">add_new_prompts</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">obj_id</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span></span>    <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Add new points or masks to a specific frame for a given object ID.</p><p>This method updates the inference state with new prompts (points or masks) for a specified object and frame index. It ensures that the prompts are either points or masks, but not both, and updates the internal state accordingly. It also handles the generation of new segmentations based on the provided prompts and the existing state.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>obj_id</code></td><td><code>int</code></td><td>The ID of the object to which the prompts are associated.</td><td><em>required</em></td></tr><tr><td><code>points</code></td><td><code>torch.Tensor, optional</code></td><td>The coordinates of the points of interest.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>torch.Tensor, optional</code></td><td>The labels corresponding to the points.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>torch.Tensor, optional</code></td><td>Binary masks for the object.</td><td><code>None</code></td></tr><tr><td><code>frame_idx</code></td><td><code>int, optional</code></td><td>The index of the frame to which the prompts are applied.</td><td><code>0</code></td></tr><tr><td><code>inference_state</code></td><td><code>dict[str, Any], optional</code></td><td>The current inference state. If None, uses the instance's<br/> inference state.</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>The flattened predicted masks.</td></tr><tr><td><code>pred_scores (torch.Tensor)</code></td><td>A tensor of ones indicating the number of objects.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>Only one type of prompt (either points or masks) can be added per call.</li><li>If the frame is being tracked for the first time, it is treated as an initial conditioning frame.</li><li>The method handles the consolidation of outputs and resizing of masks to the original video resolution.</li></ul></div><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>AssertionError</code></td><td>If both <code>masks</code> and <code>points</code> are provided, or neither is provided.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L993-L1099"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">add_new_prompts</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">obj_id</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span></span>    <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Add new points or masks to a specific frame for a given object ID.</span>
<span></span>
<span></span><span class="sd">    This method updates the inference state with new prompts (points or masks) for a specified object and frame</span>
<span></span><span class="sd">    index. It ensures that the prompts are either points or masks, but not both, and updates the internal state</span>
<span></span><span class="sd">    accordingly. It also handles the generation of new segmentations based on the provided prompts and the existing</span>
<span></span><span class="sd">    state.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        obj_id (int): The ID of the object to which the prompts are associated.</span>
<span></span><span class="sd">        points (torch.Tensor, optional): The coordinates of the points of interest.</span>
<span></span><span class="sd">        labels (torch.Tensor, optional): The labels corresponding to the points.</span>
<span></span><span class="sd">        masks (torch.Tensor, optional): Binary masks for the object.</span>
<span></span><span class="sd">        frame_idx (int, optional): The index of the frame to which the prompts are applied.</span>
<span></span><span class="sd">        inference_state (dict[str, Any], optional): The current inference state. If None, uses the instance's</span>
<span></span><span class="sd">            inference state.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): The flattened predicted masks.</span>
<span></span><span class="sd">        pred_scores (torch.Tensor): A tensor of ones indicating the number of objects.</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        AssertionError: If both `masks` and `points` are provided, or neither is provided.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - Only one type of prompt (either points or masks) can be added per call.</span>
<span></span><span class="sd">        - If the frame is being tracked for the first time, it is treated as an initial conditioning frame.</span>
<span></span><span class="sd">        - The method handles the consolidation of outputs and resizing of masks to the original video resolution.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="n">inference_state</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span>
<span></span>    <span class="k">assert</span> <span class="p">(</span><span class="n">masks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span><span class="n">points</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span> <span class="s2">"'masks' and 'points' prompts are not compatible with each other."</span>
<span></span>    <span class="n">obj_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_obj_id_to_idx</span><span class="p">(</span><span class="n">obj_id</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">point_inputs</span> <span class="o">=</span> <span class="kc">None</span>
<span></span>    <span class="n">pop_key</span> <span class="o">=</span> <span class="s2">"point_inputs_per_obj"</span>
<span></span>    <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">point_inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"point_coords"</span><span class="p">:</span> <span class="n">points</span><span class="p">,</span> <span class="s2">"point_labels"</span><span class="p">:</span> <span class="n">labels</span><span class="p">}</span>
<span></span>        <span class="n">inference_state</span><span class="p">[</span><span class="s2">"point_inputs_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">point_inputs</span>
<span></span>        <span class="n">pop_key</span> <span class="o">=</span> <span class="s2">"mask_inputs_per_obj"</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"mask_inputs_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">masks</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="n">pop_key</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>    <span class="c1"># If this frame hasn't been tracked before, we treat it as an initial conditioning</span>
<span></span>    <span class="c1"># frame, meaning that the inputs points are to generate segments on this frame without</span>
<span></span>    <span class="c1"># using any memory from other frames, like in SAM. Otherwise (if it has been tracked),</span>
<span></span>    <span class="c1"># the input points will be used to correct the already tracked masks.</span>
<span></span>    <span class="n">is_init_cond_frame</span> <span class="o">=</span> <span class="n">frame_idx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"frames_already_tracked"</span><span class="p">]</span>
<span></span>    <span class="n">obj_output_dict</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span>
<span></span>    <span class="n">obj_temp_output_dict</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"temp_output_dict_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span>
<span></span>    <span class="c1"># Add a frame to conditioning output if it's an initial conditioning frame or</span>
<span></span>    <span class="c1"># if the model sees all frames receiving clicks/mask as conditioning frames.</span>
<span></span>    <span class="n">is_cond</span> <span class="o">=</span> <span class="n">is_init_cond_frame</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add_all_frames_to_correct_as_cond</span>
<span></span>    <span class="n">storage_key</span> <span class="o">=</span> <span class="s2">"cond_frame_outputs"</span> <span class="k">if</span> <span class="n">is_cond</span> <span class="k">else</span> <span class="s2">"non_cond_frame_outputs"</span>
<span></span>
<span></span>    <span class="c1"># Get any previously predicted mask logits on this object and feed it along with</span>
<span></span>    <span class="c1"># the new clicks into the SAM mask decoder.</span>
<span></span>    <span class="n">prev_sam_mask_logits</span> <span class="o">=</span> <span class="kc">None</span>
<span></span>    <span class="c1"># lookup temporary output dict first, which contains the most recent output</span>
<span></span>    <span class="c1"># (if not found, then lookup conditioning and non-conditioning frame output)</span>
<span></span>    <span class="k">if</span> <span class="n">point_inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">prev_out</span> <span class="o">=</span> <span class="p">(</span>
<span></span>            <span class="n">obj_temp_output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>            <span class="ow">or</span> <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>            <span class="ow">or</span> <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>        <span class="k">if</span> <span class="n">prev_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">prev_out</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"pred_masks"</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">prev_sam_mask_logits</span> <span class="o">=</span> <span class="n">prev_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
<span></span>                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"cuda"</span>
<span></span>            <span class="p">)</span>
<span></span>            <span class="c1"># Clamp the scale of prev_sam_mask_logits to avoid rare numerical issues.</span>
<span></span>            <span class="n">prev_sam_mask_logits</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="o">-</span><span class="mf">32.0</span><span class="p">,</span> <span class="mf">32.0</span><span class="p">)</span>
<span></span>    <span class="n">current_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_single_frame_inference</span><span class="p">(</span>
<span></span>        <span class="n">output_dict</span><span class="o">=</span><span class="n">obj_output_dict</span><span class="p">,</span>  <span class="c1"># run on the slice of a single object</span>
<span></span>        <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span><span class="p">,</span>
<span></span>        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># run on the slice of a single object</span>
<span></span>        <span class="n">is_init_cond_frame</span><span class="o">=</span><span class="n">is_init_cond_frame</span><span class="p">,</span>
<span></span>        <span class="n">point_inputs</span><span class="o">=</span><span class="n">point_inputs</span><span class="p">,</span>
<span></span>        <span class="n">mask_inputs</span><span class="o">=</span><span class="n">masks</span><span class="p">,</span>
<span></span>        <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="c1"># Skip the memory encoder when adding clicks or mask. We execute the memory encoder</span>
<span></span>        <span class="c1"># at the beginning of `propagate_in_video` (after user finalize their clicks). This</span>
<span></span>        <span class="c1"># allows us to enforce non-overlapping constraints on all objects before encoding</span>
<span></span>        <span class="c1"># them into memory.</span>
<span></span>        <span class="n">run_mem_encoder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="n">prev_sam_mask_logits</span><span class="o">=</span><span class="n">prev_sam_mask_logits</span><span class="p">,</span>
<span></span>        <span class="n">inference_state</span><span class="o">=</span><span class="n">inference_state</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="c1"># Add the output to the output dict (to be used as future memory)</span>
<span></span>    <span class="n">obj_temp_output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_out</span>
<span></span>
<span></span>    <span class="c1"># Resize the output mask to the original video resolution</span>
<span></span>    <span class="n">consolidated_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_consolidate_temp_output_across_obj</span><span class="p">(</span>
<span></span>        <span class="n">frame_idx</span><span class="p">,</span>
<span></span>        <span class="n">is_cond</span><span class="o">=</span><span class="n">is_cond</span><span class="p">,</span>
<span></span>        <span class="n">run_mem_encoder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="n">inference_state</span><span class="o">=</span><span class="n">inference_state</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.clear_all_points_in_frame"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.clear_all_points_in_frame</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">clear_all_points_in_frame</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">)</span>
</code></pre></div><p>Remove all input points or mask in a specific frame for a given object.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>inference_state</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>frame_idx</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>obj_id</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1754-L1800"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">clear_all_points_in_frame</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Remove all input points or mask in a specific frame for a given object."""</span>
<span></span>    <span class="n">obj_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_obj_id_to_idx</span><span class="p">(</span><span class="n">obj_id</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Clear the conditioning information on the given frame</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"point_inputs_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"mask_inputs_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">temp_output_dict_per_obj</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"temp_output_dict_per_obj"</span><span class="p">]</span>
<span></span>    <span class="n">temp_output_dict_per_obj</span><span class="p">[</span><span class="n">obj_idx</span><span class="p">][</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>    <span class="n">temp_output_dict_per_obj</span><span class="p">[</span><span class="n">obj_idx</span><span class="p">][</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Check and see if there are still any inputs left on this frame</span>
<span></span>    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_idx_to_id"</span><span class="p">])</span>
<span></span>    <span class="n">frame_has_input</span> <span class="o">=</span> <span class="kc">False</span>
<span></span>    <span class="k">for</span> <span class="n">obj_idx2</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
<span></span>        <span class="k">if</span> <span class="n">frame_idx</span> <span class="ow">in</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"point_inputs_per_obj"</span><span class="p">][</span><span class="n">obj_idx2</span><span class="p">]:</span>
<span></span>            <span class="n">frame_has_input</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>            <span class="k">break</span>
<span></span>        <span class="k">if</span> <span class="n">frame_idx</span> <span class="ow">in</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"mask_inputs_per_obj"</span><span class="p">][</span><span class="n">obj_idx2</span><span class="p">]:</span>
<span></span>            <span class="n">frame_has_input</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>            <span class="k">break</span>
<span></span>
<span></span>    <span class="c1"># If this frame has no remaining inputs for any objects, we further clear its</span>
<span></span>    <span class="c1"># conditioning frame status</span>
<span></span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">frame_has_input</span><span class="p">:</span>
<span></span>        <span class="n">output_dict</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict"</span><span class="p">]</span>
<span></span>        <span class="n">consolidated_frame_inds</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"consolidated_frame_inds"</span><span class="p">]</span>
<span></span>        <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>        <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>        <span class="c1"># Remove the frame's conditioning output (possibly downgrading it to non-conditioning)</span>
<span></span>        <span class="n">out</span> <span class="o">=</span> <span class="n">output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>        <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="c1"># The frame is not a conditioning frame anymore since it's not receiving inputs,</span>
<span></span>            <span class="c1"># so we "downgrade" its output (if exists) to a non-conditioning frame output.</span>
<span></span>            <span class="n">output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span>
<span></span>            <span class="n">inference_state</span><span class="p">[</span><span class="s2">"frames_already_tracked"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>        <span class="c1"># Similarly, do it for the sliced output on each object.</span>
<span></span>        <span class="k">for</span> <span class="n">obj_idx2</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
<span></span>            <span class="n">obj_output_dict</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">][</span><span class="n">obj_idx2</span><span class="p">]</span>
<span></span>            <span class="n">obj_out</span> <span class="o">=</span> <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>            <span class="k">if</span> <span class="n">obj_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>                <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj_out</span>
<span></span>
<span></span>        <span class="c1"># If all the conditioning frames have been removed, we also clear the tracking outputs</span>
<span></span>        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>            <span class="bp">self</span><span class="o">.</span><span class="n">_reset_tracking_results</span><span class="p">(</span><span class="n">inference_state</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.clear_all_points_in_video"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.clear_all_points_in_video</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">clear_all_points_in_video</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">)</span>
</code></pre></div><p>Remove all input points or mask in all frames throughout the video.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>inference_state</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1803-L1813"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">clear_all_points_in_video</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Remove all input points or mask in all frames throughout the video."""</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">_reset_tracking_results</span><span class="p">(</span><span class="n">inference_state</span><span class="p">)</span>
<span></span>    <span class="c1"># Remove all object ids</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_id_to_idx"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_idx_to_id"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"point_inputs_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"mask_inputs_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"temp_output_dict_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.get_im_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.get_im_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>Extract and process image features using SAM2's image encoder for subsequent segmentation tasks.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor</code></td><td>The input image tensor.</td><td><em>required</em></td></tr><tr><td><code>batch</code></td><td><code>int, optional</code></td><td>The batch size for expanding features if there are multiple prompts.</td><td><code>1</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>vis_feats (torch.Tensor)</code></td><td>The visual features extracted from the image.</td></tr><tr><td><code>vis_pos_embed (torch.Tensor)</code></td><td>The positional embeddings for the visual features.</td></tr><tr><td><code>feat_sizes (list[tuple])</code></td><td>A list containing the sizes of the extracted features.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>If <code>batch</code> is greater than 1, the features are expanded to fit the batch size.</li><li>The method leverages the model's <code>_prepare_backbone_features</code> method to prepare the backbone features.</li></ul></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1235-L1256"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Extract and process image features using SAM2's image encoder for subsequent segmentation tasks.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (torch.Tensor): The input image tensor.</span>
<span></span><span class="sd">        batch (int, optional): The batch size for expanding features if there are multiple prompts.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        vis_feats (torch.Tensor): The visual features extracted from the image.</span>
<span></span><span class="sd">        vis_pos_embed (torch.Tensor): The positional embeddings for the visual features.</span>
<span></span><span class="sd">        feat_sizes (list[tuple]): A list containing the sizes of the extracted features.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - If `batch` is greater than 1, the features are expanded to fit the batch size.</span>
<span></span><span class="sd">        - The method leverages the model's `_prepare_backbone_features` method to prepare the backbone features.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="c1"># check if there's precomputed backbone output</span>
<span></span>    <span class="n">backbone_out</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">"backbone_out"</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">backbone_out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">backbone_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward_image</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
<span></span>    <span class="n">_</span><span class="p">,</span> <span class="n">vis_feats</span><span class="p">,</span> <span class="n">vis_pos_embed</span><span class="p">,</span> <span class="n">feat_sizes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_prepare_backbone_features</span><span class="p">(</span><span class="n">backbone_out</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">vis_feats</span><span class="p">,</span> <span class="n">vis_pos_embed</span><span class="p">,</span> <span class="n">feat_sizes</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.get_model"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.get_model</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div><p>Retrieve and configure the model with binarization enabled.</p><div class="admonition note"><p class="admonition-title">Notes</p><p>This method overrides the base class implementation to set the binarize flag to True.</p></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L883-L891"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Retrieve and configure the model with binarization enabled.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        This method overrides the base class implementation to set the binarize flag to True.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">model</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_model</span><span class="p">()</span>
<span></span>    <span class="n">model</span><span class="o">.</span><span class="n">set_binarize</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.inference"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.inference</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">points</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Perform image segmentation inference based on the given input cues, using the currently loaded image. This</p><p>method leverages SAM's (Segment Anything Model) architecture consisting of image encoder, prompt encoder, and mask decoder for real-time and promptable segmentation tasks.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor</code></td><td>The preprocessed input image in tensor format, with shape (N, C, H, W).</td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td><code>np.ndarray | list, optional</code></td><td>Bounding boxes with shape (N, 4), in XYXY format.</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list, optional</code></td><td>Points indicating object locations with shape (N, 2), in pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list, optional</code></td><td>Labels for point prompts, shape (N, ). 1 = foreground, 0 = background.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>np.ndarray, optional</code></td><td>Low-resolution masks from previous predictions shape (N,H,W). For SAM H=W=256.</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>The output masks in shape CxHxW, where C is the number of generated masks.</td></tr><tr><td><code>pred_scores (torch.Tensor)</code></td><td>An array of length C containing predicted quality scores for each mask.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L893-L964"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform image segmentation inference based on the given input cues, using the currently loaded image. This</span>
<span></span><span class="sd">    method leverages SAM's (Segment Anything Model) architecture consisting of image encoder, prompt</span>
<span></span><span class="sd">    encoder, and mask decoder for real-time and promptable segmentation tasks.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (torch.Tensor): The preprocessed input image in tensor format, with shape (N, C, H, W).</span>
<span></span><span class="sd">        bboxes (np.ndarray | list, optional): Bounding boxes with shape (N, 4), in XYXY format.</span>
<span></span><span class="sd">        points (np.ndarray | list, optional): Points indicating object locations with shape (N, 2), in pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list, optional): Labels for point prompts, shape (N, ). 1 = foreground, 0 = background.</span>
<span></span><span class="sd">        masks (np.ndarray, optional): Low-resolution masks from previous predictions shape (N,H,W). For SAM H=W=256.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): The output masks in shape CxHxW, where C is the number of generated masks.</span>
<span></span><span class="sd">        pred_scores (torch.Tensor): An array of length C containing predicted quality scores for each mask.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="c1"># Override prompts if any stored in self.prompts</span>
<span></span>    <span class="n">bboxes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"bboxes"</span><span class="p">,</span> <span class="n">bboxes</span><span class="p">)</span>
<span></span>    <span class="n">points</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"points"</span><span class="p">,</span> <span class="n">points</span><span class="p">)</span>
<span></span>    <span class="n">masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"masks"</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">frame</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">frame</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"im"</span><span class="p">]</span> <span class="o">=</span> <span class="n">im</span>
<span></span>    <span class="n">output_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict"</span><span class="p">]</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># initialize prompts</span>
<span></span>        <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_prompts</span><span class="p">(</span>
<span></span>            <span class="n">im</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)):</span>
<span></span>                <span class="bp">self</span><span class="o">.</span><span class="n">add_new_prompts</span><span class="p">(</span><span class="n">obj_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="n">points</span><span class="p">[[</span><span class="n">i</span><span class="p">]],</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">[[</span><span class="n">i</span><span class="p">]],</span> <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame</span><span class="p">)</span>
<span></span>        <span class="k">elif</span> <span class="n">masks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">masks</span><span class="p">)):</span>
<span></span>                <span class="bp">self</span><span class="o">.</span><span class="n">add_new_prompts</span><span class="p">(</span><span class="n">obj_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="n">masks</span><span class="p">[[</span><span class="n">i</span><span class="p">]],</span> <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">propagate_in_video_preflight</span><span class="p">()</span>
<span></span>
<span></span>    <span class="n">consolidated_frame_inds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"consolidated_frame_inds"</span><span class="p">]</span>
<span></span>    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_idx_to_id"</span><span class="p">])</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">"No points are provided; please add points first"</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]:</span>
<span></span>        <span class="n">storage_key</span> <span class="o">=</span> <span class="s2">"cond_frame_outputs"</span>
<span></span>        <span class="n">current_out</span> <span class="o">=</span> <span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame</span><span class="p">]</span>
<span></span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem_around_input</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem_for_multi_obj</span> <span class="ow">or</span> <span class="n">batch_size</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">):</span>
<span></span>            <span class="c1"># clear non-conditioning memory of the surrounding frames</span>
<span></span>            <span class="bp">self</span><span class="o">.</span><span class="n">_clear_non_cond_mem_around_input</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
<span></span>    <span class="k">elif</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]:</span>
<span></span>        <span class="n">storage_key</span> <span class="o">=</span> <span class="s2">"non_cond_frame_outputs"</span>
<span></span>        <span class="n">current_out</span> <span class="o">=</span> <span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame</span><span class="p">]</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="n">storage_key</span> <span class="o">=</span> <span class="s2">"non_cond_frame_outputs"</span>
<span></span>        <span class="n">current_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_single_frame_inference</span><span class="p">(</span>
<span></span>            <span class="n">output_dict</span><span class="o">=</span><span class="n">output_dict</span><span class="p">,</span>
<span></span>            <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame</span><span class="p">,</span>
<span></span>            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span></span>            <span class="n">is_init_cond_frame</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>            <span class="n">point_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>            <span class="n">mask_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>            <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>            <span class="n">run_mem_encoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_out</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">_prune_non_cond_memory</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
<span></span>    <span class="c1"># Create slices of per-object outputs for subsequent interaction with each</span>
<span></span>    <span class="c1"># individual object after tracking.</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">_add_output_per_object</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">current_out</span><span class="p">,</span> <span class="n">storage_key</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"frames_already_tracked"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
<span></span>    <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">pred_masks</span><span class="p">[(</span><span class="n">pred_masks</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">mask_threshold</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># filter blank masks</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.init_state"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.init_state</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">init_state</span><span class="p">(</span><span class="n">predictor</span><span class="p">)</span>
</code></pre></div><p>Initialize an inference state for the predictor.</p><p>This function sets up the initial state required for performing inference on video data. It includes initializing various dictionaries and ordered dictionaries that will store inputs, outputs, and other metadata relevant to the tracking process.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>predictor</code></td><td><code>SAM2VideoPredictor</code></td><td>The predictor object for which to initialize the state.</td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1177-L1191"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@staticmethod</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">init_state</span><span class="p">(</span><span class="n">predictor</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Initialize an inference state for the predictor.</span>
<span></span>
<span></span><span class="sd">    This function sets up the initial state required for performing inference on video data. It includes</span>
<span></span><span class="sd">    initializing various dictionaries and ordered dictionaries that will store inputs, outputs, and other metadata</span>
<span></span><span class="sd">    relevant to the tracking process.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        predictor (SAM2VideoPredictor): The predictor object for which to initialize the state.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">predictor</span><span class="o">.</span><span class="n">inference_state</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># means initialized</span>
<span></span>        <span class="k">return</span>
<span></span>    <span class="k">assert</span> <span class="n">predictor</span><span class="o">.</span><span class="n">dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span></span>    <span class="k">assert</span> <span class="n">predictor</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">"video"</span>
<span></span>    <span class="n">predictor</span><span class="o">.</span><span class="n">inference_state</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">_init_state</span><span class="p">(</span><span class="n">predictor</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">frames</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.postprocess"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.postprocess</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">)</span>
</code></pre></div><p>Post-process the predictions to apply non-overlapping constraints if required.</p><p>This method extends the post-processing functionality by applying non-overlapping constraints to the predicted masks if the <code>non_overlap_masks</code> flag is set to True. This ensures that the masks do not overlap, which can be useful for certain applications.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>preds</code></td><td><code>tuple[torch.Tensor, torch.Tensor]</code></td><td>The predicted masks and scores from the model.</td><td><em>required</em></td></tr><tr><td><code>img</code></td><td><code>torch.Tensor</code></td><td>The processed image tensor.</td><td><em>required</em></td></tr><tr><td><code>orig_imgs</code></td><td><code>list[np.ndarray]</code></td><td>The original images before processing.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>list</code></td><td>The post-processed predictions.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><p>If <code>non_overlap_masks</code> is True, the method applies constraints to ensure non-overlapping masks.</p></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L966-L990"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Post-process the predictions to apply non-overlapping constraints if required.</span>
<span></span>
<span></span><span class="sd">    This method extends the post-processing functionality by applying non-overlapping constraints to the predicted</span>
<span></span><span class="sd">    masks if the `non_overlap_masks` flag is set to True. This ensures that the masks do not overlap, which can be</span>
<span></span><span class="sd">    useful for certain applications.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        preds (tuple[torch.Tensor, torch.Tensor]): The predicted masks and scores from the model.</span>
<span></span><span class="sd">        img (torch.Tensor): The processed image tensor.</span>
<span></span><span class="sd">        orig_imgs (list[np.ndarray]): The original images before processing.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (list): The post-processed predictions.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        If `non_overlap_masks` is True, the method applies constraints to ensure non-overlapping masks.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">results</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">postprocess</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_overlap_masks</span><span class="p">:</span>
<span></span>        <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
<span></span>            <span class="k">if</span> <span class="n">result</span><span class="o">.</span><span class="n">masks</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">masks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>                <span class="k">continue</span>
<span></span>            <span class="n">result</span><span class="o">.</span><span class="n">masks</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_apply_non_overlapping_constraints</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">masks</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span></span>    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.propagate_in_video_preflight"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.propagate_in_video_preflight</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">propagate_in_video_preflight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Prepare inference_state and consolidate temporary outputs before tracking.</p><p>This method marks the start of tracking, disallowing the addition of new objects until the session is reset. It consolidates temporary outputs from <code>temp_output_dict_per_obj</code> and merges them into <code>output_dict</code>. Additionally, it clears non-conditioning memory around input frames and ensures that the state is consistent with the provided inputs.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>inference_state</code></td><td><code>dict[str, Any], optional</code></td><td>The current inference state. If None, uses the instance's<br/> inference state.</td><td><code>None</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1102-L1174"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">propagate_in_video_preflight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Prepare inference_state and consolidate temporary outputs before tracking.</span>
<span></span>
<span></span><span class="sd">    This method marks the start of tracking, disallowing the addition of new objects until the session is reset. It</span>
<span></span><span class="sd">    consolidates temporary outputs from `temp_output_dict_per_obj` and merges them into `output_dict`. Additionally,</span>
<span></span><span class="sd">    it clears non-conditioning memory around input frames and ensures that the state is consistent with the provided</span>
<span></span><span class="sd">    inputs.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        inference_state (dict[str, Any], optional): The current inference state. If None, uses the instance's</span>
<span></span><span class="sd">            inference state.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="n">inference_state</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span>
<span></span>    <span class="c1"># Tracking has started and we don't allow adding new objects until session is reset.</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"tracking_has_started"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_idx_to_id"</span><span class="p">])</span>
<span></span>
<span></span>    <span class="c1"># Consolidate per-object temporary outputs in "temp_output_dict_per_obj" and</span>
<span></span>    <span class="c1"># add them into "output_dict".</span>
<span></span>    <span class="n">temp_output_dict_per_obj</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"temp_output_dict_per_obj"</span><span class="p">]</span>
<span></span>    <span class="n">output_dict</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict"</span><span class="p">]</span>
<span></span>    <span class="c1"># "consolidated_frame_inds" contains indices of those frames where consolidated</span>
<span></span>    <span class="c1"># temporary outputs have been added (either in this call or any previous calls</span>
<span></span>    <span class="c1"># to `propagate_in_video_preflight`).</span>
<span></span>    <span class="n">consolidated_frame_inds</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"consolidated_frame_inds"</span><span class="p">]</span>
<span></span>    <span class="k">for</span> <span class="n">is_cond</span> <span class="ow">in</span> <span class="p">{</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">}:</span>
<span></span>        <span class="c1"># Separately consolidate conditioning and non-conditioning temp outputs</span>
<span></span>        <span class="n">storage_key</span> <span class="o">=</span> <span class="s2">"cond_frame_outputs"</span> <span class="k">if</span> <span class="n">is_cond</span> <span class="k">else</span> <span class="s2">"non_cond_frame_outputs"</span>
<span></span>        <span class="c1"># Find all the frames that contain temporary outputs for any objects</span>
<span></span>        <span class="c1"># (these should be the frames that have just received clicks for mask inputs</span>
<span></span>        <span class="c1"># via `add_new_points` or `add_new_mask`)</span>
<span></span>        <span class="n">temp_frame_inds</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span></span>        <span class="k">for</span> <span class="n">obj_temp_output_dict</span> <span class="ow">in</span> <span class="n">temp_output_dict_per_obj</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>            <span class="n">temp_frame_inds</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">obj_temp_output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span></span>        <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="n">storage_key</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">temp_frame_inds</span><span class="p">)</span>
<span></span>        <span class="c1"># consolidate the temporary output across all objects on this frame</span>
<span></span>        <span class="k">for</span> <span class="n">frame_idx</span> <span class="ow">in</span> <span class="n">temp_frame_inds</span><span class="p">:</span>
<span></span>            <span class="n">consolidated_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_consolidate_temp_output_across_obj</span><span class="p">(</span>
<span></span>                <span class="n">frame_idx</span><span class="p">,</span> <span class="n">is_cond</span><span class="o">=</span><span class="n">is_cond</span><span class="p">,</span> <span class="n">run_mem_encoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inference_state</span><span class="o">=</span><span class="n">inference_state</span>
<span></span>            <span class="p">)</span>
<span></span>            <span class="c1"># merge them into "output_dict" and also create per-object slices</span>
<span></span>            <span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">consolidated_out</span>
<span></span>            <span class="bp">self</span><span class="o">.</span><span class="n">_add_output_per_object</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="n">consolidated_out</span><span class="p">,</span> <span class="n">storage_key</span><span class="p">,</span> <span class="n">inference_state</span><span class="o">=</span><span class="n">inference_state</span><span class="p">)</span>
<span></span>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem_around_input</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem_for_multi_obj</span> <span class="ow">or</span> <span class="n">batch_size</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">):</span>
<span></span>                <span class="c1"># clear non-conditioning memory of the surrounding frames</span>
<span></span>                <span class="bp">self</span><span class="o">.</span><span class="n">_clear_non_cond_mem_around_input</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>
<span></span>        <span class="c1"># clear temporary outputs in `temp_output_dict_per_obj`</span>
<span></span>        <span class="k">for</span> <span class="n">obj_temp_output_dict</span> <span class="ow">in</span> <span class="n">temp_output_dict_per_obj</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>            <span class="n">obj_temp_output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>
<span></span>    <span class="c1"># edge case: if an output is added to "cond_frame_outputs", we remove any prior</span>
<span></span>    <span class="c1"># output on the same frame in "non_cond_frame_outputs"</span>
<span></span>    <span class="k">for</span> <span class="n">frame_idx</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]:</span>
<span></span>        <span class="n">output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>    <span class="k">for</span> <span class="n">obj_output_dict</span> <span class="ow">in</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>        <span class="k">for</span> <span class="n">frame_idx</span> <span class="ow">in</span> <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]:</span>
<span></span>            <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>    <span class="k">for</span> <span class="n">frame_idx</span> <span class="ow">in</span> <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]:</span>
<span></span>        <span class="k">assert</span> <span class="n">frame_idx</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span>
<span></span>        <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Make sure that the frame indices in "consolidated_frame_inds" are exactly those frames</span>
<span></span>    <span class="c1"># with either points or mask inputs (which should be true under a correct workflow).</span>
<span></span>    <span class="n">all_consolidated_frame_inds</span> <span class="o">=</span> <span class="p">(</span>
<span></span>        <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span> <span class="o">|</span> <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="n">input_frames_inds</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span></span>    <span class="k">for</span> <span class="n">point_inputs_per_frame</span> <span class="ow">in</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"point_inputs_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>        <span class="n">input_frames_inds</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">point_inputs_per_frame</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span></span>    <span class="k">for</span> <span class="n">mask_inputs_per_frame</span> <span class="ow">in</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"mask_inputs_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>        <span class="n">input_frames_inds</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">mask_inputs_per_frame</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span></span>    <span class="k">assert</span> <span class="n">all_consolidated_frame_inds</span> <span class="o">==</span> <span class="n">input_frames_inds</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.remove_object"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.remove_object</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">remove_object</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">,</span> <span class="n">strict</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</code></pre></div><p>Remove an object id from the tracking state. If strict is True, we check whether the object id actually</p><p>exists and raise an error if it doesn't exist.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>inference_state</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>obj_id</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>strict</code></td><td></td><td></td><td><code>False</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1675-L1751"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">remove_object</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Remove an object id from the tracking state. If strict is True, we check whether the object id actually</span>
<span></span><span class="sd">    exists and raise an error if it doesn't exist.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">old_obj_idx_to_rm</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_id_to_idx"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">obj_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>    <span class="c1"># Check whether this object_id to remove actually exists and possibly raise an error.</span>
<span></span>    <span class="k">if</span> <span class="n">old_obj_idx_to_rm</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="k">if</span> <span class="ow">not</span> <span class="n">strict</span><span class="p">:</span>
<span></span>            <span class="k">return</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span>
<span></span>        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
<span></span>            <span class="sa">f</span><span class="s2">"Cannot remove object id </span><span class="si">{</span><span class="n">obj_id</span><span class="si">}</span><span class="s2"> as it doesn't exist. "</span>
<span></span>            <span class="sa">f</span><span class="s2">"All existing object ids: </span><span class="si">{</span><span class="n">inference_state</span><span class="p">[</span><span class="s1">'obj_ids'</span><span class="p">]</span><span class="si">}</span><span class="s2">."</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># If this is the only remaining object id, we simply reset the state.</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_id_to_idx"</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">clear_all_points_in_video</span><span class="p">(</span><span class="n">inference_state</span><span class="p">)</span>
<span></span>        <span class="k">return</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span>
<span></span>
<span></span>    <span class="c1"># There are still remaining objects after removing this object id. In this case,</span>
<span></span>    <span class="c1"># we need to delete the object storage from inference state tensors.</span>
<span></span>    <span class="c1"># Step 0: clear the input on those frames where this object id has point or mask input</span>
<span></span>    <span class="c1"># (note that this step is required as it might downgrade conditioning frames to</span>
<span></span>    <span class="c1"># non-conditioning ones)</span>
<span></span>    <span class="n">obj_input_frames_inds</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span></span>    <span class="n">obj_input_frames_inds</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"point_inputs_per_obj"</span><span class="p">][</span><span class="n">old_obj_idx_to_rm</span><span class="p">])</span>
<span></span>    <span class="n">obj_input_frames_inds</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"mask_inputs_per_obj"</span><span class="p">][</span><span class="n">old_obj_idx_to_rm</span><span class="p">])</span>
<span></span>    <span class="k">for</span> <span class="n">frame_idx</span> <span class="ow">in</span> <span class="n">obj_input_frames_inds</span><span class="p">:</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">clear_all_points_in_frame</span><span class="p">(</span><span class="n">inference_state</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Step 1: Update the object id mapping (note that it must be done after Step 0,</span>
<span></span>    <span class="c1"># since Step 0 still requires the old object id mappings in inference_state)</span>
<span></span>    <span class="n">old_obj_ids</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span>
<span></span>    <span class="n">old_obj_inds</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">old_obj_ids</span><span class="p">)))</span>
<span></span>    <span class="n">remain_old_obj_inds</span> <span class="o">=</span> <span class="n">old_obj_inds</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span></span>    <span class="n">remain_old_obj_inds</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">old_obj_idx_to_rm</span><span class="p">)</span>
<span></span>    <span class="n">new_obj_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">old_obj_ids</span><span class="p">[</span><span class="n">old_idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">old_idx</span> <span class="ow">in</span> <span class="n">remain_old_obj_inds</span><span class="p">]</span>
<span></span>    <span class="n">new_obj_inds</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">new_obj_ids</span><span class="p">)))</span>
<span></span>    <span class="c1"># build new mappings</span>
<span></span>    <span class="n">old_idx_to_new_idx</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">remain_old_obj_inds</span><span class="p">,</span> <span class="n">new_obj_inds</span><span class="p">))</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_id_to_idx"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">new_obj_ids</span><span class="p">,</span> <span class="n">new_obj_inds</span><span class="p">))</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_idx_to_id"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">new_obj_inds</span><span class="p">,</span> <span class="n">new_obj_ids</span><span class="p">))</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_obj_ids</span>
<span></span>
<span></span>    <span class="c1"># Step 2: For per-object tensor storage, we shift their obj_idx in the dict keys.</span>
<span></span>    <span class="c1"># (note that "consolidated_frame_inds" doesn't need to be updated in this step as</span>
<span></span>    <span class="c1"># it's already handled in Step 0)</span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="nf">_map_keys</span><span class="p">(</span><span class="n">container</span><span class="p">):</span>
<span></span>        <span class="n">new_kvs</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">old_obj_inds</span><span class="p">:</span>
<span></span>            <span class="n">v</span> <span class="o">=</span> <span class="n">container</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<span></span>            <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">old_idx_to_new_idx</span><span class="p">:</span>
<span></span>                <span class="n">new_kvs</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">old_idx_to_new_idx</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">v</span><span class="p">))</span>
<span></span>        <span class="n">container</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">new_kvs</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">_map_keys</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"point_inputs_per_obj"</span><span class="p">])</span>
<span></span>    <span class="n">_map_keys</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"mask_inputs_per_obj"</span><span class="p">])</span>
<span></span>    <span class="n">_map_keys</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">])</span>
<span></span>    <span class="n">_map_keys</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"temp_output_dict_per_obj"</span><span class="p">])</span>
<span></span>
<span></span>    <span class="c1"># Step 3: For packed tensor storage, we index the remaining ids and rebuild the per-object slices.</span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="nf">_slice_state</span><span class="p">(</span><span class="n">output_dict</span><span class="p">,</span> <span class="n">storage_key</span><span class="p">):</span>
<span></span>        <span class="k">for</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<span></span>            <span class="n">out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">][</span><span class="n">remain_old_obj_inds</span><span class="p">]</span>
<span></span>            <span class="n">out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="n">remain_old_obj_inds</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]]</span>
<span></span>            <span class="c1"># "maskmem_pos_enc" is the same across frames, so we only need to store one copy of it</span>
<span></span>            <span class="n">out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_maskmem_pos_enc</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">],</span> <span class="n">inference_state</span><span class="p">)</span>
<span></span>            <span class="n">out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">][</span><span class="n">remain_old_obj_inds</span><span class="p">]</span>
<span></span>            <span class="n">out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">][</span><span class="n">remain_old_obj_inds</span><span class="p">]</span>
<span></span>            <span class="n">out</span><span class="p">[</span><span class="s2">"object_score_logits"</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">"object_score_logits"</span><span class="p">][</span><span class="n">remain_old_obj_inds</span><span class="p">]</span>
<span></span>            <span class="c1"># also update the per-object slices</span>
<span></span>            <span class="bp">self</span><span class="o">.</span><span class="n">_add_output_per_object</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">storage_key</span><span class="p">,</span> <span class="n">inference_state</span><span class="o">=</span><span class="n">inference_state</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">_slice_state</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict"</span><span class="p">],</span> <span class="s2">"cond_frame_outputs"</span><span class="p">)</span>
<span></span>    <span class="n">_slice_state</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict"</span><span class="p">],</span> <span class="s2">"non_cond_frame_outputs"</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span>
</code></pre></div></details><p><br/><br/><hr/><br/></p><h2 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor"><span class="doc-kind doc-kind-class">class</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor</code></h2><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">cfg</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">DEFAULT_CFG</span><span class="p">,</span>
<span></span>    <span class="n">overrides</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">max_obj_num</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
<span></span>    <span class="n">_callbacks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div><p><strong>Bases:</strong> <code>SAM2Predictor</code></p><p>SAM2DynamicInteractivePredictor extends SAM2Predictor to support dynamic interactions with video frames or a</p><p>sequence of images.</p><p>This constructor initializes the SAM2DynamicInteractivePredictor with a given configuration, applies any specified overrides</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>cfg</code></td><td><code>dict[str, Any]</code></td><td>Configuration dictionary containing default settings.</td><td><code>DEFAULT_CFG</code></td></tr><tr><td><code>overrides</code></td><td><code>dict[str, Any] | None</code></td><td>Dictionary of values to override default configuration.</td><td><code>None</code></td></tr><tr><td><code>max_obj_num</code></td><td><code>int</code></td><td>Maximum number of objects to track. Default is 3. this is set to keep fix feature size<br/> for the model.</td><td><code>3</code></td></tr><tr><td><code>_callbacks</code></td><td><code>dict[str, Any] | None</code></td><td>Dictionary of callback functions to customize behavior.</td><td><code>None</code></td></tr></tbody></table><p><strong>Attributes</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>memory_bank</code></td><td><code>list</code></td><td>OrderedDict: Stores the states of each image with prompts.</td></tr><tr><td><code>obj_idx_set</code></td><td><code>set</code></td><td>A set to keep track of the object indices that have been added.</td></tr><tr><td><code>obj_id_to_idx</code></td><td><code>OrderedDict</code></td><td>Maps object IDs to their corresponding indices.</td></tr><tr><td><code>obj_idx_to_id</code></td><td><code>OrderedDict</code></td><td>Maps object indices to their corresponding IDs.</td></tr></tbody></table><p><strong>Methods</strong></p><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._obj_id_to_idx"><code>_obj_id_to_idx</code></a></td><td>Map client-side object id to model-side object index.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._prepare_memory_conditioned_features"><code>_prepare_memory_conditioned_features</code></a></td><td>Prepare memory-conditioned features for the current image state.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_im_features"><code>get_im_features</code></a></td><td>Initialize the image state by processing the input image and extracting features.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_maskmem_enc"><code>get_maskmem_enc</code></a></td><td>Get memory and positional encoding from memory, which is used to condition the current image features.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.inference"><code>inference</code></a></td><td>Perform inference on a single image with optional bounding boxes, masks, points and object IDs. It has two</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.track_step"><code>track_step</code></a></td><td>Tracking step for the current image state to predict masks.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.update_memory"><code>update_memory</code></a></td><td>Append the imgState to the memory_bank and update the memory for the model.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">SAM2DynamicInteractivePredictor</span><span class="p">(</span><span class="n">cfg</span><span class="o">=</span><span class="n">DEFAULT_CFG</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">support_img1</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes1</span><span class="p">,</span> <span class="n">obj_ids</span><span class="o">=</span><span class="n">labels1</span><span class="p">,</span> <span class="n">update_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">results1</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">query_img1</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">support_img2</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes2</span><span class="p">,</span> <span class="n">obj_ids</span><span class="o">=</span><span class="n">labels2</span><span class="p">,</span> <span class="n">update_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">results2</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">query_img2</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1856-L2186"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SAM2DynamicInteractivePredictor</span><span class="p">(</span><span class="n">SAM2Predictor</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""SAM2DynamicInteractivePredictor extends SAM2Predictor to support dynamic interactions with video frames or a</span>
<span></span><span class="sd">    sequence of images.</span>
<span></span>
<span></span><span class="sd">    Attributes:</span>
<span></span><span class="sd">        memory_bank (list): OrderedDict: Stores the states of each image with prompts.</span>
<span></span><span class="sd">        obj_idx_set (set): A set to keep track of the object indices that have been added.</span>
<span></span><span class="sd">        obj_id_to_idx (OrderedDict): Maps object IDs to their corresponding indices.</span>
<span></span><span class="sd">        obj_idx_to_id (OrderedDict): Maps object indices to their corresponding IDs.</span>
<span></span>
<span></span><span class="sd">    Methods:</span>
<span></span><span class="sd">        get_model: Retrieves and configures the model with binarization enabled.</span>
<span></span><span class="sd">        inference: Performs inference on a single image with optional prompts and object IDs.</span>
<span></span><span class="sd">        postprocess: Post-processes the predictions to apply non-overlapping constraints if required.</span>
<span></span><span class="sd">        update_memory: Append the imgState to the memory_bank and update the memory for the model.</span>
<span></span><span class="sd">        track_step: Tracking step for the current image state to predict masks.</span>
<span></span><span class="sd">        get_maskmem_enc: Get memory and positional encoding from the memory bank.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">            &gt;&gt;&gt; predictor = SAM2DynamicInteractivePredictor(cfg=DEFAULT_CFG)</span>
<span></span><span class="sd">            &gt;&gt;&gt; predictor(source=support_img1, bboxes=bboxes1, obj_ids=labels1, update_memory=True)</span>
<span></span><span class="sd">            &gt;&gt;&gt; results1 = predictor(source=query_img1)</span>
<span></span><span class="sd">            &gt;&gt;&gt; predictor(source=support_img2, bboxes=bboxes2, obj_ids=labels2, update_memory=True)</span>
<span></span><span class="sd">            &gt;&gt;&gt; results2 = predictor(source=query_img2)</span>
<span></span><span class="sd">    """</span>
<span></span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<span></span>        <span class="bp">self</span><span class="p">,</span>
<span></span>        <span class="n">cfg</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">DEFAULT_CFG</span><span class="p">,</span>
<span></span>        <span class="n">overrides</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="n">max_obj_num</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
<span></span>        <span class="n">_callbacks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span></span><span class="w">        </span><span class="sd">"""Initialize the predictor with configuration and optional overrides.</span>
<span></span>
<span></span><span class="sd">        This constructor initializes the SAM2DynamicInteractivePredictor with a given configuration, applies any</span>
<span></span><span class="sd">        specified overrides</span>
<span></span>
<span></span><span class="sd">        Args:</span>
<span></span><span class="sd">            cfg (dict[str, Any]): Configuration dictionary containing default settings.</span>
<span></span><span class="sd">            overrides (dict[str, Any] | None): Dictionary of values to override default configuration.</span>
<span></span><span class="sd">            max_obj_num (int): Maximum number of objects to track. Default is 3. this is set to keep fix feature size</span>
<span></span><span class="sd">                for the model.</span>
<span></span><span class="sd">            _callbacks (dict[str, Any] | None): Dictionary of callback functions to customize behavior.</span>
<span></span><span class="sd">        """</span>
<span></span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">overrides</span><span class="p">,</span> <span class="n">_callbacks</span><span class="p">)</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">non_overlap_masks</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>
<span></span>        <span class="c1"># Initialize the memory bank to store image states</span>
<span></span>        <span class="c1"># NOTE: probably need to use dict for better query</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">memory_bank</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>
<span></span>        <span class="c1"># Initialize the object index set and mappings</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">obj_idx_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">obj_id_to_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">obj_idx_to_id</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">max_obj_num</span><span class="p">)))</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">_max_obj_num</span> <span class="o">=</span> <span class="n">max_obj_num</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._obj_id_to_idx"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._obj_id_to_idx</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_obj_id_to_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span>
</code></pre></div><p>Map client-side object id to model-side object index.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>obj_id</code></td><td><code>int</code></td><td>The client-side object ID.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>int</code></td><td>The model-side object index, or None if not found.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2126-L2135"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_obj_id_to_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span></span><span class="w">    </span><span class="sd">"""Map client-side object id to model-side object index.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        obj_id (int): The client-side object ID.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (int): The model-side object index, or None if not found.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">obj_id_to_idx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">obj_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._prepare_memory_conditioned_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._prepare_memory_conditioned_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prepare_memory_conditioned_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
</code></pre></div><p>Prepare memory-conditioned features for the current image state.</p><p>If <code>obj_idx</code> is provided, features are prepared for a specific prompted object in the image. If <code>obj_idx</code> is None, features are prepared for all objects. If no memory is available, a no-memory embedding is added to the current vision features. Otherwise, memory from previous frames is used to condition the current vision features via a transformer attention mechanism.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>obj_idx</code></td><td><code>int | None</code></td><td>The index of the object for which to prepare the features.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pix_feat_with_mem (torch.Tensor)</code></td><td>The memory-conditioned pixel features.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2078-L2111"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prepare_memory_conditioned_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span></span><span class="w">    </span><span class="sd">"""Prepare memory-conditioned features for the current image state.</span>
<span></span>
<span></span><span class="sd">    If ``obj_idx`` is provided, features are prepared for a specific prompted object in the image. If ``obj_idx`` is</span>
<span></span><span class="sd">    None, features are prepared for all objects. If no memory is available, a no-memory embedding is added to the</span>
<span></span><span class="sd">    current vision features. Otherwise, memory from previous frames is used to condition the current vision features</span>
<span></span><span class="sd">    via a transformer attention mechanism.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        obj_idx (int | None): The index of the object for which to prepare the features.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pix_feat_with_mem (torch.Tensor): The memory-conditioned pixel features.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory_bank</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj_idx</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
<span></span>        <span class="c1"># For initial conditioning frames, encode without using any previous memory.</span>
<span></span>        <span class="c1"># Directly add the no-memory embedding (instead of using the transformer encoder).</span>
<span></span>        <span class="n">pix_feat_with_mem</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_feats</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">no_mem_embed</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="c1"># for inference frames, use the memory features from previous frames</span>
<span></span>        <span class="n">memory</span><span class="p">,</span> <span class="n">memory_pos_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_maskmem_enc</span><span class="p">()</span>
<span></span>        <span class="n">pix_feat_with_mem</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">memory_attention</span><span class="p">(</span>
<span></span>            <span class="n">curr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vision_feats</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:],</span>
<span></span>            <span class="n">curr_pos</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vision_pos_embeds</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:],</span>
<span></span>            <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">,</span>
<span></span>            <span class="n">memory_pos</span><span class="o">=</span><span class="n">memory_pos_embed</span><span class="p">,</span>
<span></span>            <span class="n">num_obj_ptr_tokens</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># num_obj_ptr_tokens</span>
<span></span>        <span class="p">)</span>
<span></span>    <span class="c1"># Reshape output (HW)BC =&gt; BCHW</span>
<span></span>    <span class="k">return</span> <span class="n">pix_feat_with_mem</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">_max_obj_num</span><span class="p">,</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">memory_attention</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
<span></span>        <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">feat_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
<span></span>    <span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_im_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_im_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div><p>Initialize the image state by processing the input image and extracting features.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>img</code></td><td><code>torch.Tensor | np.ndarray</code></td><td>The input image tensor or numpy array.</td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1981-L1995"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span></span><span class="w">    </span><span class="sd">"""Initialize the image state by processing the input image and extracting features.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        img (torch.Tensor | np.ndarray): The input image tensor or numpy array.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">vis_feats</span><span class="p">,</span> <span class="n">vis_pos_embed</span><span class="p">,</span> <span class="n">feat_sizes</span> <span class="o">=</span> <span class="n">SAM2VideoPredictor</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_obj_num</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">high_res_features</span> <span class="o">=</span> <span class="p">[</span>
<span></span>        <span class="n">feat</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">feat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="o">*</span><span class="n">feat_size</span><span class="p">)</span>
<span></span>        <span class="k">for</span> <span class="n">feat</span><span class="p">,</span> <span class="n">feat_size</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">vis_feats</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">feat_sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span></span>    <span class="p">]</span>
<span></span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">vision_feats</span> <span class="o">=</span> <span class="n">vis_feats</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">vision_pos_embeds</span> <span class="o">=</span> <span class="n">vis_pos_embed</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">feat_sizes</span> <span class="o">=</span> <span class="n">feat_sizes</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_maskmem_enc"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_maskmem_enc</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_maskmem_enc</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
</code></pre></div><p>Get memory and positional encoding from memory, which is used to condition the current image features.</p><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2113-L2124"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_maskmem_enc</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span></span><span class="w">    </span><span class="sd">"""Get memory and positional encoding from memory, which is used to condition the current image features."""</span>
<span></span>    <span class="n">to_cat_memory</span><span class="p">,</span> <span class="n">to_cat_memory_pos_embed</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span></span>    <span class="k">for</span> <span class="n">consolidated_out</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_bank</span><span class="p">:</span>
<span></span>        <span class="n">to_cat_memory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># (H*W, B, C)</span>
<span></span>        <span class="n">maskmem_enc</span> <span class="o">=</span> <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span></span>        <span class="n">maskmem_enc</span> <span class="o">=</span> <span class="n">maskmem_enc</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">maskmem_tpos_enc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_maskmem</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
<span></span>        <span class="n">to_cat_memory_pos_embed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">maskmem_enc</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">to_cat_memory</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span></span>    <span class="n">memory_pos_embed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">to_cat_memory_pos_embed</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">memory</span><span class="p">,</span> <span class="n">memory_pos_embed</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.inference"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.inference</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">im</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">obj_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">update_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
</code></pre></div><p>Perform inference on a single image with optional bounding boxes, masks, points and object IDs. It has two</p><p>modes: one is to run inference on a single image without updating the memory, and the other is to update the memory with the provided prompts and object IDs. When update_memory is True, it will update the memory with the provided prompts and obj_ids. When update_memory is False, it will only run inference on the provided image without updating the memory.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor | np.ndarray</code></td><td>The input image tensor or numpy array.</td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td><code>list[list[float]] | None</code></td><td>Optional list of bounding boxes to update the memory.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>list[torch.Tensor | np.ndarray] | None</code></td><td>Optional masks to update the memory.</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>list[list[float]] | None</code></td><td>Optional list of points to update the memory, each point is [x, y].</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>list[int] | None</code></td><td>Optional list of object IDs corresponding to the points (&gt;0 for positive, 0 for<br/> negative).</td><td><code>None</code></td></tr><tr><td><code>obj_ids</code></td><td><code>list[int] | None</code></td><td>Optional list of object IDs corresponding to the prompts.</td><td><code>None</code></td></tr><tr><td><code>update_memory</code></td><td><code>bool</code></td><td>Flag to indicate whether to update the memory with new objects.</td><td><code>False</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>res_masks (torch.Tensor)</code></td><td>The output masks in shape (C, H, W)</td></tr><tr><td><code>object_score_logits (torch.Tensor)</code></td><td>Quality scores for each mask</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1914-L1979"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">im</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">obj_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">update_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span></span><span class="w">    </span><span class="sd">"""Perform inference on a single image with optional bounding boxes, masks, points and object IDs. It has two</span>
<span></span><span class="sd">    modes: one is to run inference on a single image without updating the memory, and the other is to update</span>
<span></span><span class="sd">    the memory with the provided prompts and object IDs. When update_memory is True, it will update the</span>
<span></span><span class="sd">    memory with the provided prompts and obj_ids. When update_memory is False, it will only run inference on</span>
<span></span><span class="sd">    the provided image without updating the memory.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (torch.Tensor | np.ndarray): The input image tensor or numpy array.</span>
<span></span><span class="sd">        bboxes (list[list[float]] | None): Optional list of bounding boxes to update the memory.</span>
<span></span><span class="sd">        masks (list[torch.Tensor | np.ndarray] | None): Optional masks to update the memory.</span>
<span></span><span class="sd">        points (list[list[float]] | None): Optional list of points to update the memory, each point is [x, y].</span>
<span></span><span class="sd">        labels (list[int] | None): Optional list of object IDs corresponding to the points (&gt;0 for positive, 0 for</span>
<span></span><span class="sd">            negative).</span>
<span></span><span class="sd">        obj_ids (list[int] | None): Optional list of object IDs corresponding to the prompts.</span>
<span></span><span class="sd">        update_memory (bool): Flag to indicate whether to update the memory with new objects.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        res_masks (torch.Tensor): The output masks in shape (C, H, W)</span>
<span></span><span class="sd">        object_score_logits (torch.Tensor): Quality scores for each mask</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
<span></span>    <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_prompts</span><span class="p">(</span>
<span></span>        <span class="n">dst_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">,</span>
<span></span>        <span class="n">src_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span>
<span></span>        <span class="n">points</span><span class="o">=</span><span class="n">points</span><span class="p">,</span>
<span></span>        <span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes</span><span class="p">,</span>
<span></span>        <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
<span></span>        <span class="n">masks</span><span class="o">=</span><span class="n">masks</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="n">update_memory</span><span class="p">:</span>
<span></span>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj_ids</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
<span></span>            <span class="n">obj_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">obj_ids</span><span class="p">]</span>
<span></span>        <span class="k">assert</span> <span class="n">obj_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">"obj_ids must be provided when update_memory is True"</span>
<span></span>        <span class="k">assert</span> <span class="n">masks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
<span></span>            <span class="s2">"bboxes, masks, or points must be provided when update_memory is True"</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># placeholder</span>
<span></span>            <span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">obj_ids</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>            <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">obj_ids</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="k">if</span> <span class="n">masks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">obj_ids</span><span class="p">),</span> <span class="s2">"masks and obj_ids must have the same length."</span>
<span></span>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">obj_ids</span><span class="p">),</span> <span class="s2">"points and obj_ids must have the same length."</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">update_memory</span><span class="p">(</span><span class="n">obj_ids</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">current_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">track_step</span><span class="p">()</span>
<span></span>    <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">],</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"object_score_logits"</span><span class="p">]</span>
<span></span>    <span class="c1"># filter the masks and logits based on the object indices</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">obj_idx_set</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">"No objects have been added to the state. Please add objects before inference."</span><span class="p">)</span>
<span></span>    <span class="n">idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">obj_idx_set</span><span class="p">)</span>  <span class="c1"># cls id</span>
<span></span>    <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">pred_masks</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">pred_scores</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span></span>    <span class="c1"># the original score are in [-32,32], and a object score larger than 0 means the object is present, we map it to [-1,1] range,</span>
<span></span>    <span class="c1"># and use a activate function to make sure the object score logits are non-negative, so that we can use it as a mask</span>
<span></span>    <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="n">pred_scores</span> <span class="o">/</span> <span class="mi">32</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">pred_scores</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.track_step"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.track_step</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">track_step</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">obj_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">point</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">label</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>
</code></pre></div><p>Tracking step for the current image state to predict masks.</p><p>This method processes the image features and runs the SAM heads to predict masks. If obj_idx is provided, it processes the features for a specific prompted object in the image. If obj_idx is None, it processes the features for all objects in the image. The method supports both mask-based output without SAM and full SAM processing with memory-conditioned features.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>obj_idx</code></td><td><code>int | None</code></td><td>The index of the object for which to predict masks. If None, it processes all objects.</td><td><code>None</code></td></tr><tr><td><code>point</code></td><td><code>torch.Tensor | None</code></td><td>The coordinates of the points of interest with shape (N, 2).</td><td><code>None</code></td></tr><tr><td><code>label</code></td><td><code>torch.Tensor | None</code></td><td>The labels corresponding to the points where 1 means positive clicks, 0 means<br/> negative clicks.</td><td><code>None</code></td></tr><tr><td><code>mask</code></td><td><code>torch.Tensor | None</code></td><td>The mask input for the object with shape (H, W).</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>current_out (dict[str, Any])</code></td><td>A dictionary containing the current output with mask predictions and object</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2137-L2186"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">track_step</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">obj_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">point</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">label</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span></span><span class="w">    </span><span class="sd">"""Tracking step for the current image state to predict masks.</span>
<span></span>
<span></span><span class="sd">    This method processes the image features and runs the SAM heads to predict masks. If obj_idx is provided, it</span>
<span></span><span class="sd">    processes the features for a specific prompted object in the image. If obj_idx is None, it processes the</span>
<span></span><span class="sd">    features for all objects in the image. The method supports both mask-based output without SAM and full SAM</span>
<span></span><span class="sd">    processing with memory-conditioned features.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        obj_idx (int | None): The index of the object for which to predict masks. If None, it processes all objects.</span>
<span></span><span class="sd">        point (torch.Tensor | None): The coordinates of the points of interest with shape (N, 2).</span>
<span></span><span class="sd">        label (torch.Tensor | None): The labels corresponding to the points where 1 means positive clicks, 0 means</span>
<span></span><span class="sd">            negative clicks.</span>
<span></span><span class="sd">        mask (torch.Tensor | None): The mask input for the object with shape (H, W).</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        current_out (dict[str, Any]): A dictionary containing the current output with mask predictions and object</span>
<span></span><span class="sd">            pointers. Keys include 'point_inputs', 'mask_inputs', 'pred_masks', 'pred_masks_high_res',</span>
<span></span><span class="sd">            'obj_ptr', 'object_score_logits'.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">use_mask_input_as_output_without_sam</span><span class="p">:</span>
<span></span>        <span class="c1"># When use_mask_input_as_output_without_sam=True, we directly output the mask input</span>
<span></span>        <span class="c1"># (see it as a GT mask) without using a SAM prompt encoder + mask decoder.</span>
<span></span>        <span class="n">pix_feat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_feats</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span></span>        <span class="n">pix_feat</span> <span class="o">=</span> <span class="n">pix_feat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">memory_attention</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">feat_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span></span>        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">low_res_masks</span><span class="p">,</span> <span class="n">high_res_masks</span><span class="p">,</span> <span class="n">obj_ptr</span><span class="p">,</span> <span class="n">object_score_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_use_mask_as_output</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="c1"># Fuse visual features with previous memory features in the memory bank.</span>
<span></span>        <span class="n">pix_feat_with_mem</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_memory_conditioned_features</span><span class="p">(</span><span class="n">obj_idx</span><span class="p">)</span>
<span></span>        <span class="c1"># If ``obj_idx`` is provided (i.e., prompts are being added), keep only the first feature map.</span>
<span></span>        <span class="n">pix_feat_with_mem</span> <span class="o">=</span> <span class="n">pix_feat_with_mem</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">obj_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">pix_feat_with_mem</span>
<span></span>        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">low_res_masks</span><span class="p">,</span> <span class="n">high_res_masks</span><span class="p">,</span> <span class="n">obj_ptr</span><span class="p">,</span> <span class="n">object_score_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_forward_sam_heads</span><span class="p">(</span>
<span></span>            <span class="n">backbone_features</span><span class="o">=</span><span class="n">pix_feat_with_mem</span><span class="p">,</span>
<span></span>            <span class="n">point_inputs</span><span class="o">=</span><span class="p">{</span><span class="s2">"point_coords"</span><span class="p">:</span> <span class="n">point</span><span class="p">,</span> <span class="s2">"point_labels"</span><span class="p">:</span> <span class="n">label</span><span class="p">}</span> <span class="k">if</span> <span class="n">obj_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
<span></span>            <span class="n">mask_inputs</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span>
<span></span>            <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>            <span class="n">high_res_features</span><span class="o">=</span><span class="p">[</span><span class="n">feat</span><span class="p">[:</span> <span class="n">pix_feat_with_mem</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">high_res_features</span><span class="p">],</span>
<span></span>        <span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="p">{</span>
<span></span>        <span class="s2">"pred_masks"</span><span class="p">:</span> <span class="n">low_res_masks</span><span class="p">,</span>
<span></span>        <span class="s2">"pred_masks_high_res"</span><span class="p">:</span> <span class="n">high_res_masks</span><span class="p">,</span>
<span></span>        <span class="s2">"obj_ptr"</span><span class="p">:</span> <span class="n">obj_ptr</span><span class="p">,</span>
<span></span>        <span class="s2">"object_score_logits"</span><span class="p">:</span> <span class="n">object_score_logits</span><span class="p">,</span>
<span></span>    <span class="p">}</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.update_memory"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.update_memory</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">update_memory</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">obj_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div><p>Append the imgState to the memory_bank and update the memory for the model.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>obj_ids</code></td><td><code>list[int]</code></td><td>List of object IDs corresponding to the prompts.</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>torch.Tensor | None</code></td><td>Tensor of shape (B, N, 2) representing the input points for N objects.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>torch.Tensor | None</code></td><td>Tensor of shape (B, N) representing the labels for the input points.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>torch.Tensor | None</code></td><td>Optional tensor of shape (N, H, W) representing the input masks for N objects.</td><td><code>None</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1998-L2076"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">update_memory</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">obj_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span></span><span class="w">    </span><span class="sd">"""Append the imgState to the memory_bank and update the memory for the model.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        obj_ids (list[int]): List of object IDs corresponding to the prompts.</span>
<span></span><span class="sd">        points (torch.Tensor | None): Tensor of shape (B, N, 2) representing the input points for N objects.</span>
<span></span><span class="sd">        labels (torch.Tensor | None): Tensor of shape (B, N) representing the labels for the input points.</span>
<span></span><span class="sd">        masks (torch.Tensor | None): Optional tensor of shape (N, H, W) representing the input masks for N objects.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">consolidated_out</span> <span class="o">=</span> <span class="p">{</span>
<span></span>        <span class="s2">"maskmem_features"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="s2">"maskmem_pos_enc"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="s2">"pred_masks"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
<span></span>            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_obj_num</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">4</span><span class="p">),</span>
<span></span>            <span class="n">fill_value</span><span class="o">=-</span><span class="mf">1024.0</span><span class="p">,</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
<span></span>            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>        <span class="p">),</span>
<span></span>        <span class="s2">"obj_ptr"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
<span></span>            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_obj_num</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">),</span>
<span></span>            <span class="n">fill_value</span><span class="o">=-</span><span class="mf">1024.0</span><span class="p">,</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
<span></span>            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>        <span class="p">),</span>
<span></span>        <span class="s2">"object_score_logits"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
<span></span>            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_obj_num</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span></span>            <span class="c1"># default to 10.0 for object_score_logits, i.e. assuming the object is</span>
<span></span>            <span class="c1"># present as sigmoid(10)=1, same as in `predict_masks` of `MaskDecoder`</span>
<span></span>            <span class="n">fill_value</span><span class="o">=-</span><span class="mi">32</span><span class="p">,</span>  <span class="c1"># 10.0,</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
<span></span>            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>        <span class="p">),</span>
<span></span>    <span class="p">}</span>
<span></span>
<span></span>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">obj_ids</span><span class="p">):</span>
<span></span>        <span class="k">assert</span> <span class="n">obj_id</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_obj_num</span>
<span></span>        <span class="n">obj_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_obj_id_to_idx</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">obj_id</span><span class="p">))</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">obj_idx_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">obj_idx</span><span class="p">)</span>
<span></span>        <span class="n">point</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">points</span><span class="p">[[</span><span class="n">i</span><span class="p">]],</span> <span class="n">labels</span><span class="p">[[</span><span class="n">i</span><span class="p">]]</span>
<span></span>        <span class="n">mask</span> <span class="o">=</span> <span class="n">masks</span><span class="p">[[</span><span class="n">i</span><span class="p">]][</span><span class="kc">None</span><span class="p">]</span> <span class="k">if</span> <span class="n">masks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
<span></span>        <span class="c1"># Currently, only bbox prompt or mask prompt is supported, so we assert that bbox is not None.</span>
<span></span>        <span class="k">assert</span> <span class="n">point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">"Either bbox, points or mask is required"</span>
<span></span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">track_step</span><span class="p">(</span><span class="n">obj_idx</span><span class="p">,</span> <span class="n">point</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span></span>        <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">obj_mask</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span>
<span></span>            <span class="k">assert</span> <span class="n">obj_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span> <span class="o">==</span> <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span> <span class="p">(</span>
<span></span>                <span class="sa">f</span><span class="s2">"Expected mask shape </span><span class="si">{</span><span class="n">consolidated_out</span><span class="p">[</span><span class="s1">'pred_masks'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="n">obj_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span><span class="si">}</span><span class="s2"> for object </span><span class="si">{</span><span class="n">obj_idx</span><span class="si">}</span><span class="s2">."</span>
<span></span>            <span class="p">)</span>
<span></span>            <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">][</span><span class="n">obj_idx</span> <span class="p">:</span> <span class="n">obj_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj_mask</span>
<span></span>            <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">][</span><span class="n">obj_idx</span> <span class="p">:</span> <span class="n">obj_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">]</span>
<span></span>
<span></span>            <span class="k">if</span> <span class="s2">"object_score_logits"</span> <span class="ow">in</span> <span class="n">out</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
<span></span>                <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"object_score_logits"</span><span class="p">][</span><span class="n">obj_idx</span> <span class="p">:</span> <span class="n">obj_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">"object_score_logits"</span><span class="p">]</span>
<span></span>
<span></span>    <span class="n">high_res_masks</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
<span></span>        <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"cuda"</span><span class="p">),</span>
<span></span>        <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">,</span>
<span></span>        <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">,</span>
<span></span>        <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">non_overlap_masks_for_mem_enc</span><span class="p">:</span>
<span></span>        <span class="n">high_res_masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_apply_non_overlapping_constraints</span><span class="p">(</span><span class="n">high_res_masks</span><span class="p">)</span>
<span></span>    <span class="n">maskmem_features</span><span class="p">,</span> <span class="n">maskmem_pos_enc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_encode_new_memory</span><span class="p">(</span>
<span></span>        <span class="n">current_vision_feats</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vision_feats</span><span class="p">,</span>
<span></span>        <span class="n">feat_sizes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">feat_sizes</span><span class="p">,</span>
<span></span>        <span class="n">pred_masks_high_res</span><span class="o">=</span><span class="n">high_res_masks</span><span class="p">,</span>
<span></span>        <span class="n">object_score_logits</span><span class="o">=</span><span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"object_score_logits"</span><span class="p">],</span>
<span></span>        <span class="n">is_mask_from_pts</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">]</span> <span class="o">=</span> <span class="n">maskmem_features</span>
<span></span>    <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span> <span class="o">=</span> <span class="n">maskmem_pos_enc</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">memory_bank</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">consolidated_out</span><span class="p">)</span>
</code></pre></div></details><p><br/><br/><hr/><br/></p><h2 id="ultralytics.models.sam.predict.SAM3Predictor"><span class="doc-kind doc-kind-class">class</span> <code>ultralytics.models.sam.predict.SAM3Predictor</code></h2><div class="highlight"><pre><span></span><code><span></span><span class="n">SAM3Predictor</span><span class="p">()</span>
</code></pre></div><p><strong>Bases:</strong> <code>SAM2Predictor</code></p><p>Segment Anything Model 3 (SAM3) Interactive Predictor for image segmentation tasks.</p><p><strong>Methods</strong></p><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><a href="#ultralytics.models.sam.predict.SAM3Predictor.get_model"><code>get_model</code></a></td><td>Retrieve and initialize the Segment Anything Model 3 (SAM3) for image segmentation tasks.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3Predictor.setup_model"><code>setup_model</code></a></td><td>Setup the SAM3 model with appropriate mean and standard deviation for preprocessing.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2189-L2210"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SAM3Predictor</span><span class="p">(</span><span class="n">SAM2Predictor</span><span class="p">):</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3Predictor.get_model"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3Predictor.get_model</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div><p>Retrieve and initialize the Segment Anything Model 3 (SAM3) for image segmentation tasks.</p><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2206-L2210"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Retrieve and initialize the Segment Anything Model 3 (SAM3) for image segmentation tasks."""</span>
<span></span>    <span class="kn">from</span><span class="w"> </span><span class="nn">.build_sam3</span><span class="w"> </span><span class="kn">import</span> <span class="n">build_interactive_sam3</span>  <span class="c1"># slow import</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">build_interactive_sam3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="nb">compile</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">compile</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3Predictor.setup_model"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3Predictor.setup_model</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</code></pre></div><p>Setup the SAM3 model with appropriate mean and standard deviation for preprocessing.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>model</code></td><td></td><td></td><td><code>None</code></td></tr><tr><td><code>verbose</code></td><td></td><td></td><td><code>True</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2199-L2204"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Setup the SAM3 model with appropriate mean and standard deviation for preprocessing."""</span>
<span></span>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">setup_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
<span></span>    <span class="c1"># update mean and std</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">127.5</span><span class="p">,</span> <span class="mf">127.5</span><span class="p">,</span> <span class="mf">127.5</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">127.5</span><span class="p">,</span> <span class="mf">127.5</span><span class="p">,</span> <span class="mf">127.5</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></details><p><br/><br/><hr/><br/></p><h2 id="ultralytics.models.sam.predict.SAM3SemanticPredictor"><span class="doc-kind doc-kind-class">class</span> <code>ultralytics.models.sam.predict.SAM3SemanticPredictor</code></h2><div class="highlight"><pre><span></span><code><span></span><span class="n">SAM3SemanticPredictor</span><span class="p">()</span>
</code></pre></div><p><strong>Bases:</strong> <code>SAM3Predictor</code></p><p>Segment Anything Model 3 (SAM3) Predictor for image segmentation tasks.</p><p><strong>Methods</strong></p><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor._get_dummy_prompt"><code>_get_dummy_prompt</code></a></td><td>Get a dummy geometric prompt with zero boxes.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor._inference_features"><code>_inference_features</code></a></td><td>Run inference on the extracted features with optional bounding boxes and labels.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor._prepare_geometric_prompts"><code>_prepare_geometric_prompts</code></a></td><td>Prepare prompts by normalizing bounding boxes and points to the destination shape.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.get_im_features"><code>get_im_features</code></a></td><td>Extract image features using the model's backbone.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.get_model"><code>get_model</code></a></td><td>Retrieve and initialize the Segment Anything Model 3 (SAM3) for image segmentation tasks.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.inference"><code>inference</code></a></td><td>Perform inference on a single image with optional prompts.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.inference_features"><code>inference_features</code></a></td><td>Perform prompts preprocessing and inference on provided image features using the SAM model.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.postprocess"><code>postprocess</code></a></td><td>Post-process the predictions to apply non-overlapping constraints if required.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.pre_transform"><code>pre_transform</code></a></td><td>Perform initial transformations on the input image for preprocessing.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3SemanticPredictor.reset_prompts"><code>reset_prompts</code></a></td><td>Reset the prompts for the predictor.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2213-L2402"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SAM3SemanticPredictor</span><span class="p">(</span><span class="n">SAM3Predictor</span><span class="p">):</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3SemanticPredictor._get_dummy_prompt"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3SemanticPredictor._get_dummy_prompt</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_get_dummy_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_prompts</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>Get a dummy geometric prompt with zero boxes.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>num_prompts</code></td><td></td><td></td><td><code>1</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2396-L2402"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_get_dummy_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_prompts</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Get a dummy geometric prompt with zero boxes."""</span>
<span></span>    <span class="n">geometric_prompt</span> <span class="o">=</span> <span class="n">Prompt</span><span class="p">(</span>
<span></span>        <span class="n">box_embeddings</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_prompts</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
<span></span>        <span class="n">box_mask</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_prompts</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">geometric_prompt</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3SemanticPredictor._inference_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3SemanticPredictor._inference_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_inference_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Run inference on the extracted features with optional bounding boxes and labels.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>features</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td></td><td></td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td></td><td></td><td><code>None</code></td></tr><tr><td><code>text</code></td><td><code>list[str] | None</code></td><td></td><td><code>None</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2273-L2290"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_inference_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Run inference on the extracted features with optional bounding boxes and labels."""</span>
<span></span>    <span class="c1"># NOTE: priority: bboxes &gt; text &gt; pre-set classes</span>
<span></span>    <span class="n">nc</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">bboxes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">if</span> <span class="n">text</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">names</span><span class="p">)</span>
<span></span>    <span class="n">geometric_prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dummy_prompt</span><span class="p">(</span><span class="n">nc</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">bboxes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bboxes</span><span class="p">)):</span>
<span></span>            <span class="n">geometric_prompt</span><span class="o">.</span><span class="n">append_boxes</span><span class="p">(</span><span class="n">bboxes</span><span class="p">[[</span><span class="n">i</span><span class="p">]],</span> <span class="n">labels</span><span class="p">[[</span><span class="n">i</span><span class="p">]])</span>
<span></span>        <span class="k">if</span> <span class="n">text</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">text</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"visual"</span><span class="p">]</span>  <span class="c1"># bboxes needs this `visual` text prompt if no text passed</span>
<span></span>    <span class="k">if</span> <span class="n">text</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">names</span> <span class="o">!=</span> <span class="n">text</span><span class="p">:</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">set_classes</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">)</span>
<span></span>    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward_grounding</span><span class="p">(</span>
<span></span>        <span class="n">backbone_out</span><span class="o">=</span><span class="n">features</span><span class="p">,</span>
<span></span>        <span class="n">text_ids</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nc</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">),</span>
<span></span>        <span class="n">geometric_prompt</span><span class="o">=</span><span class="n">geometric_prompt</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3SemanticPredictor._prepare_geometric_prompts"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3SemanticPredictor._prepare_geometric_prompts</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prepare_geometric_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Prepare prompts by normalizing bounding boxes and points to the destination shape.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>src_shape</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td></td><td></td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td></td><td></td><td><code>None</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2253-L2271"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prepare_geometric_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Prepare prompts by normalizing bounding boxes and points to the destination shape."""</span>
<span></span>    <span class="k">if</span> <span class="n">bboxes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="n">bboxes</span> <span class="o">=</span> <span class="n">bboxes</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="k">if</span> <span class="n">bboxes</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">bboxes</span>
<span></span>        <span class="c1"># needs xywh as input</span>
<span></span>        <span class="n">bboxes</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">xyxy2xywh</span><span class="p">(</span><span class="n">bboxes</span><span class="p">)</span>
<span></span>        <span class="n">bboxes</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">/=</span> <span class="n">src_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span></span>        <span class="n">bboxes</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">/=</span> <span class="n">src_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span></span>        <span class="c1"># Assuming labels are all positive if users don't pass labels.</span>
<span></span>        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">bboxes</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span></span>        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="k">assert</span> <span class="n">bboxes</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span>
<span></span>            <span class="sa">f</span><span class="s2">"Number of points </span><span class="si">{</span><span class="n">bboxes</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> should match number of labels </span><span class="si">{</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">."</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="n">bboxes</span> <span class="o">=</span> <span class="n">bboxes</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># (N, 1, 4)</span>
<span></span>        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (N, 1)</span>
<span></span>    <span class="k">return</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">labels</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3SemanticPredictor.get_im_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3SemanticPredictor.get_im_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">)</span>
</code></pre></div><p>Extract image features using the model's backbone.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2223-L2225"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Extract image features using the model's backbone."""</span>
<span></span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">forward_image</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3SemanticPredictor.get_model"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3SemanticPredictor.get_model</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div><p>Retrieve and initialize the Segment Anything Model 3 (SAM3) for image segmentation tasks.</p><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2216-L2220"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Retrieve and initialize the Segment Anything Model 3 (SAM3) for image segmentation tasks."""</span>
<span></span>    <span class="kn">from</span><span class="w"> </span><span class="nn">.build_sam3</span><span class="w"> </span><span class="kn">import</span> <span class="n">build_sam3_image_model</span>  <span class="c1"># slow import</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">build_sam3_image_model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="nb">compile</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">compile</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3SemanticPredictor.inference"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3SemanticPredictor.inference</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div><p>Perform inference on a single image with optional prompts.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td></td><td></td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td></td><td></td><td><code>None</code></td></tr><tr><td><code>text</code></td><td><code>list[str] | None</code></td><td></td><td><code>None</code></td></tr><tr><td><code>*args</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>**kwargs</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2326-L2333"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform inference on a single image with optional prompts."""</span>
<span></span>    <span class="n">bboxes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"bboxes"</span><span class="p">,</span> <span class="n">bboxes</span><span class="p">)</span>
<span></span>    <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"labels"</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span></span>    <span class="n">text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"text"</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span></span>    <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span><span class="n">im</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span>
<span></span>    <span class="n">prompts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_geometric_prompts</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inference_features</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="o">*</span><span class="n">prompts</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3SemanticPredictor.inference_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3SemanticPredictor.inference_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Perform prompts preprocessing and inference on provided image features using the SAM model.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>features</code></td><td><code>dict[str, Any]</code></td><td>Extracted image features from the SAM3 model image encoder.</td><td><em>required</em></td></tr><tr><td><code>src_shape</code></td><td><code>tuple[int, int]</code></td><td>The source shape (height, width) of the input image.</td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td><code>np.ndarray | list[list[float]] | None</code></td><td>Bounding boxes in xyxy format with shape (N, 4). pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list[int] | None</code></td><td>Point prompt labels with shape (N, ).</td><td><code>None</code></td></tr><tr><td><code>text</code></td><td><code>list[str] | None</code></td><td>List of text prompts corresponding to the classes.</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>The output masks in shape (C, H, W), where C is the number of generated masks.</td></tr><tr><td><code>pred_bboxes (torch.Tensor)</code></td><td>Bounding boxes for each mask with shape (N, 6), where N is the number of boxes.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>The input features is a torch.Tensor of shape (B, C, H, W) if performing on SAM, or a dict[str, Any] if performing on SAM2.</li></ul></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2336-L2389"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference_features</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">features</span><span class="p">,</span>
<span></span>    <span class="n">src_shape</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">text</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform prompts preprocessing and inference on provided image features using the SAM model.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        features (dict[str, Any]): Extracted image features from the SAM3 model image encoder.</span>
<span></span><span class="sd">        src_shape (tuple[int, int]): The source shape (height, width) of the input image.</span>
<span></span><span class="sd">        bboxes (np.ndarray | list[list[float]] | None): Bounding boxes in xyxy format with shape (N, 4). pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list[int] | None): Point prompt labels with shape (N, ).</span>
<span></span><span class="sd">        text (list[str] | None): List of text prompts corresponding to the classes.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): The output masks in shape (C, H, W), where C is the number of generated masks.</span>
<span></span><span class="sd">        pred_bboxes (torch.Tensor): Bounding boxes for each mask with shape (N, 6), where N is the number of boxes.</span>
<span></span><span class="sd">            Each box is in xyxy format with additional columns for score and class.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - The input features is a torch.Tensor of shape (B, C, H, W) if performing on SAM, or a dict[str, Any] if performing on SAM2.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">prompts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_geometric_prompts</span><span class="p">(</span><span class="n">src_shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span></span>    <span class="n">preds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inference_features</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="o">*</span><span class="n">prompts</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">)</span>
<span></span>    <span class="n">pred_boxes</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="s2">"pred_boxes"</span><span class="p">]</span>  <span class="c1"># (nc, num_query, 4)</span>
<span></span>    <span class="n">pred_logits</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="s2">"pred_logits"</span><span class="p">]</span>
<span></span>    <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span>
<span></span>    <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">pred_logits</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
<span></span>    <span class="n">presence_score</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="s2">"presence_logit_dec"</span><span class="p">]</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="n">pred_scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred_scores</span> <span class="o">*</span> <span class="n">presence_score</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="n">pred_cls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span></span>        <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">pred_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span>
<span></span>        <span class="n">dtype</span><span class="o">=</span><span class="n">pred_scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span></span>        <span class="n">device</span><span class="o">=</span><span class="n">pred_scores</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>    <span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">pred_scores</span><span class="p">)</span>
<span></span>    <span class="n">pred_boxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pred_boxes</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">pred_cls</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">keep</span> <span class="o">=</span> <span class="n">pred_scores</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">conf</span>
<span></span>    <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">pred_masks</span><span class="p">[</span><span class="n">keep</span><span class="p">]</span>
<span></span>    <span class="n">pred_boxes</span> <span class="o">=</span> <span class="n">pred_boxes</span><span class="p">[</span><span class="n">keep</span><span class="p">]</span>
<span></span>    <span class="n">pred_boxes</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">xywh2xyxy</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">4</span><span class="p">])</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="n">pred_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_boxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">float</span><span class="p">()[</span><span class="kc">None</span><span class="p">],</span> <span class="n">src_shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span></span>        <span class="n">pred_boxes</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*=</span> <span class="n">src_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span></span>        <span class="n">pred_boxes</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*=</span> <span class="n">src_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span></span>        <span class="n">pred_boxes</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">*=</span> <span class="n">src_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span></span>        <span class="n">pred_boxes</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">*=</span> <span class="n">src_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_boxes</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3SemanticPredictor.postprocess"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3SemanticPredictor.postprocess</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">)</span>
</code></pre></div><p>Post-process the predictions to apply non-overlapping constraints if required.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>preds</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>img</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>orig_imgs</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2292-L2324"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Post-process the predictions to apply non-overlapping constraints if required."""</span>
<span></span>    <span class="n">pred_boxes</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="s2">"pred_boxes"</span><span class="p">]</span>  <span class="c1"># (nc, num_query, 4)</span>
<span></span>    <span class="n">pred_logits</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="s2">"pred_logits"</span><span class="p">]</span>
<span></span>    <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span>
<span></span>    <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">pred_logits</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
<span></span>    <span class="n">presence_score</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="s2">"presence_logit_dec"</span><span class="p">]</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="n">pred_scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred_scores</span> <span class="o">*</span> <span class="n">presence_score</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="n">pred_cls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span></span>        <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">pred_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span>
<span></span>        <span class="n">dtype</span><span class="o">=</span><span class="n">pred_scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span></span>        <span class="n">device</span><span class="o">=</span><span class="n">pred_scores</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>    <span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">pred_scores</span><span class="p">)</span>
<span></span>    <span class="n">pred_boxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pred_boxes</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">pred_cls</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">keep</span> <span class="o">=</span> <span class="n">pred_scores</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">conf</span>
<span></span>    <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">pred_masks</span><span class="p">[</span><span class="n">keep</span><span class="p">]</span>
<span></span>    <span class="n">pred_boxes</span> <span class="o">=</span> <span class="n">pred_boxes</span><span class="p">[</span><span class="n">keep</span><span class="p">]</span>
<span></span>    <span class="n">pred_boxes</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">xywh2xyxy</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">4</span><span class="p">])</span>
<span></span>
<span></span>    <span class="n">names</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">"names"</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pred_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>
<span></span>    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">orig_imgs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>  <span class="c1"># input images are a torch.Tensor, not a list</span>
<span></span>        <span class="n">orig_imgs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_torch2numpy_batch</span><span class="p">(</span><span class="n">orig_imgs</span><span class="p">)</span>
<span></span>    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>    <span class="k">for</span> <span class="n">masks</span><span class="p">,</span> <span class="n">boxes</span><span class="p">,</span> <span class="n">orig_img</span><span class="p">,</span> <span class="n">img_path</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">pred_masks</span><span class="p">],</span> <span class="p">[</span><span class="n">pred_boxes</span><span class="p">],</span> <span class="n">orig_imgs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
<span></span>        <span class="k">if</span> <span class="n">masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>            <span class="n">masks</span><span class="p">,</span> <span class="n">boxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="k">else</span><span class="p">:</span>
<span></span>            <span class="n">masks</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">masks</span><span class="o">.</span><span class="n">float</span><span class="p">()[</span><span class="kc">None</span><span class="p">],</span> <span class="n">orig_img</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span></span>            <span class="n">boxes</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span> <span class="o">*=</span> <span class="n">orig_img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span></span>            <span class="n">boxes</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span> <span class="o">*=</span> <span class="n">orig_img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span></span>        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Results</span><span class="p">(</span><span class="n">orig_img</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="n">img_path</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">names</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="n">masks</span><span class="p">,</span> <span class="n">boxes</span><span class="o">=</span><span class="n">boxes</span><span class="p">))</span>
<span></span>    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3SemanticPredictor.pre_transform"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3SemanticPredictor.pre_transform</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">pre_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">)</span>
</code></pre></div><p>Perform initial transformations on the input image for preprocessing.</p><p>This method applies transformations such as resizing to prepare the image for further preprocessing. Currently, batched inference is not supported; hence the list length should be 1.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>list[np.ndarray]</code></td><td>List containing a single image in HWC numpy array format.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>list[np.ndarray]</code></td><td>List containing the transformed image.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">480</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># Single HWC image</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">transformed</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">pre_transform</span><span class="p">([</span><span class="n">image</span><span class="p">])</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">transformed</span><span class="p">))</span>
<span></span><span class="mi">1</span>
</code></pre></div><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>AssertionError</code></td><td>If the input list contains more than one image.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2227-L2251"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">pre_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform initial transformations on the input image for preprocessing.</span>
<span></span>
<span></span><span class="sd">    This method applies transformations such as resizing to prepare the image for further preprocessing. Currently,</span>
<span></span><span class="sd">    batched inference is not supported; hence the list length should be 1.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (list[np.ndarray]): List containing a single image in HWC numpy array format.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (list[np.ndarray]): List containing the transformed image.</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        AssertionError: If the input list contains more than one image.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; image = np.random.rand(480, 640, 3)  # Single HWC image</span>
<span></span><span class="sd">        &gt;&gt;&gt; transformed = predictor.pre_transform([image])</span>
<span></span><span class="sd">        &gt;&gt;&gt; print(len(transformed))</span>
<span></span><span class="sd">        1</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">im</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"SAM model does not currently support batched inference"</span>
<span></span>    <span class="n">letterbox</span> <span class="o">=</span> <span class="n">LetterBox</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">,</span> <span class="n">auto</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">scale_fill</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># hardcode here for sam3</span>
<span></span>    <span class="k">return</span> <span class="p">[</span><span class="n">letterbox</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">im</span><span class="p">]</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3SemanticPredictor.reset_prompts"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3SemanticPredictor.reset_prompts</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">reset_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div><p>Reset the prompts for the predictor.</p><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2391-L2394"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">reset_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Reset the prompts for the predictor."""</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">text_embeddings</span> <span class="o">=</span> <span class="p">{}</span>
</code></pre></div></details><p><br/><br/><hr/><br/></p><h2 id="ultralytics.models.sam.predict.SAM3VideoPredictor"><span class="doc-kind doc-kind-class">class</span> <code>ultralytics.models.sam.predict.SAM3VideoPredictor</code></h2><div class="highlight"><pre><span></span><code><span></span><span class="n">SAM3VideoPredictor</span><span class="p">()</span>
</code></pre></div><p><strong>Bases:</strong> <code>SAM2VideoPredictor</code>, <code>SAM3Predictor</code></p><p>Segment Anything Model 3 (SAM3) Video Predictor for video segmentation tasks.</p><p><strong>Methods</strong></p><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoPredictor.propagate_in_video"><code>propagate_in_video</code></a></td><td>Perform image segmentation inference based on the given input cues, using the currently loaded image. This</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2405-L2456"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SAM3VideoPredictor</span><span class="p">(</span><span class="n">SAM2VideoPredictor</span><span class="p">,</span> <span class="n">SAM3Predictor</span><span class="p">):</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoPredictor.propagate_in_video"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoPredictor.propagate_in_video</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">propagate_in_video</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">)</span>
</code></pre></div><p>Perform image segmentation inference based on the given input cues, using the currently loaded image. This</p><p>method leverages SAM's (Segment Anything Model) architecture consisting of image encoder, prompt encoder, and mask decoder for real-time and promptable segmentation tasks.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>inference_state</code></td><td><code>dict</code></td><td>The current state of inference, including input cues and previous outputs.</td><td><em>required</em></td></tr><tr><td><code>frame_idx</code></td><td><code>int</code></td><td>The index of the current frame in the video sequence.</td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2408-L2456"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">propagate_in_video</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_state</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform image segmentation inference based on the given input cues, using the currently loaded image. This</span>
<span></span><span class="sd">    method leverages SAM's (Segment Anything Model) architecture consisting of image encoder, prompt</span>
<span></span><span class="sd">    encoder, and mask decoder for real-time and promptable segmentation tasks.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        inference_state (dict): The current state of inference, including input cues and previous outputs.</span>
<span></span><span class="sd">        frame_idx (int): The index of the current frame in the video sequence.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">frame</span> <span class="o">=</span> <span class="n">frame_idx</span>
<span></span>    <span class="n">output_dict</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict"</span><span class="p">]</span>
<span></span>    <span class="n">obj_ids</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span>
<span></span>    <span class="n">consolidated_frame_inds</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"consolidated_frame_inds"</span><span class="p">]</span>
<span></span>    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_idx_to_id"</span><span class="p">])</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">"No points are provided; please add points first"</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]:</span>
<span></span>        <span class="n">storage_key</span> <span class="o">=</span> <span class="s2">"cond_frame_outputs"</span>
<span></span>        <span class="n">current_out</span> <span class="o">=</span> <span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame</span><span class="p">]</span>
<span></span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem_around_input</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem_for_multi_obj</span> <span class="ow">or</span> <span class="n">batch_size</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">):</span>
<span></span>            <span class="c1"># clear non-conditioning memory of the surrounding frames</span>
<span></span>            <span class="bp">self</span><span class="o">.</span><span class="n">_clear_non_cond_mem_around_input</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
<span></span>    <span class="k">elif</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]:</span>
<span></span>        <span class="n">storage_key</span> <span class="o">=</span> <span class="s2">"non_cond_frame_outputs"</span>
<span></span>        <span class="n">current_out</span> <span class="o">=</span> <span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame</span><span class="p">]</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="n">storage_key</span> <span class="o">=</span> <span class="s2">"non_cond_frame_outputs"</span>
<span></span>        <span class="n">current_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_single_frame_inference</span><span class="p">(</span>
<span></span>            <span class="n">output_dict</span><span class="o">=</span><span class="n">output_dict</span><span class="p">,</span>
<span></span>            <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame</span><span class="p">,</span>
<span></span>            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span></span>            <span class="n">is_init_cond_frame</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>            <span class="n">point_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>            <span class="n">mask_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>            <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>            <span class="n">run_mem_encoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span></span>            <span class="n">inference_state</span><span class="o">=</span><span class="n">inference_state</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_out</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">_prune_non_cond_memory</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">inference_state</span><span class="o">=</span><span class="n">inference_state</span><span class="p">)</span>
<span></span>    <span class="c1"># Create slices of per-object outputs for subsequent interaction with each</span>
<span></span>    <span class="c1"># individual object after tracking.</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">_add_output_per_object</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">current_out</span><span class="p">,</span> <span class="n">storage_key</span><span class="p">,</span> <span class="n">inference_state</span><span class="o">=</span><span class="n">inference_state</span><span class="p">)</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"frames_already_tracked"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
<span></span>    <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="n">obj_scores</span> <span class="o">=</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"object_score_logits"</span><span class="p">]</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">obj_ids</span><span class="p">,</span> <span class="n">pred_masks</span><span class="p">,</span> <span class="n">obj_scores</span>
</code></pre></div></details><p><br/><br/><hr/><br/></p><h2 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor"><span class="doc-kind doc-kind-class">class</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor</code></h2><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">cfg</span><span class="o">=</span><span class="n">DEFAULT_CFG</span><span class="p">,</span>
<span></span>    <span class="n">overrides</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">_callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="c1"># prob threshold for detection outputs -- only keep detections above this threshold</span>
<span></span>    <span class="c1"># enters NMS and det-to-track matching</span>
<span></span>    <span class="n">score_threshold_detection</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
<span></span>    <span class="c1"># IoU threshold for detection NMS</span>
<span></span>    <span class="n">det_nms_thresh</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span></span>    <span class="c1"># IoU threshold for det-to-track matching -- a detection is considered "matched" to a tracklet it</span>
<span></span>    <span class="c1"># overlaps with a tracklet above this threshold -- it is often a loose threshold like 0.1</span>
<span></span>    <span class="n">assoc_iou_thresh</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
<span></span>    <span class="c1"># IoU threshold for det-to-track matching, which is used to determine whether a masklet is "unmatched"</span>
<span></span>    <span class="c1"># by any detections -- it is often a stricter threshold like 0.5</span>
<span></span>    <span class="n">trk_assoc_iou_thresh</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
<span></span>    <span class="c1"># prob threshold for a detection to be added as a new object</span>
<span></span>    <span class="n">new_det_thresh</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span></span>    <span class="c1"># hotstart parameters: we hold off the outputs for `hotstart_delay` frames and</span>
<span></span>    <span class="c1"># 1) remove those tracklets unmatched by any detections based on `hotstart_unmatch_thresh`</span>
<span></span>    <span class="c1"># 2) remove those tracklets overlapping with one another based on `hotstart_dup_thresh`</span>
<span></span>    <span class="n">hotstart_delay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span></span>    <span class="n">hotstart_unmatch_thresh</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span></span>    <span class="n">hotstart_dup_thresh</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span></span>    <span class="n">init_trk_keep_alive</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
<span></span>    <span class="n">max_trk_keep_alive</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
<span></span>    <span class="n">min_trk_keep_alive</span><span class="o">=-</span><span class="mi">4</span><span class="p">,</span>
<span></span>    <span class="c1"># Threshold for suppressing overlapping objects based on recent occlusion</span>
<span></span>    <span class="n">suppress_overlapping_based_on_recent_occlusion_threshold</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span></span>    <span class="n">decrease_trk_keep_alive_for_empty_masklets</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span></span>    <span class="n">o2o_matching_masklets_enable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Enable hungarian matching to match existing masklets</span>
<span></span>    <span class="n">suppress_det_close_to_boundary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="n">fill_hole_area</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span></span>    <span class="c1"># The maximum number of objects (masklets) to track across all GPUs (for no limit, set it to -1)</span>
<span></span>    <span class="n">max_num_objects</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<span></span>    <span class="n">recondition_every_nth_frame</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<span></span>    <span class="c1"># masket confirmation status (to suppress unconfirmed masklets)</span>
<span></span>    <span class="n">masklet_confirmation_enable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="c1"># a masklet is confirmed after being consecutively detected and matched for</span>
<span></span>    <span class="c1"># `masklet_confirmation_consecutive_det_thresh`</span>
<span></span>    <span class="n">masklet_confirmation_consecutive_det_thresh</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span></span>    <span class="c1"># bbox heuristic parameters</span>
<span></span>    <span class="n">reconstruction_bbox_iou_thresh</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span></span>    <span class="n">reconstruction_bbox_det_score</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p><strong>Bases:</strong> <code>SAM3SemanticPredictor</code></p><p>Segment Anything Model 3 (SAM3) Video Semantic Predictor.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>cfg</code></td><td></td><td></td><td><code>DEFAULT_CFG</code></td></tr><tr><td><code>overrides</code></td><td></td><td></td><td><code>None</code></td></tr><tr><td><code>_callbacks</code></td><td></td><td></td><td><code>None</code></td></tr><tr><td><code>score_threshold_detection</code></td><td></td><td></td><td><code>0.5</code></td></tr><tr><td><code>det_nms_thresh</code></td><td></td><td></td><td><code>0.0</code></td></tr><tr><td><code>assoc_iou_thresh</code></td><td></td><td></td><td><code>0.5</code></td></tr><tr><td><code>trk_assoc_iou_thresh</code></td><td></td><td></td><td><code>0.5</code></td></tr><tr><td><code>new_det_thresh</code></td><td></td><td></td><td><code>0.0</code></td></tr><tr><td><code>hotstart_delay</code></td><td></td><td></td><td><code>0</code></td></tr><tr><td><code>hotstart_unmatch_thresh</code></td><td></td><td></td><td><code>3</code></td></tr><tr><td><code>hotstart_dup_thresh</code></td><td></td><td></td><td><code>3</code></td></tr><tr><td><code>init_trk_keep_alive</code></td><td></td><td></td><td><code>30</code></td></tr><tr><td><code>max_trk_keep_alive</code></td><td></td><td></td><td><code>30</code></td></tr><tr><td><code>min_trk_keep_alive</code></td><td></td><td></td><td><code>-4</code></td></tr><tr><td><code>suppress_overlapping_based_on_recent_occlusion_threshold</code></td><td></td><td></td><td><code>0.0</code></td></tr><tr><td><code>decrease_trk_keep_alive_for_empty_masklets</code></td><td></td><td></td><td><code>True</code></td></tr><tr><td><code>o2o_matching_masklets_enable</code></td><td></td><td></td><td><code>False</code></td></tr><tr><td><code>suppress_det_close_to_boundary</code></td><td></td><td></td><td><code>False</code></td></tr><tr><td><code>fill_hole_area</code></td><td></td><td></td><td><code>16</code></td></tr><tr><td><code>max_num_objects</code></td><td></td><td></td><td><code>-1</code></td></tr><tr><td><code>recondition_every_nth_frame</code></td><td></td><td></td><td><code>-1</code></td></tr><tr><td><code>masklet_confirmation_enable</code></td><td></td><td></td><td><code>False</code></td></tr><tr><td><code>masklet_confirmation_consecutive_det_thresh</code></td><td></td><td></td><td><code>3</code></td></tr><tr><td><code>reconstruction_bbox_iou_thresh</code></td><td></td><td></td><td><code>0.0</code></td></tr><tr><td><code>reconstruction_bbox_det_score</code></td><td></td><td></td><td><code>0.0</code></td></tr></tbody></table><p><strong>Methods</strong></p><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._apply_object_wise_non_overlapping_constraints"><code>_apply_object_wise_non_overlapping_constraints</code></a></td><td>Applies non-overlapping constraints object wise (i.e. only one object can claim the overlapping region).</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._associate_det_trk"><code>_associate_det_trk</code></a></td><td>Match detections on the current frame with the existing masklets.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._cache_backbone_features"><code>_cache_backbone_features</code></a></td><td>Build and cache SAM2 backbone features.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._det_track_one_frame"><code>_det_track_one_frame</code></a></td><td>This function handles one-step inference for the DenseTracking model in an SPMD manner. At a high-level, all</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._drop_new_det_with_obj_limit"><code>_drop_new_det_with_obj_limit</code></a></td><td>Drop a few new detections based on the maximum number of objects. We drop new objects based on their</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._extract_detection_outputs"><code>_extract_detection_outputs</code></a></td><td>Extract and filter detection outputs.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._initialize_metadata"><code>_initialize_metadata</code></a></td><td>Initialize metadata for the masklets.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._process_hotstart"><code>_process_hotstart</code></a></td><td>Handle hotstart heuristics to remove unmatched or duplicated objects.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._propogate_tracker_one_frame_local_gpu"><code>_propogate_tracker_one_frame_local_gpu</code></a></td><td>Inference_states: list of inference states, each state corresponds to a different set of objects.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._recondition_masklets"><code>_recondition_masklets</code></a></td><td>Recondition masklets based on new high-confidence detections.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._run_single_frame_inference"><code>_run_single_frame_inference</code></a></td><td>Perform inference on a single frame and get its inference results.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._suppress_detections_close_to_boundary"><code>_suppress_detections_close_to_boundary</code></a></td><td>Suppress detections too close to image edges (for normalized boxes).</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._suppress_overlapping_based_on_recent_occlusion"><code>_suppress_overlapping_based_on_recent_occlusion</code></a></td><td>Suppress overlapping masks based on the most recent occlusion information. If an object is removed by</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_add_new_objects"><code>_tracker_add_new_objects</code></a></td><td>Add a new object to SAM2 inference states.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_remove_objects"><code>_tracker_remove_objects</code></a></td><td>Remove an object from SAM2 inference states. This would remove the object from all frames in the video.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_update_memories"><code>_tracker_update_memories</code></a></td><td>Run Sam2 memory encoder, enforcing non-overlapping constraints globally.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.add_prompt"><code>add_prompt</code></a></td><td>Add text, point or box prompts on a single frame. This method returns the inference outputs only on the</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.build_outputs"><code>build_outputs</code></a></td><td>Build the output masks for the current frame.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.inference"><code>inference</code></a></td><td>Perform inference on a video sequence with optional prompts.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.init_state"><code>init_state</code></a></td><td>Initialize an inference state for the predictor.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.postprocess"><code>postprocess</code></a></td><td>Post-process the predictions to apply non-overlapping constraints if required.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_backbone_and_detection"><code>run_backbone_and_detection</code></a></td><td>Run backbone and detection for a single frame.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_propagation"><code>run_tracker_propagation</code></a></td><td>Run the tracker propagation phase for a single frame in an SPMD manner.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_update_execution_phase"><code>run_tracker_update_execution_phase</code></a></td><td>Execute the tracker update plan for a single frame in an SPMD manner.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_update_planning_phase"><code>run_tracker_update_planning_phase</code></a></td><td>Run the tracker update planning phase for a single frame in an SPMD manner.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.setup_model"><code>setup_model</code></a></td><td>Setup the SAM3VideoSemanticPredictor model.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.setup_source"><code>setup_source</code></a></td><td>Setup the source for the SAM3VideoSemanticPredictor model.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.update_masklet_confirmation_status"><code>update_masklet_confirmation_status</code></a></td><td>Update the confirmation status of masklets based on the current frame's detection results.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2459-L3940"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SAM3VideoSemanticPredictor</span><span class="p">(</span><span class="n">SAM3SemanticPredictor</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Segment Anything Model 3 (SAM3) Video Semantic Predictor."""</span>
<span></span>
<span></span>    <span class="n">HIGH_CONF_THRESH</span> <span class="o">=</span> <span class="mf">0.8</span>
<span></span>    <span class="n">HIGH_IOU_THRESH</span> <span class="o">=</span> <span class="mf">0.8</span>
<span></span>    <span class="n">NO_OBJ_LOGIT</span> <span class="o">=</span> <span class="o">-</span><span class="mf">10.0</span>
<span></span>    <span class="n">NEVER_OCCLUDED</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span></span>    <span class="n">ALWAYS_OCCLUDED</span> <span class="o">=</span> <span class="mi">100000</span>
<span></span>
<span></span>    <span class="n">UNCONFIRMED</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># newly added masklet, not confirmed by any detection yet</span>
<span></span>    <span class="n">CONFIRMED</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># confirmed by at least one detection</span>
<span></span>    <span class="n">_bb_feat_sizes</span> <span class="o">=</span> <span class="p">[</span>
<span></span>        <span class="p">(</span><span class="mi">288</span><span class="p">,</span> <span class="mi">288</span><span class="p">),</span>
<span></span>        <span class="p">(</span><span class="mi">144</span><span class="p">,</span> <span class="mi">144</span><span class="p">),</span>
<span></span>        <span class="p">(</span><span class="mi">72</span><span class="p">,</span> <span class="mi">72</span><span class="p">),</span>
<span></span>    <span class="p">]</span>
<span></span>    <span class="n">stride</span> <span class="o">=</span> <span class="mi">14</span>
<span></span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<span></span>        <span class="bp">self</span><span class="p">,</span>
<span></span>        <span class="n">cfg</span><span class="o">=</span><span class="n">DEFAULT_CFG</span><span class="p">,</span>
<span></span>        <span class="n">overrides</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="n">_callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="c1"># prob threshold for detection outputs -- only keep detections above this threshold</span>
<span></span>        <span class="c1"># enters NMS and det-to-track matching</span>
<span></span>        <span class="n">score_threshold_detection</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
<span></span>        <span class="c1"># IoU threshold for detection NMS</span>
<span></span>        <span class="n">det_nms_thresh</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span></span>        <span class="c1"># IoU threshold for det-to-track matching -- a detection is considered "matched" to a tracklet it</span>
<span></span>        <span class="c1"># overlaps with a tracklet above this threshold -- it is often a loose threshold like 0.1</span>
<span></span>        <span class="n">assoc_iou_thresh</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
<span></span>        <span class="c1"># IoU threshold for det-to-track matching, which is used to determine whether a masklet is "unmatched"</span>
<span></span>        <span class="c1"># by any detections -- it is often a stricter threshold like 0.5</span>
<span></span>        <span class="n">trk_assoc_iou_thresh</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
<span></span>        <span class="c1"># prob threshold for a detection to be added as a new object</span>
<span></span>        <span class="n">new_det_thresh</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span></span>        <span class="c1"># hotstart parameters: we hold off the outputs for `hotstart_delay` frames and</span>
<span></span>        <span class="c1"># 1) remove those tracklets unmatched by any detections based on `hotstart_unmatch_thresh`</span>
<span></span>        <span class="c1"># 2) remove those tracklets overlapping with one another based on `hotstart_dup_thresh`</span>
<span></span>        <span class="n">hotstart_delay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span></span>        <span class="n">hotstart_unmatch_thresh</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span></span>        <span class="n">hotstart_dup_thresh</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span></span>        <span class="n">init_trk_keep_alive</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
<span></span>        <span class="n">max_trk_keep_alive</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
<span></span>        <span class="n">min_trk_keep_alive</span><span class="o">=-</span><span class="mi">4</span><span class="p">,</span>
<span></span>        <span class="c1"># Threshold for suppressing overlapping objects based on recent occlusion</span>
<span></span>        <span class="n">suppress_overlapping_based_on_recent_occlusion_threshold</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span></span>        <span class="n">decrease_trk_keep_alive_for_empty_masklets</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span></span>        <span class="n">o2o_matching_masklets_enable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Enable hungarian matching to match existing masklets</span>
<span></span>        <span class="n">suppress_det_close_to_boundary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="n">fill_hole_area</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span></span>        <span class="c1"># The maximum number of objects (masklets) to track across all GPUs (for no limit, set it to -1)</span>
<span></span>        <span class="n">max_num_objects</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<span></span>        <span class="n">recondition_every_nth_frame</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<span></span>        <span class="c1"># masket confirmation status (to suppress unconfirmed masklets)</span>
<span></span>        <span class="n">masklet_confirmation_enable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="c1"># a masklet is confirmed after being consecutively detected and matched for</span>
<span></span>        <span class="c1"># `masklet_confirmation_consecutive_det_thresh`</span>
<span></span>        <span class="n">masklet_confirmation_consecutive_det_thresh</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span></span>        <span class="c1"># bbox heuristic parameters</span>
<span></span>        <span class="n">reconstruction_bbox_iou_thresh</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span></span>        <span class="n">reconstruction_bbox_det_score</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span></span>    <span class="p">):</span>
<span></span><span class="w">        </span><span class="sd">"""Initialize the SAM3VideoSemanticPredictor with configuration and optional overrides."""</span>
<span></span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">overrides</span><span class="p">,</span> <span class="n">_callbacks</span><span class="p">)</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">score_threshold_detection</span> <span class="o">=</span> <span class="n">score_threshold_detection</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">det_nms_thresh</span> <span class="o">=</span> <span class="n">det_nms_thresh</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">assoc_iou_thresh</span> <span class="o">=</span> <span class="n">assoc_iou_thresh</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">trk_assoc_iou_thresh</span> <span class="o">=</span> <span class="n">trk_assoc_iou_thresh</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">new_det_thresh</span> <span class="o">=</span> <span class="n">new_det_thresh</span>
<span></span>
<span></span>        <span class="c1"># hotstart parameters</span>
<span></span>        <span class="k">if</span> <span class="n">hotstart_delay</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span></span>            <span class="k">assert</span> <span class="n">hotstart_unmatch_thresh</span> <span class="o">&lt;=</span> <span class="n">hotstart_delay</span>
<span></span>            <span class="k">assert</span> <span class="n">hotstart_dup_thresh</span> <span class="o">&lt;=</span> <span class="n">hotstart_delay</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">hotstart_delay</span> <span class="o">=</span> <span class="n">hotstart_delay</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">hotstart_unmatch_thresh</span> <span class="o">=</span> <span class="n">hotstart_unmatch_thresh</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">hotstart_dup_thresh</span> <span class="o">=</span> <span class="n">hotstart_dup_thresh</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">init_trk_keep_alive</span> <span class="o">=</span> <span class="n">init_trk_keep_alive</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">max_trk_keep_alive</span> <span class="o">=</span> <span class="n">max_trk_keep_alive</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">min_trk_keep_alive</span> <span class="o">=</span> <span class="n">min_trk_keep_alive</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">suppress_overlapping_based_on_recent_occlusion_threshold</span> <span class="o">=</span> <span class="p">(</span>
<span></span>            <span class="n">suppress_overlapping_based_on_recent_occlusion_threshold</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">suppress_det_close_to_boundary</span> <span class="o">=</span> <span class="n">suppress_det_close_to_boundary</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">decrease_trk_keep_alive_for_empty_masklets</span> <span class="o">=</span> <span class="n">decrease_trk_keep_alive_for_empty_masklets</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">o2o_matching_masklets_enable</span> <span class="o">=</span> <span class="n">o2o_matching_masklets_enable</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">fill_hole_area</span> <span class="o">=</span> <span class="n">fill_hole_area</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">_dist_pg_cpu</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># CPU process group (lazy-initialized on first use)</span>
<span></span>
<span></span>        <span class="n">max_num_objects</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># no limit</span>
<span></span>        <span class="n">num_obj_for_compile</span> <span class="o">=</span> <span class="mi">16</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">max_num_objects</span> <span class="o">=</span> <span class="n">max_num_objects</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">num_obj_for_compile</span> <span class="o">=</span> <span class="n">num_obj_for_compile</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">recondition_every_nth_frame</span> <span class="o">=</span> <span class="n">recondition_every_nth_frame</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">masklet_confirmation_enable</span> <span class="o">=</span> <span class="n">masklet_confirmation_enable</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">masklet_confirmation_consecutive_det_thresh</span> <span class="o">=</span> <span class="n">masklet_confirmation_consecutive_det_thresh</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_bbox_iou_thresh</span> <span class="o">=</span> <span class="n">reconstruction_bbox_iou_thresh</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_bbox_det_score</span> <span class="o">=</span> <span class="n">reconstruction_bbox_det_score</span>
<span></span>
<span></span>        <span class="c1"># build SAM3 tracker</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span> <span class="o">=</span> <span class="n">SAM3VideoPredictor</span><span class="p">(</span><span class="n">overrides</span><span class="o">=</span><span class="n">overrides</span><span class="p">)</span>
<span></span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">[</span><span class="s2">"on_predict_start"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_state</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._apply_object_wise_non_overlapping_constraints"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._apply_object_wise_non_overlapping_constraints</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_apply_object_wise_non_overlapping_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred_masks</span><span class="p">,</span> <span class="n">obj_scores</span><span class="p">,</span> <span class="n">background_value</span> <span class="o">=</span> <span class="o">-</span><span class="mf">10.0</span><span class="p">)</span>
</code></pre></div><p>Applies non-overlapping constraints object wise (i.e. only one object can claim the overlapping region).</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>pred_masks</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>obj_scores</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>background_value</code></td><td></td><td></td><td><code>-10.0</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2761-L2775"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_apply_object_wise_non_overlapping_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred_masks</span><span class="p">,</span> <span class="n">obj_scores</span><span class="p">,</span> <span class="n">background_value</span><span class="o">=-</span><span class="mf">10.0</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Applies non-overlapping constraints object wise (i.e. only one object can claim the overlapping region)."""</span>
<span></span>    <span class="c1"># Replace pixel scores with object scores</span>
<span></span>    <span class="n">pred_masks_single_score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">pred_masks</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">obj_scores</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">background_value</span><span class="p">)</span>
<span></span>    <span class="c1"># Apply pixel-wise non-overlapping constraint based on mask scores</span>
<span></span>    <span class="n">pixel_level_non_overlapping_masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_apply_non_overlapping_constraints</span><span class="p">(</span>
<span></span>        <span class="n">pred_masks_single_score</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="c1"># Replace object scores with pixel scores. Note, that now only one object can claim the overlapping region</span>
<span></span>    <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
<span></span>        <span class="n">pixel_level_non_overlapping_masks</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span>
<span></span>        <span class="n">pred_masks</span><span class="p">,</span>
<span></span>        <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">pred_masks</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">background_value</span><span class="p">),</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._associate_det_trk"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._associate_det_trk</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_associate_det_trk</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">det_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">det_scores_np</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">trk_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">trk_obj_ids</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Match detections on the current frame with the existing masklets.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>det_masks</code></td><td><code>torch.Tensor</code></td><td>(N, H, W) tensor of predicted masks</td><td><em>required</em></td></tr><tr><td><code>det_scores_np</code></td><td><code>np.ndarray</code></td><td>(N,) array of detection scores</td><td><em>required</em></td></tr><tr><td><code>trk_masks</code></td><td><code>torch.Tensor</code></td><td>(M, H, W) tensor of track masks</td><td><em>required</em></td></tr><tr><td><code>trk_obj_ids</code></td><td><code>np.ndarray</code></td><td>(M,) array of object IDs corresponding to trk_masks</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>new_det_fa_inds</code></td><td>array of new object indices.</td></tr><tr><td><code>unmatched_trk_obj_ids</code></td><td>array of existing masklet object IDs that are not matched to any detections on this</td></tr><tr><td><code>det_to_matched_trk_obj_ids</code></td><td>dict[int, np.ndarray]: mapping from detector's detection indices to the list of</td></tr><tr><td><code>empty_trk_obj_ids</code></td><td>array of existing masklet object IDs with zero area in SAM2 prediction</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L3467-L3598"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_associate_det_trk</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">det_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">det_scores_np</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">trk_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">trk_obj_ids</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Match detections on the current frame with the existing masklets.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        det_masks: (N, H, W) tensor of predicted masks</span>
<span></span><span class="sd">        det_scores_np: (N,) array of detection scores</span>
<span></span><span class="sd">        trk_masks: (M, H, W) tensor of track masks</span>
<span></span><span class="sd">        trk_obj_ids: (M,) array of object IDs corresponding to trk_masks</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        new_det_fa_inds: array of new object indices.</span>
<span></span><span class="sd">        unmatched_trk_obj_ids: array of existing masklet object IDs that are not matched to any detections on this</span>
<span></span><span class="sd">            frame (for unmatched, we only count masklets with &gt;0 area)</span>
<span></span><span class="sd">        det_to_matched_trk_obj_ids: dict[int, np.ndarray]: mapping from detector's detection indices to the list of</span>
<span></span><span class="sd">            matched tracklet object IDs</span>
<span></span><span class="sd">        empty_trk_obj_ids: array of existing masklet object IDs with zero area in SAM2 prediction</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">iou_threshold</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assoc_iou_thresh</span>
<span></span>    <span class="n">iou_threshold_trk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trk_assoc_iou_thresh</span>
<span></span>    <span class="n">new_det_thresh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_det_thresh</span>
<span></span>
<span></span>    <span class="k">assert</span> <span class="n">det_masks</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(),</span> <span class="s2">"float tensor expected (do not binarize)"</span>
<span></span>    <span class="k">assert</span> <span class="n">trk_masks</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(),</span> <span class="s2">"float tensor expected (do not binarize)"</span>
<span></span>    <span class="k">assert</span> <span class="n">trk_masks</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">trk_obj_ids</span><span class="p">),</span> <span class="p">(</span>
<span></span>        <span class="sa">f</span><span class="s2">"trk_masks and trk_obj_ids should have the same length, </span><span class="si">{</span><span class="n">trk_masks</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2"> vs </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">trk_obj_ids</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">trk_masks</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="c1"># all detections are new</span>
<span></span>        <span class="n">new_det_fa_inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">det_masks</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span></span>        <span class="n">unmatched_trk_obj_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span></span>        <span class="n">empty_trk_obj_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span></span>        <span class="n">det_to_matched_trk_obj_ids</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>        <span class="n">trk_id_to_max_iou_high_conf_det</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>        <span class="k">return</span> <span class="p">(</span>
<span></span>            <span class="n">new_det_fa_inds</span><span class="p">,</span>
<span></span>            <span class="n">unmatched_trk_obj_ids</span><span class="p">,</span>
<span></span>            <span class="n">det_to_matched_trk_obj_ids</span><span class="p">,</span>
<span></span>            <span class="n">trk_id_to_max_iou_high_conf_det</span><span class="p">,</span>
<span></span>            <span class="n">empty_trk_obj_ids</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>    <span class="k">elif</span> <span class="n">det_masks</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="c1"># all previous tracklets are unmatched if they have a non-zero area</span>
<span></span>        <span class="n">new_det_fa_inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span></span>        <span class="n">trk_is_nonempty</span> <span class="o">=</span> <span class="p">(</span><span class="n">trk_masks</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span></span>        <span class="n">unmatched_trk_obj_ids</span> <span class="o">=</span> <span class="n">trk_obj_ids</span><span class="p">[</span><span class="n">trk_is_nonempty</span><span class="p">]</span>
<span></span>        <span class="n">empty_trk_obj_ids</span> <span class="o">=</span> <span class="n">trk_obj_ids</span><span class="p">[</span><span class="o">~</span><span class="n">trk_is_nonempty</span><span class="p">]</span>
<span></span>        <span class="n">det_to_matched_trk_obj_ids</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>        <span class="n">trk_id_to_max_iou_high_conf_det</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>        <span class="k">return</span> <span class="p">(</span>
<span></span>            <span class="n">new_det_fa_inds</span><span class="p">,</span>
<span></span>            <span class="n">unmatched_trk_obj_ids</span><span class="p">,</span>
<span></span>            <span class="n">det_to_matched_trk_obj_ids</span><span class="p">,</span>
<span></span>            <span class="n">trk_id_to_max_iou_high_conf_det</span><span class="p">,</span>
<span></span>            <span class="n">empty_trk_obj_ids</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="n">det_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span> <span class="o">!=</span> <span class="n">trk_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]:</span>
<span></span>        <span class="c1"># resize to the smaller size to save GPU memory</span>
<span></span>        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">det_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:])</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">trk_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]):</span>
<span></span>            <span class="n">trk_masks</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
<span></span>                <span class="n">trk_masks</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
<span></span>                <span class="n">size</span><span class="o">=</span><span class="n">det_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span>
<span></span>                <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">,</span>
<span></span>                <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>            <span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span></span>        <span class="k">else</span><span class="p">:</span>
<span></span>            <span class="c1"># resize detections to track size</span>
<span></span>            <span class="n">det_masks</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
<span></span>                <span class="n">det_masks</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
<span></span>                <span class="n">size</span><span class="o">=</span><span class="n">trk_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span>
<span></span>                <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">,</span>
<span></span>                <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>            <span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">det_masks_binary</span> <span class="o">=</span> <span class="n">det_masks</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span></span>    <span class="n">trk_masks_binary</span> <span class="o">=</span> <span class="n">trk_masks</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span></span>    <span class="n">ious</span> <span class="o">=</span> <span class="n">mask_iou</span><span class="p">(</span><span class="n">det_masks_binary</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">trk_masks_binary</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>  <span class="c1"># (N, M)</span>
<span></span>
<span></span>    <span class="n">ious_np</span> <span class="o">=</span> <span class="n">ious</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">o2o_matching_masklets_enable</span><span class="p">:</span>
<span></span>        <span class="kn">from</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="kn">import</span> <span class="n">linear_sum_assignment</span>
<span></span>
<span></span>        <span class="c1"># Hungarian matching for tracks (one-to-one: each track matches at most one detection)</span>
<span></span>        <span class="n">cost_matrix</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">ious_np</span>  <span class="c1"># Hungarian solves for minimum cost</span>
<span></span>        <span class="n">row_ind</span><span class="p">,</span> <span class="n">col_ind</span> <span class="o">=</span> <span class="n">linear_sum_assignment</span><span class="p">(</span><span class="n">cost_matrix</span><span class="p">)</span>
<span></span>        <span class="n">trk_is_matched</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">trk_masks</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span></span>        <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">row_ind</span><span class="p">,</span> <span class="n">col_ind</span><span class="p">):</span>
<span></span>            <span class="k">if</span> <span class="n">ious_np</span><span class="p">[</span><span class="n">d</span><span class="p">,</span> <span class="n">t</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">iou_threshold_trk</span><span class="p">:</span>
<span></span>                <span class="n">trk_is_matched</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="n">trk_is_matched</span> <span class="o">=</span> <span class="p">(</span><span class="n">ious_np</span> <span class="o">&gt;=</span> <span class="n">iou_threshold_trk</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span></span>    <span class="c1"># Non-empty tracks not matched by Hungarian assignment above threshold are unmatched</span>
<span></span>    <span class="n">trk_is_nonempty</span> <span class="o">=</span> <span class="n">trk_masks_binary</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span></span>    <span class="n">trk_is_unmatched</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">trk_is_nonempty</span><span class="p">,</span> <span class="o">~</span><span class="n">trk_is_matched</span><span class="p">)</span>
<span></span>    <span class="n">unmatched_trk_obj_ids</span> <span class="o">=</span> <span class="n">trk_obj_ids</span><span class="p">[</span><span class="n">trk_is_unmatched</span><span class="p">]</span>
<span></span>    <span class="c1"># also record masklets that have zero area in SAM 2 prediction</span>
<span></span>    <span class="n">empty_trk_obj_ids</span> <span class="o">=</span> <span class="n">trk_obj_ids</span><span class="p">[</span><span class="o">~</span><span class="n">trk_is_nonempty</span><span class="p">]</span>
<span></span>
<span></span>    <span class="c1"># For detections: allow many tracks to match to the same detection (many-to-one)</span>
<span></span>    <span class="c1"># So, a detection is 'new' if it does not match any track above threshold</span>
<span></span>    <span class="n">is_new_det</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span>
<span></span>        <span class="n">det_scores_np</span> <span class="o">&gt;=</span> <span class="n">new_det_thresh</span><span class="p">,</span>
<span></span>        <span class="n">np</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">ious_np</span> <span class="o">&gt;=</span> <span class="n">iou_threshold</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)),</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="n">new_det_fa_inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">is_new_det</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span></span>
<span></span>    <span class="c1"># for each detection, which tracks it matched to (above threshold)</span>
<span></span>    <span class="n">det_to_matched_trk_obj_ids</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>    <span class="n">trk_id_to_max_iou_high_conf_det</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># trk id --&gt; exactly one detection idx</span>
<span></span>    <span class="n">det_to_max_iou_trk_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">ious_np</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="n">det_is_high_conf</span> <span class="o">=</span> <span class="p">(</span><span class="n">det_scores_np</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">HIGH_CONF_THRESH</span><span class="p">)</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">is_new_det</span>
<span></span>    <span class="n">det_is_high_iou</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">ious_np</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">HIGH_IOU_THRESH</span>
<span></span>    <span class="n">det_is_high_conf_and_iou</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">det_is_high_conf</span> <span class="o">&amp;</span> <span class="n">det_is_high_iou</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span></span>    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">det_masks</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
<span></span>        <span class="n">det_to_matched_trk_obj_ids</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">trk_obj_ids</span><span class="p">[</span><span class="n">ious_np</span><span class="p">[</span><span class="n">d</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&gt;=</span> <span class="n">iou_threshold</span><span class="p">]</span>
<span></span>        <span class="k">if</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">det_is_high_conf_and_iou</span><span class="p">:</span>
<span></span>            <span class="n">trk_obj_id</span> <span class="o">=</span> <span class="n">trk_obj_ids</span><span class="p">[</span><span class="n">det_to_max_iou_trk_idx</span><span class="p">[</span><span class="n">d</span><span class="p">]]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span></span>            <span class="n">trk_id_to_max_iou_high_conf_det</span><span class="p">[</span><span class="n">trk_obj_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">d</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="p">(</span>
<span></span>        <span class="n">new_det_fa_inds</span><span class="p">,</span>
<span></span>        <span class="n">unmatched_trk_obj_ids</span><span class="p">,</span>
<span></span>        <span class="n">det_to_matched_trk_obj_ids</span><span class="p">,</span>
<span></span>        <span class="n">trk_id_to_max_iou_high_conf_det</span><span class="p">,</span>
<span></span>        <span class="n">empty_trk_obj_ids</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._cache_backbone_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._cache_backbone_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_cache_backbone_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sam3_image_out</span><span class="p">)</span>
</code></pre></div><p>Build and cache SAM2 backbone features.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>sam3_image_out</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2933-L2948"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_cache_backbone_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sam3_image_out</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Build and cache SAM2 backbone features."""</span>
<span></span>    <span class="n">sam_mask_decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">sam_mask_decoder</span>
<span></span>    <span class="n">feats</span> <span class="o">=</span> <span class="n">sam3_image_out</span><span class="p">[</span><span class="s2">"backbone_out"</span><span class="p">][</span><span class="s2">"sam2_backbone_out"</span><span class="p">]</span>
<span></span>    <span class="n">tracker_backbone_fpn</span> <span class="o">=</span> <span class="p">[</span>
<span></span>        <span class="n">sam_mask_decoder</span><span class="o">.</span><span class="n">conv_s0</span><span class="p">(</span><span class="n">feats</span><span class="p">[</span><span class="s2">"backbone_fpn"</span><span class="p">][</span><span class="mi">0</span><span class="p">]),</span>
<span></span>        <span class="n">sam_mask_decoder</span><span class="o">.</span><span class="n">conv_s1</span><span class="p">(</span><span class="n">feats</span><span class="p">[</span><span class="s2">"backbone_fpn"</span><span class="p">][</span><span class="mi">1</span><span class="p">]),</span>
<span></span>        <span class="n">feats</span><span class="p">[</span><span class="s2">"backbone_fpn"</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span>
<span></span>    <span class="p">]</span>
<span></span>    <span class="n">tracker_backbone_out</span> <span class="o">=</span> <span class="p">{</span>
<span></span>        <span class="s2">"vision_features"</span><span class="p">:</span> <span class="n">tracker_backbone_fpn</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
<span></span>        <span class="s2">"vision_pos_enc"</span><span class="p">:</span> <span class="n">feats</span><span class="p">[</span><span class="s2">"vision_pos_enc"</span><span class="p">],</span>
<span></span>        <span class="s2">"backbone_fpn"</span><span class="p">:</span> <span class="n">tracker_backbone_fpn</span><span class="p">,</span>
<span></span>    <span class="p">}</span>
<span></span>    <span class="c1"># cache the SAM2 backbone features for `frame_idx` in the tracker</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">backbone_out</span> <span class="o">=</span> <span class="n">tracker_backbone_out</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._det_track_one_frame"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._det_track_one_frame</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_det_track_one_frame</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">im</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">text_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">num_frames</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">reverse</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span></span>    <span class="n">geometric_prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span>
<span></span>    <span class="n">tracker_states_local</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
<span></span>    <span class="n">tracker_metadata_prev</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
<span></span>    <span class="n">allow_new_detections</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>This function handles one-step inference for the DenseTracking model in an SPMD manner. At a high-level, all</p><p>GPUs execute the same function calls as if it's done on a single GPU, while under the hood, some
function calls involve distributed computation based on sharded SAM2 states.</p><ul><li><code>input_batch</code> contains image and other inputs on the entire video; it should be identical across GPUs</li><li><code>tracker_states_local</code> holds the local masklet information in this GPU shard</li><li><code>tracker_metadata_prev</code> manages the metadata for SAM2 objects, such as which masklet is hold on which GPUs it contains both global and local masklet information</li></ul><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor</code></td><td></td><td><em>required</em></td></tr><tr><td><code>text_ids</code></td><td><code>torch.Tensor</code></td><td></td><td><em>required</em></td></tr><tr><td><code>frame_idx</code></td><td><code>int</code></td><td></td><td><em>required</em></td></tr><tr><td><code>num_frames</code></td><td><code>int</code></td><td></td><td><em>required</em></td></tr><tr><td><code>reverse</code></td><td><code>bool</code></td><td></td><td><em>required</em></td></tr><tr><td><code>geometric_prompt</code></td><td><code>Prompt</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_states_local</code></td><td><code>list[Any]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_metadata_prev</code></td><td><code>dict[str, Any]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>allow_new_detections</code></td><td><code>bool</code></td><td></td><td><code>True</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2777-L2882"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_det_track_one_frame</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">im</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">text_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">num_frames</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">reverse</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span></span>    <span class="n">geometric_prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span>
<span></span>    <span class="n">tracker_states_local</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
<span></span>    <span class="n">tracker_metadata_prev</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
<span></span>    <span class="n">allow_new_detections</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""This function handles one-step inference for the DenseTracking model in an SPMD manner. At a high-level, all</span>
<span></span><span class="sd">    GPUs execute the same function calls as if it's done on a single GPU, while under the hood, some</span>
<span></span><span class="sd">    function calls involve distributed computation based on sharded SAM2 states.</span>
<span></span>
<span></span><span class="sd">    - `input_batch` contains image and other inputs on the entire video; it should be identical across GPUs</span>
<span></span><span class="sd">    - `tracker_states_local` holds the local masklet information in this GPU shard</span>
<span></span><span class="sd">    - `tracker_metadata_prev` manages the metadata for SAM2 objects, such as which masklet is hold on which GPUs</span>
<span></span><span class="sd">      it contains both global and local masklet information</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="c1"># Step 1: run backbone and detector in a distributed manner -- this is done via Sam3ImageOnVideoMultiGPU,</span>
<span></span>    <span class="c1"># a MultiGPU model (assigned to `self.detector`) that shards frames in a round-robin manner.</span>
<span></span>    <span class="n">det_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_backbone_and_detection</span><span class="p">(</span>
<span></span>        <span class="n">im</span><span class="o">=</span><span class="n">im</span><span class="p">,</span>
<span></span>        <span class="n">text_ids</span><span class="o">=</span><span class="n">text_ids</span><span class="p">,</span>
<span></span>        <span class="n">geometric_prompt</span><span class="o">=</span><span class="n">geometric_prompt</span><span class="p">,</span>
<span></span>        <span class="n">allow_new_detections</span><span class="o">=</span><span class="n">allow_new_detections</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Step 2: each GPU propagates its local SAM2 states to get the SAM2 prediction masks.</span>
<span></span>    <span class="c1"># the returned `tracker_low_res_masks_global` contains the concatenated masklet predictions</span>
<span></span>    <span class="c1"># gathered from all GPUs (as if they are propagated on a single GPU). Note that this step only</span>
<span></span>    <span class="c1"># runs the SAM2 propagation step, but doesn't encode new memory for the predicted masks;</span>
<span></span>    <span class="c1"># we defer memory encoding to `run_tracker_update_execution_phase` after resolving all heuristics.</span>
<span></span>    <span class="k">if</span> <span class="n">tracker_metadata_prev</span> <span class="o">==</span> <span class="p">{}:</span>
<span></span>        <span class="c1"># initialize masklet metadata if it's uninitialized (empty dict)</span>
<span></span>        <span class="n">tracker_metadata_prev</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_initialize_metadata</span><span class="p">())</span>
<span></span>    <span class="n">tracker_low_res_masks_global</span><span class="p">,</span> <span class="n">tracker_obj_scores_global</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_tracker_propagation</span><span class="p">(</span>
<span></span>        <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span><span class="p">,</span>
<span></span>        <span class="n">tracker_states_local</span><span class="o">=</span><span class="n">tracker_states_local</span><span class="p">,</span>
<span></span>        <span class="n">tracker_metadata_prev</span><span class="o">=</span><span class="n">tracker_metadata_prev</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Step 3: based on detection outputs and the propagated SAM2 prediction masks, we make plans</span>
<span></span>    <span class="c1"># for SAM2 masklet updates (i.e. which objects to add and remove, how to load-balance them, etc).</span>
<span></span>    <span class="c1"># We also run SAM2 memory encoder globally in this step to resolve non-overlapping constraints.</span>
<span></span>    <span class="c1"># **This step should involve all the heuristics needed for any updates.** Most of the update</span>
<span></span>    <span class="c1"># planning will be done on the master rank (GPU 0) and the resulting plan `tracker_update_plan` is</span>
<span></span>    <span class="c1"># broadcasted to other GPUs (to be executed in a distributed manner). This step also generates the</span>
<span></span>    <span class="c1"># new masklet metadata `tracker_metadata_new` (based on its previous version `tracker_metadata_prev`).</span>
<span></span>    <span class="n">tracker_update_plan</span><span class="p">,</span> <span class="n">tracker_metadata_new</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_tracker_update_planning_phase</span><span class="p">(</span>
<span></span>        <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span><span class="p">,</span>
<span></span>        <span class="n">reverse</span><span class="o">=</span><span class="n">reverse</span><span class="p">,</span>
<span></span>        <span class="n">det_out</span><span class="o">=</span><span class="n">det_out</span><span class="p">,</span>
<span></span>        <span class="n">tracker_low_res_masks_global</span><span class="o">=</span><span class="n">tracker_low_res_masks_global</span><span class="p">,</span>
<span></span>        <span class="n">tracker_obj_scores_global</span><span class="o">=</span><span class="n">tracker_obj_scores_global</span><span class="p">,</span>
<span></span>        <span class="n">tracker_metadata_prev</span><span class="o">=</span><span class="n">tracker_metadata_prev</span><span class="p">,</span>
<span></span>        <span class="n">tracker_states_local</span><span class="o">=</span><span class="n">tracker_states_local</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Get reconditioning info from the update plan</span>
<span></span>    <span class="n">reconditioned_obj_ids</span> <span class="o">=</span> <span class="n">tracker_update_plan</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"reconditioned_obj_ids"</span><span class="p">,</span> <span class="nb">set</span><span class="p">())</span>
<span></span>
<span></span>    <span class="c1"># Step 4: based on `tracker_update_plan`, each GPU executes the update w.r.t. its local SAM2 inference states</span>
<span></span>    <span class="n">tracker_states_local_new</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_tracker_update_execution_phase</span><span class="p">(</span>
<span></span>        <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span><span class="p">,</span>
<span></span>        <span class="n">num_frames</span><span class="o">=</span><span class="n">num_frames</span><span class="p">,</span>
<span></span>        <span class="n">det_out</span><span class="o">=</span><span class="n">det_out</span><span class="p">,</span>
<span></span>        <span class="n">tracker_states_local</span><span class="o">=</span><span class="n">tracker_states_local</span><span class="p">,</span>
<span></span>        <span class="n">tracker_update_plan</span><span class="o">=</span><span class="n">tracker_update_plan</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Step 5: finally, build the outputs for this frame (it only needs to be done on GPU 0 since</span>
<span></span>    <span class="c1"># only GPU 0 will send outputs to the server).</span>
<span></span>    <span class="n">obj_id_to_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_outputs</span><span class="p">(</span>
<span></span>        <span class="n">det_out</span><span class="o">=</span><span class="n">det_out</span><span class="p">,</span>
<span></span>        <span class="n">tracker_low_res_masks_global</span><span class="o">=</span><span class="n">tracker_low_res_masks_global</span><span class="p">,</span>
<span></span>        <span class="n">tracker_metadata_prev</span><span class="o">=</span><span class="n">tracker_metadata_prev</span><span class="p">,</span>
<span></span>        <span class="n">tracker_update_plan</span><span class="o">=</span><span class="n">tracker_update_plan</span><span class="p">,</span>
<span></span>        <span class="n">reconditioned_obj_ids</span><span class="o">=</span><span class="n">reconditioned_obj_ids</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="n">obj_id_to_score</span> <span class="o">=</span> <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"obj_id_to_score"</span><span class="p">]</span>
<span></span>    <span class="n">obj_id_to_cls</span> <span class="o">=</span> <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"obj_id_to_cls"</span><span class="p">]</span>
<span></span>    <span class="c1"># a few statistics for the current frame as a part of the output</span>
<span></span>    <span class="n">frame_stats</span> <span class="o">=</span> <span class="p">{</span>
<span></span>        <span class="s2">"num_obj_tracked"</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"num_obj"</span><span class="p">]),</span>
<span></span>        <span class="s2">"num_obj_dropped"</span><span class="p">:</span> <span class="n">tracker_update_plan</span><span class="p">[</span><span class="s2">"num_obj_dropped_due_to_limit"</span><span class="p">],</span>
<span></span>    <span class="p">}</span>
<span></span>    <span class="c1"># add tracker scores to metadata, it should be fired for frames except the first frame</span>
<span></span>    <span class="k">if</span> <span class="n">tracker_obj_scores_global</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="c1"># Convert tracker_obj_scores_global to sigmoid scores before updating</span>
<span></span>        <span class="n">tracker_obj_scores_global</span> <span class="o">=</span> <span class="n">tracker_obj_scores_global</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span></span>        <span class="n">tracker_obj_ids</span> <span class="o">=</span> <span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span>
<span></span>        <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"obj_id_to_tracker_score_frame_wise"</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
<span></span>            <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">tracker_obj_ids</span><span class="p">,</span> <span class="n">tracker_obj_scores_global</span><span class="p">))</span>
<span></span>        <span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="p">(</span>
<span></span>        <span class="n">obj_id_to_mask</span><span class="p">,</span>  <span class="c1"># a dict: obj_id --&gt; output mask</span>
<span></span>        <span class="n">obj_id_to_score</span><span class="p">,</span>  <span class="c1"># a dict: obj_id --&gt; output score (prob)</span>
<span></span>        <span class="n">obj_id_to_cls</span><span class="p">,</span>  <span class="c1"># a dict: obj_id --&gt; output cls (int)</span>
<span></span>        <span class="n">tracker_states_local_new</span><span class="p">,</span>
<span></span>        <span class="n">tracker_metadata_new</span><span class="p">,</span>
<span></span>        <span class="n">frame_stats</span><span class="p">,</span>
<span></span>        <span class="n">tracker_obj_scores_global</span><span class="p">,</span>  <span class="c1"># a dict: obj_id --&gt; tracker frame-level scores</span>
<span></span>    <span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._drop_new_det_with_obj_limit"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._drop_new_det_with_obj_limit</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_drop_new_det_with_obj_limit</span><span class="p">(</span><span class="n">new_det_fa_inds</span><span class="p">,</span> <span class="n">det_scores_np</span><span class="p">,</span> <span class="n">num_to_keep</span><span class="p">)</span>
</code></pre></div><p>Drop a few new detections based on the maximum number of objects. We drop new objects based on their</p><p>detection scores, keeping the high-scoring ones and dropping the low-scoring ones.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>new_det_fa_inds</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>det_scores_np</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>num_to_keep</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L3927-L3940"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@staticmethod</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">_drop_new_det_with_obj_limit</span><span class="p">(</span><span class="n">new_det_fa_inds</span><span class="p">,</span> <span class="n">det_scores_np</span><span class="p">,</span> <span class="n">num_to_keep</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Drop a few new detections based on the maximum number of objects. We drop new objects based on their</span>
<span></span><span class="sd">    detection scores, keeping the high-scoring ones and dropping the low-scoring ones.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">num_to_keep</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_det_fa_inds</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">num_to_keep</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>  <span class="c1"># keep none</span>
<span></span>    <span class="k">if</span> <span class="n">num_to_keep</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_det_fa_inds</span><span class="p">):</span>
<span></span>        <span class="k">return</span> <span class="n">new_det_fa_inds</span>  <span class="c1"># keep all</span>
<span></span>
<span></span>    <span class="c1"># keep the top-scoring detections</span>
<span></span>    <span class="n">score_order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">det_scores_np</span><span class="p">[</span><span class="n">new_det_fa_inds</span><span class="p">])[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span></span>    <span class="n">new_det_fa_inds</span> <span class="o">=</span> <span class="n">new_det_fa_inds</span><span class="p">[</span><span class="n">score_order</span><span class="p">[:</span><span class="n">num_to_keep</span><span class="p">]]</span>
<span></span>    <span class="k">return</span> <span class="n">new_det_fa_inds</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._extract_detection_outputs"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._extract_detection_outputs</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_extract_detection_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sam3_image_out</span><span class="p">,</span> <span class="n">allow_new_detections</span><span class="p">)</span>
</code></pre></div><p>Extract and filter detection outputs.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>sam3_image_out</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>allow_new_detections</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2910-L2931"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_extract_detection_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sam3_image_out</span><span class="p">,</span> <span class="n">allow_new_detections</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Extract and filter detection outputs."""</span>
<span></span>    <span class="n">pred_probs</span> <span class="o">=</span> <span class="n">sam3_image_out</span><span class="p">[</span><span class="s2">"pred_logits"</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
<span></span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">allow_new_detections</span><span class="p">:</span>
<span></span>        <span class="n">pred_probs</span> <span class="o">=</span> <span class="n">pred_probs</span> <span class="o">-</span> <span class="mf">1e8</span>
<span></span>
<span></span>    <span class="n">pred_cls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span></span>        <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">pred_probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span>
<span></span>        <span class="n">dtype</span><span class="o">=</span><span class="n">pred_probs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span></span>        <span class="n">device</span><span class="o">=</span><span class="n">pred_probs</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>    <span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">pred_probs</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">pred_boxes_xyxy</span> <span class="o">=</span> <span class="n">sam3_image_out</span><span class="p">[</span><span class="s2">"pred_boxes_xyxy"</span><span class="p">]</span>
<span></span>    <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">sam3_image_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span>
<span></span>
<span></span>    <span class="n">keep</span> <span class="o">=</span> <span class="n">pred_probs</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_threshold_detection</span>
<span></span>    <span class="k">return</span> <span class="p">{</span>
<span></span>        <span class="s2">"bbox"</span><span class="p">:</span> <span class="n">pred_boxes_xyxy</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span>
<span></span>        <span class="s2">"mask"</span><span class="p">:</span> <span class="n">pred_masks</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span>
<span></span>        <span class="s2">"scores"</span><span class="p">:</span> <span class="n">pred_probs</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span>
<span></span>        <span class="s2">"cls"</span><span class="p">:</span> <span class="n">pred_cls</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span>
<span></span>    <span class="p">}</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._initialize_metadata"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._initialize_metadata</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_initialize_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div><p>Initialize metadata for the masklets.</p><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L3831-L3865"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_initialize_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Initialize metadata for the masklets."""</span>
<span></span>    <span class="n">tracker_metadata</span> <span class="o">=</span> <span class="p">{</span>
<span></span>        <span class="s2">"obj_ids"</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
<span></span>        <span class="s2">"num_obj"</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
<span></span>        <span class="s2">"max_obj_id"</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<span></span>        <span class="s2">"obj_id_to_score"</span><span class="p">:</span> <span class="p">{},</span>
<span></span>        <span class="s2">"obj_id_to_cls"</span><span class="p">:</span> <span class="p">{},</span>
<span></span>        <span class="s2">"obj_id_to_tracker_score_frame_wise"</span><span class="p">:</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">),</span>
<span></span>        <span class="s2">"obj_id_to_last_occluded"</span><span class="p">:</span> <span class="p">{},</span>
<span></span>    <span class="p">}</span>
<span></span>    <span class="c1"># "metadata" contains metadata that is only stored on (and accessible to) GPU 0</span>
<span></span>    <span class="c1"># - obj_first_frame_idx: obj_id --&gt; first frame index where the object was detected</span>
<span></span>    <span class="c1"># - unmatched_frame_inds: obj_id --&gt; [mismatched frame indices]</span>
<span></span>    <span class="c1"># - overlap_pair_to_frame_inds: (first_appear_obj_id, obj_id) --&gt; [overlap frame indices]</span>
<span></span>    <span class="c1"># - removed_obj_ids: object IDs that are suppressed via hot-start</span>
<span></span>    <span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span>
<span></span>        <span class="s2">"obj_first_frame_idx"</span><span class="p">:</span> <span class="p">{},</span>
<span></span>        <span class="s2">"unmatched_frame_inds"</span><span class="p">:</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">),</span>
<span></span>        <span class="s2">"trk_keep_alive"</span><span class="p">:</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span>  <span class="c1"># This is used only for object suppression not for removal</span>
<span></span>        <span class="s2">"overlap_pair_to_frame_inds"</span><span class="p">:</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">),</span>
<span></span>        <span class="s2">"removed_obj_ids"</span><span class="p">:</span> <span class="nb">set</span><span class="p">(),</span>
<span></span>    <span class="p">}</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">masklet_confirmation_enable</span><span class="p">:</span>
<span></span>        <span class="c1"># all the following are np.ndarray with the same shape as `obj_ids_all_gpu`</span>
<span></span>        <span class="n">metadata</span><span class="p">[</span><span class="s2">"masklet_confirmation"</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
<span></span>            <span class="c1"># "status" is the confirmation status of each masklet</span>
<span></span>            <span class="s2">"status"</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
<span></span>            <span class="c1"># "consecutive_det_num" is the number of consecutive frames where the masklet is</span>
<span></span>            <span class="c1"># detected by the detector (with a matched detection)</span>
<span></span>            <span class="s2">"consecutive_det_num"</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
<span></span>        <span class="p">}</span>
<span></span>    <span class="n">tracker_metadata</span><span class="p">[</span><span class="s2">"metadata"</span><span class="p">]</span> <span class="o">=</span> <span class="n">metadata</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">tracker_metadata</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._process_hotstart"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._process_hotstart</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_process_hotstart</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">reverse</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span></span>    <span class="n">det_to_matched_trk_obj_ids</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
<span></span>    <span class="n">new_det_obj_ids</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">empty_trk_obj_ids</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">unmatched_trk_obj_ids</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">metadata</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Handle hotstart heuristics to remove unmatched or duplicated objects.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td><code>int</code></td><td></td><td><em>required</em></td></tr><tr><td><code>reverse</code></td><td><code>bool</code></td><td></td><td><em>required</em></td></tr><tr><td><code>det_to_matched_trk_obj_ids</code></td><td><code>dict[int, np.ndarray]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>new_det_obj_ids</code></td><td><code>np.ndarray</code></td><td></td><td><em>required</em></td></tr><tr><td><code>empty_trk_obj_ids</code></td><td><code>np.ndarray</code></td><td></td><td><em>required</em></td></tr><tr><td><code>unmatched_trk_obj_ids</code></td><td><code>np.ndarray</code></td><td></td><td><em>required</em></td></tr><tr><td><code>metadata</code></td><td><code>dict[str, Any]</code></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L3600-L3709"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_process_hotstart</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">reverse</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span></span>    <span class="n">det_to_matched_trk_obj_ids</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
<span></span>    <span class="n">new_det_obj_ids</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">empty_trk_obj_ids</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">unmatched_trk_obj_ids</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">metadata</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Handle hotstart heuristics to remove unmatched or duplicated objects."""</span>
<span></span>    <span class="c1"># obj_id --&gt; first frame index where the object was detected</span>
<span></span>    <span class="n">obj_first_frame_idx</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">"obj_first_frame_idx"</span><span class="p">]</span>
<span></span>    <span class="c1"># obj_id --&gt; [mismatched frame indices]</span>
<span></span>    <span class="n">unmatched_frame_inds</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">"unmatched_frame_inds"</span><span class="p">]</span>
<span></span>    <span class="n">trk_keep_alive</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">"trk_keep_alive"</span><span class="p">]</span>
<span></span>    <span class="c1"># (first_appear_obj_id, obj_id) --&gt; [overlap frame indices]</span>
<span></span>    <span class="n">overlap_pair_to_frame_inds</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">"overlap_pair_to_frame_inds"</span><span class="p">]</span>
<span></span>    <span class="c1"># removed_obj_ids: object IDs that are suppressed via hot-start</span>
<span></span>    <span class="n">removed_obj_ids</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">"removed_obj_ids"</span><span class="p">]</span>
<span></span>
<span></span>    <span class="n">obj_ids_newly_removed</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>  <span class="c1"># object IDs to be newly removed on this frame</span>
<span></span>    <span class="n">hotstart_diff</span> <span class="o">=</span> <span class="n">frame_idx</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">hotstart_delay</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">reverse</span> <span class="k">else</span> <span class="n">frame_idx</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hotstart_delay</span>
<span></span>
<span></span>    <span class="c1"># Step 1: log the frame index where each object ID first appears</span>
<span></span>    <span class="k">for</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">new_det_obj_ids</span><span class="p">:</span>
<span></span>        <span class="k">if</span> <span class="n">obj_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">obj_first_frame_idx</span><span class="p">:</span>
<span></span>            <span class="n">obj_first_frame_idx</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">frame_idx</span>
<span></span>        <span class="k">assert</span> <span class="n">obj_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">trk_keep_alive</span>
<span></span>        <span class="n">trk_keep_alive</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_trk_keep_alive</span>
<span></span>
<span></span>    <span class="n">matched_trks</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span></span>    <span class="c1"># We use the det--&gt;tracks list to check for matched objects. Otherwise, we need to compute areas to decide whether they're occluded</span>
<span></span>    <span class="k">for</span> <span class="n">matched_trks_per_det</span> <span class="ow">in</span> <span class="n">det_to_matched_trk_obj_ids</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>        <span class="n">matched_trks</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">matched_trks_per_det</span><span class="p">)</span>
<span></span>    <span class="k">for</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">matched_trks</span><span class="p">:</span>
<span></span>        <span class="c1"># NOTE: To minimize number of configurable params, we use the hotstart_unmatch_thresh to set the max value of trk_keep_alive</span>
<span></span>        <span class="n">trk_keep_alive</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_trk_keep_alive</span><span class="p">,</span> <span class="n">trk_keep_alive</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="k">for</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">unmatched_trk_obj_ids</span><span class="p">:</span>
<span></span>        <span class="n">unmatched_frame_inds</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>        <span class="c1"># NOTE: To minimize number of configurable params, we use the hotstart_unmatch_thresh to set the min value of trk_keep_alive</span>
<span></span>        <span class="c1"># The max keep alive is 2x the min, means the model prefers to keep the prediction rather than suppress it if it was matched long enough.</span>
<span></span>        <span class="n">trk_keep_alive</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_trk_keep_alive</span><span class="p">,</span> <span class="n">trk_keep_alive</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">decrease_trk_keep_alive_for_empty_masklets</span><span class="p">:</span>
<span></span>        <span class="k">for</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">empty_trk_obj_ids</span><span class="p">:</span>
<span></span>            <span class="c1"># NOTE: To minimize number of configurable params, we use the hotstart_unmatch_thresh to set the min value of trk_keep_alive</span>
<span></span>            <span class="n">trk_keep_alive</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_trk_keep_alive</span><span class="p">,</span> <span class="n">trk_keep_alive</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Step 2: removed tracks that has not matched with detections for `hotstart_unmatch_thresh` frames with hotstart period</span>
<span></span>    <span class="c1"># a) add unmatched frame indices for each existing object ID</span>
<span></span>    <span class="c1"># note that `unmatched_trk_obj_ids` contains those frames where the SAM2 output mask</span>
<span></span>    <span class="c1"># doesn't match any detection; it excludes those frames where SAM2 gives an empty mask</span>
<span></span>    <span class="c1"># b) remove a masklet if it first appears after `hotstart_diff` and is unmatched for more</span>
<span></span>    <span class="c1"># than `self.hotstart_unmatch_thresh` frames</span>
<span></span>    <span class="k">for</span> <span class="n">obj_id</span><span class="p">,</span> <span class="n">frame_indices</span> <span class="ow">in</span> <span class="n">unmatched_frame_inds</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<span></span>        <span class="k">if</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">removed_obj_ids</span> <span class="ow">or</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">obj_ids_newly_removed</span><span class="p">:</span>
<span></span>            <span class="k">continue</span>  <span class="c1"># skip if the object is already removed</span>
<span></span>        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">frame_indices</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hotstart_unmatch_thresh</span><span class="p">:</span>
<span></span>            <span class="n">is_within_hotstart</span> <span class="o">=</span> <span class="p">(</span><span class="n">obj_first_frame_idx</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">hotstart_diff</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">reverse</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
<span></span>                <span class="n">obj_first_frame_idx</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">hotstart_diff</span> <span class="ow">and</span> <span class="n">reverse</span>
<span></span>            <span class="p">)</span>
<span></span>            <span class="k">if</span> <span class="n">is_within_hotstart</span><span class="p">:</span>
<span></span>                <span class="n">obj_ids_newly_removed</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">obj_id</span><span class="p">)</span>
<span></span>                <span class="n">LOGGER</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
<span></span>                    <span class="sa">f</span><span class="s2">"Removing object </span><span class="si">{</span><span class="n">obj_id</span><span class="si">}</span><span class="s2"> at frame </span><span class="si">{</span><span class="n">frame_idx</span><span class="si">}</span><span class="s2"> "</span>
<span></span>                    <span class="sa">f</span><span class="s2">"since it is unmatched for frames: </span><span class="si">{</span><span class="n">frame_indices</span><span class="si">}</span><span class="s2">"</span>
<span></span>                <span class="p">)</span>
<span></span>        <span class="k">if</span> <span class="p">(</span>
<span></span>            <span class="n">trk_keep_alive</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">0</span>  <span class="c1"># Object has not been matched for too long</span>
<span></span>            <span class="ow">and</span> <span class="n">obj_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">removed_obj_ids</span>
<span></span>            <span class="ow">and</span> <span class="n">obj_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">obj_ids_newly_removed</span>
<span></span>        <span class="p">):</span>
<span></span>            <span class="n">LOGGER</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Removing object </span><span class="si">{</span><span class="n">obj_id</span><span class="si">}</span><span class="s2"> at frame </span><span class="si">{</span><span class="n">frame_idx</span><span class="si">}</span><span class="s2">, due to being unmatched"</span><span class="p">)</span>
<span></span>            <span class="c1"># directly removed the object instead of suppressing it</span>
<span></span>            <span class="n">obj_ids_newly_removed</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">obj_id</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Step 3: removed tracks that overlaps with another track for `hotstart_dup_thresh` frames</span>
<span></span>    <span class="c1"># a) find overlaps tracks -- we consider overlap if they match to the same detection</span>
<span></span>    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">matched_trk_obj_ids</span> <span class="ow">in</span> <span class="n">det_to_matched_trk_obj_ids</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<span></span>        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">matched_trk_obj_ids</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
<span></span>            <span class="k">continue</span>  <span class="c1"># only count detections that are matched to multiple (&gt;=2) masklets</span>
<span></span>        <span class="c1"># if there are multiple matched track ids, we need to find the one that appeared first;</span>
<span></span>        <span class="c1"># these later appearing ids may be removed since they may be considered as duplicates</span>
<span></span>        <span class="n">first_appear_obj_id</span> <span class="o">=</span> <span class="p">(</span>
<span></span>            <span class="nb">min</span><span class="p">(</span><span class="n">matched_trk_obj_ids</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">obj_first_frame_idx</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
<span></span>            <span class="k">if</span> <span class="ow">not</span> <span class="n">reverse</span>
<span></span>            <span class="k">else</span> <span class="nb">max</span><span class="p">(</span><span class="n">matched_trk_obj_ids</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">obj_first_frame_idx</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="k">for</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">matched_trk_obj_ids</span><span class="p">:</span>
<span></span>            <span class="k">if</span> <span class="n">obj_id</span> <span class="o">!=</span> <span class="n">first_appear_obj_id</span><span class="p">:</span>
<span></span>                <span class="n">key</span> <span class="o">=</span> <span class="p">(</span><span class="n">first_appear_obj_id</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">)</span>
<span></span>                <span class="n">overlap_pair_to_frame_inds</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># b) remove a masklet if it first appears after `hotstart_diff` and it overlaps with another</span>
<span></span>    <span class="c1"># masklet (that appears earlier) for more than `self.hotstart_dup_thresh` frames</span>
<span></span>    <span class="k">for</span> <span class="p">(</span><span class="n">first_obj_id</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">),</span> <span class="n">frame_indices</span> <span class="ow">in</span> <span class="n">overlap_pair_to_frame_inds</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<span></span>        <span class="k">if</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">removed_obj_ids</span> <span class="ow">or</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">obj_ids_newly_removed</span><span class="p">:</span>
<span></span>            <span class="k">continue</span>  <span class="c1"># skip if the object is already removed</span>
<span></span>        <span class="k">if</span> <span class="p">(</span><span class="n">obj_first_frame_idx</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">hotstart_diff</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">reverse</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
<span></span>            <span class="n">obj_first_frame_idx</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">hotstart_diff</span> <span class="ow">and</span> <span class="n">reverse</span>
<span></span>        <span class="p">):</span>
<span></span>            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">frame_indices</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hotstart_dup_thresh</span><span class="p">:</span>
<span></span>                <span class="n">obj_ids_newly_removed</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">obj_id</span><span class="p">)</span>
<span></span>                <span class="n">LOGGER</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
<span></span>                    <span class="sa">f</span><span class="s2">"Removing object </span><span class="si">{</span><span class="n">obj_id</span><span class="si">}</span><span class="s2"> at frame </span><span class="si">{</span><span class="n">frame_idx</span><span class="si">}</span><span class="s2"> "</span>
<span></span>                    <span class="sa">f</span><span class="s2">"since it overlaps with another track </span><span class="si">{</span><span class="n">first_obj_id</span><span class="si">}</span><span class="s2"> at frames: </span><span class="si">{</span><span class="n">frame_indices</span><span class="si">}</span><span class="s2">"</span>
<span></span>                <span class="p">)</span>
<span></span>
<span></span>    <span class="n">removed_obj_ids</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">obj_ids_newly_removed</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">obj_ids_newly_removed</span><span class="p">,</span> <span class="n">metadata</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._propogate_tracker_one_frame_local_gpu"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._propogate_tracker_one_frame_local_gpu</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_propogate_tracker_one_frame_local_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_states</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span>
</code></pre></div><p>Inference_states: list of inference states, each state corresponds to a different set of objects.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>inference_states</code></td><td><code>list[Any]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>frame_idx</code></td><td><code>int</code></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L3439-L3465"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_propogate_tracker_one_frame_local_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_states</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Inference_states: list of inference states, each state corresponds to a different set of objects."""</span>
<span></span>    <span class="n">obj_ids_local</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>    <span class="n">low_res_masks_list</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>    <span class="n">obj_scores_list</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>    <span class="k">for</span> <span class="n">inference_state</span> <span class="ow">in</span> <span class="n">inference_states</span><span class="p">:</span>
<span></span>        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>            <span class="k">continue</span>  <span class="c1"># skip propagation on empty inference states</span>
<span></span>
<span></span>        <span class="n">out_obj_ids</span><span class="p">,</span> <span class="n">out_low_res_masks</span><span class="p">,</span> <span class="n">out_obj_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">propagate_in_video</span><span class="p">(</span>
<span></span>            <span class="n">inference_state</span><span class="p">,</span> <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out_obj_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
<span></span>        <span class="n">obj_ids_local</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">out_obj_ids</span><span class="p">)</span>
<span></span>        <span class="n">low_res_masks_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_low_res_masks</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span></span>        <span class="n">obj_scores_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_obj_scores</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span></span>
<span></span>    <span class="c1"># concatenate the output masklets from all local inference states</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">low_res_masks_list</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="n">low_res_masks_local</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">low_res_masks_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span></span>        <span class="n">obj_scores_local</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">obj_scores_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span></span>        <span class="n">low_res_masks_local</span> <span class="o">=</span> <span class="n">low_res_masks_local</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="n">low_res_masks_local</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_bb_feat_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="n">obj_scores_local</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">obj_ids_local</span><span class="p">,</span> <span class="n">low_res_masks_local</span><span class="p">,</span> <span class="n">obj_scores_local</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._recondition_masklets"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._recondition_masklets</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_recondition_masklets</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">,</span>
<span></span>    <span class="n">det_out</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<span></span>    <span class="n">trk_id_to_max_iou_high_conf_det</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
<span></span>    <span class="n">tracker_states_local</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
<span></span>    <span class="n">tracker_metadata</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
<span></span>    <span class="n">tracker_obj_scores_global</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Recondition masklets based on new high-confidence detections.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>det_out</code></td><td><code>dict[str, torch.Tensor]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>trk_id_to_max_iou_high_conf_det</code></td><td><code>list[int]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_states_local</code></td><td><code>list[Any]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_metadata</code></td><td><code>dict[str, np.ndarray]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_obj_scores_global</code></td><td><code>torch.Tensor</code></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2972-L3012"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_recondition_masklets</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">,</span>
<span></span>    <span class="n">det_out</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<span></span>    <span class="n">trk_id_to_max_iou_high_conf_det</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
<span></span>    <span class="n">tracker_states_local</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
<span></span>    <span class="n">tracker_metadata</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
<span></span>    <span class="n">tracker_obj_scores_global</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Recondition masklets based on new high-confidence detections."""</span>
<span></span>    <span class="c1"># Recondition the masklets based on the new detections</span>
<span></span>    <span class="k">for</span> <span class="n">trk_obj_id</span><span class="p">,</span> <span class="n">det_idx</span> <span class="ow">in</span> <span class="n">trk_id_to_max_iou_high_conf_det</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<span></span>        <span class="n">new_mask</span> <span class="o">=</span> <span class="n">det_out</span><span class="p">[</span><span class="s2">"mask"</span><span class="p">][</span><span class="n">det_idx</span> <span class="p">:</span> <span class="n">det_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
<span></span>        <span class="n">new_mask_binary</span> <span class="o">=</span> <span class="p">(</span>
<span></span>            <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">new_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">interpol_size</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="n">HIGH_CONF_THRESH</span> <span class="o">=</span> <span class="mf">0.8</span>
<span></span>        <span class="n">reconditioned_states_idx</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span></span>        <span class="n">obj_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tracker_metadata</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span> <span class="o">==</span> <span class="n">trk_obj_id</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span></span>        <span class="n">obj_score</span> <span class="o">=</span> <span class="n">tracker_obj_scores_global</span><span class="p">[</span><span class="n">obj_idx</span><span class="p">]</span>
<span></span>        <span class="k">for</span> <span class="n">state_idx</span><span class="p">,</span> <span class="n">inference_state</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tracker_states_local</span><span class="p">):</span>
<span></span>            <span class="k">if</span> <span class="p">(</span>
<span></span>                <span class="n">trk_obj_id</span> <span class="ow">in</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span>
<span></span>                <span class="c1"># NOTE: Goal of this condition is to avoid reconditioning masks that are occluded/low qualiy.</span>
<span></span>                <span class="c1"># Unfortunately, these can get reconditioned anyway due to batching. We should consider removing these heuristics.</span>
<span></span>                <span class="ow">and</span> <span class="n">obj_score</span> <span class="o">&gt;</span> <span class="n">HIGH_CONF_THRESH</span>
<span></span>            <span class="p">):</span>
<span></span>                <span class="n">LOGGER</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
<span></span>                    <span class="sa">f</span><span class="s2">"Adding new mask for track </span><span class="si">{</span><span class="n">trk_obj_id</span><span class="si">}</span><span class="s2"> at frame </span><span class="si">{</span><span class="n">frame_idx</span><span class="si">}</span><span class="s2">. Objects </span><span class="si">{</span><span class="n">inference_state</span><span class="p">[</span><span class="s1">'obj_ids'</span><span class="p">]</span><span class="si">}</span><span class="s2"> are all reconditioned."</span>
<span></span>                <span class="p">)</span>
<span></span>                <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">add_new_prompts</span><span class="p">(</span>
<span></span>                    <span class="n">inference_state</span><span class="o">=</span><span class="n">inference_state</span><span class="p">,</span>
<span></span>                    <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span><span class="p">,</span>
<span></span>                    <span class="n">obj_id</span><span class="o">=</span><span class="n">trk_obj_id</span><span class="p">,</span>
<span></span>                    <span class="n">masks</span><span class="o">=</span><span class="n">new_mask_binary</span><span class="p">,</span>
<span></span>                <span class="p">)</span>
<span></span>                <span class="n">reconditioned_states_idx</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">state_idx</span><span class="p">)</span>
<span></span>
<span></span>        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">reconditioned_states_idx</span><span class="p">:</span>
<span></span>            <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">propagate_in_video_preflight</span><span class="p">(</span><span class="n">tracker_states_local</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
<span></span>    <span class="k">return</span> <span class="n">tracker_states_local</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._run_single_frame_inference"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._run_single_frame_inference</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_run_single_frame_inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">reverse</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">inference_state</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Perform inference on a single frame and get its inference results.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>reverse</code></td><td></td><td></td><td><code>False</code></td></tr><tr><td><code>inference_state</code></td><td></td><td></td><td><code>None</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2667-L2719"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_run_single_frame_inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inference_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform inference on a single frame and get its inference results."""</span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="n">inference_state</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span>
<span></span>    <span class="c1"># prepare inputs</span>
<span></span>    <span class="n">tracker_states_local</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"tracker_inference_states"</span><span class="p">]</span>
<span></span>    <span class="n">has_text_prompt</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"text_prompt"</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span></span>    <span class="n">has_geometric_prompt</span> <span class="o">=</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"per_frame_geometric_prompt"</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span></span>    <span class="c1"># run inference for the current frame</span>
<span></span>    <span class="p">(</span>
<span></span>        <span class="n">obj_id_to_mask</span><span class="p">,</span>
<span></span>        <span class="n">obj_id_to_score</span><span class="p">,</span>
<span></span>        <span class="n">obj_id_to_cls</span><span class="p">,</span>
<span></span>        <span class="n">tracker_states_local_new</span><span class="p">,</span>
<span></span>        <span class="n">tracker_metadata_new</span><span class="p">,</span>
<span></span>        <span class="n">frame_stats</span><span class="p">,</span>
<span></span>        <span class="n">_</span><span class="p">,</span>
<span></span>    <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_det_track_one_frame</span><span class="p">(</span>
<span></span>        <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span><span class="p">,</span>
<span></span>        <span class="n">num_frames</span><span class="o">=</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"num_frames"</span><span class="p">],</span>
<span></span>        <span class="n">reverse</span><span class="o">=</span><span class="n">reverse</span><span class="p">,</span>
<span></span>        <span class="n">im</span><span class="o">=</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"im"</span><span class="p">],</span>
<span></span>        <span class="n">text_ids</span><span class="o">=</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"text_ids"</span><span class="p">],</span>
<span></span>        <span class="n">geometric_prompt</span><span class="o">=</span><span class="p">(</span>
<span></span>            <span class="bp">self</span><span class="o">.</span><span class="n">_get_dummy_prompt</span><span class="p">(</span><span class="n">num_prompts</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"text_ids"</span><span class="p">]))</span>
<span></span>            <span class="k">if</span> <span class="ow">not</span> <span class="n">has_geometric_prompt</span>
<span></span>            <span class="k">else</span> <span class="n">inference_state</span><span class="p">[</span><span class="s2">"per_frame_geometric_prompt"</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span>
<span></span>        <span class="p">),</span>
<span></span>        <span class="n">tracker_states_local</span><span class="o">=</span><span class="n">tracker_states_local</span><span class="p">,</span>
<span></span>        <span class="n">tracker_metadata_prev</span><span class="o">=</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"tracker_metadata"</span><span class="p">],</span>
<span></span>        <span class="n">allow_new_detections</span><span class="o">=</span><span class="n">has_text_prompt</span> <span class="ow">or</span> <span class="n">has_geometric_prompt</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="c1"># update inference state</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"tracker_inference_states"</span><span class="p">]</span> <span class="o">=</span> <span class="n">tracker_states_local_new</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"tracker_metadata"</span><span class="p">]</span> <span class="o">=</span> <span class="n">tracker_metadata_new</span>
<span></span>
<span></span>    <span class="n">out</span> <span class="o">=</span> <span class="p">{</span>
<span></span>        <span class="s2">"obj_id_to_mask"</span><span class="p">:</span> <span class="n">obj_id_to_mask</span><span class="p">,</span>
<span></span>        <span class="s2">"obj_id_to_score"</span><span class="p">:</span> <span class="n">obj_id_to_score</span><span class="p">,</span>  <span class="c1"># first frame detection score</span>
<span></span>        <span class="s2">"obj_id_to_cls"</span><span class="p">:</span> <span class="n">obj_id_to_cls</span><span class="p">,</span>  <span class="c1"># first frame detection score</span>
<span></span>        <span class="s2">"obj_id_to_tracker_score"</span><span class="p">:</span> <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"obj_id_to_tracker_score_frame_wise"</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">],</span>
<span></span>    <span class="p">}</span>
<span></span>    <span class="c1"># removed_obj_ids is only needed on rank 0 to handle hotstart delay buffer</span>
<span></span>    <span class="n">metadata</span> <span class="o">=</span> <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"metadata"</span><span class="p">]</span>
<span></span>    <span class="n">removed_obj_ids</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">"removed_obj_ids"</span><span class="p">]</span>
<span></span>    <span class="n">out</span><span class="p">[</span><span class="s2">"removed_obj_ids"</span><span class="p">]</span> <span class="o">=</span> <span class="n">removed_obj_ids</span>
<span></span>    <span class="n">out</span><span class="p">[</span><span class="s2">"frame_stats"</span><span class="p">]</span> <span class="o">=</span> <span class="n">frame_stats</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">masklet_confirmation_enable</span><span class="p">:</span>
<span></span>        <span class="n">status</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">"masklet_confirmation"</span><span class="p">][</span><span class="s2">"status"</span><span class="p">]</span>
<span></span>        <span class="n">is_unconfirmed</span> <span class="o">=</span> <span class="n">status</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">UNCONFIRMED</span>
<span></span>        <span class="n">out</span><span class="p">[</span><span class="s2">"unconfirmed_obj_ids"</span><span class="p">]</span> <span class="o">=</span> <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"obj_ids_all_gpu"</span><span class="p">][</span><span class="n">is_unconfirmed</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="n">out</span><span class="p">[</span><span class="s2">"unconfirmed_obj_ids"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._suppress_detections_close_to_boundary"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._suppress_detections_close_to_boundary</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_suppress_detections_close_to_boundary</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span> <span class="n">margin</span> <span class="o">=</span> <span class="mf">0.025</span><span class="p">)</span>
</code></pre></div><p>Suppress detections too close to image edges (for normalized boxes).</p><p>boxes: (N, 4) in xyxy format, normalized [0,1] margin: fraction of image</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>boxes</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>margin</code></td><td></td><td></td><td><code>0.025</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2885-L2896"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@staticmethod</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">_suppress_detections_close_to_boundary</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">0.025</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Suppress detections too close to image edges (for normalized boxes).</span>
<span></span>
<span></span><span class="sd">    boxes: (N, 4) in xyxy format, normalized [0,1]</span>
<span></span><span class="sd">    margin: fraction of image</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">boxes</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="n">x_c</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_min</span> <span class="o">+</span> <span class="n">x_max</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span></span>    <span class="n">y_c</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_min</span> <span class="o">+</span> <span class="n">y_max</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span></span>    <span class="n">keep</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_c</span> <span class="o">&gt;</span> <span class="n">margin</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x_c</span> <span class="o">&lt;</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">margin</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y_c</span> <span class="o">&gt;</span> <span class="n">margin</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y_c</span> <span class="o">&lt;</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">margin</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">keep</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._suppress_overlapping_based_on_recent_occlusion"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._suppress_overlapping_based_on_recent_occlusion</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_suppress_overlapping_based_on_recent_occlusion</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">tracker_low_res_masks_global</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">tracker_metadata_prev</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
<span></span>    <span class="n">tracker_metadata_new</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
<span></span>    <span class="n">obj_ids_newly_removed</span><span class="p">:</span> <span class="nb">set</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
<span></span>    <span class="n">reverse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Suppress overlapping masks based on the most recent occlusion information. If an object is removed by</p><p>hotstart, we always suppress it if it overlaps with any other object.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td><code>int</code></td><td>The current frame index.</td><td><em>required</em></td></tr><tr><td><code>tracker_low_res_masks_global</code></td><td><code>torch.Tensor</code></td><td>The low-resolution masks for the current frame.</td><td><em>required</em></td></tr><tr><td><code>tracker_metadata_prev</code></td><td><code>dict[str, Any]</code></td><td>The metadata from the previous frame.</td><td><em>required</em></td></tr><tr><td><code>tracker_metadata_new</code></td><td><code>dict[str, Any]</code></td><td>The metadata for the current frame.</td><td><em>required</em></td></tr><tr><td><code>obj_ids_newly_removed</code></td><td><code>set[int]</code></td><td>The object IDs that have been removed.</td><td><em>required</em></td></tr><tr><td><code>reverse</code></td><td><code>bool</code></td><td>Whether the tracking is in reverse order.</td><td><code>False</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>torch.Tensor</code></td><td>The updated low-resolution masks with some objects suppressed.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L3226-L3294"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_suppress_overlapping_based_on_recent_occlusion</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">tracker_low_res_masks_global</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">tracker_metadata_prev</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
<span></span>    <span class="n">tracker_metadata_new</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
<span></span>    <span class="n">obj_ids_newly_removed</span><span class="p">:</span> <span class="nb">set</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
<span></span>    <span class="n">reverse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Suppress overlapping masks based on the most recent occlusion information. If an object is removed by</span>
<span></span><span class="sd">    hotstart, we always suppress it if it overlaps with any other object.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        frame_idx (int): The current frame index.</span>
<span></span><span class="sd">        tracker_low_res_masks_global (torch.Tensor): The low-resolution masks for the current frame.</span>
<span></span><span class="sd">        tracker_metadata_prev (dict[str, Any]): The metadata from the previous frame.</span>
<span></span><span class="sd">        tracker_metadata_new (dict[str, Any]): The metadata for the current frame.</span>
<span></span><span class="sd">        obj_ids_newly_removed (set[int]): The object IDs that have been removed.</span>
<span></span><span class="sd">        reverse (bool): Whether the tracking is in reverse order.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (torch.Tensor): The updated low-resolution masks with some objects suppressed.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">obj_ids_global</span> <span class="o">=</span> <span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span>
<span></span>    <span class="n">binary_tracker_low_res_masks_global</span> <span class="o">=</span> <span class="n">tracker_low_res_masks_global</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span></span>    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tracker_low_res_masks_global</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">obj_ids_global</span><span class="p">)</span> <span class="o">==</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span>
<span></span>            <span class="sa">f</span><span class="s2">"Mismatch in number of objects: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">obj_ids_global</span><span class="p">)</span><span class="si">}</span><span class="s2"> vs </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">"</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="n">last_occluded_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
<span></span>            <span class="p">[</span>
<span></span>                <span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"obj_id_to_last_occluded"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
<span></span>                    <span class="n">obj_id</span><span class="p">,</span>
<span></span>                    <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
<span></span>                        <span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
<span></span>                        <span class="n">fill_value</span><span class="o">=</span><span class="p">(</span>
<span></span>                            <span class="bp">self</span><span class="o">.</span><span class="n">NEVER_OCCLUDED</span> <span class="k">if</span> <span class="n">obj_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">obj_ids_newly_removed</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">ALWAYS_OCCLUDED</span>
<span></span>                        <span class="p">),</span>
<span></span>                        <span class="n">device</span><span class="o">=</span><span class="n">binary_tracker_low_res_masks_global</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>                        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span>
<span></span>                    <span class="p">),</span>
<span></span>                <span class="p">)</span>
<span></span>                <span class="k">for</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">obj_ids_global</span>
<span></span>            <span class="p">],</span>
<span></span>            <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="n">to_suppress</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_objects_to_suppress_based_on_most_recently_occluded</span><span class="p">(</span>
<span></span>            <span class="n">binary_tracker_low_res_masks_global</span><span class="p">,</span>
<span></span>            <span class="n">last_occluded_prev</span><span class="p">,</span>
<span></span>            <span class="n">obj_ids_global</span><span class="p">,</span>
<span></span>            <span class="n">frame_idx</span><span class="p">,</span>
<span></span>            <span class="n">reverse</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>        <span class="c1"># Update metadata with occlusion information</span>
<span></span>        <span class="n">is_obj_occluded</span> <span class="o">=</span> <span class="o">~</span><span class="p">(</span><span class="n">binary_tracker_low_res_masks_global</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)))</span>
<span></span>        <span class="n">is_obj_occluded_or_suppressed</span> <span class="o">=</span> <span class="n">is_obj_occluded</span> <span class="o">|</span> <span class="n">to_suppress</span>
<span></span>        <span class="n">last_occluded_new</span> <span class="o">=</span> <span class="n">last_occluded_prev</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span></span>        <span class="n">last_occluded_new</span><span class="p">[</span><span class="n">is_obj_occluded_or_suppressed</span><span class="p">]</span> <span class="o">=</span> <span class="n">frame_idx</span>
<span></span>        <span class="c1"># Slice out the last occluded frame for each object</span>
<span></span>        <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"obj_id_to_last_occluded"</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
<span></span>            <span class="n">obj_id</span><span class="p">:</span> <span class="n">last_occluded_new</span><span class="p">[</span><span class="n">obj_idx</span> <span class="p">:</span> <span class="n">obj_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">obj_idx</span><span class="p">,</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">obj_ids_global</span><span class="p">)</span>
<span></span>        <span class="p">}</span>
<span></span>
<span></span>        <span class="c1"># Zero out suppressed masks before memory encoding</span>
<span></span>        <span class="n">tracker_low_res_masks_global</span><span class="p">[</span><span class="n">to_suppress</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">NO_OBJ_LOGIT</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">tracker_low_res_masks_global</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_add_new_objects"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_add_new_objects</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_tracker_add_new_objects</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">num_frames</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">new_obj_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
<span></span>    <span class="n">new_obj_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">tracker_states_local</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Add a new object to SAM2 inference states.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td><code>int</code></td><td></td><td><em>required</em></td></tr><tr><td><code>num_frames</code></td><td><code>int</code></td><td></td><td><em>required</em></td></tr><tr><td><code>new_obj_ids</code></td><td><code>list[int]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>new_obj_masks</code></td><td><code>torch.Tensor</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_states_local</code></td><td><code>list[Any]</code></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L3769-L3811"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_tracker_add_new_objects</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">num_frames</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">new_obj_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
<span></span>    <span class="n">new_obj_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">tracker_states_local</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Add a new object to SAM2 inference states."""</span>
<span></span>    <span class="n">prev_tracker_state</span> <span class="o">=</span> <span class="n">tracker_states_local</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tracker_states_local</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
<span></span>
<span></span>    <span class="c1"># prepare inference_state</span>
<span></span>    <span class="c1"># batch objects that first appear on the same frame together</span>
<span></span>    <span class="c1"># Clear inference state. Keep the cached image features if available.</span>
<span></span>    <span class="n">new_tracker_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">_init_state</span><span class="p">(</span><span class="n">num_frames</span><span class="o">=</span><span class="n">num_frames</span><span class="p">)</span>
<span></span>    <span class="c1"># NOTE: adding image placeholder</span>
<span></span>    <span class="n">new_tracker_state</span><span class="p">[</span><span class="s2">"im"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span></span>    <span class="n">new_tracker_state</span><span class="p">[</span><span class="s2">"backbone_out"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
<span></span>        <span class="n">prev_tracker_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"backbone_out"</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">if</span> <span class="n">prev_tracker_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_obj_ids</span><span class="p">)</span> <span class="o">==</span> <span class="n">new_obj_masks</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span></span>    <span class="k">assert</span> <span class="n">new_obj_masks</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span>
<span></span>    <span class="n">new_obj_masks</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
<span></span>        <span class="n">new_obj_masks</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
<span></span>        <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">interpol_size</span><span class="p">,</span>
<span></span>        <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">,</span>
<span></span>        <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span></span>    <span class="n">new_obj_masks</span> <span class="o">=</span> <span class="n">new_obj_masks</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span></span>
<span></span>    <span class="c1"># add object one by one</span>
<span></span>    <span class="k">for</span> <span class="n">new_obj_id</span><span class="p">,</span> <span class="n">new_mask</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">new_obj_ids</span><span class="p">,</span> <span class="n">new_obj_masks</span><span class="p">):</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">add_new_prompts</span><span class="p">(</span>
<span></span>            <span class="n">inference_state</span><span class="o">=</span><span class="n">new_tracker_state</span><span class="p">,</span>
<span></span>            <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span><span class="p">,</span>
<span></span>            <span class="n">obj_id</span><span class="o">=</span><span class="n">new_obj_id</span><span class="p">,</span>
<span></span>            <span class="n">masks</span><span class="o">=</span><span class="n">new_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>  <span class="c1"># add bs, channel</span>
<span></span>        <span class="p">)</span>
<span></span>    <span class="c1"># NOTE: we skip enforcing the non-overlapping constraint **globally** when adding new objects.</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">propagate_in_video_preflight</span><span class="p">(</span><span class="n">new_tracker_state</span><span class="p">)</span>
<span></span>    <span class="n">tracker_states_local</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_tracker_state</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">tracker_states_local</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_remove_objects"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_remove_objects</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_tracker_remove_objects</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tracker_states_local</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">obj_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span>
</code></pre></div><p>Remove an object from SAM2 inference states. This would remove the object from all frames in the video.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>tracker_states_local</code></td><td><code>list[Any]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>obj_ids</code></td><td><code>list[int]</code></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L3813-L3829"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_tracker_remove_objects</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tracker_states_local</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">obj_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
<span></span><span class="w">    </span><span class="sd">"""Remove an object from SAM2 inference states. This would remove the object from all frames in the video."""</span>
<span></span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">obj_ids</span><span class="p">:</span>
<span></span>        <span class="k">return</span>
<span></span>    <span class="c1"># Filter out states that become empty after removal</span>
<span></span>    <span class="n">active_states</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">tracker_states_local</span><span class="p">:</span>
<span></span>        <span class="k">for</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">obj_ids</span><span class="p">:</span>
<span></span>            <span class="c1"># we try to remove `obj_id` on every inference state with `strict=False`</span>
<span></span>            <span class="c1"># it will not do anything if an inference state doesn't contain `obj_id`</span>
<span></span>            <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">remove_object</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span></span>
<span></span>        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span></span>            <span class="n">active_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Update the list in-place</span>
<span></span>    <span class="n">tracker_states_local</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">active_states</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_update_memories"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor._tracker_update_memories</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_tracker_update_memories</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tracker_inference_states</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">low_res_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
</code></pre></div><p>Run Sam2 memory encoder, enforcing non-overlapping constraints globally.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>tracker_inference_states</code></td><td><code>list[Any]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>frame_idx</code></td><td><code>int</code></td><td></td><td><em>required</em></td></tr><tr><td><code>low_res_masks</code></td><td><code>torch.Tensor</code></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L3711-L3767"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_tracker_update_memories</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span> <span class="n">tracker_inference_states</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">low_res_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Run Sam2 memory encoder, enforcing non-overlapping constraints globally."""</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tracker_inference_states</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="k">return</span>
<span></span>    <span class="c1"># NOTE: inspect this part if we observe OOMs in the demo</span>
<span></span>    <span class="n">high_res_masks</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
<span></span>        <span class="n">low_res_masks</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
<span></span>        <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">interpol_size</span><span class="p">,</span>
<span></span>        <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">,</span>
<span></span>        <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="c1"># We first apply non-overlapping constraints before memory encoding. This may include some suppression heuristics.</span>
<span></span>    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">"_warm_up_complete"</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_warm_up_complete</span><span class="p">:</span>
<span></span>        <span class="n">high_res_masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_suppress_object_pw_area_shrinkage</span><span class="p">(</span><span class="n">high_res_masks</span><span class="p">)</span>
<span></span>    <span class="c1"># Instead of gathering the predicted object scores, we use mask areas as a proxy.</span>
<span></span>    <span class="n">object_score_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">high_res_masks</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)),</span> <span class="mf">10.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.0</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Run the memory encoder on local slices for each GPU</span>
<span></span>    <span class="n">start_idx_gpu</span> <span class="o">=</span> <span class="mi">0</span>
<span></span>    <span class="n">start_idx_state</span> <span class="o">=</span> <span class="n">start_idx_gpu</span>
<span></span>    <span class="k">for</span> <span class="n">tracker_state</span> <span class="ow">in</span> <span class="n">tracker_inference_states</span><span class="p">:</span>
<span></span>        <span class="n">num_obj_per_state</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tracker_state</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">])</span>
<span></span>        <span class="k">if</span> <span class="n">num_obj_per_state</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>            <span class="k">continue</span>
<span></span>        <span class="c1"># Get the local high-res masks and object score logits for this inference state</span>
<span></span>        <span class="n">end_idx_state</span> <span class="o">=</span> <span class="n">start_idx_state</span> <span class="o">+</span> <span class="n">num_obj_per_state</span>
<span></span>        <span class="n">local_high_res_masks</span> <span class="o">=</span> <span class="n">high_res_masks</span><span class="p">[</span><span class="n">start_idx_state</span><span class="p">:</span><span class="n">end_idx_state</span><span class="p">]</span>
<span></span>        <span class="n">local_object_score_logits</span> <span class="o">=</span> <span class="n">object_score_logits</span><span class="p">[</span><span class="n">start_idx_state</span><span class="p">:</span><span class="n">end_idx_state</span><span class="p">]</span>
<span></span>        <span class="n">local_batch_size</span> <span class="o">=</span> <span class="n">local_high_res_masks</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span></span>        <span class="c1"># Run Sam2 memory encoder. Note that we do not re-enforce the non-overlapping constraint as it is turned off by default</span>
<span></span>
<span></span>        <span class="n">encoded_mem</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">_run_memory_encoder</span><span class="p">(</span>
<span></span>            <span class="n">local_batch_size</span><span class="p">,</span>
<span></span>            <span class="n">local_high_res_masks</span><span class="p">,</span>
<span></span>            <span class="n">local_object_score_logits</span><span class="p">,</span>
<span></span>            <span class="n">is_mask_from_pts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>            <span class="n">inference_state</span><span class="o">=</span><span class="n">tracker_state</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="n">local_maskmem_features</span><span class="p">,</span> <span class="n">local_maskmem_pos_enc</span> <span class="o">=</span> <span class="n">encoded_mem</span>
<span></span>        <span class="c1"># Store encoded memories in the local inference state</span>
<span></span>        <span class="n">output_dict</span> <span class="o">=</span> <span class="n">tracker_state</span><span class="p">[</span><span class="s2">"output_dict"</span><span class="p">]</span>
<span></span>        <span class="k">for</span> <span class="n">storage_key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">,</span> <span class="s2">"non_cond_frame_outputs"</span><span class="p">]:</span>
<span></span>            <span class="k">if</span> <span class="n">frame_idx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">]:</span>
<span></span>                <span class="k">continue</span>
<span></span>            <span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">][</span><span class="s2">"maskmem_features"</span><span class="p">]</span> <span class="o">=</span> <span class="n">local_maskmem_features</span>
<span></span>            <span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">][</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">pos</span> <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="n">local_maskmem_pos_enc</span><span class="p">]</span>
<span></span>            <span class="c1"># for batched inference state, we also need to add per-object</span>
<span></span>            <span class="c1"># memory slides to support instance interactivity</span>
<span></span>            <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">_add_output_per_object</span><span class="p">(</span>
<span></span>                <span class="n">inference_state</span><span class="o">=</span><span class="n">tracker_state</span><span class="p">,</span>
<span></span>                <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span><span class="p">,</span>
<span></span>                <span class="n">current_out</span><span class="o">=</span><span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">],</span>
<span></span>                <span class="n">storage_key</span><span class="o">=</span><span class="n">storage_key</span><span class="p">,</span>
<span></span>            <span class="p">)</span>
<span></span>        <span class="n">start_idx_state</span> <span class="o">+=</span> <span class="n">num_obj_per_state</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.add_prompt"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.add_prompt</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">add_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">text</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">inference_state</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Add text, point or box prompts on a single frame. This method returns the inference outputs only on the</p><p>prompted frame.</p><p>Note that text prompts are NOT associated with a particular frame (i.e. they apply to all frames). However, we only run inference on the frame specified in <code>frame_idx</code>.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>text</code></td><td></td><td></td><td><code>None</code></td></tr><tr><td><code>bboxes</code></td><td></td><td></td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td></td><td></td><td><code>None</code></td></tr><tr><td><code>inference_state</code></td><td></td><td></td><td><code>None</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2722-L2759"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">add_prompt</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">,</span>
<span></span>    <span class="n">text</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">inference_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Add text, point or box prompts on a single frame. This method returns the inference outputs only on the</span>
<span></span><span class="sd">    prompted frame.</span>
<span></span>
<span></span><span class="sd">    Note that text prompts are NOT associated with a particular frame (i.e. they apply</span>
<span></span><span class="sd">    to all frames). However, we only run inference on the frame specified in `frame_idx`.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="n">inference_state</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span>
<span></span>    <span class="k">assert</span> <span class="n">text</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">bboxes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">"at least one type of prompt (text, boxes) must be provided"</span>
<span></span>
<span></span>    <span class="c1"># 1) handle text prompt</span>
<span></span>    <span class="n">use_text</span> <span class="o">=</span> <span class="n">text</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span></span>    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span> <span class="k">if</span> <span class="n">use_text</span> <span class="k">else</span> <span class="s2">"visual"</span>
<span></span>    <span class="n">text_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">text</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">text</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"text_prompt"</span><span class="p">]</span> <span class="o">=</span> <span class="n">text</span> <span class="k">if</span> <span class="n">use_text</span> <span class="k">else</span> <span class="kc">None</span>
<span></span>    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_batch</span><span class="p">)</span>
<span></span>    <span class="n">text_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"text_ids"</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_ids</span>
<span></span>    <span class="k">if</span> <span class="n">text</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">names</span> <span class="o">!=</span> <span class="n">text</span><span class="p">:</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">set_classes</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># 2) handle box prompt</span>
<span></span>    <span class="n">bboxes</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_geometric_prompts</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span></span>    <span class="k">assert</span> <span class="p">(</span><span class="n">bboxes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
<span></span>    <span class="n">geometric_prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dummy_prompt</span><span class="p">(</span><span class="n">num_prompts</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">bboxes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bboxes</span><span class="p">)):</span>
<span></span>            <span class="n">geometric_prompt</span><span class="o">.</span><span class="n">append_boxes</span><span class="p">(</span><span class="n">bboxes</span><span class="p">[[</span><span class="n">i</span><span class="p">]],</span> <span class="n">labels</span><span class="p">[[</span><span class="n">i</span><span class="p">]])</span>
<span></span>    <span class="n">inference_state</span><span class="p">[</span><span class="s2">"per_frame_geometric_prompt"</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">geometric_prompt</span>
<span></span>    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_single_frame_inference</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inference_state</span><span class="o">=</span><span class="n">inference_state</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">out</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.build_outputs"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.build_outputs</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">build_outputs</span><span class="p">(</span>
<span></span>    <span class="n">det_out</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<span></span>    <span class="n">tracker_low_res_masks_global</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">tracker_metadata_prev</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
<span></span>    <span class="n">tracker_update_plan</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
<span></span>    <span class="n">reconditioned_obj_ids</span><span class="p">:</span> <span class="nb">set</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Build the output masks for the current frame.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>det_out</code></td><td><code>dict[str, torch.Tensor]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_low_res_masks_global</code></td><td><code>torch.Tensor</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_metadata_prev</code></td><td><code>dict[str, np.ndarray]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_update_plan</code></td><td><code>dict[str, np.ndarray]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>reconditioned_obj_ids</code></td><td><code>set | None</code></td><td></td><td><code>None</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L3333-L3369"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@staticmethod</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">build_outputs</span><span class="p">(</span>
<span></span>    <span class="n">det_out</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<span></span>    <span class="n">tracker_low_res_masks_global</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">tracker_metadata_prev</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
<span></span>    <span class="n">tracker_update_plan</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
<span></span>    <span class="n">reconditioned_obj_ids</span><span class="p">:</span> <span class="nb">set</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Build the output masks for the current frame."""</span>
<span></span>    <span class="n">new_det_fa_inds</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">tracker_update_plan</span><span class="p">[</span><span class="s2">"new_det_fa_inds"</span><span class="p">]</span>
<span></span>    <span class="n">new_det_obj_ids</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">tracker_update_plan</span><span class="p">[</span><span class="s2">"new_det_obj_ids"</span><span class="p">]</span>
<span></span>    <span class="n">obj_id_to_mask</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># obj_id --&gt; output mask tensor</span>
<span></span>
<span></span>    <span class="c1"># Part 1: masks from previous SAM2 propagation</span>
<span></span>    <span class="n">existing_masklet_obj_ids</span> <span class="o">=</span> <span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span>
<span></span>    <span class="n">existing_masklet_binary</span> <span class="o">=</span> <span class="n">tracker_low_res_masks_global</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">existing_masklet_obj_ids</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">existing_masklet_binary</span><span class="p">)</span>
<span></span>    <span class="k">for</span> <span class="n">obj_id</span><span class="p">,</span> <span class="n">mask</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">existing_masklet_obj_ids</span><span class="p">,</span> <span class="n">existing_masklet_binary</span><span class="p">):</span>
<span></span>        <span class="n">obj_id_to_mask</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask</span>  <span class="c1"># (1, H_video, W_video)</span>
<span></span>
<span></span>    <span class="c1"># Part 2: masks from new detections</span>
<span></span>    <span class="n">new_det_fa_inds_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">new_det_fa_inds</span><span class="p">)</span>
<span></span>    <span class="n">new_det_low_res_masks</span> <span class="o">=</span> <span class="n">det_out</span><span class="p">[</span><span class="s2">"mask"</span><span class="p">][</span><span class="n">new_det_fa_inds_t</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_det_obj_ids</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_det_low_res_masks</span><span class="p">)</span>
<span></span>    <span class="k">for</span> <span class="n">obj_id</span><span class="p">,</span> <span class="n">mask</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">new_det_obj_ids</span><span class="p">,</span> <span class="n">new_det_low_res_masks</span><span class="p">):</span>
<span></span>        <span class="n">obj_id_to_mask</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask</span>  <span class="c1"># (1, H_video, W_video)</span>
<span></span>
<span></span>    <span class="c1"># Part 3: Override masks for reconditioned objects using detection masks</span>
<span></span>    <span class="k">if</span> <span class="n">reconditioned_obj_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">reconditioned_obj_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="n">trk_id_to_max_iou_high_conf_det</span> <span class="o">=</span> <span class="n">tracker_update_plan</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"trk_id_to_max_iou_high_conf_det"</span><span class="p">,</span> <span class="p">{})</span>
<span></span>
<span></span>        <span class="k">for</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">reconditioned_obj_ids</span><span class="p">:</span>
<span></span>            <span class="n">det_idx</span> <span class="o">=</span> <span class="n">trk_id_to_max_iou_high_conf_det</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">obj_id</span><span class="p">)</span>
<span></span>
<span></span>            <span class="k">if</span> <span class="n">det_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>                <span class="n">obj_id_to_mask</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">det_out</span><span class="p">[</span><span class="s2">"mask"</span><span class="p">][</span><span class="n">det_idx</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">obj_id_to_mask</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.inference"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.inference</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div><p>Perform inference on a video sequence with optional prompts.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td></td><td></td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td></td><td></td><td><code>None</code></td></tr><tr><td><code>text</code></td><td><code>list[str] | None</code></td><td></td><td><code>None</code></td></tr><tr><td><code>*args</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>**kwargs</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2607-L2613"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform inference on a video sequence with optional prompts."""</span>
<span></span>    <span class="n">frame</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">frame</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># align frame index to be 0-based</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"im"</span><span class="p">]</span> <span class="o">=</span> <span class="n">im</span>  <span class="c1"># only pass image for subsequent frames</span>
<span></span>    <span class="k">if</span> <span class="s2">"text_ids"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">:</span>  <span class="c1"># first frame processing</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">add_prompt</span><span class="p">(</span><span class="n">frame_idx</span><span class="o">=</span><span class="n">frame</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_single_frame_inference</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.init_state"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.init_state</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">init_state</span><span class="p">(</span><span class="n">predictor</span><span class="p">)</span>
</code></pre></div><p>Initialize an inference state for the predictor.</p><p>This function sets up the initial state required for performing inference on video data. It includes initializing various dictionaries and ordered dictionaries that will store inputs, outputs, and other metadata relevant to the tracking process.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>predictor</code></td><td><code>SAM3VideoSemanticPredictor</code></td><td>The predictor object for which to initialize the state.</td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2583-L2605"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@staticmethod</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">init_state</span><span class="p">(</span><span class="n">predictor</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Initialize an inference state for the predictor.</span>
<span></span>
<span></span><span class="sd">    This function sets up the initial state required for performing inference on video data. It includes</span>
<span></span><span class="sd">    initializing various dictionaries and ordered dictionaries that will store inputs, outputs, and other metadata</span>
<span></span><span class="sd">    relevant to the tracking process.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        predictor (SAM3VideoSemanticPredictor): The predictor object for which to initialize the state.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">predictor</span><span class="o">.</span><span class="n">inference_state</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># means initialized</span>
<span></span>        <span class="k">return</span>
<span></span>    <span class="k">assert</span> <span class="n">predictor</span><span class="o">.</span><span class="n">dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span></span>    <span class="k">assert</span> <span class="n">predictor</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">"video"</span>
<span></span>    <span class="n">num_frames</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">frames</span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="p">{</span>
<span></span>        <span class="s2">"num_frames"</span><span class="p">:</span> <span class="n">num_frames</span><span class="p">,</span>
<span></span>        <span class="s2">"tracker_inference_states"</span><span class="p">:</span> <span class="p">[],</span>
<span></span>        <span class="s2">"tracker_metadata"</span><span class="p">:</span> <span class="p">{},</span>
<span></span>        <span class="s2">"text_prompt"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="s2">"per_frame_geometric_prompt"</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_frames</span><span class="p">,</span>
<span></span>    <span class="p">}</span>
<span></span>    <span class="n">predictor</span><span class="o">.</span><span class="n">inference_state</span> <span class="o">=</span> <span class="n">inference_state</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.postprocess"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.postprocess</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">)</span>
</code></pre></div><p>Post-process the predictions to apply non-overlapping constraints if required.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>preds</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>img</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>orig_imgs</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2615-L2665"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Post-process the predictions to apply non-overlapping constraints if required."""</span>
<span></span>    <span class="n">obj_id_to_mask</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="s2">"obj_id_to_mask"</span><span class="p">]</span>  <span class="c1"># low res masks</span>
<span></span>    <span class="n">curr_obj_ids</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">obj_id_to_mask</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span></span>    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">orig_imgs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>  <span class="c1"># input images are a torch.Tensor, not a list</span>
<span></span>        <span class="n">orig_imgs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_torch2numpy_batch</span><span class="p">(</span><span class="n">orig_imgs</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">curr_obj_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_boxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">obj_id_to_mask</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="k">for</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">curr_obj_ids</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span></span>        <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">float</span><span class="p">()[</span><span class="kc">None</span><span class="p">],</span> <span class="n">orig_imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span></span>        <span class="n">pred_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">curr_obj_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span></span>            <span class="p">[</span><span class="n">preds</span><span class="p">[</span><span class="s2">"obj_id_to_score"</span><span class="p">][</span><span class="n">obj_id</span><span class="p">]</span> <span class="k">for</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">curr_obj_ids</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="n">pred_cls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span></span>            <span class="p">[</span><span class="n">preds</span><span class="p">[</span><span class="s2">"obj_id_to_cls"</span><span class="p">][</span><span class="n">obj_id</span><span class="p">]</span> <span class="k">for</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">curr_obj_ids</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="n">keep</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred_scores</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">conf</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">pred_masks</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span></span>        <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">pred_masks</span><span class="p">[</span><span class="n">keep</span><span class="p">]</span>
<span></span>        <span class="n">pred_boxes</span> <span class="o">=</span> <span class="n">batched_mask_to_box</span><span class="p">(</span><span class="n">pred_masks</span><span class="p">)</span>
<span></span>        <span class="n">pred_boxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
<span></span>            <span class="p">[</span><span class="n">pred_boxes</span><span class="p">,</span> <span class="n">pred_ids</span><span class="p">[</span><span class="n">keep</span><span class="p">][:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">pred_scores</span><span class="p">[</span><span class="n">keep</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">pred_cls</span><span class="p">[</span><span class="n">keep</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="k">if</span> <span class="n">pred_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<span></span>            <span class="n">tracker_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span></span>                <span class="p">[</span>
<span></span>                    <span class="p">(</span>
<span></span>                        <span class="n">preds</span><span class="p">[</span><span class="s2">"obj_id_to_tracker_score"</span><span class="p">][</span><span class="n">obj_id</span><span class="p">]</span>
<span></span>                        <span class="k">if</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">preds</span><span class="p">[</span><span class="s2">"obj_id_to_tracker_score"</span><span class="p">]</span>
<span></span>                        <span class="k">else</span> <span class="mf">0.0</span>
<span></span>                    <span class="p">)</span>
<span></span>                    <span class="k">for</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">curr_obj_ids</span>
<span></span>                <span class="p">],</span>
<span></span>                <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>            <span class="p">)[</span><span class="n">keep</span><span class="p">]</span>
<span></span>            <span class="n">pred_masks</span> <span class="o">=</span> <span class="p">(</span>
<span></span>                <span class="bp">self</span><span class="o">.</span><span class="n">_apply_object_wise_non_overlapping_constraints</span><span class="p">(</span>
<span></span>                    <span class="n">pred_masks</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
<span></span>                    <span class="n">tracker_scores</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
<span></span>                    <span class="n">background_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span></span>                <span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span></span>            <span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span></span>
<span></span>    <span class="c1"># names = getattr(self.model, "names", [str(i) for i in range(pred_scores.shape[0])])</span>
<span></span>    <span class="n">names</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pred_boxes</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
<span></span>    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>    <span class="k">for</span> <span class="n">masks</span><span class="p">,</span> <span class="n">boxes</span><span class="p">,</span> <span class="n">orig_img</span><span class="p">,</span> <span class="n">img_path</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">pred_masks</span><span class="p">],</span> <span class="p">[</span><span class="n">pred_boxes</span><span class="p">],</span> <span class="n">orig_imgs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
<span></span>        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Results</span><span class="p">(</span><span class="n">orig_img</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="n">img_path</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">names</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="n">masks</span><span class="p">,</span> <span class="n">boxes</span><span class="o">=</span><span class="n">boxes</span><span class="p">))</span>
<span></span>    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_backbone_and_detection"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_backbone_and_detection</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_backbone_and_detection</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">text_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">geometric_prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">allow_new_detections</span><span class="p">:</span> <span class="nb">bool</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Run backbone and detection for a single frame.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor</code></td><td></td><td><em>required</em></td></tr><tr><td><code>text_ids</code></td><td><code>torch.Tensor</code></td><td></td><td><em>required</em></td></tr><tr><td><code>geometric_prompt</code></td><td><code>Prompt</code></td><td></td><td><em>required</em></td></tr><tr><td><code>allow_new_detections</code></td><td><code>bool</code></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2898-L2908"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_backbone_and_detection</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">text_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">geometric_prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">allow_new_detections</span><span class="p">:</span> <span class="nb">bool</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Run backbone and detection for a single frame."""</span>
<span></span>    <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
<span></span>    <span class="n">sam3_image_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward_grounding</span><span class="p">(</span>
<span></span>        <span class="n">backbone_out</span><span class="o">=</span><span class="n">features</span><span class="p">,</span> <span class="n">text_ids</span><span class="o">=</span><span class="n">text_ids</span><span class="p">,</span> <span class="n">geometric_prompt</span><span class="o">=</span><span class="n">geometric_prompt</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="n">det_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_detection_outputs</span><span class="p">(</span><span class="n">sam3_image_out</span><span class="p">,</span> <span class="n">allow_new_detections</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">_cache_backbone_features</span><span class="p">(</span><span class="n">sam3_image_out</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">det_out</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_propagation"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_propagation</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_tracker_propagation</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">tracker_states_local</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">tracker_metadata_prev</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Run the tracker propagation phase for a single frame in an SPMD manner.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td><code>int</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_states_local</code></td><td><code>list[Any]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_metadata_prev</code></td><td><code>dict[str, np.ndarray]</code></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2950-L2970"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_tracker_propagation</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">tracker_states_local</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">tracker_metadata_prev</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Run the tracker propagation phase for a single frame in an SPMD manner."""</span>
<span></span>    <span class="c1"># Step 1: propagate the local SAM2 states to get the current frame's prediction</span>
<span></span>    <span class="c1"># `low_res_masks_local` of the existing masklets on this GPU</span>
<span></span>    <span class="c1"># - obj_ids_local: list[int] -- list of object IDs</span>
<span></span>    <span class="c1"># - low_res_masks_local: Tensor -- (num_local_obj, H_mask, W_mask)</span>
<span></span>    <span class="n">obj_ids_local</span><span class="p">,</span> <span class="n">low_res_masks_local</span><span class="p">,</span> <span class="n">obj_scores_local</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_propogate_tracker_one_frame_local_gpu</span><span class="p">(</span>
<span></span>        <span class="n">tracker_states_local</span><span class="p">,</span> <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">obj_ids_local</span> <span class="o">==</span> <span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]),</span> <span class="s2">"</span><span class="si">{}</span><span class="s2"> != </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
<span></span>        <span class="n">obj_ids_local</span><span class="p">,</span> <span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Step 2: all-gather `low_res_masks_local` into `low_res_masks_global`</span>
<span></span>    <span class="c1"># - low_res_masks_global: Tensor -- (num_global_obj, H_mask, W_mask)</span>
<span></span>    <span class="n">low_res_masks_global</span> <span class="o">=</span> <span class="n">low_res_masks_local</span>
<span></span>    <span class="n">obj_scores_global</span> <span class="o">=</span> <span class="n">obj_scores_local</span>
<span></span>    <span class="k">return</span> <span class="n">low_res_masks_global</span><span class="p">,</span> <span class="n">obj_scores_global</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_update_execution_phase"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_update_execution_phase</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_tracker_update_execution_phase</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">num_frames</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">det_out</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<span></span>    <span class="n">tracker_states_local</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
<span></span>    <span class="n">tracker_update_plan</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Execute the tracker update plan for a single frame in an SPMD manner.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td><code>int</code></td><td></td><td><em>required</em></td></tr><tr><td><code>num_frames</code></td><td><code>int</code></td><td></td><td><em>required</em></td></tr><tr><td><code>det_out</code></td><td><code>dict[str, torch.Tensor]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_states_local</code></td><td><code>list[Any]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_update_plan</code></td><td><code>dict[str, np.ndarray]</code></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L3296-L3330"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_tracker_update_execution_phase</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">num_frames</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">det_out</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<span></span>    <span class="n">tracker_states_local</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
<span></span>    <span class="n">tracker_update_plan</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Execute the tracker update plan for a single frame in an SPMD manner."""</span>
<span></span>    <span class="c1"># initialize tracking scores with detection scores</span>
<span></span>    <span class="n">new_det_fa_inds</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">tracker_update_plan</span><span class="p">[</span><span class="s2">"new_det_fa_inds"</span><span class="p">]</span>
<span></span>    <span class="n">new_det_obj_ids</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">tracker_update_plan</span><span class="p">[</span><span class="s2">"new_det_obj_ids"</span><span class="p">]</span>
<span></span>    <span class="c1"># new_det_gpu_ids: np.ndarray = tracker_update_plan["new_det_gpu_ids"]</span>
<span></span>    <span class="n">new_det_obj_ids_local</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">new_det_obj_ids</span>
<span></span>    <span class="n">new_det_fa_inds_local</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">new_det_fa_inds</span>
<span></span>    <span class="n">obj_ids_newly_removed</span><span class="p">:</span> <span class="nb">set</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">tracker_update_plan</span><span class="p">[</span><span class="s2">"obj_ids_newly_removed"</span><span class="p">]</span>
<span></span>
<span></span>    <span class="c1"># Step 1: add new objects from the detector to SAM2 inference states</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_det_fa_inds_local</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="n">new_det_fa_inds_local_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">new_det_fa_inds_local</span><span class="p">)</span>
<span></span>        <span class="n">new_det_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">det_out</span><span class="p">[</span><span class="s2">"mask"</span><span class="p">][</span><span class="n">new_det_fa_inds_local_t</span><span class="p">]</span>
<span></span>        <span class="c1"># initialize SAM2 with new object masks</span>
<span></span>        <span class="n">tracker_states_local</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tracker_add_new_objects</span><span class="p">(</span>
<span></span>            <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span><span class="p">,</span>
<span></span>            <span class="n">num_frames</span><span class="o">=</span><span class="n">num_frames</span><span class="p">,</span>
<span></span>            <span class="n">new_obj_ids</span><span class="o">=</span><span class="n">new_det_obj_ids_local</span><span class="p">,</span>
<span></span>            <span class="n">new_obj_masks</span><span class="o">=</span><span class="n">new_det_masks</span><span class="p">,</span>
<span></span>            <span class="n">tracker_states_local</span><span class="o">=</span><span class="n">tracker_states_local</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Step 2: remove from SAM2 inference states those objects removed by heuristics</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">obj_ids_newly_removed</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">_tracker_remove_objects</span><span class="p">(</span><span class="n">tracker_states_local</span><span class="p">,</span> <span class="n">obj_ids_newly_removed</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">tracker_states_local</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_update_planning_phase"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.run_tracker_update_planning_phase</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_tracker_update_planning_phase</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">reverse</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span></span>    <span class="n">det_out</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<span></span>    <span class="n">tracker_low_res_masks_global</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">tracker_obj_scores_global</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">tracker_metadata_prev</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
<span></span>    <span class="n">tracker_states_local</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Run the tracker update planning phase for a single frame in an SPMD manner.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td><code>int</code></td><td></td><td><em>required</em></td></tr><tr><td><code>reverse</code></td><td><code>bool</code></td><td></td><td><em>required</em></td></tr><tr><td><code>det_out</code></td><td><code>dict[str, torch.Tensor]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_low_res_masks_global</code></td><td><code>torch.Tensor</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_obj_scores_global</code></td><td><code>torch.Tensor</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_metadata_prev</code></td><td><code>dict[str, np.ndarray]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>tracker_states_local</code></td><td><code>list[Any]</code></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L3014-L3224"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_tracker_update_planning_phase</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">reverse</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span></span>    <span class="n">det_out</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<span></span>    <span class="n">tracker_low_res_masks_global</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">tracker_obj_scores_global</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">tracker_metadata_prev</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
<span></span>    <span class="n">tracker_states_local</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Run the tracker update planning phase for a single frame in an SPMD manner."""</span>
<span></span>    <span class="c1"># initialize new metadata from previous metadata (its values will be updated later)</span>
<span></span>    <span class="n">tracker_metadata_new</span> <span class="o">=</span> <span class="p">{</span>
<span></span>        <span class="s2">"obj_ids"</span><span class="p">:</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]),</span>
<span></span>        <span class="s2">"num_obj"</span><span class="p">:</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"num_obj"</span><span class="p">]),</span>
<span></span>        <span class="s2">"obj_id_to_score"</span><span class="p">:</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"obj_id_to_score"</span><span class="p">]),</span>
<span></span>        <span class="s2">"obj_id_to_cls"</span><span class="p">:</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"obj_id_to_cls"</span><span class="p">]),</span>
<span></span>        <span class="s2">"obj_id_to_tracker_score_frame_wise"</span><span class="p">:</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"obj_id_to_tracker_score_frame_wise"</span><span class="p">]),</span>
<span></span>        <span class="s2">"obj_id_to_last_occluded"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># will be filled later</span>
<span></span>        <span class="s2">"max_obj_id"</span><span class="p">:</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"max_obj_id"</span><span class="p">]),</span>
<span></span>    <span class="p">}</span>
<span></span>
<span></span>    <span class="c1"># Initialize reconditioned_obj_ids early to avoid UnboundLocalError</span>
<span></span>    <span class="n">reconditioned_obj_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span></span>
<span></span>    <span class="c1"># Step 1: make the update plan and resolve heuristics on GPU 0</span>
<span></span>    <span class="n">det_mask_preds</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">det_out</span><span class="p">[</span><span class="s2">"mask"</span><span class="p">]</span>  <span class="c1"># low-res mask logits</span>
<span></span>    <span class="n">det_scores_np</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">det_out</span><span class="p">[</span><span class="s2">"scores"</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span></span>    <span class="n">det_cls_np</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">det_out</span><span class="p">[</span><span class="s2">"cls"</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span></span>    <span class="n">det_bbox_xyxy</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">det_out</span><span class="p">[</span><span class="s2">"bbox"</span><span class="p">]</span>
<span></span>    <span class="c1"># a) match detector and tracker masks and find new objects</span>
<span></span>    <span class="p">(</span>
<span></span>        <span class="n">new_det_fa_inds</span><span class="p">,</span>
<span></span>        <span class="n">unmatched_trk_obj_ids</span><span class="p">,</span>
<span></span>        <span class="n">det_to_matched_trk_obj_ids</span><span class="p">,</span>
<span></span>        <span class="n">trk_id_to_max_iou_high_conf_det</span><span class="p">,</span>
<span></span>        <span class="n">empty_trk_obj_ids</span><span class="p">,</span>
<span></span>    <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_associate_det_trk</span><span class="p">(</span>
<span></span>        <span class="n">det_masks</span><span class="o">=</span><span class="n">det_mask_preds</span><span class="p">,</span>
<span></span>        <span class="n">det_scores_np</span><span class="o">=</span><span class="n">det_scores_np</span><span class="p">,</span>
<span></span>        <span class="n">trk_masks</span><span class="o">=</span><span class="n">tracker_low_res_masks_global</span><span class="p">,</span>
<span></span>        <span class="n">trk_obj_ids</span><span class="o">=</span><span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">],</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">suppress_det_close_to_boundary</span><span class="p">:</span>
<span></span>        <span class="n">keep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_suppress_detections_close_to_boundary</span><span class="p">(</span><span class="n">det_bbox_xyxy</span><span class="p">[</span><span class="n">new_det_fa_inds</span><span class="p">])</span>
<span></span>        <span class="n">new_det_fa_inds</span> <span class="o">=</span> <span class="n">new_det_fa_inds</span><span class="p">[</span><span class="n">keep</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span>
<span></span>
<span></span>    <span class="c1"># check whether we've hit the maximum number of objects we can track (and if so, drop some detections)</span>
<span></span>    <span class="n">prev_obj_num</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"num_obj"</span><span class="p">])</span>
<span></span>    <span class="n">new_det_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_det_fa_inds</span><span class="p">)</span>
<span></span>    <span class="n">num_obj_dropped_due_to_limit</span> <span class="o">=</span> <span class="mi">0</span>
<span></span>    <span class="k">if</span> <span class="n">prev_obj_num</span> <span class="o">+</span> <span class="n">new_det_num</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_num_objects</span><span class="p">:</span>
<span></span>        <span class="n">LOGGER</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">"hitting </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_num_objects</span><span class="si">=}</span><span class="s2"> with </span><span class="si">{</span><span class="n">new_det_num</span><span class="si">=}</span><span class="s2"> and </span><span class="si">{</span><span class="n">prev_obj_num</span><span class="si">=}</span><span class="s2">"</span><span class="p">)</span>
<span></span>        <span class="n">new_det_num_to_keep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_num_objects</span> <span class="o">-</span> <span class="n">prev_obj_num</span>
<span></span>        <span class="n">num_obj_dropped_due_to_limit</span> <span class="o">=</span> <span class="n">new_det_num</span> <span class="o">-</span> <span class="n">new_det_num_to_keep</span>
<span></span>        <span class="n">new_det_fa_inds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_drop_new_det_with_obj_limit</span><span class="p">(</span><span class="n">new_det_fa_inds</span><span class="p">,</span> <span class="n">det_scores_np</span><span class="p">,</span> <span class="n">new_det_num_to_keep</span><span class="p">)</span>
<span></span>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_det_fa_inds</span><span class="p">)</span> <span class="o">==</span> <span class="n">new_det_num_to_keep</span>
<span></span>        <span class="n">new_det_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_det_fa_inds</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># assign object IDs to new detections and decide which GPU to place them</span>
<span></span>    <span class="n">new_det_obj_ids</span> <span class="o">=</span> <span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"max_obj_id"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">new_det_num</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># b) handle hotstart heuristics to remove objects</span>
<span></span>    <span class="c1"># here `metadata` contains metadata stored on (and only accessible to) GPU 0;</span>
<span></span>    <span class="c1"># we avoid broadcasting them to other GPUs to save communication cost, assuming</span>
<span></span>    <span class="c1"># that `metadata` is not needed by other GPUs</span>
<span></span>    <span class="n">metadata_new</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"metadata"</span><span class="p">])</span>
<span></span>    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">"_warm_up_complete"</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_warm_up_complete</span><span class="p">:</span>
<span></span>        <span class="n">obj_ids_newly_removed</span><span class="p">,</span> <span class="n">metadata_new</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_hotstart</span><span class="p">(</span>
<span></span>            <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span><span class="p">,</span>
<span></span>            <span class="n">reverse</span><span class="o">=</span><span class="n">reverse</span><span class="p">,</span>
<span></span>            <span class="n">det_to_matched_trk_obj_ids</span><span class="o">=</span><span class="n">det_to_matched_trk_obj_ids</span><span class="p">,</span>
<span></span>            <span class="n">new_det_obj_ids</span><span class="o">=</span><span class="n">new_det_obj_ids</span><span class="p">,</span>
<span></span>            <span class="n">empty_trk_obj_ids</span><span class="o">=</span><span class="n">empty_trk_obj_ids</span><span class="p">,</span>
<span></span>            <span class="n">unmatched_trk_obj_ids</span><span class="o">=</span><span class="n">unmatched_trk_obj_ids</span><span class="p">,</span>
<span></span>            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata_new</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="c1"># if warm-up is not complete, we don't remove any objects</span>
<span></span>        <span class="n">obj_ids_newly_removed</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span></span>    <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"metadata"</span><span class="p">]</span> <span class="o">=</span> <span class="n">metadata_new</span>
<span></span>
<span></span>    <span class="c1"># `tracker_update_plan` should be identical on all GPUs after broadcasting</span>
<span></span>    <span class="n">tracker_update_plan</span> <span class="o">=</span> <span class="p">{</span>
<span></span>        <span class="s2">"new_det_fa_inds"</span><span class="p">:</span> <span class="n">new_det_fa_inds</span><span class="p">,</span>  <span class="c1"># np.ndarray</span>
<span></span>        <span class="s2">"new_det_obj_ids"</span><span class="p">:</span> <span class="n">new_det_obj_ids</span><span class="p">,</span>  <span class="c1"># np.ndarray</span>
<span></span>        <span class="c1"># "new_det_gpu_ids": new_det_gpu_ids,  # np.ndarray</span>
<span></span>        <span class="s2">"unmatched_trk_obj_ids"</span><span class="p">:</span> <span class="n">unmatched_trk_obj_ids</span><span class="p">,</span>  <span class="c1"># np.ndarray</span>
<span></span>        <span class="s2">"det_to_matched_trk_obj_ids"</span><span class="p">:</span> <span class="n">det_to_matched_trk_obj_ids</span><span class="p">,</span>  <span class="c1"># dict</span>
<span></span>        <span class="s2">"obj_ids_newly_removed"</span><span class="p">:</span> <span class="n">obj_ids_newly_removed</span><span class="p">,</span>  <span class="c1"># set</span>
<span></span>        <span class="s2">"num_obj_dropped_due_to_limit"</span><span class="p">:</span> <span class="n">num_obj_dropped_due_to_limit</span><span class="p">,</span>  <span class="c1"># int</span>
<span></span>        <span class="s2">"trk_id_to_max_iou_high_conf_det"</span><span class="p">:</span> <span class="n">trk_id_to_max_iou_high_conf_det</span><span class="p">,</span>  <span class="c1"># dict</span>
<span></span>        <span class="s2">"reconditioned_obj_ids"</span><span class="p">:</span> <span class="n">reconditioned_obj_ids</span><span class="p">,</span>  <span class="c1"># set</span>
<span></span>    <span class="p">}</span>
<span></span>
<span></span>    <span class="c1"># Step 3 (optional): recondition masklets based on high-confidence detections before memory encoding</span>
<span></span>    <span class="c1"># NOTE: Running this in execution phase (after memory encoding) can lead to suboptimal results</span>
<span></span>    <span class="n">should_recondition_iou</span> <span class="o">=</span> <span class="kc">False</span>
<span></span>
<span></span>    <span class="c1"># Evaluate tracklets for reconditioning based on bbox IoU mismatch with detections</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_bbox_iou_thresh</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">trk_id_to_max_iou_high_conf_det</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="k">for</span> <span class="n">trk_obj_id</span><span class="p">,</span> <span class="n">det_idx</span> <span class="ow">in</span> <span class="n">trk_id_to_max_iou_high_conf_det</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<span></span>            <span class="n">det_box</span> <span class="o">=</span> <span class="n">det_out</span><span class="p">[</span><span class="s2">"bbox"</span><span class="p">][</span><span class="n">det_idx</span><span class="p">]</span>
<span></span>            <span class="n">det_score</span> <span class="o">=</span> <span class="n">det_out</span><span class="p">[</span><span class="s2">"scores"</span><span class="p">][</span><span class="n">det_idx</span><span class="p">]</span>
<span></span>
<span></span>            <span class="k">try</span><span class="p">:</span>
<span></span>                <span class="n">trk_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">])</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">trk_obj_id</span><span class="p">)</span>
<span></span>            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
<span></span>                <span class="k">continue</span>  <span class="c1"># Skip if tracklet not found</span>
<span></span>
<span></span>            <span class="n">tracker_mask</span> <span class="o">=</span> <span class="n">tracker_low_res_masks_global</span><span class="p">[</span><span class="n">trk_idx</span><span class="p">]</span>
<span></span>            <span class="n">mask_binary</span> <span class="o">=</span> <span class="n">tracker_mask</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span></span>            <span class="n">mask_area</span> <span class="o">=</span> <span class="n">mask_binary</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span></span>
<span></span>            <span class="k">if</span> <span class="n">mask_area</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>                <span class="k">continue</span>  <span class="c1"># Skip tracklets with zero mask area</span>
<span></span>
<span></span>            <span class="c1"># Get bounding box from SAM2 mask and convert to normalized coordinates</span>
<span></span>            <span class="n">tracker_box_pixels</span> <span class="o">=</span> <span class="n">batched_mask_to_box</span><span class="p">(</span><span class="n">mask_binary</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span></span>            <span class="n">mask_height</span><span class="p">,</span> <span class="n">mask_width</span> <span class="o">=</span> <span class="n">tracker_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
<span></span>            <span class="n">tracker_box_normalized</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span></span>                <span class="p">[</span>
<span></span>                    <span class="n">tracker_box_pixels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">mask_width</span><span class="p">,</span>
<span></span>                    <span class="n">tracker_box_pixels</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">mask_height</span><span class="p">,</span>
<span></span>                    <span class="n">tracker_box_pixels</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="n">mask_width</span><span class="p">,</span>
<span></span>                    <span class="n">tracker_box_pixels</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">/</span> <span class="n">mask_height</span><span class="p">,</span>
<span></span>                <span class="p">],</span>
<span></span>                <span class="n">device</span><span class="o">=</span><span class="n">tracker_box_pixels</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>            <span class="p">)</span>
<span></span>
<span></span>            <span class="c1"># Compute IoU between detection and SAM2 tracklet bounding boxes</span>
<span></span>            <span class="n">det_box_batch</span> <span class="o">=</span> <span class="n">det_box</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span></span>            <span class="n">tracker_box_batch</span> <span class="o">=</span> <span class="n">tracker_box_normalized</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span></span>            <span class="n">iou</span> <span class="o">=</span> <span class="n">box_iou</span><span class="p">(</span><span class="n">det_box_batch</span><span class="p">,</span> <span class="n">tracker_box_batch</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span></span>
<span></span>            <span class="k">if</span> <span class="n">iou</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_bbox_iou_thresh</span> <span class="ow">and</span> <span class="n">det_score</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_bbox_det_score</span><span class="p">:</span>
<span></span>                <span class="n">should_recondition_iou</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>                <span class="n">reconditioned_obj_ids</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">trk_obj_id</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">should_recondition_periodic</span> <span class="o">=</span> <span class="p">(</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">recondition_every_nth_frame</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span></span>        <span class="ow">and</span> <span class="n">frame_idx</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">recondition_every_nth_frame</span> <span class="o">==</span> <span class="mi">0</span>
<span></span>        <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">trk_id_to_max_iou_high_conf_det</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Recondition if periodic or IoU condition met</span>
<span></span>    <span class="k">if</span> <span class="n">should_recondition_periodic</span> <span class="ow">or</span> <span class="n">should_recondition_iou</span><span class="p">:</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">_recondition_masklets</span><span class="p">(</span>
<span></span>            <span class="n">frame_idx</span><span class="p">,</span>
<span></span>            <span class="n">det_out</span><span class="p">,</span>
<span></span>            <span class="n">trk_id_to_max_iou_high_conf_det</span><span class="p">,</span>
<span></span>            <span class="n">tracker_states_local</span><span class="p">,</span>
<span></span>            <span class="n">tracker_metadata_prev</span><span class="p">,</span>
<span></span>            <span class="n">tracker_obj_scores_global</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Step 4: Run SAM2 memory encoder on the current frame's prediction masks</span>
<span></span>    <span class="c1"># This is done on all GPUs</span>
<span></span>    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tracker_low_res_masks_global</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">"_warm_up_complete"</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_warm_up_complete</span><span class="p">:</span>
<span></span>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">suppress_overlapping_based_on_recent_occlusion_threshold</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
<span></span>                <span class="c1"># NOTE: tracker_low_res_masks_global is updated in-place then returned</span>
<span></span>                <span class="n">tracker_low_res_masks_global</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_suppress_overlapping_based_on_recent_occlusion</span><span class="p">(</span>
<span></span>                    <span class="n">frame_idx</span><span class="p">,</span>
<span></span>                    <span class="n">tracker_low_res_masks_global</span><span class="p">,</span>
<span></span>                    <span class="n">tracker_metadata_prev</span><span class="p">,</span>
<span></span>                    <span class="n">tracker_metadata_new</span><span class="p">,</span>
<span></span>                    <span class="n">obj_ids_newly_removed</span><span class="p">,</span>
<span></span>                    <span class="n">reverse</span><span class="p">,</span>
<span></span>                <span class="p">)</span>
<span></span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">_tracker_update_memories</span><span class="p">(</span><span class="n">tracker_states_local</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">low_res_masks</span><span class="o">=</span><span class="n">tracker_low_res_masks_global</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Step 4: update the SAM2 metadata based on the update plan</span>
<span></span>    <span class="n">updated_obj_ids_this_gpu</span> <span class="o">=</span> <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_det_obj_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="n">updated_obj_ids_this_gpu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">updated_obj_ids_this_gpu</span><span class="p">,</span> <span class="n">new_det_obj_ids</span><span class="p">])</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">obj_ids_newly_removed</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="n">is_removed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">updated_obj_ids_this_gpu</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">obj_ids_newly_removed</span><span class="p">))</span>
<span></span>        <span class="n">updated_obj_ids_this_gpu</span> <span class="o">=</span> <span class="n">updated_obj_ids_this_gpu</span><span class="p">[</span><span class="o">~</span><span class="n">is_removed</span><span class="p">]</span>
<span></span>    <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span> <span class="o">=</span> <span class="n">updated_obj_ids_this_gpu</span>
<span></span>    <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"num_obj"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">updated_obj_ids_this_gpu</span><span class="p">)</span>
<span></span>    <span class="c1"># update object scores and the maximum object ID assigned so far</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_det_obj_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"obj_id_to_score"</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">new_det_obj_ids</span><span class="p">,</span> <span class="n">det_scores_np</span><span class="p">[</span><span class="n">new_det_fa_inds</span><span class="p">]))</span>
<span></span>        <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"obj_id_to_cls"</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">new_det_obj_ids</span><span class="p">,</span> <span class="n">det_cls_np</span><span class="p">[</span><span class="n">new_det_fa_inds</span><span class="p">]))</span>
<span></span>        <span class="c1"># tracker scores are not available for new objects, use det score instead.</span>
<span></span>        <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"obj_id_to_tracker_score_frame_wise"</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
<span></span>            <span class="nb">zip</span><span class="p">(</span><span class="n">new_det_obj_ids</span><span class="p">,</span> <span class="n">det_scores_np</span><span class="p">[</span><span class="n">new_det_fa_inds</span><span class="p">])</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"max_obj_id"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"max_obj_id"</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">new_det_obj_ids</span><span class="p">))</span>
<span></span>    <span class="c1"># for removed objects, we set their scores to a very low value (-1e4) but still</span>
<span></span>    <span class="c1"># keep them in "obj_id_to_score" (it's easier to handle outputs this way)</span>
<span></span>    <span class="k">for</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">obj_ids_newly_removed</span><span class="p">:</span>
<span></span>        <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"obj_id_to_score"</span><span class="p">][</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e4</span>
<span></span>        <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"obj_id_to_tracker_score_frame_wise"</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">][</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e4</span>
<span></span>        <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"obj_id_to_last_occluded"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">obj_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>    <span class="c1"># check that "metadata" is in tracker_metadata_new if and only if it's GPU 0</span>
<span></span>    <span class="k">assert</span> <span class="s2">"metadata"</span> <span class="ow">in</span> <span class="n">tracker_metadata_new</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">masklet_confirmation_enable</span><span class="p">:</span>
<span></span>        <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_masklet_confirmation_status</span><span class="p">(</span>
<span></span>            <span class="n">metadata</span><span class="o">=</span><span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"metadata"</span><span class="p">],</span>
<span></span>            <span class="n">obj_ids_all_gpu_prev</span><span class="o">=</span><span class="n">tracker_metadata_prev</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">],</span>
<span></span>            <span class="n">obj_ids_all_gpu_updated</span><span class="o">=</span><span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">],</span>
<span></span>            <span class="n">det_to_matched_trk_obj_ids</span><span class="o">=</span><span class="n">det_to_matched_trk_obj_ids</span><span class="p">,</span>
<span></span>            <span class="n">new_det_obj_ids</span><span class="o">=</span><span class="n">new_det_obj_ids</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="n">tracker_metadata_new</span><span class="p">[</span><span class="s2">"metadata"</span><span class="p">]</span> <span class="o">=</span> <span class="n">metadata</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">tracker_update_plan</span><span class="p">,</span> <span class="n">tracker_metadata_new</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.setup_model"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.setup_model</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</code></pre></div><p>Setup the SAM3VideoSemanticPredictor model.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>model</code></td><td></td><td></td><td><code>None</code></td></tr><tr><td><code>verbose</code></td><td></td><td></td><td><code>True</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2565-L2572"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Setup the SAM3VideoSemanticPredictor model."""</span>
<span></span>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">setup_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
<span></span>    <span class="kn">from</span><span class="w"> </span><span class="nn">.build_sam3</span><span class="w"> </span><span class="kn">import</span> <span class="n">build_interactive_sam3</span>
<span></span>
<span></span>    <span class="c1"># Initialize the SAM3 tracker model without backbone (backbone is handled in the detector)</span>
<span></span>    <span class="n">model</span> <span class="o">=</span> <span class="n">build_interactive_sam3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">with_backbone</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">setup_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.setup_source"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.setup_source</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_source</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">)</span>
</code></pre></div><p>Setup the source for the SAM3VideoSemanticPredictor model.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>source</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L2574-L2580"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_source</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Setup the source for the SAM3VideoSemanticPredictor model."""</span>
<span></span>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">setup_source</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">imgsz</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">set_imgsz</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">_bb_feat_sizes</span> <span class="o">=</span> <span class="p">[[</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">*</span> <span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">interpol_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">memory_encoder</span><span class="o">.</span><span class="n">mask_downsampler</span><span class="o">.</span><span class="n">interpol_size</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.update_masklet_confirmation_status"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM3VideoSemanticPredictor.update_masklet_confirmation_status</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">update_masklet_confirmation_status</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">metadata</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
<span></span>    <span class="n">obj_ids_all_gpu_prev</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">obj_ids_all_gpu_updated</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">det_to_matched_trk_obj_ids</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
<span></span>    <span class="n">new_det_obj_ids</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Update the confirmation status of masklets based on the current frame's detection results.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>metadata</code></td><td><code>dict[str, Any]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>obj_ids_all_gpu_prev</code></td><td><code>np.ndarray</code></td><td></td><td><em>required</em></td></tr><tr><td><code>obj_ids_all_gpu_updated</code></td><td><code>np.ndarray</code></td><td></td><td><em>required</em></td></tr><tr><td><code>det_to_matched_trk_obj_ids</code></td><td><code>dict[int, np.ndarray]</code></td><td></td><td><em>required</em></td></tr><tr><td><code>new_det_obj_ids</code></td><td><code>np.ndarray</code></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L3867-L3913"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">update_masklet_confirmation_status</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">metadata</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
<span></span>    <span class="n">obj_ids_all_gpu_prev</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">obj_ids_all_gpu_updated</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">det_to_matched_trk_obj_ids</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
<span></span>    <span class="n">new_det_obj_ids</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Update the confirmation status of masklets based on the current frame's detection results."""</span>
<span></span>    <span class="n">confirmation_data</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">"masklet_confirmation"</span><span class="p">]</span>
<span></span>
<span></span>    <span class="c1"># a) first, expand "confirmation_data" to include new masklets added in this frame</span>
<span></span>    <span class="n">status_prev</span> <span class="o">=</span> <span class="n">confirmation_data</span><span class="p">[</span><span class="s2">"status"</span><span class="p">]</span>
<span></span>    <span class="n">consecutive_det_num_prev</span> <span class="o">=</span> <span class="n">confirmation_data</span><span class="p">[</span><span class="s2">"consecutive_det_num"</span><span class="p">]</span>
<span></span>    <span class="k">assert</span> <span class="n">status_prev</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">obj_ids_all_gpu_prev</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span>
<span></span>        <span class="sa">f</span><span class="s2">"Got </span><span class="si">{</span><span class="n">status_prev</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> vs </span><span class="si">{</span><span class="n">obj_ids_all_gpu_prev</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="n">obj_id_to_updated_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">obj_id</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">obj_ids_all_gpu_updated</span><span class="p">)}</span>
<span></span>    <span class="n">prev_elem_is_in_updated</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">obj_ids_all_gpu_prev</span><span class="p">,</span> <span class="n">obj_ids_all_gpu_updated</span><span class="p">)</span>
<span></span>    <span class="n">prev_elem_obj_ids_in_updated</span> <span class="o">=</span> <span class="n">obj_ids_all_gpu_prev</span><span class="p">[</span><span class="n">prev_elem_is_in_updated</span><span class="p">]</span>
<span></span>    <span class="n">prev_elem_inds_in_updated</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span></span>        <span class="p">[</span><span class="n">obj_id_to_updated_idx</span><span class="p">[</span><span class="n">obj_id</span><span class="p">]</span> <span class="k">for</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="n">prev_elem_obj_ids_in_updated</span><span class="p">],</span>
<span></span>        <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="c1"># newly added masklets are initialized to "UNCONFIRMED" status</span>
<span></span>    <span class="n">unconfirmed_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">UNCONFIRMED</span>
<span></span>    <span class="n">status</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">obj_ids_all_gpu_updated</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="n">unconfirmed_val</span><span class="p">)</span>
<span></span>    <span class="n">status</span><span class="p">[</span><span class="n">prev_elem_inds_in_updated</span><span class="p">]</span> <span class="o">=</span> <span class="n">status_prev</span><span class="p">[</span><span class="n">prev_elem_is_in_updated</span><span class="p">]</span>
<span></span>    <span class="n">consecutive_det_num</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">obj_ids_all_gpu_updated</span><span class="p">)</span>
<span></span>    <span class="n">consecutive_det_num</span><span class="p">[</span><span class="n">prev_elem_inds_in_updated</span><span class="p">]</span> <span class="o">=</span> <span class="n">consecutive_det_num_prev</span><span class="p">[</span><span class="n">prev_elem_is_in_updated</span><span class="p">]</span>
<span></span>
<span></span>    <span class="c1"># b) update the confirmation status of all masklets based on the current frame</span>
<span></span>    <span class="c1"># b.1) update "consecutive_det_num"</span>
<span></span>    <span class="c1"># "is_matched": whether a masklet is matched to a detection on this frame</span>
<span></span>    <span class="n">is_matched</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">obj_ids_all_gpu_updated</span><span class="p">,</span> <span class="n">new_det_obj_ids</span><span class="p">)</span>
<span></span>    <span class="k">for</span> <span class="n">matched_trk_obj_ids</span> <span class="ow">in</span> <span class="n">det_to_matched_trk_obj_ids</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>        <span class="n">is_matched</span> <span class="o">|=</span> <span class="n">np</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">obj_ids_all_gpu_updated</span><span class="p">,</span> <span class="n">matched_trk_obj_ids</span><span class="p">)</span>
<span></span>    <span class="n">consecutive_det_num</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_matched</span><span class="p">,</span> <span class="n">consecutive_det_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># b.2) update "status"</span>
<span></span>    <span class="n">change_to_confirmed</span> <span class="o">=</span> <span class="n">consecutive_det_num</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">masklet_confirmation_consecutive_det_thresh</span>
<span></span>    <span class="n">status</span><span class="p">[</span><span class="n">change_to_confirmed</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">CONFIRMED</span>
<span></span>
<span></span>    <span class="n">confirmation_data</span><span class="p">[</span><span class="s2">"status"</span><span class="p">]</span> <span class="o">=</span> <span class="n">status</span>
<span></span>    <span class="n">confirmation_data</span><span class="p">[</span><span class="s2">"consecutive_det_num"</span><span class="p">]</span> <span class="o">=</span> <span class="n">consecutive_det_num</span>
<span></span>    <span class="k">return</span> <span class="n">metadata</span>
</code></pre></div></details><p><br/><br/></p><br/><br/><div class="git-info"><div class="dates-container"><span class="date-item" title="This page was first created on November 12, 2023"><span class="hover-item">ğŸ“…</span> Created 2 years ago </span><span class="date-item" title="This page was last updated on December 12, 2025"><span class="hover-item">âœï¸</span> Updated 1 month ago </span></div><div class="authors-container"><a class="author-link" href="https://github.com/glenn-jocher" title="glenn-jocher (7 changes)"><img alt="glenn-jocher" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/26833433?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/Laughing-q" title="Laughing-q (3 changes)"><img alt="Laughing-q" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/61612323?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/ShuaiLYU" title="ShuaiLYU (1 change)"><img alt="ShuaiLYU" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/31230805?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/jk4e" title="jk4e (1 change)"><img alt="jk4e" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/116908874?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/Burhan-Q" title="Burhan-Q (1 change)"><img alt="Burhan-Q" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/62214284?v=4&amp;s=96"/></a></div></div><div class="share-buttons"><button class="share-button hover-item" onclick="window.open('https://twitter.com/intent/tweet?url=https%3A%2F%2Fdocs.ultralytics.com%2Freference%2Fmodels%2Fsam%2Fpredict%2F', 'TwitterShare', 'width=550,height=680,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-x-twitter"></i> Tweet </button><button class="share-button hover-item linkedin" onclick="window.open('https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fdocs.ultralytics.com%2Freference%2Fmodels%2Fsam%2Fpredict%2F', 'LinkedinShare', 'width=550,height=730,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-linkedin-in"></i> Share </button></div><br/></article></div>
<script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div><button class="md-top md-icon" data-md-component="top" hidden="" type="button"><svg class="lucide lucide-circle-arrow-up" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><path d="m16 12-4-4-4 4M12 16V8"></path></svg> Back to top
</button></main><footer class="md-footer"><nav aria-label="Footer" class="md-footer__inner md-grid"><a aria-label="Previous: utils" class="md-footer__link md-footer__link--prev" href="../modules/utils/"><div class="md-footer__button md-icon"><svg class="lucide lucide-arrow-left" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m12 19-7-7 7-7M19 12H5"></path></svg></div><div class="md-footer__title"><span class="md-footer__direction"> Previous </span><div class="md-ellipsis"> utils </div></div></a><a aria-label="Next: decoder" class="md-footer__link md-footer__link--next" href="../sam3/decoder/"><div class="md-footer__title"><span class="md-footer__direction"> Next </span><div class="md-ellipsis"> decoder </div></div><div class="md-footer__button md-icon"><svg class="lucide lucide-arrow-right" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M5 12h14M12 5l7 7-7 7"></path></svg></div></a></nav><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class="md-copyright"><div class="md-copyright__highlight"><a href="https://www.ultralytics.com/" target="_blank">Â© 2026 Ultralytics Inc.</a> All rights reserved. </div> Made with <a href="https://zensical.org/" rel="noopener" target="_blank"> Zensical </a></div><div class="md-social"><a class="md-social__link" href="https://github.com/ultralytics" rel="noopener" target="_blank" title="github.com"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://www.linkedin.com/company/ultralytics/" rel="noopener" target="_blank" title="www.linkedin.com"><svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://x.com/ultralytics" rel="noopener" target="_blank" title="x.com"><svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://youtube.com/ultralytics?sub_confirmation=1" rel="noopener" target="_blank" title="youtube.com"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M549.7 124.1c-6.2-23.7-24.8-42.3-48.3-48.6C458.9 64 288.1 64 288.1 64S117.3 64 74.7 75.5c-23.5 6.3-42 24.9-48.3 48.6C15 167 15 256.4 15 256.4s0 89.4 11.4 132.3c6.3 23.6 24.8 41.5 48.3 47.8C117.3 448 288.1 448 288.1 448s170.8 0 213.4-11.5c23.5-6.3 42-24.2 48.3-47.8 11.4-42.9 11.4-132.3 11.4-132.3s0-89.4-11.4-132.3zM232.2 337.6V175.2l142.7 81.2z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://hub.docker.com/r/ultralytics/ultralytics/" rel="noopener" target="_blank" title="hub.docker.com"><svg viewbox="0 0 640 512" xmlns="http://www.w3.org/2000/svg"><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://pypi.org/project/ultralytics/" rel="noopener" target="_blank" title="pypi.org"><svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 444.7a20.4 20.4 0 1 1 0-40.7 20.4 20.4 0 1 1 0 40.7M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.6-183.4a20.4 20.4 0 1 1 0 40.8 20.4 20.4 0 1 1 0-40.8" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://discord.com/invite/ultralytics" rel="noopener" target="_blank" title="discord.com"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M492.5 69.8c-.2-.3-.4-.6-.8-.7-38.1-17.5-78.4-30-119.7-37.1-.4-.1-.8 0-1.1.1s-.6.4-.8.8c-5.5 9.9-10.5 20.2-14.9 30.6-44.6-6.8-89.9-6.8-134.4 0-4.5-10.5-9.5-20.7-15.1-30.6-.2-.3-.5-.6-.8-.8s-.7-.2-1.1-.2C162.5 39 122.2 51.5 84.1 69c-.3.1-.6.4-.8.7C7.1 183.5-13.8 294.6-3.6 404.2c0 .3.1.5.2.8s.3.4.5.6c44.4 32.9 94 58 146.8 74.2.4.1.8.1 1.1 0s.7-.4.9-.7c11.3-15.4 21.4-31.8 30-48.8.1-.2.2-.5.2-.8s0-.5-.1-.8-.2-.5-.4-.6-.4-.3-.7-.4c-15.8-6.1-31.2-13.4-45.9-21.9-.3-.2-.5-.4-.7-.6s-.3-.6-.3-.9 0-.6.2-.9.3-.5.6-.7c3.1-2.3 6.2-4.7 9.1-7.1.3-.2.6-.4.9-.4s.7 0 1 .1c96.2 43.9 200.4 43.9 295.5 0 .3-.1.7-.2 1-.2s.7.2.9.4c2.9 2.4 6 4.9 9.1 7.2.2.2.4.4.6.7s.2.6.2.9-.1.6-.3.9-.4.5-.6.6c-14.7 8.6-30 15.9-45.9 21.8-.2.1-.5.2-.7.4s-.3.4-.4.7-.1.5-.1.8.1.5.2.8c8.8 17 18.8 33.3 30 48.8.2.3.6.6.9.7s.8.1 1.1 0c52.9-16.2 102.6-41.3 147.1-74.2.2-.2.4-.4.5-.6s.2-.5.2-.8c12.3-126.8-20.5-236.9-86.9-334.5zm-302 267.7c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.4 59.2-52.8 59.2m195.4 0c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.2 59.2-52.8 59.2" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://reddit.com/r/ultralytics" rel="noopener" target="_blank" title="reddit.com"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M0 256C0 114.6 114.6 0 256 0s256 114.6 256 256-114.6 256-256 256H37.1c-13.7 0-20.5-16.5-10.9-26.2L75 437C28.7 390.7 0 326.7 0 256m349.6-102.4c23.6 0 42.7-19.1 42.7-42.7s-19.1-42.7-42.7-42.7c-20.6 0-37.8 14.6-41.8 34-34.5 3.7-61.4 33-61.4 68.4v.2c-37.5 1.6-71.8 12.3-99 29.1-10.1-7.8-22.8-12.5-36.5-12.5-33 0-59.8 26.8-59.8 59.8 0 24 14.1 44.6 34.4 54.1 2 69.4 77.6 125.2 170.6 125.2s168.7-55.9 170.6-125.3c20.2-9.6 34.1-30.2 34.1-54 0-33-26.8-59.8-59.8-59.8-13.7 0-26.3 4.6-36.4 12.4-27.4-17-62.1-27.7-100-29.1v-.2c0-25.4 18.9-46.5 43.4-49.9 4.4 18.8 21.3 32.8 41.5 32.8zm-172.5 93.3c16.7 0 29.5 17.6 28.5 39.3s-13.5 29.6-30.3 29.6-31.4-8.8-30.4-30.5S160.3 247 177 247zm190.1 38.3c1 21.7-13.7 30.5-30.4 30.5s-29.3-7.9-30.3-29.6 11.8-39.3 28.5-39.3 31.2 16.6 32.1 38.3zm-48.1 56.7c-10.3 24.6-34.6 41.9-63 41.9s-52.7-17.3-63-41.9c-1.2-2.9.8-6.2 3.9-6.5 18.4-1.9 38.3-2.9 59.1-2.9s40.7 1 59.1 2.9c3.1.3 5.1 3.6 3.9 6.5" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://weixin.qq.com/r/mp/LxckPDfEgWr_rXNf90I9" rel="noopener" target="_blank" title="weixin.qq.com"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M385.2 167.6c6.4 0 12.6.3 18.8 1.1C387.4 90.3 303.3 32 207.7 32 100.5 32 13 104.8 13 197.4c0 53.4 29.3 97.5 77.9 131.6l-19.3 58.6 68-34.1c24.4 4.8 43.8 9.7 68.2 9.7 6.2 0 12.1-.3 18.3-.8-4-12.9-6.2-26.6-6.2-40.8-.1-84.9 72.9-154 165.3-154m-104.5-52.9c14.5 0 24.2 9.7 24.2 24.4 0 14.5-9.7 24.2-24.2 24.2-14.8 0-29.3-9.7-29.3-24.2.1-14.7 14.6-24.4 29.3-24.4m-136.4 48.6c-14.5 0-29.3-9.7-29.3-24.2 0-14.8 14.8-24.4 29.3-24.4 14.8 0 24.4 9.7 24.4 24.4 0 14.6-9.6 24.2-24.4 24.2M563 319.4c0-77.9-77.9-141.3-165.4-141.3-92.7 0-165.4 63.4-165.4 141.3S305 460.7 397.6 460.7c19.3 0 38.9-5.1 58.6-9.9l53.4 29.3-14.8-48.6C534 402.1 563 363.2 563 319.4m-219.1-24.5c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.8 0 24.4 9.7 24.4 19.3 0 10-9.7 19.6-24.4 19.6m107.1 0c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.5 0 24.4 9.7 24.4 19.3.1 10-9.9 19.6-24.4 19.6" fill="currentColor"></path></svg></a></div></div></div></footer></div><div class="md-dialog" data-md-component="dialog"><div class="md-dialog__inner md-typeset"></div></div><div class="md-progress" data-md-component="progress" role="progressbar"></div>
<script id="__config" type="application/json">{"annotate":null,"base":"../../../..","features":["content.action.edit","content.code.annotate","content.code.copy","content.tooltips","toc.follow","navigation.top","navigation.tabs","navigation.tabs.sticky","navigation.prune","navigation.footer","navigation.tracking","navigation.instant","navigation.instant.progress","navigation.indexes","navigation.sections","content.tabs.link"],"search":"../../../../assets/javascripts/workers/search.e2d2d235.min.js","tags":null,"translations":{"clipboard.copied":"Copied to clipboard","clipboard.copy":"Copy to clipboard","search.result.more.one":"1 more on this page","search.result.more.other":"# more on this page","search.result.none":"No matching documents","search.result.one":"1 matching document","search.result.other":"# matching documents","search.result.placeholder":"Type to start searching","search.result.term.missing":"Missing","select.version":"Select version"},"version":null}</script>
<script src="../../../../assets/javascripts/bundle.00c35305.min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/ultralytics/llm@v0.1.8/js/chat.min.js"></script>
<script src="https://unpkg.com/tablesort@5.6.0/dist/tablesort.min.js"></script>
<script src="../../../../javascript/extra.js"></script>
<script src="../../../../javascript/giscus.js"></script>
<script src="../../../../javascript/tablesort.js"></script>
</body></html>