 <!DOCTYPE html><html class="no-js" lang="en"><head><meta charset="utf-8"/><meta content="width=device-width,initial-scale=1" name="viewport"/><meta content="Explore Ultralytics SAM and SAM 2 Predictor for advanced, real-time image segmentation using the Segment Anything Model (SAM and SAM 2). Complete implementation details and auxiliary utilities." name="description"/><meta content="Ultralytics" name="author"/><link href="https://docs.ultralytics.com/reference/models/sam/predict/" rel="canonical"/><link href="../modules/utils/" rel="prev"/><link href="../../utils/loss/" rel="next"/><link href="/" hreflang="en" rel="alternate"/><link href="/zh/" hreflang="zh" rel="alternate"/><link href="/ko/" hreflang="ko" rel="alternate"/><link href="/ja/" hreflang="ja" rel="alternate"/><link href="/ru/" hreflang="ru" rel="alternate"/><link href="/de/" hreflang="de" rel="alternate"/><link href="/fr/" hreflang="fr" rel="alternate"/><link href="/es/" hreflang="es" rel="alternate"/><link href="/pt/" hreflang="pt" rel="alternate"/><link href="/it/" hreflang="it" rel="alternate"/><link href="/tr/" hreflang="tr" rel="alternate"/><link href="/vi/" hreflang="vi" rel="alternate"/><link href="/ar/" hreflang="ar" rel="alternate"/><link href="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/logo/favicon-yolo.png" rel="icon"/><meta content="zensical-0.0.9" name="generator"/><title>Reference for ultralytics/models/sam/predict.py</title><link href="../../../../assets/stylesheets/modern/main.2644c6b7.min.css" rel="stylesheet"/><link href="../../../../assets/stylesheets/modern/palette.dfe2e883.min.css" rel="stylesheet"/><style>:root{}</style><link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/><link href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,500,500i,700,700i%7CJetBrains+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/><style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style><link href="../../../../stylesheets/style.css" rel="stylesheet"/>
<script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,t)=>(e<<5)-e+t.charCodeAt(0)),0),__md_get=(e,t=localStorage,a=__md_scope)=>JSON.parse(t.getItem(a.pathname+"."+e)),__md_set=(e,t,a=localStorage,_=__md_scope)=>{try{a.setItem(_.pathname+"."+e,JSON.stringify(t))}catch(e){}},document.documentElement.setAttribute("data-platform",navigator.platform)</script>
<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-2M5EHKC0BH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-2M5EHKC0BH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-2M5EHKC0BH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
<style data-doc-kind="true">.doc-kind{display:inline-flex;align-items:center;gap:0.25em;padding:0.21em 0.59em;border-radius:999px;font-weight:700;font-size:0.81em;letter-spacing:0.06em;text-transform:uppercase;line-height:1;color:var(--doc-kind-color,#f8fafc);background:var(--doc-kind-bg,rgba(255,255,255,0.12));}.doc-kind-class{--doc-kind-color:#039dfc;--doc-kind-bg:rgba(3,157,252,0.22);}.doc-kind-function{--doc-kind-color:#fc9803;--doc-kind-bg:rgba(252,152,3,0.22);}.doc-kind-method{--doc-kind-color:#ef5eff;--doc-kind-bg:rgba(239,94,255,0.22);}.doc-kind-property{--doc-kind-color:#02e835;--doc-kind-bg:rgba(2,232,53,0.22);}</style><meta content="Reference for ultralytics/models/sam/predict.py" name="title"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet"/><meta content="website" property="og:type"/><meta content="https://docs.ultralytics.com/reference/models/sam/predict/" property="og:url"/><meta content="Reference for ultralytics/models/sam/predict.py" property="og:title"/><meta content="" property="og:description"/><meta content="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" property="og:image"/><meta content="summary_large_image" property="twitter:card"/><meta content="https://docs.ultralytics.com/reference/models/sam/predict/" property="twitter:url"/><meta content="Reference for ultralytics/models/sam/predict.py" property="twitter:title"/><meta content="" property="twitter:description"/><meta content="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" property="twitter:image"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": "Article", "headline": "Reference for ultralytics/models/sam/predict.py", "image": ["https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png"], "datePublished": "2023-11-12 02:49:37 +0100", "dateModified": "2025-11-23 19:53:50 +0100", "author": [{"@type": "Organization", "name": "Ultralytics", "url": "https://ultralytics.com/"}], "abstract": ""}</script></head><body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="default" dir="ltr"><input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/><input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/><label class="md-overlay" for="__drawer"></label><div data-md-component="skip"><a class="md-skip" href="#reference-for-ultralyticsmodelssampredictpy"> Skip to content </a></div><div data-md-component="announce"><aside class="md-banner"><div class="md-banner__inner md-grid md-typeset"><a class="banner-wrapper" href="https://www.ultralytics.com/news/ultralytics-raises-30m-series-a" target="_blank"><div class="banner-content-wrapper"><img alt="Ultralytics raises $30M Series A" height="40" src="https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/691dac336b5de2ae8b398bca_writting.svg"/><div class="vc-wrapper"><img alt="Elephant" height="28" src="https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/691dac33c95408144846afc7_image%201.png"/><img alt="SquareOne" height="28" src="https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/691dac3333068d9632cc6df8_image%202.png"/></div></div></a></div></aside></div><header class="md-header md-header--shadow md-header--lifted" data-md-component="header"><nav aria-label="Header" class="md-header__inner md-grid"><a aria-label="Ultralytics YOLO Docs" class="md-header__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs"><img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/></a><label class="md-header__button md-icon" for="__drawer"><svg class="lucide lucide-menu" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 5h16M4 12h16M4 19h16"></path></svg></label><div class="md-header__title" data-md-component="header-title"><div class="md-header__ellipsis"><div class="md-header__topic"><span class="md-ellipsis"> Ultralytics YOLO Docs </span></div><div class="md-header__topic" data-md-component="header-topic"><span class="md-ellipsis"> Reference for ultralytics/models/sam/predict.py </span></div></div></div><form class="md-header__option" data-md-component="palette"><input aria-label="Switch to light mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_0" name="__palette" type="radio"/><label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg></label><input aria-label="Switch to system preference" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="black" data-md-color-scheme="slate" id="__palette_1" name="__palette" type="radio"/><label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to system preference"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label><input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_2" name="__palette" type="radio"/><label class="md-header__button md-icon" for="__palette_0" hidden="" title="Switch to dark mode"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg></label></form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<div class="md-header__option"><div class="md-select"><button aria-label="Select language" class="md-header__button md-icon"><svg class="lucide lucide-languages" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m5 8 6 6M4 14l6-6 2-3M2 5h12M7 2h1M22 22l-5-10-5 10M14 18h6"></path></svg></button><div class="md-select__inner"><ul class="md-select__list"><li class="md-select__item"><a class="md-select__link" href="/" hreflang="en"> ğŸ‡¬ğŸ‡§ English </a></li><li class="md-select__item"><a class="md-select__link" href="/zh/" hreflang="zh"> ğŸ‡¨ğŸ‡³ ç®€ä½“ä¸­æ–‡ </a></li><li class="md-select__item"><a class="md-select__link" href="/ko/" hreflang="ko"> ğŸ‡°ğŸ‡· í•œêµ­ì–´ </a></li><li class="md-select__item"><a class="md-select__link" href="/ja/" hreflang="ja"> ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª </a></li><li class="md-select__item"><a class="md-select__link" href="/ru/" hreflang="ru"> ğŸ‡·ğŸ‡º Ğ ÑƒÑÑĞºĞ¸Ğ¹ </a></li><li class="md-select__item"><a class="md-select__link" href="/de/" hreflang="de"> ğŸ‡©ğŸ‡ª Deutsch </a></li><li class="md-select__item"><a class="md-select__link" href="/fr/" hreflang="fr"> ğŸ‡«ğŸ‡· FranÃ§ais </a></li><li class="md-select__item"><a class="md-select__link" href="/es/" hreflang="es"> ğŸ‡ªğŸ‡¸ EspaÃ±ol </a></li><li class="md-select__item"><a class="md-select__link" href="/pt/" hreflang="pt"> ğŸ‡µğŸ‡¹ PortuguÃªs </a></li><li class="md-select__item"><a class="md-select__link" href="/it/" hreflang="it"> ğŸ‡®ğŸ‡¹ Italiano </a></li><li class="md-select__item"><a class="md-select__link" href="/tr/" hreflang="tr"> ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e </a></li><li class="md-select__item"><a class="md-select__link" href="/vi/" hreflang="vi"> ğŸ‡»ğŸ‡³ Tiáº¿ng Viá»‡t </a></li><li class="md-select__item"><a class="md-select__link" href="/ar/" hreflang="ar"> ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© </a></li></ul></div></div></div><label class="md-header__button md-icon" for="__search"><svg class="lucide lucide-search" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m21 21-4.34-4.34"></path><circle cx="11" cy="11" r="8"></circle></svg></label><div aria-label="Search" class="md-search" data-md-component="search" role="dialog"><button class="md-search__button" type="button"> Search </button></div><div class="md-header__source"><a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository"><div class="md-source__icon md-icon"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg></div><div class="md-source__repository"> ultralytics/ultralytics </div></a></div></nav><nav aria-label="Tabs" class="md-tabs" data-md-component="tabs"><div class="md-grid"><ul class="md-tabs__list"><li class="md-tabs__item"><a class="md-tabs__link" href="../../../.."> Home </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../quickstart/"> Quickstart </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../modes/"> Modes </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../tasks/"> Tasks </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../models/"> Models </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../datasets/"> Datasets </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../solutions/"> Solutions ğŸš€ </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../guides/"> Guides </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../integrations/"> Integrations </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../hub/"> HUB </a></li><li class="md-tabs__item md-tabs__item--active"><a class="md-tabs__link" href="../../../__init__/"> Reference </a></li><li class="md-tabs__item"><a class="md-tabs__link" href="../../../../help/"> Help </a></li></ul></div></nav></header><div class="md-container" data-md-component="container"><main class="md-main" data-md-component="main"><div class="md-main__inner md-grid"><div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0"><label class="md-nav__title" for="__drawer"><a aria-label="Ultralytics YOLO Docs" class="md-nav__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs"><img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/></a> Ultralytics YOLO Docs </label><div class="md-nav__source"><a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository"><div class="md-source__icon md-icon"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg></div><div class="md-source__repository"> ultralytics/ultralytics </div></a></div><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../.."><span class="md-ellipsis"> Home </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../quickstart/"><span class="md-ellipsis"> Quickstart </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../modes/"><span class="md-ellipsis"> none </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../tasks/"><span class="md-ellipsis"> Computer Vision Tasks supported by Ultralytics YOLO11 </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../models/"><span class="md-ellipsis"> Models Supported by Ultralytics </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../datasets/"><span class="md-ellipsis"> Datasets Overview </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../solutions/"><span class="md-ellipsis"> Ultralytics Solutions: Harness YOLO11 to Solve Real-World Problems </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../guides/"><span class="md-ellipsis"> Comprehensive Tutorials for Ultralytics YOLO </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../integrations/"><span class="md-ellipsis"> Ultralytics Integrations </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../hub/"><span class="md-ellipsis"> Ultralytics HUB </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"><input checked="" class="md-nav__toggle md-toggle" id="__nav_11" type="checkbox"/><label class="md-nav__link" for="__nav_11" id="__nav_11_label" tabindex=""><span class="md-ellipsis"> Reference </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="true" aria-labelledby="__nav_11_label" class="md-nav" data-md-level="1"><label class="md-nav__title" for="__nav_11"><span class="md-nav__icon md-icon"></span> Reference </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../__init__/"><span class="md-ellipsis"> __init__ </span></a></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_2" type="checkbox"/><label class="md-nav__link" for="__nav_11_2" id="__nav_11_2_label" tabindex=""><span class="md-ellipsis"> cfg </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_2_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_2"><span class="md-nav__icon md-icon"></span> cfg </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../cfg/__init__/"><span class="md-ellipsis"> __init__ </span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_3" type="checkbox"/><label class="md-nav__link" for="__nav_11_3" id="__nav_11_3_label" tabindex=""><span class="md-ellipsis"> data </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_3_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_3"><span class="md-nav__icon md-icon"></span> data </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../data/annotator/"><span class="md-ellipsis"> annotator </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/augment/"><span class="md-ellipsis"> augment </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/base/"><span class="md-ellipsis"> base </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/build/"><span class="md-ellipsis"> build </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/converter/"><span class="md-ellipsis"> converter </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/dataset/"><span class="md-ellipsis"> dataset </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/loaders/"><span class="md-ellipsis"> loaders </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/split/"><span class="md-ellipsis"> split </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/split_dota/"><span class="md-ellipsis"> split_dota </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../data/utils/"><span class="md-ellipsis"> utils </span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_4" type="checkbox"/><label class="md-nav__link" for="__nav_11_4" id="__nav_11_4_label" tabindex=""><span class="md-ellipsis"> engine </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_4_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_4"><span class="md-nav__icon md-icon"></span> engine </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../engine/exporter/"><span class="md-ellipsis"> exporter </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../engine/model/"><span class="md-ellipsis"> model </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../engine/predictor/"><span class="md-ellipsis"> predictor </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../engine/results/"><span class="md-ellipsis"> results </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../engine/trainer/"><span class="md-ellipsis"> trainer </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../engine/tuner/"><span class="md-ellipsis"> tuner </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../engine/validator/"><span class="md-ellipsis"> validator </span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_5" type="checkbox"/><label class="md-nav__link" for="__nav_11_5" id="__nav_11_5_label" tabindex=""><span class="md-ellipsis"> hub </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_5_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_5"><span class="md-nav__icon md-icon"></span> hub </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../hub/__init__/"><span class="md-ellipsis"> __init__ </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../hub/auth/"><span class="md-ellipsis"> auth </span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../hub/google/__init__/"><span class="md-ellipsis"> google </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../hub/session/"><span class="md-ellipsis"> session </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../hub/utils/"><span class="md-ellipsis"> utils </span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"><input checked="" class="md-nav__toggle md-toggle" id="__nav_11_6" type="checkbox"/><label class="md-nav__link" for="__nav_11_6" id="__nav_11_6_label" tabindex=""><span class="md-ellipsis"> models </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="true" aria-labelledby="__nav_11_6_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_6"><span class="md-nav__icon md-icon"></span> models </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../fastsam/model/"><span class="md-ellipsis"> fastsam </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../nas/model/"><span class="md-ellipsis"> nas </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../rtdetr/model/"><span class="md-ellipsis"> rtdetr </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input checked="" class="md-nav__toggle md-toggle" id="__nav_11_6_4" type="checkbox"/><label class="md-nav__link" for="__nav_11_6_4" id="__nav_11_6_4_label" tabindex="0"><span class="md-ellipsis"> sam </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="true" aria-labelledby="__nav_11_6_4_label" class="md-nav" data-md-level="3"><label class="md-nav__title" for="__nav_11_6_4"><span class="md-nav__icon md-icon"></span> sam </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../amg/"><span class="md-ellipsis"> amg </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../build/"><span class="md-ellipsis"> build </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../model/"><span class="md-ellipsis"> model </span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../modules/blocks/"><span class="md-ellipsis"> modules </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--active"><input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/><label class="md-nav__link md-nav__link--active" for="__toc"><span class="md-ellipsis"> predict </span><span class="md-nav__icon md-icon"></span></label><a class="md-nav__link md-nav__link--active" href="./"><span class="md-ellipsis"> predict </span></a><nav aria-label="On this page" class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc"><span class="md-nav__icon md-icon"></span> On this page </label><ul class="md-nav__list" data-md-component="toc" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> Predictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.Predictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor._inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor._prepare_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prepare_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.generate"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> generate</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.postprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> postprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.pre_transform"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> pre_transform</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.preprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> preprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.prompt_inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> prompt_inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.remove_small_regions"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> remove_small_regions</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.reset_image"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> reset_image</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.set_image"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> set_image</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.set_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> set_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.setup_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.setup_source"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_source</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM2Predictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM2Predictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor._inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor._prepare_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prepare_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor.set_image"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> set_image</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM2VideoPredictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM2VideoPredictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._add_output_per_object"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _add_output_per_object</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._clear_non_cond_mem_around_input"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _clear_non_cond_mem_around_input</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._consolidate_temp_output_across_obj"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _consolidate_temp_output_across_obj</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_empty_mask_ptr"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _get_empty_mask_ptr</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_maskmem_pos_enc"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _get_maskmem_pos_enc</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._obj_id_to_idx"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _obj_id_to_idx</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_memory_encoder"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _run_memory_encoder</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_single_frame_inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _run_single_frame_inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.add_new_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> add_new_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.init_state"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> init_state</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.postprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> postprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.propagate_in_video_preflight"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> propagate_in_video_preflight</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM2DynamicInteractivePredictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._obj_id_to_idx"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _obj_id_to_idx</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._prepare_memory_conditioned_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prepare_memory_conditioned_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_maskmem_enc"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_maskmem_enc</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.track_step"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> track_step</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.update_memory"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> update_memory</span></a></li></ul></nav></li></ul></nav></li></ul></nav></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../utils/loss/"><span class="md-ellipsis"> utils </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../yolo/classify/predict/"><span class="md-ellipsis"> yolo </span><span class="md-nav__icon md-icon"></span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_7" type="checkbox"/><label class="md-nav__link" for="__nav_11_7" id="__nav_11_7_label" tabindex=""><span class="md-ellipsis"> nn </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_7_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_7"><span class="md-nav__icon md-icon"></span> nn </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../nn/autobackend/"><span class="md-ellipsis"> autobackend </span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../nn/modules/activation/"><span class="md-ellipsis"> modules </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../nn/tasks/"><span class="md-ellipsis"> tasks </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../nn/text_model/"><span class="md-ellipsis"> text_model </span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_8" type="checkbox"/><label class="md-nav__link" for="__nav_11_8" id="__nav_11_8_label" tabindex=""><span class="md-ellipsis"> solutions </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_8_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_8"><span class="md-nav__icon md-icon"></span> solutions </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/ai_gym/"><span class="md-ellipsis"> ai_gym </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/analytics/"><span class="md-ellipsis"> analytics </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/config/"><span class="md-ellipsis"> config </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/distance_calculation/"><span class="md-ellipsis"> distance_calculation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/heatmap/"><span class="md-ellipsis"> heatmap </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/instance_segmentation/"><span class="md-ellipsis"> instance_segmentation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/object_blurrer/"><span class="md-ellipsis"> object_blurrer </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/object_counter/"><span class="md-ellipsis"> object_counter </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/object_cropper/"><span class="md-ellipsis"> object_cropper </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/parking_management/"><span class="md-ellipsis"> parking_management </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/queue_management/"><span class="md-ellipsis"> queue_management </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/region_counter/"><span class="md-ellipsis"> region_counter </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/security_alarm/"><span class="md-ellipsis"> security_alarm </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/similarity_search/"><span class="md-ellipsis"> similarity_search </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/solutions/"><span class="md-ellipsis"> solutions </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/speed_estimation/"><span class="md-ellipsis"> speed_estimation </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/streamlit_inference/"><span class="md-ellipsis"> streamlit_inference </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/trackzone/"><span class="md-ellipsis"> trackzone </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../solutions/vision_eye/"><span class="md-ellipsis"> vision_eye </span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_9" type="checkbox"/><label class="md-nav__link" for="__nav_11_9" id="__nav_11_9_label" tabindex=""><span class="md-ellipsis"> trackers </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_9_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_9"><span class="md-nav__icon md-icon"></span> trackers </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../trackers/basetrack/"><span class="md-ellipsis"> basetrack </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../trackers/bot_sort/"><span class="md-ellipsis"> bot_sort </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../trackers/byte_tracker/"><span class="md-ellipsis"> byte_tracker </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../trackers/track/"><span class="md-ellipsis"> track </span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../trackers/utils/gmc/"><span class="md-ellipsis"> utils </span><span class="md-nav__icon md-icon"></span></a></li></ul></nav></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle" id="__nav_11_10" type="checkbox"/><label class="md-nav__link" for="__nav_11_10" id="__nav_11_10_label" tabindex=""><span class="md-ellipsis"> utils </span><span class="md-nav__icon md-icon"></span></label><nav aria-expanded="false" aria-labelledby="__nav_11_10_label" class="md-nav" data-md-level="2"><label class="md-nav__title" for="__nav_11_10"><span class="md-nav__icon md-icon"></span> utils </label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/__init__/"><span class="md-ellipsis"> __init__ </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/autobatch/"><span class="md-ellipsis"> autobatch </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/autodevice/"><span class="md-ellipsis"> autodevice </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/benchmarks/"><span class="md-ellipsis"> benchmarks </span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../utils/callbacks/base/"><span class="md-ellipsis"> callbacks </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/checks/"><span class="md-ellipsis"> checks </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/cpu/"><span class="md-ellipsis"> cpu </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/dist/"><span class="md-ellipsis"> dist </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/downloads/"><span class="md-ellipsis"> downloads </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/errors/"><span class="md-ellipsis"> errors </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/events/"><span class="md-ellipsis"> events </span></a></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../utils/export/engine/"><span class="md-ellipsis"> export </span><span class="md-nav__icon md-icon"></span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/files/"><span class="md-ellipsis"> files </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/git/"><span class="md-ellipsis"> git </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/instance/"><span class="md-ellipsis"> instance </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/logger/"><span class="md-ellipsis"> logger </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/loss/"><span class="md-ellipsis"> loss </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/metrics/"><span class="md-ellipsis"> metrics </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/nms/"><span class="md-ellipsis"> nms </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/ops/"><span class="md-ellipsis"> ops </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/patches/"><span class="md-ellipsis"> patches </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/plotting/"><span class="md-ellipsis"> plotting </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/tal/"><span class="md-ellipsis"> tal </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/torch_utils/"><span class="md-ellipsis"> torch_utils </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/tqdm/"><span class="md-ellipsis"> tqdm </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/triton/"><span class="md-ellipsis"> triton </span></a></li><li class="md-nav__item"><a class="md-nav__link" href="../../../utils/tuner/"><span class="md-ellipsis"> tuner </span></a></li></ul></nav></li></ul></nav></li><li class="md-nav__item md-nav__item--pruned md-nav__item--nested"><a class="md-nav__link" href="../../../../help/"><span class="md-ellipsis"> Help </span><span class="md-nav__icon md-icon"></span></a></li></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav aria-label="On this page" class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc"><span class="md-nav__icon md-icon"></span> On this page </label><ul class="md-nav__list" data-md-component="toc" data-md-scrollfix=""><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> Predictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.Predictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor._inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor._prepare_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prepare_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.generate"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> generate</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.postprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> postprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.pre_transform"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> pre_transform</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.preprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> preprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.prompt_inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> prompt_inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.remove_small_regions"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> remove_small_regions</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.reset_image"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> reset_image</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.set_image"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> set_image</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.set_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> set_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.setup_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.Predictor.setup_source"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> setup_source</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM2Predictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM2Predictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor._inference_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _inference_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor._prepare_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prepare_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2Predictor.set_image"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> set_image</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM2VideoPredictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM2VideoPredictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._add_output_per_object"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _add_output_per_object</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._clear_non_cond_mem_around_input"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _clear_non_cond_mem_around_input</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._consolidate_temp_output_across_obj"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _consolidate_temp_output_across_obj</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_empty_mask_ptr"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _get_empty_mask_ptr</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_maskmem_pos_enc"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _get_maskmem_pos_enc</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._obj_id_to_idx"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _obj_id_to_idx</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_memory_encoder"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _run_memory_encoder</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_single_frame_inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _run_single_frame_inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.add_new_prompts"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> add_new_prompts</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_model"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_model</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.init_state"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> init_state</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.postprocess"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> postprocess</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2VideoPredictor.propagate_in_video_preflight"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> propagate_in_video_preflight</span></a></li></ul></nav></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor"><span class="md-ellipsis"><span class="doc-kind doc-kind-class">class</span> SAM2DynamicInteractivePredictor</span></a><nav aria-label="Class ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor" class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._obj_id_to_idx"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _obj_id_to_idx</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._prepare_memory_conditioned_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> _prepare_memory_conditioned_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_im_features"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_im_features</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_maskmem_enc"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> get_maskmem_enc</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.inference"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> inference</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.track_step"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> track_step</span></a></li><li class="md-nav__item"><a class="md-nav__link" href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.update_memory"><span class="md-ellipsis"><span class="doc-kind doc-kind-method">method</span> update_memory</span></a></li></ul></nav></li></ul></nav></div></div></div><div class="md-content" data-md-component="content"><article class="md-content__inner md-typeset"><a class="md-content__button md-icon" href="https://github.com/ultralytics/ultralytics/tree/main/docs/en/reference/models/sam/predict.md" rel="edit" title="Edit this page"><svg class="lucide lucide-file-pen" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12.5 22H18a2 2 0 0 0 2-2V7l-5-5H6a2 2 0 0 0-2 2v9.5"></path><path d="M14 2v4a2 2 0 0 0 2 2h4M13.378 15.626a1 1 0 1 0-3.004-3.004l-5.01 5.012a2 2 0 0 0-.506.854l-.837 2.87a.5.5 0 0 0 .62.62l2.87-.837a2 2 0 0 0 .854-.506z"></path></svg></a><h1 id="reference-for-ultralyticsmodelssampredictpy">Reference for <code>ultralytics/models/sam/predict.py</code></h1><div class="admonition success"><p class="admonition-title">Improvements</p><p>This page is sourced from <a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py">https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py</a>. Have an improvement or example to add? Open a <a href="https://docs.ultralytics.com/help/contributing/">Pull Request</a> â€” thank you! ğŸ™</p></div><p><br/></p><div class="admonition abstract"><p class="admonition-title">Summary</p><div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="classes" name="__tabbed_1" type="radio"/><input id="methods" name="__tabbed_1" type="radio"/><div class="tabbed-labels"><label for="classes"><span class="doc-kind doc-kind-class">Classes</span></label><label for="methods"><span class="doc-kind doc-kind-method">Methods</span></label></div><div class="tabbed-content"><div class="tabbed-block"><ul><li><a href="#ultralytics.models.sam.predict.Predictor"><code>Predictor</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2Predictor"><code>SAM2Predictor</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor"><code>SAM2VideoPredictor</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor"><code>SAM2DynamicInteractivePredictor</code></a></li></ul></div><div class="tabbed-block"><ul><li><a href="#ultralytics.models.sam.predict.Predictor.preprocess"><code>Predictor.preprocess</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.pre_transform"><code>Predictor.pre_transform</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.inference"><code>Predictor.inference</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.prompt_inference"><code>Predictor.prompt_inference</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor._inference_features"><code>Predictor._inference_features</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor._prepare_prompts"><code>Predictor._prepare_prompts</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.generate"><code>Predictor.generate</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.setup_model"><code>Predictor.setup_model</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.get_model"><code>Predictor.get_model</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.postprocess"><code>Predictor.postprocess</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.setup_source"><code>Predictor.setup_source</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.set_image"><code>Predictor.set_image</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.get_im_features"><code>Predictor.get_im_features</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.set_prompts"><code>Predictor.set_prompts</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.reset_image"><code>Predictor.reset_image</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.remove_small_regions"><code>Predictor.remove_small_regions</code></a></li><li><a href="#ultralytics.models.sam.predict.Predictor.inference_features"><code>Predictor.inference_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2Predictor.get_model"><code>SAM2Predictor.get_model</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2Predictor._prepare_prompts"><code>SAM2Predictor._prepare_prompts</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2Predictor.set_image"><code>SAM2Predictor.set_image</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2Predictor.get_im_features"><code>SAM2Predictor.get_im_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2Predictor._inference_features"><code>SAM2Predictor._inference_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_model"><code>SAM2VideoPredictor.get_model</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.inference"><code>SAM2VideoPredictor.inference</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.postprocess"><code>SAM2VideoPredictor.postprocess</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.add_new_prompts"><code>SAM2VideoPredictor.add_new_prompts</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.propagate_in_video_preflight"><code>SAM2VideoPredictor.propagate_in_video_preflight</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.init_state"><code>SAM2VideoPredictor.init_state</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_im_features"><code>SAM2VideoPredictor.get_im_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._obj_id_to_idx"><code>SAM2VideoPredictor._obj_id_to_idx</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_single_frame_inference"><code>SAM2VideoPredictor._run_single_frame_inference</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_maskmem_pos_enc"><code>SAM2VideoPredictor._get_maskmem_pos_enc</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._consolidate_temp_output_across_obj"><code>SAM2VideoPredictor._consolidate_temp_output_across_obj</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_empty_mask_ptr"><code>SAM2VideoPredictor._get_empty_mask_ptr</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_memory_encoder"><code>SAM2VideoPredictor._run_memory_encoder</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._add_output_per_object"><code>SAM2VideoPredictor._add_output_per_object</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._clear_non_cond_mem_around_input"><code>SAM2VideoPredictor._clear_non_cond_mem_around_input</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.inference"><code>SAM2DynamicInteractivePredictor.inference</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_im_features"><code>SAM2DynamicInteractivePredictor.get_im_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.update_memory"><code>SAM2DynamicInteractivePredictor.update_memory</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._prepare_memory_conditioned_features"><code>SAM2DynamicInteractivePredictor._prepare_memory_conditioned_features</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_maskmem_enc"><code>SAM2DynamicInteractivePredictor.get_maskmem_enc</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._obj_id_to_idx"><code>SAM2DynamicInteractivePredictor._obj_id_to_idx</code></a></li><li><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.track_step"><code>SAM2DynamicInteractivePredictor.track_step</code></a></li></ul></div></div></div></div><h2 id="ultralytics.models.sam.predict.Predictor"><span class="doc-kind doc-kind-class">class</span> <code>ultralytics.models.sam.predict.Predictor</code></h2><div class="highlight"><pre><span></span><code><span></span><span class="n">Predictor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span> <span class="o">=</span> <span class="n">DEFAULT_CFG</span><span class="p">,</span> <span class="n">overrides</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">_callbacks</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p><strong>Bases:</strong> <code>BasePredictor</code></p><p>Predictor class for SAM, enabling real-time image segmentation with promptable capabilities.</p><p>This class extends BasePredictor and implements the Segment Anything Model (SAM) for advanced image segmentation tasks. It supports various input prompts like points, bounding boxes, and masks for fine-grained control over segmentation results.</p><p>Sets up the Predictor object for SAM (Segment Anything Model) and applies any configuration overrides or callbacks provided. Initializes task-specific settings for SAM, such as retina_masks being set to True for optimal results.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>cfg</code></td><td><code>dict</code></td><td>Configuration dictionary containing default settings.</td><td><code>DEFAULT_CFG</code></td></tr><tr><td><code>overrides</code></td><td><code>dict | None</code></td><td>Dictionary of values to override default configuration.</td><td><code>None</code></td></tr><tr><td><code>_callbacks</code></td><td><code>dict | None</code></td><td>Dictionary of callback functions to customize behavior.</td><td><code>None</code></td></tr></tbody></table><p><strong>Attributes</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>args</code></td><td><code>SimpleNamespace</code></td><td>Configuration arguments for the predictor.</td></tr><tr><td><code>model</code></td><td><code>torch.nn.Module</code></td><td>The loaded SAM model.</td></tr><tr><td><code>device</code></td><td><code>torch.device</code></td><td>The device (CPU or GPU) on which the model is loaded.</td></tr><tr><td><code>im</code></td><td><code>torch.Tensor</code></td><td>The preprocessed input image.</td></tr><tr><td><code>features</code></td><td><code>torch.Tensor</code></td><td>Extracted image features.</td></tr><tr><td><code>prompts</code></td><td><code>dict[str, Any]</code></td><td>Dictionary to store various types of prompts (e.g., bboxes, points, masks).</td></tr><tr><td><code>segment_all</code></td><td><code>bool</code></td><td>Flag to indicate if full image segmentation should be performed.</td></tr><tr><td><code>mean</code></td><td><code>torch.Tensor</code></td><td>Mean values for image normalization.</td></tr><tr><td><code>std</code></td><td><code>torch.Tensor</code></td><td>Standard deviation values for image normalization.</td></tr></tbody></table><p><strong>Methods</strong></p><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><a href="#ultralytics.models.sam.predict.Predictor._inference_features"><code>_inference_features</code></a></td><td>Perform inference on image features using the SAM model.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor._prepare_prompts"><code>_prepare_prompts</code></a></td><td>Prepare and transform the input prompts for processing based on the destination shape.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.generate"><code>generate</code></a></td><td>Perform image segmentation using the Segment Anything Model (SAM).</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.get_im_features"><code>get_im_features</code></a></td><td>Extract image features using the SAM model's image encoder for subsequent mask prediction.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.get_model"><code>get_model</code></a></td><td>Retrieve or build the Segment Anything Model (SAM) for image segmentation tasks.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.inference"><code>inference</code></a></td><td>Perform image segmentation inference based on the given input cues, using the currently loaded image.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.inference_features"><code>inference_features</code></a></td><td>Perform prompts preprocessing and inference on provided image features using the SAM model.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.postprocess"><code>postprocess</code></a></td><td>Post-process SAM's inference outputs to generate object detection masks and bounding boxes.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.pre_transform"><code>pre_transform</code></a></td><td>Perform initial transformations on the input image for preprocessing.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.preprocess"><code>preprocess</code></a></td><td>Preprocess the input image for model inference.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.prompt_inference"><code>prompt_inference</code></a></td><td>Perform image segmentation inference based on input cues using SAM's specialized architecture.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.remove_small_regions"><code>remove_small_regions</code></a></td><td>Remove small disconnected regions and holes from segmentation masks.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.reset_image"><code>reset_image</code></a></td><td>Reset the current image and its features, clearing them for subsequent inference.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.set_image"><code>set_image</code></a></td><td>Preprocess and set a single image for inference.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.set_prompts"><code>set_prompts</code></a></td><td>Set prompts for subsequent inference operations.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.setup_model"><code>setup_model</code></a></td><td>Initialize the Segment Anything Model (SAM) for inference.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.Predictor.setup_source"><code>setup_source</code></a></td><td>Set up the data source for inference.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">setup_model</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s2">"sam_model.pt"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="s2">"image.jpg"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">]]</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">results</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L40-L693"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Predictor</span><span class="p">(</span><span class="n">BasePredictor</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Predictor class for SAM, enabling real-time image segmentation with promptable capabilities.</span>
<span></span>
<span></span><span class="sd">    This class extends BasePredictor and implements the Segment Anything Model (SAM) for advanced image segmentation</span>
<span></span><span class="sd">    tasks. It supports various input prompts like points, bounding boxes, and masks for fine-grained control over</span>
<span></span><span class="sd">    segmentation results.</span>
<span></span>
<span></span><span class="sd">    Attributes:</span>
<span></span><span class="sd">        args (SimpleNamespace): Configuration arguments for the predictor.</span>
<span></span><span class="sd">        model (torch.nn.Module): The loaded SAM model.</span>
<span></span><span class="sd">        device (torch.device): The device (CPU or GPU) on which the model is loaded.</span>
<span></span><span class="sd">        im (torch.Tensor): The preprocessed input image.</span>
<span></span><span class="sd">        features (torch.Tensor): Extracted image features.</span>
<span></span><span class="sd">        prompts (dict[str, Any]): Dictionary to store various types of prompts (e.g., bboxes, points, masks).</span>
<span></span><span class="sd">        segment_all (bool): Flag to indicate if full image segmentation should be performed.</span>
<span></span><span class="sd">        mean (torch.Tensor): Mean values for image normalization.</span>
<span></span><span class="sd">        std (torch.Tensor): Standard deviation values for image normalization.</span>
<span></span>
<span></span><span class="sd">    Methods:</span>
<span></span><span class="sd">        preprocess: Prepare input images for model inference.</span>
<span></span><span class="sd">        pre_transform: Perform initial transformations on the input image.</span>
<span></span><span class="sd">        inference: Perform segmentation inference based on input prompts.</span>
<span></span><span class="sd">        prompt_inference: Internal function for prompt-based segmentation inference.</span>
<span></span><span class="sd">        generate: Generate segmentation masks for an entire image.</span>
<span></span><span class="sd">        setup_model: Initialize the SAM model for inference.</span>
<span></span><span class="sd">        get_model: Build and return a SAM model.</span>
<span></span><span class="sd">        postprocess: Post-process model outputs to generate final results.</span>
<span></span><span class="sd">        setup_source: Set up the data source for inference.</span>
<span></span><span class="sd">        set_image: Set and preprocess a single image for inference.</span>
<span></span><span class="sd">        get_im_features: Extract image features using the SAM image encoder.</span>
<span></span><span class="sd">        set_prompts: Set prompts for subsequent inference.</span>
<span></span><span class="sd">        reset_image: Reset the current image and its features.</span>
<span></span><span class="sd">        remove_small_regions: Remove small disconnected regions and holes from masks.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.setup_model(model_path="sam_model.pt")</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.set_image("image.jpg")</span>
<span></span><span class="sd">        &gt;&gt;&gt; bboxes = [[100, 100, 200, 200]]</span>
<span></span><span class="sd">        &gt;&gt;&gt; results = predictor(bboxes=bboxes)</span>
<span></span><span class="sd">    """</span>
<span></span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="o">=</span><span class="n">DEFAULT_CFG</span><span class="p">,</span> <span class="n">overrides</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">        </span><span class="sd">"""Initialize the Predictor with configuration, overrides, and callbacks.</span>
<span></span>
<span></span><span class="sd">        Sets up the Predictor object for SAM (Segment Anything Model) and applies any configuration overrides or</span>
<span></span><span class="sd">        callbacks provided. Initializes task-specific settings for SAM, such as retina_masks being set to True for</span>
<span></span><span class="sd">        optimal results.</span>
<span></span>
<span></span><span class="sd">        Args:</span>
<span></span><span class="sd">            cfg (dict): Configuration dictionary containing default settings.</span>
<span></span><span class="sd">            overrides (dict | None): Dictionary of values to override default configuration.</span>
<span></span><span class="sd">            _callbacks (dict | None): Dictionary of callback functions to customize behavior.</span>
<span></span><span class="sd">        """</span>
<span></span>        <span class="k">if</span> <span class="n">overrides</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">overrides</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>        <span class="n">overrides</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"segment"</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"predict"</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span></span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">overrides</span><span class="p">,</span> <span class="n">_callbacks</span><span class="p">)</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">retina_masks</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">im</span> <span class="o">=</span> <span class="kc">None</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="kc">None</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">segment_all</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor._inference_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor._inference_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_inference_features</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">features</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Perform inference on image features using the SAM model.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>features</code></td><td><code>torch.Tensor</code></td><td>Extracted image features with shape (B, C, H, W) from the SAM model image encoder.</td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td><code>np.ndarray | list[list[float]] | None</code></td><td>Bounding boxes in XYXY format with shape (N, 4).</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list[list[float]] | None</code></td><td>Object location points with shape (N, 2), in pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list[int] | None</code></td><td>Point prompt labels with shape (N,). 1 = foreground, 0 = background.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>list[np.ndarray] | np.ndarray | None</code></td><td>Masks for the objects, where each mask is a 2D array.</td><td><code>None</code></td></tr><tr><td><code>multimask_output</code></td><td><code>bool</code></td><td>Flag to return multiple masks for ambiguous prompts.</td><td><code>False</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>Output masks with shape (C, H, W), where C is the number of generated masks.</td></tr><tr><td><code>pred_scores (torch.Tensor)</code></td><td>Quality scores for each mask, with length C.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L231-L269"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_inference_features</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">features</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform inference on image features using the SAM model.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        features (torch.Tensor): Extracted image features with shape (B, C, H, W) from the SAM model image encoder.</span>
<span></span><span class="sd">        bboxes (np.ndarray | list[list[float]] | None): Bounding boxes in XYXY format with shape (N, 4).</span>
<span></span><span class="sd">        points (np.ndarray | list[list[float]] | None): Object location points with shape (N, 2), in pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list[int] | None): Point prompt labels with shape (N,). 1 = foreground, 0 = background.</span>
<span></span><span class="sd">        masks (list[np.ndarray] | np.ndarray | None): Masks for the objects, where each mask is a 2D array.</span>
<span></span><span class="sd">        multimask_output (bool): Flag to return multiple masks for ambiguous prompts.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): Output masks with shape (C, H, W), where C is the number of generated masks.</span>
<span></span><span class="sd">        pred_scores (torch.Tensor): Quality scores for each mask, with length C.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">points</span> <span class="o">=</span> <span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
<span></span>    <span class="c1"># Embed prompts</span>
<span></span>    <span class="n">sparse_embeddings</span><span class="p">,</span> <span class="n">dense_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">prompt_encoder</span><span class="p">(</span><span class="n">points</span><span class="o">=</span><span class="n">points</span><span class="p">,</span> <span class="n">boxes</span><span class="o">=</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="n">masks</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Predict masks</span>
<span></span>    <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">mask_decoder</span><span class="p">(</span>
<span></span>        <span class="n">image_embeddings</span><span class="o">=</span><span class="n">features</span><span class="p">,</span>
<span></span>        <span class="n">image_pe</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">prompt_encoder</span><span class="o">.</span><span class="n">get_dense_pe</span><span class="p">(),</span>
<span></span>        <span class="n">sparse_prompt_embeddings</span><span class="o">=</span><span class="n">sparse_embeddings</span><span class="p">,</span>
<span></span>        <span class="n">dense_prompt_embeddings</span><span class="o">=</span><span class="n">dense_embeddings</span><span class="p">,</span>
<span></span>        <span class="n">multimask_output</span><span class="o">=</span><span class="n">multimask_output</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># (N, d, H, W) --&gt; (N*d, H, W), (N, d) --&gt; (N*d, )</span>
<span></span>    <span class="c1"># `d` could be 1 or 3 depends on `multimask_output`.</span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">pred_scores</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor._prepare_prompts"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor._prepare_prompts</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prepare_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dst_shape</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">points</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Prepare and transform the input prompts for processing based on the destination shape.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>dst_shape</code></td><td><code>tuple[int, int]</code></td><td>The target shape (height, width) for the prompts.</td><td><em>required</em></td></tr><tr><td><code>src_shape</code></td><td><code>tuple[int, int]</code></td><td>The source shape (height, width) of the input image.</td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td><code>np.ndarray | list | None</code></td><td>Bounding boxes in XYXY format with shape (N, 4).</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list | None</code></td><td>Points indicating object locations with shape (N, 2) or (N, num_points, 2), in pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list | None</code></td><td>Point prompt labels with shape (N) or (N, num_points). 1 for foreground, 0 for background.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>list[np.ndarray] | np.ndarray | None</code></td><td>Masks for the objects, where each mask is a 2D array with shape (H, W).</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>bboxes (torch.Tensor | None)</code></td><td>Transformed bounding boxes.</td></tr><tr><td><code>points (torch.Tensor | None)</code></td><td>Transformed points.</td></tr><tr><td><code>labels (torch.Tensor | None)</code></td><td>Transformed labels.</td></tr><tr><td><code>masks (torch.Tensor | None)</code></td><td>Transformed masks.</td></tr></tbody></table><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>AssertionError</code></td><td>If the number of points don't match the number of labels, in case labels were passed.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L271-L320"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prepare_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dst_shape</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Prepare and transform the input prompts for processing based on the destination shape.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        dst_shape (tuple[int, int]): The target shape (height, width) for the prompts.</span>
<span></span><span class="sd">        src_shape (tuple[int, int]): The source shape (height, width) of the input image.</span>
<span></span><span class="sd">        bboxes (np.ndarray | list | None): Bounding boxes in XYXY format with shape (N, 4).</span>
<span></span><span class="sd">        points (np.ndarray | list | None): Points indicating object locations with shape (N, 2) or (N, num_points,</span>
<span></span><span class="sd">            2), in pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list | None): Point prompt labels with shape (N) or (N, num_points). 1 for foreground,</span>
<span></span><span class="sd">            0 for background.</span>
<span></span><span class="sd">        masks (list[np.ndarray] | np.ndarray | None): Masks for the objects, where each mask is a 2D array with</span>
<span></span><span class="sd">            shape (H, W).</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        bboxes (torch.Tensor | None): Transformed bounding boxes.</span>
<span></span><span class="sd">        points (torch.Tensor | None): Transformed points.</span>
<span></span><span class="sd">        labels (torch.Tensor | None): Transformed labels.</span>
<span></span><span class="sd">        masks (torch.Tensor | None): Transformed masks.</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        AssertionError: If the number of points don't match the number of labels, in case labels were passed.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">r</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">segment_all</span> <span class="k">else</span> <span class="nb">min</span><span class="p">(</span><span class="n">dst_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">src_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dst_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">src_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span></span>    <span class="c1"># Transform input prompts</span>
<span></span>    <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="n">points</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="k">if</span> <span class="n">points</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">points</span>
<span></span>        <span class="c1"># Assuming labels are all positive if users don't pass labels.</span>
<span></span>        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">points</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span></span>        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="k">assert</span> <span class="n">points</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span>
<span></span>            <span class="sa">f</span><span class="s2">"Number of points </span><span class="si">{</span><span class="n">points</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> should match number of labels </span><span class="si">{</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">."</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="n">points</span> <span class="o">*=</span> <span class="n">r</span>
<span></span>        <span class="k">if</span> <span class="n">points</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
<span></span>            <span class="c1"># (N, 2) --&gt; (N, 1, 2), (N, ) --&gt; (N, 1)</span>
<span></span>            <span class="n">points</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">points</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="n">labels</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
<span></span>    <span class="k">if</span> <span class="n">bboxes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="n">bboxes</span> <span class="o">=</span> <span class="n">bboxes</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="k">if</span> <span class="n">bboxes</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">bboxes</span>
<span></span>        <span class="n">bboxes</span> <span class="o">*=</span> <span class="n">r</span>
<span></span>    <span class="k">if</span> <span class="n">masks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">masks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span></span>        <span class="n">masks</span> <span class="o">=</span> <span class="n">masks</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="k">if</span> <span class="n">masks</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="n">masks</span>
<span></span>        <span class="n">letterbox</span> <span class="o">=</span> <span class="n">LetterBox</span><span class="p">(</span><span class="n">dst_shape</span><span class="p">,</span> <span class="n">auto</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">INTER_NEAREST</span><span class="p">)</span>
<span></span>        <span class="n">masks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">letterbox</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">masks</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span></span>        <span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.generate"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.generate</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">im</span><span class="p">,</span>
<span></span>    <span class="n">crop_n_layers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span></span>    <span class="n">crop_overlap_ratio</span><span class="o">=</span><span class="mi">512</span> <span class="o">/</span> <span class="mi">1500</span><span class="p">,</span>
<span></span>    <span class="n">crop_downscale_factor</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span></span>    <span class="n">point_grids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points_stride</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
<span></span>    <span class="n">points_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span></span>    <span class="n">conf_thres</span><span class="o">=</span><span class="mf">0.88</span><span class="p">,</span>
<span></span>    <span class="n">stability_score_thresh</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
<span></span>    <span class="n">stability_score_offset</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
<span></span>    <span class="n">crop_nms_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Perform image segmentation using the Segment Anything Model (SAM).</p><p>This method segments an entire image into constituent parts by leveraging SAM's advanced architecture and real-time performance capabilities. It can optionally work on image crops for finer segmentation.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor</code></td><td>Input tensor representing the preprocessed image with shape (N, C, H, W).</td><td><em>required</em></td></tr><tr><td><code>crop_n_layers</code></td><td><code>int</code></td><td>Number of layers for additional mask predictions on image crops.</td><td><code>0</code></td></tr><tr><td><code>crop_overlap_ratio</code></td><td><code>float</code></td><td>Overlap between crops, scaled down in subsequent layers.</td><td><code>512 / 1500</code></td></tr><tr><td><code>crop_downscale_factor</code></td><td><code>int</code></td><td>Scaling factor for sampled points-per-side in each layer.</td><td><code>1</code></td></tr><tr><td><code>point_grids</code></td><td><code>list[np.ndarray] | None</code></td><td>Custom grids for point sampling normalized to [0,1].</td><td><code>None</code></td></tr><tr><td><code>points_stride</code></td><td><code>int</code></td><td>Number of points to sample along each side of the image.</td><td><code>32</code></td></tr><tr><td><code>points_batch_size</code></td><td><code>int</code></td><td>Batch size for the number of points processed simultaneously.</td><td><code>64</code></td></tr><tr><td><code>conf_thres</code></td><td><code>float</code></td><td>Confidence threshold [0,1] for filtering based on mask quality prediction.</td><td><code>0.88</code></td></tr><tr><td><code>stability_score_thresh</code></td><td><code>float</code></td><td>Stability threshold [0,1] for mask filtering based on stability.</td><td><code>0.95</code></td></tr><tr><td><code>stability_score_offset</code></td><td><code>float</code></td><td>Offset value for calculating stability score.</td><td><code>0.95</code></td></tr><tr><td><code>crop_nms_thresh</code></td><td><code>float</code></td><td>IoU cutoff for NMS to remove duplicate masks between crops.</td><td><code>0.7</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>Segmented masks with shape (N, H, W).</td></tr><tr><td><code>pred_scores (torch.Tensor)</code></td><td>Confidence scores for each mask with shape (N,).</td></tr><tr><td><code>pred_bboxes (torch.Tensor)</code></td><td>Bounding boxes for each mask with shape (N, 4).</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">im</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>  <span class="c1"># Example input image</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">masks</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">boxes</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L322-L431"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">im</span><span class="p">,</span>
<span></span>    <span class="n">crop_n_layers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span></span>    <span class="n">crop_overlap_ratio</span><span class="o">=</span><span class="mi">512</span> <span class="o">/</span> <span class="mi">1500</span><span class="p">,</span>
<span></span>    <span class="n">crop_downscale_factor</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span></span>    <span class="n">point_grids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points_stride</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
<span></span>    <span class="n">points_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span></span>    <span class="n">conf_thres</span><span class="o">=</span><span class="mf">0.88</span><span class="p">,</span>
<span></span>    <span class="n">stability_score_thresh</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
<span></span>    <span class="n">stability_score_offset</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
<span></span>    <span class="n">crop_nms_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform image segmentation using the Segment Anything Model (SAM).</span>
<span></span>
<span></span><span class="sd">    This method segments an entire image into constituent parts by leveraging SAM's advanced architecture and</span>
<span></span><span class="sd">    real-time performance capabilities. It can optionally work on image crops for finer segmentation.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (torch.Tensor): Input tensor representing the preprocessed image with shape (N, C, H, W).</span>
<span></span><span class="sd">        crop_n_layers (int): Number of layers for additional mask predictions on image crops.</span>
<span></span><span class="sd">        crop_overlap_ratio (float): Overlap between crops, scaled down in subsequent layers.</span>
<span></span><span class="sd">        crop_downscale_factor (int): Scaling factor for sampled points-per-side in each layer.</span>
<span></span><span class="sd">        point_grids (list[np.ndarray] | None): Custom grids for point sampling normalized to [0,1].</span>
<span></span><span class="sd">        points_stride (int): Number of points to sample along each side of the image.</span>
<span></span><span class="sd">        points_batch_size (int): Batch size for the number of points processed simultaneously.</span>
<span></span><span class="sd">        conf_thres (float): Confidence threshold [0,1] for filtering based on mask quality prediction.</span>
<span></span><span class="sd">        stability_score_thresh (float): Stability threshold [0,1] for mask filtering based on stability.</span>
<span></span><span class="sd">        stability_score_offset (float): Offset value for calculating stability score.</span>
<span></span><span class="sd">        crop_nms_thresh (float): IoU cutoff for NMS to remove duplicate masks between crops.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): Segmented masks with shape (N, H, W).</span>
<span></span><span class="sd">        pred_scores (torch.Tensor): Confidence scores for each mask with shape (N,).</span>
<span></span><span class="sd">        pred_bboxes (torch.Tensor): Bounding boxes for each mask with shape (N, 4).</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; im = torch.rand(1, 3, 1024, 1024)  # Example input image</span>
<span></span><span class="sd">        &gt;&gt;&gt; masks, scores, boxes = predictor.generate(im)</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>  <span class="c1"># scope for faster 'import ultralytics'</span>
<span></span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">segment_all</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>    <span class="n">ih</span><span class="p">,</span> <span class="n">iw</span> <span class="o">=</span> <span class="n">im</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
<span></span>    <span class="n">crop_regions</span><span class="p">,</span> <span class="n">layer_idxs</span> <span class="o">=</span> <span class="n">generate_crop_boxes</span><span class="p">((</span><span class="n">ih</span><span class="p">,</span> <span class="n">iw</span><span class="p">),</span> <span class="n">crop_n_layers</span><span class="p">,</span> <span class="n">crop_overlap_ratio</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">point_grids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">point_grids</span> <span class="o">=</span> <span class="n">build_all_layer_point_grids</span><span class="p">(</span><span class="n">points_stride</span><span class="p">,</span> <span class="n">crop_n_layers</span><span class="p">,</span> <span class="n">crop_downscale_factor</span><span class="p">)</span>
<span></span>    <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">pred_bboxes</span><span class="p">,</span> <span class="n">region_areas</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
<span></span>    <span class="k">for</span> <span class="n">crop_region</span><span class="p">,</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">crop_regions</span><span class="p">,</span> <span class="n">layer_idxs</span><span class="p">):</span>
<span></span>        <span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">crop_region</span>
<span></span>        <span class="n">w</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">-</span> <span class="n">x1</span><span class="p">,</span> <span class="n">y2</span> <span class="o">-</span> <span class="n">y1</span>
<span></span>        <span class="n">area</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">h</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">im</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="n">points_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">]])</span>  <span class="c1"># w, h</span>
<span></span>        <span class="c1"># Crop image and interpolate to input size</span>
<span></span>        <span class="n">crop_im</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">im</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">y1</span><span class="p">:</span><span class="n">y2</span><span class="p">,</span> <span class="n">x1</span><span class="p">:</span><span class="n">x2</span><span class="p">],</span> <span class="p">(</span><span class="n">ih</span><span class="p">,</span> <span class="n">iw</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span></span>        <span class="c1"># (num_points, 2)</span>
<span></span>        <span class="n">points_for_image</span> <span class="o">=</span> <span class="n">point_grids</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">points_scale</span>
<span></span>        <span class="n">crop_masks</span><span class="p">,</span> <span class="n">crop_scores</span><span class="p">,</span> <span class="n">crop_bboxes</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
<span></span>        <span class="k">for</span> <span class="p">(</span><span class="n">points</span><span class="p">,)</span> <span class="ow">in</span> <span class="n">batch_iterator</span><span class="p">(</span><span class="n">points_batch_size</span><span class="p">,</span> <span class="n">points_for_image</span><span class="p">):</span>
<span></span>            <span class="n">pred_mask</span><span class="p">,</span> <span class="n">pred_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompt_inference</span><span class="p">(</span><span class="n">crop_im</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="n">points</span><span class="p">,</span> <span class="n">multimask_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span></span>            <span class="c1"># Interpolate predicted masks to input size</span>
<span></span>            <span class="n">pred_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">pred_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">],</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span></span>            <span class="n">idx</span> <span class="o">=</span> <span class="n">pred_score</span> <span class="o">&gt;</span> <span class="n">conf_thres</span>
<span></span>            <span class="n">pred_mask</span><span class="p">,</span> <span class="n">pred_score</span> <span class="o">=</span> <span class="n">pred_mask</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">pred_score</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span></span>
<span></span>            <span class="n">stability_score</span> <span class="o">=</span> <span class="n">calculate_stability_score</span><span class="p">(</span>
<span></span>                <span class="n">pred_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">mask_threshold</span><span class="p">,</span> <span class="n">stability_score_offset</span>
<span></span>            <span class="p">)</span>
<span></span>            <span class="n">idx</span> <span class="o">=</span> <span class="n">stability_score</span> <span class="o">&gt;</span> <span class="n">stability_score_thresh</span>
<span></span>            <span class="n">pred_mask</span><span class="p">,</span> <span class="n">pred_score</span> <span class="o">=</span> <span class="n">pred_mask</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">pred_score</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span></span>            <span class="c1"># Bool type is much more memory-efficient.</span>
<span></span>            <span class="n">pred_mask</span> <span class="o">=</span> <span class="n">pred_mask</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">mask_threshold</span>
<span></span>            <span class="c1"># (N, 4)</span>
<span></span>            <span class="n">pred_bbox</span> <span class="o">=</span> <span class="n">batched_mask_to_box</span><span class="p">(</span><span class="n">pred_mask</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span></span>            <span class="n">keep_mask</span> <span class="o">=</span> <span class="o">~</span><span class="n">is_box_near_crop_edge</span><span class="p">(</span><span class="n">pred_bbox</span><span class="p">,</span> <span class="n">crop_region</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">iw</span><span class="p">,</span> <span class="n">ih</span><span class="p">])</span>
<span></span>            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">keep_mask</span><span class="p">):</span>
<span></span>                <span class="n">pred_bbox</span><span class="p">,</span> <span class="n">pred_mask</span><span class="p">,</span> <span class="n">pred_score</span> <span class="o">=</span> <span class="n">pred_bbox</span><span class="p">[</span><span class="n">keep_mask</span><span class="p">],</span> <span class="n">pred_mask</span><span class="p">[</span><span class="n">keep_mask</span><span class="p">],</span> <span class="n">pred_score</span><span class="p">[</span><span class="n">keep_mask</span><span class="p">]</span>
<span></span>
<span></span>            <span class="n">crop_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_mask</span><span class="p">)</span>
<span></span>            <span class="n">crop_bboxes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_bbox</span><span class="p">)</span>
<span></span>            <span class="n">crop_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_score</span><span class="p">)</span>
<span></span>
<span></span>        <span class="c1"># Do nms within this crop</span>
<span></span>        <span class="n">crop_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">crop_masks</span><span class="p">)</span>
<span></span>        <span class="n">crop_bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">crop_bboxes</span><span class="p">)</span>
<span></span>        <span class="n">crop_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">crop_scores</span><span class="p">)</span>
<span></span>        <span class="n">keep</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">nms</span><span class="p">(</span><span class="n">crop_bboxes</span><span class="p">,</span> <span class="n">crop_scores</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">iou</span><span class="p">)</span>  <span class="c1"># NMS</span>
<span></span>        <span class="n">crop_bboxes</span> <span class="o">=</span> <span class="n">uncrop_boxes_xyxy</span><span class="p">(</span><span class="n">crop_bboxes</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span> <span class="n">crop_region</span><span class="p">)</span>
<span></span>        <span class="n">crop_masks</span> <span class="o">=</span> <span class="n">uncrop_masks</span><span class="p">(</span><span class="n">crop_masks</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span> <span class="n">crop_region</span><span class="p">,</span> <span class="n">ih</span><span class="p">,</span> <span class="n">iw</span><span class="p">)</span>
<span></span>        <span class="n">crop_scores</span> <span class="o">=</span> <span class="n">crop_scores</span><span class="p">[</span><span class="n">keep</span><span class="p">]</span>
<span></span>
<span></span>        <span class="n">pred_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">crop_masks</span><span class="p">)</span>
<span></span>        <span class="n">pred_bboxes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">crop_bboxes</span><span class="p">)</span>
<span></span>        <span class="n">pred_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">crop_scores</span><span class="p">)</span>
<span></span>        <span class="n">region_areas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">area</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">crop_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span></span>
<span></span>    <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">pred_masks</span><span class="p">)</span>
<span></span>    <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">pred_bboxes</span><span class="p">)</span>
<span></span>    <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">pred_scores</span><span class="p">)</span>
<span></span>    <span class="n">region_areas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">region_areas</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Remove duplicate masks between crops</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">crop_regions</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<span></span>        <span class="n">scores</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">region_areas</span>
<span></span>        <span class="n">keep</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">nms</span><span class="p">(</span><span class="n">pred_bboxes</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">crop_nms_thresh</span><span class="p">)</span>
<span></span>        <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_bboxes</span><span class="p">,</span> <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">pred_masks</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span> <span class="n">pred_bboxes</span><span class="p">[</span><span class="n">keep</span><span class="p">],</span> <span class="n">pred_scores</span><span class="p">[</span><span class="n">keep</span><span class="p">]</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">pred_bboxes</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.get_im_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.get_im_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">)</span>
</code></pre></div><p>Extract image features using the SAM model's image encoder for subsequent mask prediction.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L579-L585"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Extract image features using the SAM model's image encoder for subsequent mask prediction."""</span>
<span></span>    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span>
<span></span>        <span class="sa">f</span><span class="s2">"SAM models only support square image size, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="si">}</span><span class="s2">."</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">set_imgsz</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">image_encoder</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.get_model"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.get_model</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div><p>Retrieve or build the Segment Anything Model (SAM) for image segmentation tasks.</p><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L465-L469"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Retrieve or build the Segment Anything Model (SAM) for image segmentation tasks."""</span>
<span></span>    <span class="kn">from</span><span class="w"> </span><span class="nn">.build</span><span class="w"> </span><span class="kn">import</span> <span class="n">build_sam</span>  <span class="c1"># slow import</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">build_sam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.inference"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.inference</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div><p>Perform image segmentation inference based on the given input cues, using the currently loaded image.</p><p>This method leverages SAM's (Segment Anything Model) architecture consisting of image encoder, prompt encoder, and mask decoder for real-time and promptable segmentation tasks.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor</code></td><td>The preprocessed input image in tensor format, with shape (N, C, H, W).</td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td><code>np.ndarray | list | None</code></td><td>Bounding boxes with shape (N, 4), in XYXY format.</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list | None</code></td><td>Points indicating object locations with shape (N, 2), in pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list | None</code></td><td>Labels for point prompts, shape (N,). 1 = foreground, 0 = background.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>np.ndarray | None</code></td><td>Low-resolution masks from previous predictions, shape (N, H, W). For SAM H=W=256.</td><td><code>None</code></td></tr><tr><td><code>multimask_output</code></td><td><code>bool</code></td><td>Flag to return multiple masks. Helpful for ambiguous prompts.</td><td><code>False</code></td></tr><tr><td><code>*args</code></td><td><code>Any</code></td><td>Additional positional arguments.</td><td><em>required</em></td></tr><tr><td><code>**kwargs</code></td><td><code>Any</code></td><td>Additional keyword arguments.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>The output masks in shape (C, H, W), where C is the number of generated masks.</td></tr><tr><td><code>pred_scores (torch.Tensor)</code></td><td>An array of length C containing quality scores predicted by the model for each</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">setup_model</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s2">"sam_model.pt"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="s2">"image.jpg"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">results</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">bboxes</span><span class="o">=</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">]])</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L162-L198"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform image segmentation inference based on the given input cues, using the currently loaded image.</span>
<span></span>
<span></span><span class="sd">    This method leverages SAM's (Segment Anything Model) architecture consisting of image encoder, prompt encoder,</span>
<span></span><span class="sd">    and mask decoder for real-time and promptable segmentation tasks.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (torch.Tensor): The preprocessed input image in tensor format, with shape (N, C, H, W).</span>
<span></span><span class="sd">        bboxes (np.ndarray | list | None): Bounding boxes with shape (N, 4), in XYXY format.</span>
<span></span><span class="sd">        points (np.ndarray | list | None): Points indicating object locations with shape (N, 2), in pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list | None): Labels for point prompts, shape (N,). 1 = foreground, 0 = background.</span>
<span></span><span class="sd">        masks (np.ndarray | None): Low-resolution masks from previous predictions, shape (N, H, W). For SAM H=W=256.</span>
<span></span><span class="sd">        multimask_output (bool): Flag to return multiple masks. Helpful for ambiguous prompts.</span>
<span></span><span class="sd">        *args (Any): Additional positional arguments.</span>
<span></span><span class="sd">        **kwargs (Any): Additional keyword arguments.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): The output masks in shape (C, H, W), where C is the number of generated masks.</span>
<span></span><span class="sd">        pred_scores (torch.Tensor): An array of length C containing quality scores predicted by the model for each</span>
<span></span><span class="sd">            mask.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.setup_model(model_path="sam_model.pt")</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.set_image("image.jpg")</span>
<span></span><span class="sd">        &gt;&gt;&gt; results = predictor(bboxes=[[0, 0, 100, 100]])</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="c1"># Override prompts if any stored in self.prompts</span>
<span></span>    <span class="n">bboxes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"bboxes"</span><span class="p">,</span> <span class="n">bboxes</span><span class="p">)</span>
<span></span>    <span class="n">points</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"points"</span><span class="p">,</span> <span class="n">points</span><span class="p">)</span>
<span></span>    <span class="n">masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"masks"</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span>    <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"labels"</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">i</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">masks</span><span class="p">]):</span>
<span></span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompt_inference</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span><span class="p">,</span> <span class="n">multimask_output</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.inference_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.inference_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference_features</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">features</span><span class="p">,</span>
<span></span>    <span class="n">src_shape</span><span class="p">,</span>
<span></span>    <span class="n">dst_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Perform prompts preprocessing and inference on provided image features using the SAM model.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>features</code></td><td><code>torch.Tensor | dict[str, Any]</code></td><td>Extracted image features from the SAM/SAM2 model image encoder.</td><td><em>required</em></td></tr><tr><td><code>src_shape</code></td><td><code>tuple[int, int]</code></td><td>The source shape (height, width) of the input image.</td><td><em>required</em></td></tr><tr><td><code>dst_shape</code></td><td><code>tuple[int, int] | None</code></td><td>The target shape (height, width) for the prompts. If None, defaults to (imgsz, imgsz).</td><td><code>None</code></td></tr><tr><td><code>bboxes</code></td><td><code>np.ndarray | list[list[float]] | None</code></td><td>Bounding boxes in xyxy format with shape (N, 4).</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list[list[float]] | None</code></td><td>Points indicating object locations with shape (N, 2), in pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list[int] | None</code></td><td>Point prompt labels with shape (N, ).</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>list[np.ndarray] | np.ndarray | None</code></td><td>Masks for the objects, where each mask is a 2D array.</td><td><code>None</code></td></tr><tr><td><code>multimask_output</code></td><td><code>bool</code></td><td>Flag to return multiple masks for ambiguous prompts.</td><td><code>False</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>The output masks in shape (C, H, W), where C is the number of generated masks.</td></tr><tr><td><code>pred_bboxes (torch.Tensor)</code></td><td>Bounding boxes for each mask with shape (N, 6), where N is the number of boxes.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>The input features is a torch.Tensor of shape (B, C, H, W) if performing on SAM, or a dict[str, Any] if performing on SAM2.</li></ul></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L648-L693"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference_features</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">features</span><span class="p">,</span>
<span></span>    <span class="n">src_shape</span><span class="p">,</span>
<span></span>    <span class="n">dst_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform prompts preprocessing and inference on provided image features using the SAM model.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        features (torch.Tensor | dict[str, Any]): Extracted image features from the SAM/SAM2 model image encoder.</span>
<span></span><span class="sd">        src_shape (tuple[int, int]): The source shape (height, width) of the input image.</span>
<span></span><span class="sd">        dst_shape (tuple[int, int] | None): The target shape (height, width) for the prompts. If None, defaults to</span>
<span></span><span class="sd">            (imgsz, imgsz).</span>
<span></span><span class="sd">        bboxes (np.ndarray | list[list[float]] | None): Bounding boxes in xyxy format with shape (N, 4).</span>
<span></span><span class="sd">        points (np.ndarray | list[list[float]] | None): Points indicating object locations with shape (N, 2), in</span>
<span></span><span class="sd">            pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list[int] | None): Point prompt labels with shape (N, ).</span>
<span></span><span class="sd">        masks (list[np.ndarray] | np.ndarray | None): Masks for the objects, where each mask is a 2D array.</span>
<span></span><span class="sd">        multimask_output (bool): Flag to return multiple masks for ambiguous prompts.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): The output masks in shape (C, H, W), where C is the number of generated masks.</span>
<span></span><span class="sd">        pred_bboxes (torch.Tensor): Bounding boxes for each mask with shape (N, 6), where N is the number of boxes.</span>
<span></span><span class="sd">            Each box is in xyxy format with additional columns for score and class.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - The input features is a torch.Tensor of shape (B, C, H, W) if performing on SAM, or a dict[str, Any] if performing on SAM2.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">dst_shape</span> <span class="o">=</span> <span class="n">dst_shape</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">imgsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">imgsz</span><span class="p">)</span>
<span></span>    <span class="n">prompts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_prompts</span><span class="p">(</span><span class="n">dst_shape</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span>    <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inference_features</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="o">*</span><span class="n">prompts</span><span class="p">,</span> <span class="n">multimask_output</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">pred_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">scale_masks</span><span class="p">(</span><span class="n">pred_masks</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span></span>        <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">pred_masks</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">mask_threshold</span>  <span class="c1"># to bool</span>
<span></span>        <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">batched_mask_to_box</span><span class="p">(</span><span class="n">pred_masks</span><span class="p">)</span>
<span></span>        <span class="c1"># NOTE: SAM models do not return cls info. This `cls` here is just a placeholder for consistency.</span>
<span></span>        <span class="bp">cls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pred_bboxes</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="bp">cls</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_bboxes</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.postprocess"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.postprocess</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">)</span>
</code></pre></div><p>Post-process SAM's inference outputs to generate object detection masks and bounding boxes.</p><p>This method scales masks and boxes to the original image size and applies a threshold to the mask predictions. It leverages SAM's advanced architecture for real-time, promptable segmentation tasks.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>preds</code></td><td><code>tuple</code></td><td>The output from SAM model inference, containing: - pred_masks (torch.Tensor): Predicted masks with shape (N, 1, H, W). - pred_scores (torch.Tensor): Confidence scores for each mask with shape (N, 1). - pred_bboxes (torch.Tensor, optional): Predicted bounding boxes if segment_all is True.</td><td><em>required</em></td></tr><tr><td><code>img</code></td><td><code>torch.Tensor</code></td><td>The processed input image tensor with shape (C, H, W).</td><td><em>required</em></td></tr><tr><td><code>orig_imgs</code></td><td><code>list[np.ndarray] | torch.Tensor</code></td><td>The original, unprocessed images.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>list[Results]</code></td><td>List of Results objects containing detection masks, bounding boxes, and other metadata for</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">results</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">postprocess</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L471-L521"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Post-process SAM's inference outputs to generate object detection masks and bounding boxes.</span>
<span></span>
<span></span><span class="sd">    This method scales masks and boxes to the original image size and applies a threshold to the mask</span>
<span></span><span class="sd">    predictions. It leverages SAM's advanced architecture for real-time, promptable segmentation tasks.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        preds (tuple): The output from SAM model inference, containing:</span>
<span></span><span class="sd">            - pred_masks (torch.Tensor): Predicted masks with shape (N, 1, H, W).</span>
<span></span><span class="sd">            - pred_scores (torch.Tensor): Confidence scores for each mask with shape (N, 1).</span>
<span></span><span class="sd">            - pred_bboxes (torch.Tensor, optional): Predicted bounding boxes if segment_all is True.</span>
<span></span><span class="sd">        img (torch.Tensor): The processed input image tensor with shape (C, H, W).</span>
<span></span><span class="sd">        orig_imgs (list[np.ndarray] | torch.Tensor): The original, unprocessed images.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (list[Results]): List of Results objects containing detection masks, bounding boxes, and other metadata for</span>
<span></span><span class="sd">            each processed image.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; preds = predictor.inference(img)</span>
<span></span><span class="sd">        &gt;&gt;&gt; results = predictor.postprocess(preds, img, orig_imgs)</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="c1"># (N, 1, H, W), (N, 1)</span>
<span></span>    <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
<span></span>    <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">segment_all</span> <span class="k">else</span> <span class="kc">None</span>
<span></span>    <span class="n">names</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">orig_imgs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>  <span class="c1"># input images are a torch.Tensor, not a list</span>
<span></span>        <span class="n">orig_imgs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_torch2numpy_batch</span><span class="p">(</span><span class="n">orig_imgs</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span></span>
<span></span>    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>    <span class="k">for</span> <span class="n">masks</span><span class="p">,</span> <span class="n">orig_img</span><span class="p">,</span> <span class="n">img_path</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">pred_masks</span><span class="p">],</span> <span class="n">orig_imgs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
<span></span>        <span class="k">if</span> <span class="n">masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>            <span class="n">masks</span><span class="p">,</span> <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="k">else</span><span class="p">:</span>
<span></span>            <span class="n">masks</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">scale_masks</span><span class="p">(</span><span class="n">masks</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">orig_img</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span></span>            <span class="n">masks</span> <span class="o">=</span> <span class="n">masks</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">mask_threshold</span>  <span class="c1"># to bool</span>
<span></span>            <span class="k">if</span> <span class="n">pred_bboxes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>                <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">scale_boxes</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="n">pred_bboxes</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">orig_img</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span></span>            <span class="k">else</span><span class="p">:</span>
<span></span>                <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">batched_mask_to_box</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span>
<span></span>            <span class="c1"># NOTE: SAM models do not return cls info. This `cls` here is just a placeholder for consistency.</span>
<span></span>            <span class="bp">cls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>            <span class="n">idx</span> <span class="o">=</span> <span class="n">pred_scores</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">conf</span>
<span></span>            <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pred_bboxes</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="bp">cls</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="n">idx</span><span class="p">]</span>
<span></span>            <span class="n">masks</span> <span class="o">=</span> <span class="n">masks</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span></span>        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Results</span><span class="p">(</span><span class="n">orig_img</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="n">img_path</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">names</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="n">masks</span><span class="p">,</span> <span class="n">boxes</span><span class="o">=</span><span class="n">pred_bboxes</span><span class="p">))</span>
<span></span>    <span class="c1"># Reset segment-all mode.</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">segment_all</span> <span class="o">=</span> <span class="kc">False</span>
<span></span>    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.pre_transform"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.pre_transform</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">pre_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">)</span>
</code></pre></div><p>Perform initial transformations on the input image for preprocessing.</p><p>This method applies transformations such as resizing to prepare the image for further preprocessing. Currently, batched inference is not supported; hence the list length should be 1.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>list[np.ndarray]</code></td><td>List containing a single image in HWC numpy array format.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>list[np.ndarray]</code></td><td>List containing the transformed image.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">480</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># Single HWC image</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">transformed</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">pre_transform</span><span class="p">([</span><span class="n">image</span><span class="p">])</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">transformed</span><span class="p">))</span>
<span></span><span class="mi">1</span>
</code></pre></div><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>AssertionError</code></td><td>If the input list contains more than one image.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L136-L160"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">pre_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform initial transformations on the input image for preprocessing.</span>
<span></span>
<span></span><span class="sd">    This method applies transformations such as resizing to prepare the image for further preprocessing. Currently,</span>
<span></span><span class="sd">    batched inference is not supported; hence the list length should be 1.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (list[np.ndarray]): List containing a single image in HWC numpy array format.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (list[np.ndarray]): List containing the transformed image.</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        AssertionError: If the input list contains more than one image.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; image = np.random.rand(480, 640, 3)  # Single HWC image</span>
<span></span><span class="sd">        &gt;&gt;&gt; transformed = predictor.pre_transform([image])</span>
<span></span><span class="sd">        &gt;&gt;&gt; print(len(transformed))</span>
<span></span><span class="sd">        1</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">im</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"SAM model does not currently support batched inference"</span>
<span></span>    <span class="n">letterbox</span> <span class="o">=</span> <span class="n">LetterBox</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">imgsz</span><span class="p">,</span> <span class="n">auto</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="p">[</span><span class="n">letterbox</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">im</span><span class="p">]</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.preprocess"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.preprocess</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">)</span>
</code></pre></div><p>Preprocess the input image for model inference.</p><p>This method prepares the input image by applying transformations and normalization. It supports both torch.Tensor and list of np.ndarray as input formats.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor | list[np.ndarray]</code></td><td>Input image(s) in BCHW tensor format or list of HWC numpy arrays.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>torch.Tensor</code></td><td>The preprocessed image tensor, normalized and converted to the appropriate dtype.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">640</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">preprocessed_image</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L104-L134"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Preprocess the input image for model inference.</span>
<span></span>
<span></span><span class="sd">    This method prepares the input image by applying transformations and normalization. It supports both</span>
<span></span><span class="sd">    torch.Tensor and list of np.ndarray as input formats.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (torch.Tensor | list[np.ndarray]): Input image(s) in BCHW tensor format or list of HWC numpy arrays.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (torch.Tensor): The preprocessed image tensor, normalized and converted to the appropriate dtype.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; image = torch.rand(1, 3, 640, 640)</span>
<span></span><span class="sd">        &gt;&gt;&gt; preprocessed_image = predictor.preprocess(image)</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">im</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">im</span>
<span></span>    <span class="n">not_tensor</span> <span class="o">=</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">not_tensor</span><span class="p">:</span>
<span></span>        <span class="n">im</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pre_transform</span><span class="p">(</span><span class="n">im</span><span class="p">))</span>
<span></span>        <span class="n">im</span> <span class="o">=</span> <span class="n">im</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span></span>        <span class="n">im</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
<span></span>        <span class="n">im</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">im</span> <span class="o">=</span> <span class="n">im</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">not_tensor</span><span class="p">:</span>
<span></span>        <span class="n">im</span> <span class="o">=</span> <span class="p">(</span><span class="n">im</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">std</span>
<span></span>    <span class="n">im</span> <span class="o">=</span> <span class="n">im</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fp16</span> <span class="k">else</span> <span class="n">im</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span></span>    <span class="k">return</span> <span class="n">im</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.prompt_inference"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.prompt_inference</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">prompt_inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">points</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">multimask_output</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</code></pre></div><p>Perform image segmentation inference based on input cues using SAM's specialized architecture.</p><p>This internal function leverages the Segment Anything Model (SAM) for prompt-based, real-time segmentation. It processes various input prompts such as bounding boxes, points, and masks to generate segmentation masks.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor</code></td><td>Preprocessed input image tensor with shape (N, C, H, W).</td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td><code>np.ndarray | list | None</code></td><td>Bounding boxes in XYXY format with shape (N, 4).</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list | None</code></td><td>Points indicating object locations with shape (N, 2) or (N, num_points, 2), in pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list | None</code></td><td>Point prompt labels with shape (N) or (N, num_points). 1 for foreground, 0 for background.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>np.ndarray | None</code></td><td>Low-res masks from previous predictions with shape (N, H, W). For SAM, H=W=256.</td><td><code>None</code></td></tr><tr><td><code>multimask_output</code></td><td><code>bool</code></td><td>Flag to return multiple masks for ambiguous prompts.</td><td><code>False</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>Output masks with shape (C, H, W), where C is the number of generated masks.</td></tr><tr><td><code>pred_scores (torch.Tensor)</code></td><td>Quality scores predicted by the model for each mask, with length C.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">im</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">]]</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">masks</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">prompt_inference</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L200-L229"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">prompt_inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform image segmentation inference based on input cues using SAM's specialized architecture.</span>
<span></span>
<span></span><span class="sd">    This internal function leverages the Segment Anything Model (SAM) for prompt-based, real-time segmentation. It</span>
<span></span><span class="sd">    processes various input prompts such as bounding boxes, points, and masks to generate segmentation masks.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (torch.Tensor): Preprocessed input image tensor with shape (N, C, H, W).</span>
<span></span><span class="sd">        bboxes (np.ndarray | list | None): Bounding boxes in XYXY format with shape (N, 4).</span>
<span></span><span class="sd">        points (np.ndarray | list | None): Points indicating object locations with shape (N, 2) or (N, num_points,</span>
<span></span><span class="sd">            2), in pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list | None): Point prompt labels with shape (N) or (N, num_points). 1 for foreground,</span>
<span></span><span class="sd">            0 for background.</span>
<span></span><span class="sd">        masks (np.ndarray | None): Low-res masks from previous predictions with shape (N, H, W). For SAM, H=W=256.</span>
<span></span><span class="sd">        multimask_output (bool): Flag to return multiple masks for ambiguous prompts.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): Output masks with shape (C, H, W), where C is the number of generated masks.</span>
<span></span><span class="sd">        pred_scores (torch.Tensor): Quality scores predicted by the model for each mask, with length C.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; im = torch.rand(1, 3, 1024, 1024)</span>
<span></span><span class="sd">        &gt;&gt;&gt; bboxes = [[100, 100, 200, 200]]</span>
<span></span><span class="sd">        &gt;&gt;&gt; masks, scores, logits = predictor.prompt_inference(im, bboxes=bboxes)</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span><span class="n">im</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span>
<span></span>
<span></span>    <span class="n">prompts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_prompts</span><span class="p">(</span><span class="n">im</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inference_features</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="o">*</span><span class="n">prompts</span><span class="p">,</span> <span class="n">multimask_output</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.remove_small_regions"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.remove_small_regions</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">remove_small_regions</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="n">min_area</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">nms_thresh</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">)</span>
</code></pre></div><p>Remove small disconnected regions and holes from segmentation masks.</p><p>This function performs post-processing on segmentation masks generated by the Segment Anything Model (SAM). It removes small disconnected regions and holes from the input masks, and then performs Non-Maximum Suppression (NMS) to eliminate any newly created duplicate boxes.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>masks</code></td><td><code>torch.Tensor</code></td><td>Segmentation masks to be processed, with shape (N, H, W) where N is the number of masks, H is height, and W is width.</td><td><em>required</em></td></tr><tr><td><code>min_area</code></td><td><code>int</code></td><td>Minimum area threshold for removing disconnected regions and holes. Regions smaller than this will be removed.</td><td><code>0</code></td></tr><tr><td><code>nms_thresh</code></td><td><code>float</code></td><td>IoU threshold for the NMS algorithm to remove duplicate boxes.</td><td><code>0.7</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>new_masks (torch.Tensor)</code></td><td>Processed masks with small regions removed, shape (N, H, W).</td></tr><tr><td><code>keep (list[int])</code></td><td>Indices of remaining masks after NMS, for filtering corresponding boxes.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">640</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span>  <span class="c1"># 5 random binary masks</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">new_masks</span><span class="p">,</span> <span class="n">keep</span> <span class="o">=</span> <span class="n">remove_small_regions</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="n">min_area</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">nms_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Original masks: </span><span class="si">{</span><span class="n">masks</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, Processed masks: </span><span class="si">{</span><span class="n">new_masks</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Indices of kept masks: </span><span class="si">{</span><span class="n">keep</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L597-L645"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@staticmethod</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">remove_small_regions</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="n">min_area</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">nms_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Remove small disconnected regions and holes from segmentation masks.</span>
<span></span>
<span></span><span class="sd">    This function performs post-processing on segmentation masks generated by the Segment Anything Model (SAM). It</span>
<span></span><span class="sd">    removes small disconnected regions and holes from the input masks, and then performs Non-Maximum Suppression</span>
<span></span><span class="sd">    (NMS) to eliminate any newly created duplicate boxes.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        masks (torch.Tensor): Segmentation masks to be processed, with shape (N, H, W) where N is the number of</span>
<span></span><span class="sd">            masks, H is height, and W is width.</span>
<span></span><span class="sd">        min_area (int): Minimum area threshold for removing disconnected regions and holes. Regions smaller than</span>
<span></span><span class="sd">            this will be removed.</span>
<span></span><span class="sd">        nms_thresh (float): IoU threshold for the NMS algorithm to remove duplicate boxes.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        new_masks (torch.Tensor): Processed masks with small regions removed, shape (N, H, W).</span>
<span></span><span class="sd">        keep (list[int]): Indices of remaining masks after NMS, for filtering corresponding boxes.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; masks = torch.rand(5, 640, 640) &gt; 0.5  # 5 random binary masks</span>
<span></span><span class="sd">        &gt;&gt;&gt; new_masks, keep = remove_small_regions(masks, min_area=100, nms_thresh=0.7)</span>
<span></span><span class="sd">        &gt;&gt;&gt; print(f"Original masks: {masks.shape}, Processed masks: {new_masks.shape}")</span>
<span></span><span class="sd">        &gt;&gt;&gt; print(f"Indices of kept masks: {keep}")</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>  <span class="c1"># scope for faster 'import ultralytics'</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="n">masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="k">return</span> <span class="n">masks</span>
<span></span>
<span></span>    <span class="c1"># Filter small disconnected regions and holes</span>
<span></span>    <span class="n">new_masks</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>    <span class="k">for</span> <span class="n">mask</span> <span class="ow">in</span> <span class="n">masks</span><span class="p">:</span>
<span></span>        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span></span>        <span class="n">mask</span><span class="p">,</span> <span class="n">changed</span> <span class="o">=</span> <span class="n">remove_small_regions</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">min_area</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"holes"</span><span class="p">)</span>
<span></span>        <span class="n">unchanged</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">changed</span>
<span></span>        <span class="n">mask</span><span class="p">,</span> <span class="n">changed</span> <span class="o">=</span> <span class="n">remove_small_regions</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">min_area</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"islands"</span><span class="p">)</span>
<span></span>        <span class="n">unchanged</span> <span class="o">=</span> <span class="n">unchanged</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">changed</span>
<span></span>
<span></span>        <span class="n">new_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span></span>        <span class="c1"># Give score=0 to changed masks and 1 to unchanged masks so NMS prefers masks not needing postprocessing</span>
<span></span>        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">unchanged</span><span class="p">))</span>
<span></span>
<span></span>    <span class="c1"># Recalculate boxes and remove any new duplicates</span>
<span></span>    <span class="n">new_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_masks</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span></span>    <span class="n">boxes</span> <span class="o">=</span> <span class="n">batched_mask_to_box</span><span class="p">(</span><span class="n">new_masks</span><span class="p">)</span>
<span></span>    <span class="n">keep</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">nms</span><span class="p">(</span><span class="n">boxes</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">nms_thresh</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">new_masks</span><span class="p">[</span><span class="n">keep</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">masks</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">masks</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">keep</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.reset_image"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.reset_image</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">reset_image</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div><p>Reset the current image and its features, clearing them for subsequent inference.</p><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L591-L594"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">reset_image</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Reset the current image and its features, clearing them for subsequent inference."""</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">im</span> <span class="o">=</span> <span class="kc">None</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.set_image"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.set_image</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">set_image</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>
</code></pre></div><p>Preprocess and set a single image for inference.</p><p>This method prepares the model for inference on a single image by setting up the model if not already initialized, configuring the data source, and preprocessing the image for feature extraction. It ensures that only one image is set at a time and extracts image features for subsequent use.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>image</code></td><td><code>str | np.ndarray</code></td><td>Path to the image file as a string, or a numpy array representing an image read by cv2.</td><td><em>required</em></td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">))</span>
</code></pre></div><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>This method should be called before performing inference on a new image.</li><li>The extracted features are stored in the <code>self.features</code> attribute for later use.</li></ul></div><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>AssertionError</code></td><td>If more than one image is attempted to be set.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L547-L577"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">set_image</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Preprocess and set a single image for inference.</span>
<span></span>
<span></span><span class="sd">    This method prepares the model for inference on a single image by setting up the model if not already</span>
<span></span><span class="sd">    initialized, configuring the data source, and preprocessing the image for feature extraction. It ensures that</span>
<span></span><span class="sd">    only one image is set at a time and extracts image features for subsequent use.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        image (str | np.ndarray): Path to the image file as a string, or a numpy array representing an image read by</span>
<span></span><span class="sd">            cv2.</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        AssertionError: If more than one image is attempted to be set.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.set_image("path/to/image.jpg")</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.set_image(cv2.imread("path/to/image.jpg"))</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - This method should be called before performing inference on a new image.</span>
<span></span><span class="sd">        - The extracted features are stored in the `self.features` attribute for later use.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">setup_model</span><span class="p">()</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">setup_source</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span></span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"`set_image` only supports setting one image!"</span>
<span></span>    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">:</span>
<span></span>        <span class="n">im</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
<span></span>        <span class="k">break</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.set_prompts"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.set_prompts</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">set_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompts</span><span class="p">)</span>
</code></pre></div><p>Set prompts for subsequent inference operations.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>prompts</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L587-L589"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">set_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompts</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Set prompts for subsequent inference operations."""</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span> <span class="o">=</span> <span class="n">prompts</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.setup_model"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.setup_model</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</code></pre></div><p>Initialize the Segment Anything Model (SAM) for inference.</p><p>This method sets up the SAM model by allocating it to the appropriate device and initializing the necessary parameters for image normalization and other Ultralytics compatibility settings.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>model</code></td><td><code>torch.nn.Module | None</code></td><td>A pretrained SAM model. If None, a new model is built based on config.</td><td><code>None</code></td></tr><tr><td><code>verbose</code></td><td><code>bool</code></td><td>If True, prints selected device information.</td><td><code>True</code></td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">setup_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">sam_model</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L433-L463"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Initialize the Segment Anything Model (SAM) for inference.</span>
<span></span>
<span></span><span class="sd">    This method sets up the SAM model by allocating it to the appropriate device and initializing the necessary</span>
<span></span><span class="sd">    parameters for image normalization and other Ultralytics compatibility settings.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        model (torch.nn.Module | None): A pretrained SAM model. If None, a new model is built based on config.</span>
<span></span><span class="sd">        verbose (bool): If True, prints selected device information.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.setup_model(model=sam_model, verbose=True)</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">device</span> <span class="o">=</span> <span class="n">select_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_model</span><span class="p">()</span>
<span></span>    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span></span>    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">half</span> <span class="k">else</span> <span class="n">model</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">123.675</span><span class="p">,</span> <span class="mf">116.28</span><span class="p">,</span> <span class="mf">103.53</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">58.395</span><span class="p">,</span> <span class="mf">57.12</span><span class="p">,</span> <span class="mf">57.375</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Ultralytics compatibility settings</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">pt</span> <span class="o">=</span> <span class="kc">False</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">triton</span> <span class="o">=</span> <span class="kc">False</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="mi">32</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fp16</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">half</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">done_warmup</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fp16</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.Predictor.setup_source"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.Predictor.setup_source</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_source</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">)</span>
</code></pre></div><p>Set up the data source for inference.</p><p>This method configures the data source from which images will be fetched for inference. It supports various input types such as image files, directories, video files, and other compatible data sources.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>source</code></td><td><code>str | Path | None</code></td><td>The path or identifier for the image data source. Can be a file path, directory path, URL, or other supported source types.</td><td><em>required</em></td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">setup_source</span><span class="p">(</span><span class="s2">"path/to/images"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">setup_source</span><span class="p">(</span><span class="s2">"video.mp4"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">setup_source</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># Uses default source if available</span>
</code></pre></div><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>If source is None, the method may use a default source if configured.</li><li>The method adapts to different source types and prepares them for subsequent inference steps.</li><li>Supported source types may include local files, directories, URLs, and video streams.</li></ul></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L523-L545"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_source</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Set up the data source for inference.</span>
<span></span>
<span></span><span class="sd">    This method configures the data source from which images will be fetched for inference. It supports various</span>
<span></span><span class="sd">    input types such as image files, directories, video files, and other compatible data sources.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        source (str | Path | None): The path or identifier for the image data source. Can be a file path, directory</span>
<span></span><span class="sd">            path, URL, or other supported source types.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.setup_source("path/to/images")</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.setup_source("video.mp4")</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.setup_source(None)  # Uses default source if available</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - If source is None, the method may use a default source if configured.</span>
<span></span><span class="sd">        - The method adapts to different source types and prepares them for subsequent inference steps.</span>
<span></span><span class="sd">        - Supported source types may include local files, directories, URLs, and video streams.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">if</span> <span class="n">source</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">setup_source</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
</code></pre></div></details><p><br/><br/><hr/><br/></p><h2 id="ultralytics.models.sam.predict.SAM2Predictor"><span class="doc-kind doc-kind-class">class</span> <code>ultralytics.models.sam.predict.SAM2Predictor</code></h2><div class="highlight"><pre><span></span><code><span></span><span class="n">SAM2Predictor</span><span class="p">()</span>
</code></pre></div><p><strong>Bases:</strong> <code>Predictor</code></p><p>SAM2Predictor class for advanced image segmentation using Segment Anything Model 2 architecture.</p><p>This class extends the base Predictor class to implement SAM2-specific functionality for image segmentation tasks. It provides methods for model initialization, feature extraction, and prompt-based inference.</p><p><strong>Attributes</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>_bb_feat_sizes</code></td><td><code>list[tuple]</code></td><td>Feature sizes for different backbone levels.</td></tr><tr><td><code>model</code></td><td><code>torch.nn.Module</code></td><td>The loaded SAM2 model.</td></tr><tr><td><code>device</code></td><td><code>torch.device</code></td><td>The device (CPU or GPU) on which the model is loaded.</td></tr><tr><td><code>features</code></td><td><code>dict</code></td><td>Cached image features for efficient inference.</td></tr><tr><td><code>segment_all</code></td><td><code>bool</code></td><td>Flag to indicate if all segments should be predicted.</td></tr><tr><td><code>prompts</code></td><td><code>dict[str, Any]</code></td><td>Dictionary to store various types of prompts for inference.</td></tr></tbody></table><p><strong>Methods</strong></p><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><a href="#ultralytics.models.sam.predict.SAM2Predictor._inference_features"><code>_inference_features</code></a></td><td>Perform inference on image features using the SAM2 model.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2Predictor._prepare_prompts"><code>_prepare_prompts</code></a></td><td>Prepare and transform the input prompts for processing based on the destination shape.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2Predictor.get_im_features"><code>get_im_features</code></a></td><td>Extract image features from the SAM image encoder for subsequent processing.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2Predictor.get_model"><code>get_model</code></a></td><td>Retrieve and initialize the Segment Anything Model 2 (SAM2) for image segmentation tasks.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2Predictor.set_image"><code>set_image</code></a></td><td>Preprocess and set a single image for inference using the SAM2 model.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">SAM2Predictor</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">]]</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Predicted </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">masks</span><span class="p">)</span><span class="si">}</span><span class="s2"> masks with average score </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">boxes</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L696-L867"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SAM2Predictor</span><span class="p">(</span><span class="n">Predictor</span><span class="p">):</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2Predictor._inference_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2Predictor._inference_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_inference_features</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">features</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="n">img_idx</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Perform inference on image features using the SAM2 model.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>features</code></td><td><code>torch.Tensor | dict[str, Any]</code></td><td>Extracted image features with shape (B, C, H, W) from the SAM2</td><td><em>required</em></td></tr><tr><td><code>model image encoder, it could also be a dictionary including:&lt;br&gt;    - image_embed (torch.Tensor): Image embedding with shape (B, C, H, W).&lt;br&gt;    - high_res_feats (list[torch.Tensor]): List of high-resolution feature maps from the backbone, each with shape (B, C, H, W).</code></td><td></td><td></td><td><em>required</em></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list[list[float]] | None</code></td><td>Object location points with shape (N, 2), in pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list[int] | None</code></td><td>Point prompt labels with shape (N,). 1 = foreground, 0 = background.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>list[np.ndarray] | np.ndarray | None</code></td><td>Masks for the objects, where each mask is a 2D array.</td><td><code>None</code></td></tr><tr><td><code>multimask_output</code></td><td><code>bool</code></td><td>Flag to return multiple masks for ambiguous prompts.</td><td><code>False</code></td></tr><tr><td><code>img_idx</code></td><td><code>int</code></td><td>Index of the image in the batch to process.</td><td><code>-1</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>Output masks with shape (C, H, W), where C is the number of generated masks.</td></tr><tr><td><code>pred_scores (torch.Tensor)</code></td><td>Quality scores for each mask, with length C.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L818-L867"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_inference_features</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">features</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="n">img_idx</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform inference on image features using the SAM2 model.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        features (torch.Tensor | dict[str, Any]): Extracted image features with shape (B, C, H, W) from the SAM2</span>
<span></span><span class="sd">        model image encoder, it could also be a dictionary including:</span>
<span></span><span class="sd">            - image_embed (torch.Tensor): Image embedding with shape (B, C, H, W).</span>
<span></span><span class="sd">            - high_res_feats (list[torch.Tensor]): List of high-resolution feature maps from the backbone, each with shape (B, C, H, W).</span>
<span></span><span class="sd">        points (np.ndarray | list[list[float]] | None): Object location points with shape (N, 2), in pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list[int] | None): Point prompt labels with shape (N,). 1 = foreground, 0 = background.</span>
<span></span><span class="sd">        masks (list[np.ndarray] | np.ndarray | None): Masks for the objects, where each mask is a 2D array.</span>
<span></span><span class="sd">        multimask_output (bool): Flag to return multiple masks for ambiguous prompts.</span>
<span></span><span class="sd">        img_idx (int): Index of the image in the batch to process.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): Output masks with shape (C, H, W), where C is the number of generated masks.</span>
<span></span><span class="sd">        pred_scores (torch.Tensor): Quality scores for each mask, with length C.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">points</span> <span class="o">=</span> <span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
<span></span>    <span class="n">sparse_embeddings</span><span class="p">,</span> <span class="n">dense_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">sam_prompt_encoder</span><span class="p">(</span>
<span></span>        <span class="n">points</span><span class="o">=</span><span class="n">points</span><span class="p">,</span>
<span></span>        <span class="n">boxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="n">masks</span><span class="o">=</span><span class="n">masks</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="c1"># Predict masks</span>
<span></span>    <span class="n">batched_mode</span> <span class="o">=</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">points</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span>  <span class="c1"># multi object prediction</span>
<span></span>    <span class="n">high_res_features</span> <span class="o">=</span> <span class="kc">None</span>
<span></span>    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
<span></span>        <span class="n">high_res_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">feat_level</span><span class="p">[</span><span class="n">img_idx</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">feat_level</span> <span class="ow">in</span> <span class="n">features</span><span class="p">[</span><span class="s2">"high_res_feats"</span><span class="p">]]</span>
<span></span>        <span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="s2">"image_embed"</span><span class="p">][[</span><span class="n">img_idx</span><span class="p">]]</span>
<span></span>    <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">sam_mask_decoder</span><span class="p">(</span>
<span></span>        <span class="n">image_embeddings</span><span class="o">=</span><span class="n">features</span><span class="p">,</span>
<span></span>        <span class="n">image_pe</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">sam_prompt_encoder</span><span class="o">.</span><span class="n">get_dense_pe</span><span class="p">(),</span>
<span></span>        <span class="n">sparse_prompt_embeddings</span><span class="o">=</span><span class="n">sparse_embeddings</span><span class="p">,</span>
<span></span>        <span class="n">dense_prompt_embeddings</span><span class="o">=</span><span class="n">dense_embeddings</span><span class="p">,</span>
<span></span>        <span class="n">multimask_output</span><span class="o">=</span><span class="n">multimask_output</span><span class="p">,</span>
<span></span>        <span class="n">repeat_image</span><span class="o">=</span><span class="n">batched_mode</span><span class="p">,</span>
<span></span>        <span class="n">high_res_features</span><span class="o">=</span><span class="n">high_res_features</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="c1"># (N, d, H, W) --&gt; (N*d, H, W), (N, d) --&gt; (N*d, )</span>
<span></span>    <span class="c1"># `d` could be 1 or 3 depends on `multimask_output`.</span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">pred_scores</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2Predictor._prepare_prompts"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2Predictor._prepare_prompts</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prepare_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dst_shape</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">points</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Prepare and transform the input prompts for processing based on the destination shape.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>dst_shape</code></td><td><code>tuple[int, int]</code></td><td>The target shape (height, width) for the prompts.</td><td><em>required</em></td></tr><tr><td><code>src_shape</code></td><td><code>tuple[int, int]</code></td><td>The source shape (height, width) of the input image.</td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td><code>np.ndarray | list | None</code></td><td>Bounding boxes in XYXY format with shape (N, 4).</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list | None</code></td><td>Points indicating object locations with shape (N, 2) or (N, num_points, 2), in pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list | None</code></td><td>Point prompt labels with shape (N,) or (N, num_points). 1 for foreground, 0 for background.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>list | np.ndarray | None</code></td><td>Masks for the objects, where each mask is a 2D array.</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>points (torch.Tensor | None)</code></td><td>Transformed points.</td></tr><tr><td><code>labels (torch.Tensor | None)</code></td><td>Transformed labels.</td></tr><tr><td><code>masks (torch.Tensor | None)</code></td><td>Transformed masks.</td></tr></tbody></table><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>AssertionError</code></td><td>If the number of points don't match the number of labels, in case labels were passed.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L736-L768"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prepare_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dst_shape</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Prepare and transform the input prompts for processing based on the destination shape.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        dst_shape (tuple[int, int]): The target shape (height, width) for the prompts.</span>
<span></span><span class="sd">        src_shape (tuple[int, int]): The source shape (height, width) of the input image.</span>
<span></span><span class="sd">        bboxes (np.ndarray | list | None): Bounding boxes in XYXY format with shape (N, 4).</span>
<span></span><span class="sd">        points (np.ndarray | list | None): Points indicating object locations with shape (N, 2) or (N, num_points,</span>
<span></span><span class="sd">            2), in pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list | None): Point prompt labels with shape (N,) or (N, num_points). 1 for foreground,</span>
<span></span><span class="sd">            0 for background.</span>
<span></span><span class="sd">        masks (list | np.ndarray | None): Masks for the objects, where each mask is a 2D array.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        points (torch.Tensor | None): Transformed points.</span>
<span></span><span class="sd">        labels (torch.Tensor | None): Transformed labels.</span>
<span></span><span class="sd">        masks (torch.Tensor | None): Transformed masks.</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        AssertionError: If the number of points don't match the number of labels, in case labels were passed.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_prepare_prompts</span><span class="p">(</span><span class="n">dst_shape</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">,</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">bboxes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">bboxes</span> <span class="o">=</span> <span class="n">bboxes</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span></span>        <span class="n">bbox_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">bboxes</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">bboxes</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span></span>        <span class="c1"># NOTE: merge "boxes" and "points" into a single "points" input</span>
<span></span>        <span class="c1"># (where boxes are added at the beginning) to model.sam_prompt_encoder</span>
<span></span>        <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span></span>            <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">bbox_labels</span><span class="p">,</span> <span class="n">labels</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span></span>        <span class="k">else</span><span class="p">:</span>
<span></span>            <span class="n">points</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">bbox_labels</span>
<span></span>    <span class="k">return</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2Predictor.get_im_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2Predictor.get_im_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">)</span>
</code></pre></div><p>Extract image features from the SAM image encoder for subsequent processing.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td></td><td></td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L801-L816"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Extract image features from the SAM image encoder for subsequent processing."""</span>
<span></span>    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span>
<span></span>        <span class="sa">f</span><span class="s2">"SAM 2 models only support square image size, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="si">}</span><span class="s2">."</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">set_imgsz</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">_bb_feat_sizes</span> <span class="o">=</span> <span class="p">[[</span><span class="n">x</span> <span class="o">//</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
<span></span>
<span></span>    <span class="n">backbone_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward_image</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
<span></span>    <span class="n">_</span><span class="p">,</span> <span class="n">vision_feats</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_prepare_backbone_features</span><span class="p">(</span><span class="n">backbone_out</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">directly_add_no_mem_embed</span><span class="p">:</span>
<span></span>        <span class="n">vision_feats</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">vision_feats</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">no_mem_embed</span>
<span></span>    <span class="n">feats</span> <span class="o">=</span> <span class="p">[</span>
<span></span>        <span class="n">feat</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">feat_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">feat</span><span class="p">,</span> <span class="n">feat_size</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">vision_feats</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bb_feat_sizes</span><span class="p">)</span>
<span></span>    <span class="p">]</span>
<span></span>    <span class="k">return</span> <span class="p">{</span><span class="s2">"image_embed"</span><span class="p">:</span> <span class="n">feats</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">"high_res_feats"</span><span class="p">:</span> <span class="n">feats</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]}</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2Predictor.get_model"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2Predictor.get_model</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div><p>Retrieve and initialize the Segment Anything Model 2 (SAM2) for image segmentation tasks.</p><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L730-L734"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Retrieve and initialize the Segment Anything Model 2 (SAM2) for image segmentation tasks."""</span>
<span></span>    <span class="kn">from</span><span class="w"> </span><span class="nn">.build</span><span class="w"> </span><span class="kn">import</span> <span class="n">build_sam</span>  <span class="c1"># slow import</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">build_sam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2Predictor.set_image"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2Predictor.set_image</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">set_image</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>
</code></pre></div><p>Preprocess and set a single image for inference using the SAM2 model.</p><p>This method initializes the model if not already done, configures the data source to the specified image, and preprocesses the image for feature extraction. It supports setting only one image at a time.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>image</code></td><td><code>str | np.ndarray</code></td><td>Path to the image file as a string, or a numpy array representing the image.</td><td><em>required</em></td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">SAM2Predictor</span><span class="p">()</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="s2">"path/to/image.jpg"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">...</span><span class="p">]))</span>  <span class="c1"># Using a numpy array</span>
</code></pre></div><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>This method must be called before performing any inference on a new image.</li><li>The method caches the extracted features for efficient subsequent inferences on the same image.</li><li>Only one image can be set at a time. To process multiple images, call this method for each new image.</li></ul></div><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>AssertionError</code></td><td>If more than one image is attempted to be set.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L770-L799"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">set_image</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Preprocess and set a single image for inference using the SAM2 model.</span>
<span></span>
<span></span><span class="sd">    This method initializes the model if not already done, configures the data source to the specified image, and</span>
<span></span><span class="sd">    preprocesses the image for feature extraction. It supports setting only one image at a time.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        image (str | np.ndarray): Path to the image file as a string, or a numpy array representing the image.</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        AssertionError: If more than one image is attempted to be set.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = SAM2Predictor()</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.set_image("path/to/image.jpg")</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.set_image(np.array([...]))  # Using a numpy array</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - This method must be called before performing any inference on a new image.</span>
<span></span><span class="sd">        - The method caches the extracted features for efficient subsequent inferences on the same image.</span>
<span></span><span class="sd">        - Only one image can be set at a time. To process multiple images, call this method for each new image.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">setup_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">setup_source</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span></span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"`set_image` only supports setting one image!"</span>
<span></span>    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">:</span>
<span></span>        <span class="n">im</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
<span></span>        <span class="k">break</span>
</code></pre></div></details><p><br/><br/><hr/><br/></p><h2 id="ultralytics.models.sam.predict.SAM2VideoPredictor"><span class="doc-kind doc-kind-class">class</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor</code></h2><div class="highlight"><pre><span></span><code><span></span><span class="n">SAM2VideoPredictor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span> <span class="o">=</span> <span class="n">DEFAULT_CFG</span><span class="p">,</span> <span class="n">overrides</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">_callbacks</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p><strong>Bases:</strong> <code>SAM2Predictor</code></p><p>SAM2VideoPredictor to handle user interactions with videos and manage inference states.</p><p>This class extends the functionality of SAM2Predictor to support video processing and maintains the state of inference operations. It includes configurations for managing non-overlapping masks, clearing memory for non-conditional inputs, and setting up callbacks for prediction events.</p><p>This constructor initializes the SAM2VideoPredictor with a given configuration, applies any specified overrides, and sets up the inference state along with certain flags that control the behavior of the predictor.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>cfg</code></td><td><code>dict</code></td><td>Configuration dictionary containing default settings.</td><td><code>DEFAULT_CFG</code></td></tr><tr><td><code>overrides</code></td><td><code>dict | None</code></td><td>Dictionary of values to override default configuration.</td><td><code>None</code></td></tr><tr><td><code>_callbacks</code></td><td><code>dict | None</code></td><td>Dictionary of callback functions to customize behavior.</td><td><code>None</code></td></tr></tbody></table><p><strong>Attributes</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>inference_state</code></td><td><code>dict</code></td><td>A dictionary to store the current state of inference operations.</td></tr><tr><td><code>non_overlap_masks</code></td><td><code>bool</code></td><td>A flag indicating whether masks should be non-overlapping.</td></tr><tr><td><code>clear_non_cond_mem_around_input</code></td><td><code>bool</code></td><td>A flag to control clearing non-conditional memory around inputs.</td></tr><tr><td><code>clear_non_cond_mem_for_multi_obj</code></td><td><code>bool</code></td><td>A flag to control clearing non-conditional memory for multi-object scenarios.</td></tr><tr><td><code>callbacks</code></td><td><code>dict</code></td><td>A dictionary of callbacks for various prediction lifecycle events.</td></tr></tbody></table><p><strong>Methods</strong></p><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._add_output_per_object"><code>_add_output_per_object</code></a></td><td>Split a multi-object output into per-object output slices and add them into Output_Dict_Per_Obj.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._clear_non_cond_mem_around_input"><code>_clear_non_cond_mem_around_input</code></a></td><td>Remove the non-conditioning memory around the input frame.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._consolidate_temp_output_across_obj"><code>_consolidate_temp_output_across_obj</code></a></td><td>Consolidate per-object temporary outputs into a single output for all objects.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_empty_mask_ptr"><code>_get_empty_mask_ptr</code></a></td><td>Get a dummy object pointer based on an empty mask on the current frame.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._get_maskmem_pos_enc"><code>_get_maskmem_pos_enc</code></a></td><td>Cache and manage the positional encoding for mask memory across frames and objects.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._obj_id_to_idx"><code>_obj_id_to_idx</code></a></td><td>Map client-side object id to model-side object index.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_memory_encoder"><code>_run_memory_encoder</code></a></td><td>Run the memory encoder on masks.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor._run_single_frame_inference"><code>_run_single_frame_inference</code></a></td><td>Run tracking on a single frame based on current inputs and previous memory.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.add_new_prompts"><code>add_new_prompts</code></a></td><td>Add new points or masks to a specific frame for a given object ID.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_im_features"><code>get_im_features</code></a></td><td>Extract and process image features using SAM2's image encoder for subsequent segmentation tasks.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.get_model"><code>get_model</code></a></td><td>Retrieve and configure the model with binarization enabled.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.inference"><code>inference</code></a></td><td>Perform image segmentation inference based on the given input cues, using the currently loaded image. This</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.init_state"><code>init_state</code></a></td><td>Initialize an inference state for the predictor.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.postprocess"><code>postprocess</code></a></td><td>Post-process the predictions to apply non-overlapping constraints if required.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2VideoPredictor.propagate_in_video_preflight"><code>propagate_in_video_preflight</code></a></td><td>Prepare inference_state and consolidate temporary outputs before tracking.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">SAM2VideoPredictor</span><span class="p">(</span><span class="n">cfg</span><span class="o">=</span><span class="n">DEFAULT_CFG</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="o">.</span><span class="n">set_image</span><span class="p">(</span><span class="s2">"path/to/video_frame.jpg"</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">]]</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">results</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes</span><span class="p">)</span>
</code></pre></div><div class="admonition note"><p class="admonition-title">Notes</p><p>The <code>fill_hole_area</code> attribute is defined but not used in the current implementation.</p></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L870-L1656"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SAM2VideoPredictor</span><span class="p">(</span><span class="n">SAM2Predictor</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""SAM2VideoPredictor to handle user interactions with videos and manage inference states.</span>
<span></span>
<span></span><span class="sd">    This class extends the functionality of SAM2Predictor to support video processing and maintains the state of</span>
<span></span><span class="sd">    inference operations. It includes configurations for managing non-overlapping masks, clearing memory for</span>
<span></span><span class="sd">    non-conditional inputs, and setting up callbacks for prediction events.</span>
<span></span>
<span></span><span class="sd">    Attributes:</span>
<span></span><span class="sd">        inference_state (dict): A dictionary to store the current state of inference operations.</span>
<span></span><span class="sd">        non_overlap_masks (bool): A flag indicating whether masks should be non-overlapping.</span>
<span></span><span class="sd">        clear_non_cond_mem_around_input (bool): A flag to control clearing non-conditional memory around inputs.</span>
<span></span><span class="sd">        clear_non_cond_mem_for_multi_obj (bool): A flag to control clearing non-conditional memory for multi-object</span>
<span></span><span class="sd">            scenarios.</span>
<span></span><span class="sd">        callbacks (dict): A dictionary of callbacks for various prediction lifecycle events.</span>
<span></span>
<span></span><span class="sd">    Methods:</span>
<span></span><span class="sd">        get_model: Retrieve and configure the model with binarization enabled.</span>
<span></span><span class="sd">        inference: Perform image segmentation inference based on the given input cues.</span>
<span></span><span class="sd">        postprocess: Post-process the predictions to apply non-overlapping constraints if required.</span>
<span></span><span class="sd">        add_new_prompts: Add new points or masks to a specific frame for a given object ID.</span>
<span></span><span class="sd">        propagate_in_video_preflight: Prepare inference_state and consolidate temporary outputs before tracking.</span>
<span></span><span class="sd">        init_state: Initialize an inference state for the predictor.</span>
<span></span><span class="sd">        get_im_features: Extract image features using SAM2's image encoder for subsequent segmentation tasks.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor = SAM2VideoPredictor(cfg=DEFAULT_CFG)</span>
<span></span><span class="sd">        &gt;&gt;&gt; predictor.set_image("path/to/video_frame.jpg")</span>
<span></span><span class="sd">        &gt;&gt;&gt; bboxes = [[100, 100, 200, 200]]</span>
<span></span><span class="sd">        &gt;&gt;&gt; results = predictor(bboxes=bboxes)</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        The `fill_hole_area` attribute is defined but not used in the current implementation.</span>
<span></span><span class="sd">    """</span>
<span></span>
<span></span>    <span class="c1"># fill_hole_area = 8  # not used</span>
<span></span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="o">=</span><span class="n">DEFAULT_CFG</span><span class="p">,</span> <span class="n">overrides</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">        </span><span class="sd">"""Initialize the predictor with configuration and optional overrides.</span>
<span></span>
<span></span><span class="sd">        This constructor initializes the SAM2VideoPredictor with a given configuration, applies any specified overrides,</span>
<span></span><span class="sd">        and sets up the inference state along with certain flags that control the behavior of the predictor.</span>
<span></span>
<span></span><span class="sd">        Args:</span>
<span></span><span class="sd">            cfg (dict): Configuration dictionary containing default settings.</span>
<span></span><span class="sd">            overrides (dict | None): Dictionary of values to override default configuration.</span>
<span></span><span class="sd">            _callbacks (dict | None): Dictionary of callback functions to customize behavior.</span>
<span></span><span class="sd">        """</span>
<span></span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">overrides</span><span class="p">,</span> <span class="n">_callbacks</span><span class="p">)</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">non_overlap_masks</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem_around_input</span> <span class="o">=</span> <span class="kc">False</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem_for_multi_obj</span> <span class="o">=</span> <span class="kc">False</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">[</span><span class="s2">"on_predict_start"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_state</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._add_output_per_object"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._add_output_per_object</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_add_output_per_object</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">current_out</span><span class="p">,</span> <span class="n">storage_key</span><span class="p">)</span>
</code></pre></div><p>Split a multi-object output into per-object output slices and add them into Output_Dict_Per_Obj.</p><p>The resulting slices share the same tensor storage.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td><code>int</code></td><td>The index of the current frame.</td><td><em>required</em></td></tr><tr><td><code>current_out</code></td><td><code>dict</code></td><td>The current output dictionary containing multi-object outputs.</td><td><em>required</em></td></tr><tr><td><code>storage_key</code></td><td><code>str</code></td><td>The key used to store the output in the per-object output dictionary.</td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1609-L1637"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_add_output_per_object</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">current_out</span><span class="p">,</span> <span class="n">storage_key</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Split a multi-object output into per-object output slices and add them into Output_Dict_Per_Obj.</span>
<span></span>
<span></span><span class="sd">    The resulting slices share the same tensor storage.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        frame_idx (int): The index of the current frame.</span>
<span></span><span class="sd">        current_out (dict): The current output dictionary containing multi-object outputs.</span>
<span></span><span class="sd">        storage_key (str): The key used to store the output in the per-object output dictionary.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">maskmem_features</span> <span class="o">=</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">]</span>
<span></span>    <span class="k">assert</span> <span class="n">maskmem_features</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">maskmem_features</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">maskmem_pos_enc</span> <span class="o">=</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span>
<span></span>    <span class="k">assert</span> <span class="n">maskmem_pos_enc</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">maskmem_pos_enc</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">for</span> <span class="n">obj_idx</span><span class="p">,</span> <span class="n">obj_output_dict</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<span></span>        <span class="n">obj_slice</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">obj_idx</span><span class="p">,</span> <span class="n">obj_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span></span>        <span class="n">obj_out</span> <span class="o">=</span> <span class="p">{</span>
<span></span>            <span class="s2">"maskmem_features"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span></span>            <span class="s2">"maskmem_pos_enc"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span></span>            <span class="s2">"pred_masks"</span><span class="p">:</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">][</span><span class="n">obj_slice</span><span class="p">],</span>
<span></span>            <span class="s2">"obj_ptr"</span><span class="p">:</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">][</span><span class="n">obj_slice</span><span class="p">],</span>
<span></span>        <span class="p">}</span>
<span></span>        <span class="k">if</span> <span class="n">maskmem_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">obj_out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">]</span> <span class="o">=</span> <span class="n">maskmem_features</span><span class="p">[</span><span class="n">obj_slice</span><span class="p">]</span>
<span></span>        <span class="k">if</span> <span class="n">maskmem_pos_enc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">obj_out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="n">obj_slice</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">maskmem_pos_enc</span><span class="p">]</span>
<span></span>        <span class="n">obj_output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj_out</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._clear_non_cond_mem_around_input"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._clear_non_cond_mem_around_input</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_clear_non_cond_mem_around_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">)</span>
</code></pre></div><p>Remove the non-conditioning memory around the input frame.</p><p>When users provide correction clicks, the surrounding frames' non-conditioning memories can still contain outdated object appearance information and could confuse the model. This method clears those non-conditioning memories surrounding the interacted frame to avoid giving the model both old and new information about the object.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td><code>int</code></td><td>The index of the current frame where user interaction occurred.</td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1639-L1656"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_clear_non_cond_mem_around_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Remove the non-conditioning memory around the input frame.</span>
<span></span>
<span></span><span class="sd">    When users provide correction clicks, the surrounding frames' non-conditioning memories can still contain</span>
<span></span><span class="sd">    outdated object appearance information and could confuse the model. This method clears those non-conditioning</span>
<span></span><span class="sd">    memories surrounding the interacted frame to avoid giving the model both old and new information about the</span>
<span></span><span class="sd">    object.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        frame_idx (int): The index of the current frame where user interaction occurred.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">memory_temporal_stride_for_eval</span>
<span></span>    <span class="n">frame_idx_begin</span> <span class="o">=</span> <span class="n">frame_idx</span> <span class="o">-</span> <span class="n">r</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_maskmem</span>
<span></span>    <span class="n">frame_idx_end</span> <span class="o">=</span> <span class="n">frame_idx</span> <span class="o">+</span> <span class="n">r</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_maskmem</span>
<span></span>    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">frame_idx_begin</span><span class="p">,</span> <span class="n">frame_idx_end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict"</span><span class="p">][</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>        <span class="k">for</span> <span class="n">obj_output_dict</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>            <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._consolidate_temp_output_across_obj"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._consolidate_temp_output_across_obj</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_consolidate_temp_output_across_obj</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">,</span> <span class="n">is_cond</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">run_mem_encoder</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</code></pre></div><p>Consolidate per-object temporary outputs into a single output for all objects.</p><p>This method combines the temporary outputs for each object on a given frame into a unified output. It fills in any missing objects either from the main output dictionary or leaves placeholders if they do not exist in the main output. Optionally, it can re-run the memory encoder after applying non-overlapping constraints to the object scores.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td><code>int</code></td><td>The index of the frame for which to consolidate outputs.</td><td><em>required</em></td></tr><tr><td><code>is_cond</code></td><td><code>bool, optional</code></td><td>Indicates if the frame is considered a conditioning frame.</td><td><code>False</code></td></tr><tr><td><code>run_mem_encoder</code></td><td><code>bool, optional</code></td><td>Specifies whether to run the memory encoder after consolidating the outputs.</td><td><code>False</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>dict</code></td><td>A consolidated output dictionary containing the combined results for all objects.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>The method initializes the consolidated output with placeholder values for missing objects.</li><li>It searches for outputs in both the temporary and main output dictionaries.</li><li>If <code>run_mem_encoder</code> is True, it applies non-overlapping constraints and re-runs the memory encoder.</li><li>The <code>maskmem_features</code> and <code>maskmem_pos_enc</code> are only populated when <code>run_mem_encoder</code> is True.</li></ul></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1442-L1545"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_consolidate_temp_output_across_obj</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">,</span>
<span></span>    <span class="n">is_cond</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="n">run_mem_encoder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Consolidate per-object temporary outputs into a single output for all objects.</span>
<span></span>
<span></span><span class="sd">    This method combines the temporary outputs for each object on a given frame into a unified</span>
<span></span><span class="sd">    output. It fills in any missing objects either from the main output dictionary or leaves</span>
<span></span><span class="sd">    placeholders if they do not exist in the main output. Optionally, it can re-run the memory encoder after</span>
<span></span><span class="sd">    applying non-overlapping constraints to the object scores.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        frame_idx (int): The index of the frame for which to consolidate outputs.</span>
<span></span><span class="sd">        is_cond (bool, optional): Indicates if the frame is considered a conditioning frame.</span>
<span></span><span class="sd">        run_mem_encoder (bool, optional): Specifies whether to run the memory encoder after consolidating the</span>
<span></span><span class="sd">            outputs.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (dict): A consolidated output dictionary containing the combined results for all objects.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - The method initializes the consolidated output with placeholder values for missing objects.</span>
<span></span><span class="sd">        - It searches for outputs in both the temporary and main output dictionaries.</span>
<span></span><span class="sd">        - If `run_mem_encoder` is True, it applies non-overlapping constraints and re-runs the memory encoder.</span>
<span></span><span class="sd">        - The `maskmem_features` and `maskmem_pos_enc` are only populated when `run_mem_encoder` is True.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_idx_to_id"</span><span class="p">])</span>
<span></span>    <span class="n">storage_key</span> <span class="o">=</span> <span class="s2">"cond_frame_outputs"</span> <span class="k">if</span> <span class="n">is_cond</span> <span class="k">else</span> <span class="s2">"non_cond_frame_outputs"</span>
<span></span>
<span></span>    <span class="c1"># Initialize `consolidated_out`. Its "maskmem_features" and "maskmem_pos_enc"</span>
<span></span>    <span class="c1"># will be added when rerunning the memory encoder after applying non-overlapping</span>
<span></span>    <span class="c1"># constraints to object scores. Its "pred_masks" are prefilled with a large</span>
<span></span>    <span class="c1"># negative value (NO_OBJ_SCORE) to represent missing objects.</span>
<span></span>    <span class="n">consolidated_out</span> <span class="o">=</span> <span class="p">{</span>
<span></span>        <span class="s2">"maskmem_features"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="s2">"maskmem_pos_enc"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="s2">"pred_masks"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
<span></span>            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">4</span><span class="p">),</span>
<span></span>            <span class="n">fill_value</span><span class="o">=-</span><span class="mf">1024.0</span><span class="p">,</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
<span></span>            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>        <span class="p">),</span>
<span></span>        <span class="s2">"obj_ptr"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
<span></span>            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">),</span>
<span></span>            <span class="n">fill_value</span><span class="o">=-</span><span class="mf">1024.0</span><span class="p">,</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
<span></span>            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>        <span class="p">),</span>
<span></span>        <span class="s2">"object_score_logits"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
<span></span>            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span></span>            <span class="c1"># default to 10.0 for object_score_logits, i.e. assuming the object is</span>
<span></span>            <span class="c1"># present as sigmoid(10)=1, same as in `predict_masks` of `MaskDecoder`</span>
<span></span>            <span class="n">fill_value</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
<span></span>            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>        <span class="p">),</span>
<span></span>    <span class="p">}</span>
<span></span>    <span class="k">for</span> <span class="n">obj_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
<span></span>        <span class="n">obj_temp_output_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"temp_output_dict_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span>
<span></span>        <span class="n">obj_output_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span>
<span></span>        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span>
<span></span>            <span class="n">obj_temp_output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>            <span class="c1"># If the object doesn't appear in "temp_output_dict_per_obj" on this frame,</span>
<span></span>            <span class="c1"># we fall back and look up its previous output in "output_dict_per_obj".</span>
<span></span>            <span class="c1"># We look up both "cond_frame_outputs" and "non_cond_frame_outputs" in</span>
<span></span>            <span class="c1"># "output_dict_per_obj" to find a previous output for this object.</span>
<span></span>            <span class="ow">or</span> <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>            <span class="ow">or</span> <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="c1"># If the object doesn't appear in "output_dict_per_obj" either, we skip it</span>
<span></span>        <span class="c1"># and leave its mask scores to the default scores (i.e. the NO_OBJ_SCORE</span>
<span></span>        <span class="c1"># placeholder above) and set its object pointer to be a dummy pointer.</span>
<span></span>        <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="c1"># Fill in dummy object pointers for those objects without any inputs or</span>
<span></span>            <span class="c1"># tracking outcomes on this frame (only do it under `run_mem_encoder=True`,</span>
<span></span>            <span class="c1"># i.e. when we need to build the memory for tracking).</span>
<span></span>            <span class="k">if</span> <span class="n">run_mem_encoder</span><span class="p">:</span>
<span></span>                <span class="c1"># fill object pointer with a dummy pointer (based on an empty mask)</span>
<span></span>                <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">][</span><span class="n">obj_idx</span> <span class="p">:</span> <span class="n">obj_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_empty_mask_ptr</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>            <span class="k">continue</span>
<span></span>        <span class="c1"># Add the temporary object output mask to consolidated output mask</span>
<span></span>        <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">][</span><span class="n">obj_idx</span> <span class="p">:</span> <span class="n">obj_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span>
<span></span>        <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">][</span><span class="n">obj_idx</span> <span class="p">:</span> <span class="n">obj_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">]</span>
<span></span>
<span></span>    <span class="c1"># Optionally, apply non-overlapping constraints on the consolidated scores and rerun the memory encoder</span>
<span></span>    <span class="k">if</span> <span class="n">run_mem_encoder</span><span class="p">:</span>
<span></span>        <span class="n">high_res_masks</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
<span></span>            <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">],</span>
<span></span>            <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">,</span>
<span></span>            <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">,</span>
<span></span>            <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">non_overlap_masks_for_mem_enc</span><span class="p">:</span>
<span></span>            <span class="n">high_res_masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_apply_non_overlapping_constraints</span><span class="p">(</span><span class="n">high_res_masks</span><span class="p">)</span>
<span></span>        <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">],</span> <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_memory_encoder</span><span class="p">(</span>
<span></span>            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span></span>            <span class="n">high_res_masks</span><span class="o">=</span><span class="n">high_res_masks</span><span class="p">,</span>
<span></span>            <span class="n">is_mask_from_pts</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># these frames are what the user interacted with</span>
<span></span>            <span class="n">object_score_logits</span><span class="o">=</span><span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"object_score_logits"</span><span class="p">],</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">consolidated_out</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._get_empty_mask_ptr"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._get_empty_mask_ptr</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_get_empty_mask_ptr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">)</span>
</code></pre></div><p>Get a dummy object pointer based on an empty mask on the current frame.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>frame_idx</code></td><td><code>int</code></td><td>The index of the current frame for which to generate the dummy object pointer.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>torch.Tensor</code></td><td>A tensor representing the dummy object pointer generated from the empty mask.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1547-L1575"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_get_empty_mask_ptr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_idx</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Get a dummy object pointer based on an empty mask on the current frame.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        frame_idx (int): The index of the current frame for which to generate the dummy object pointer.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (torch.Tensor): A tensor representing the dummy object pointer generated from the empty mask.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="c1"># Retrieve correct image features</span>
<span></span>    <span class="n">current_vision_feats</span><span class="p">,</span> <span class="n">current_vision_pos_embeds</span><span class="p">,</span> <span class="n">feat_sizes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"im"</span><span class="p">])</span>
<span></span>
<span></span>    <span class="c1"># Feed the empty mask and image feature above to get a dummy object pointer</span>
<span></span>    <span class="n">current_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">track_step</span><span class="p">(</span>
<span></span>        <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span><span class="p">,</span>
<span></span>        <span class="n">is_init_cond_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span></span>        <span class="n">current_vision_feats</span><span class="o">=</span><span class="n">current_vision_feats</span><span class="p">,</span>
<span></span>        <span class="n">current_vision_pos_embeds</span><span class="o">=</span><span class="n">current_vision_pos_embeds</span><span class="p">,</span>
<span></span>        <span class="n">feat_sizes</span><span class="o">=</span><span class="n">feat_sizes</span><span class="p">,</span>
<span></span>        <span class="n">point_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="c1"># A dummy (empty) mask with a single object</span>
<span></span>        <span class="n">mask_inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
<span></span>        <span class="n">output_dict</span><span class="o">=</span><span class="p">{},</span>
<span></span>        <span class="n">num_frames</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"num_frames"</span><span class="p">],</span>
<span></span>        <span class="n">track_in_reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="n">run_mem_encoder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="n">prev_sam_mask_logits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">]</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._get_maskmem_pos_enc"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._get_maskmem_pos_enc</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_get_maskmem_pos_enc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out_maskmem_pos_enc</span><span class="p">)</span>
</code></pre></div><p>Cache and manage the positional encoding for mask memory across frames and objects.</p><p>This method optimizes storage by caching the positional encoding (<code>maskmem_pos_enc</code>) for mask memory, which is constant across frames and objects, thus reducing the amount of redundant information stored during an inference session. It checks if the positional encoding has already been cached; if not, it caches a slice of the provided encoding. If the batch size is greater than one, it expands the cached positional encoding to match the current batch size.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>out_maskmem_pos_enc</code></td><td><code>list[torch.Tensor] | None</code></td><td>The positional encoding for mask memory. Should be a list of tensors or None.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>list[torch.Tensor]</code></td><td>The positional encoding for mask memory, either cached or expanded.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>The method assumes that <code>out_maskmem_pos_enc</code> is a list of tensors or None.</li><li>Only a single object's slice is cached since the encoding is the same across objects.</li><li>The method checks if the positional encoding has already been cached in the session's constants.</li><li>If the batch size is greater than one, the cached encoding is expanded to fit the batch size.</li></ul></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1404-L1440"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_get_maskmem_pos_enc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out_maskmem_pos_enc</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Cache and manage the positional encoding for mask memory across frames and objects.</span>
<span></span>
<span></span><span class="sd">    This method optimizes storage by caching the positional encoding (`maskmem_pos_enc`) for mask memory, which is</span>
<span></span><span class="sd">    constant across frames and objects, thus reducing the amount of redundant information stored during an inference</span>
<span></span><span class="sd">    session. It checks if the positional encoding has already been cached; if not, it caches a slice of the provided</span>
<span></span><span class="sd">    encoding. If the batch size is greater than one, it expands the cached positional encoding to match the current</span>
<span></span><span class="sd">    batch size.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        out_maskmem_pos_enc (list[torch.Tensor] | None): The positional encoding for mask memory. Should be a list</span>
<span></span><span class="sd">            of tensors or None.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (list[torch.Tensor]): The positional encoding for mask memory, either cached or expanded.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - The method assumes that `out_maskmem_pos_enc` is a list of tensors or None.</span>
<span></span><span class="sd">        - Only a single object's slice is cached since the encoding is the same across objects.</span>
<span></span><span class="sd">        - The method checks if the positional encoding has already been cached in the session's constants.</span>
<span></span><span class="sd">        - If the batch size is greater than one, the cached encoding is expanded to fit the batch size.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">model_constants</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"constants"</span><span class="p">]</span>
<span></span>    <span class="c1"># "out_maskmem_pos_enc" should be either a list of tensors or None</span>
<span></span>    <span class="k">if</span> <span class="n">out_maskmem_pos_enc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="k">if</span> <span class="s2">"maskmem_pos_enc"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_constants</span><span class="p">:</span>
<span></span>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out_maskmem_pos_enc</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
<span></span>            <span class="c1"># only take the slice for one object, since it's same across objects</span>
<span></span>            <span class="n">maskmem_pos_enc</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">out_maskmem_pos_enc</span><span class="p">]</span>
<span></span>            <span class="n">model_constants</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span> <span class="o">=</span> <span class="n">maskmem_pos_enc</span>
<span></span>        <span class="k">else</span><span class="p">:</span>
<span></span>            <span class="n">maskmem_pos_enc</span> <span class="o">=</span> <span class="n">model_constants</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span>
<span></span>        <span class="c1"># expand the cached maskmem_pos_enc to the actual batch size</span>
<span></span>        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">out_maskmem_pos_enc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span></span>        <span class="k">if</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<span></span>            <span class="n">out_maskmem_pos_enc</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">maskmem_pos_enc</span><span class="p">]</span>
<span></span>    <span class="k">return</span> <span class="n">out_maskmem_pos_enc</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._obj_id_to_idx"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._obj_id_to_idx</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_obj_id_to_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">)</span>
</code></pre></div><p>Map client-side object id to model-side object index.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>obj_id</code></td><td><code>int</code></td><td>The unique identifier of the object provided by the client side.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>int</code></td><td>The index of the object on the model side.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>The method updates or retrieves mappings between object IDs and indices stored in <code>inference_state</code>.</li><li>It ensures that new objects can only be added before tracking commences.</li><li>It maintains two-way mappings between IDs and indices (<code>obj_id_to_idx</code> and <code>obj_idx_to_id</code>).</li><li>Additional data structures are initialized for the new object to store inputs and outputs.</li></ul></div><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>RuntimeError</code></td><td>If an attempt is made to add a new object after tracking has started.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1279-L1328"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_obj_id_to_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Map client-side object id to model-side object index.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        obj_id (int): The unique identifier of the object provided by the client side.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (int): The index of the object on the model side.</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        RuntimeError: If an attempt is made to add a new object after tracking has started.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - The method updates or retrieves mappings between object IDs and indices stored in</span>
<span></span><span class="sd">          `inference_state`.</span>
<span></span><span class="sd">        - It ensures that new objects can only be added before tracking commences.</span>
<span></span><span class="sd">        - It maintains two-way mappings between IDs and indices (`obj_id_to_idx` and `obj_idx_to_id`).</span>
<span></span><span class="sd">        - Additional data structures are initialized for the new object to store inputs and outputs.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">obj_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_id_to_idx"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">obj_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">obj_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="k">return</span> <span class="n">obj_idx</span>
<span></span>
<span></span>    <span class="c1"># This is a new object id not sent to the server before. We only allow adding</span>
<span></span>    <span class="c1"># new objects *before* the tracking starts.</span>
<span></span>    <span class="n">allow_new_object</span> <span class="o">=</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"tracking_has_started"</span><span class="p">]</span>
<span></span>    <span class="k">if</span> <span class="n">allow_new_object</span><span class="p">:</span>
<span></span>        <span class="c1"># get the next object slot</span>
<span></span>        <span class="n">obj_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_id_to_idx"</span><span class="p">])</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_id_to_idx"</span><span class="p">][</span><span class="n">obj_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj_idx</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_idx_to_id"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj_id</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_ids"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_id_to_idx"</span><span class="p">])</span>
<span></span>        <span class="c1"># set up input and output structures for this object</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"point_inputs_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"mask_inputs_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
<span></span>            <span class="s2">"cond_frame_outputs"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># dict containing {frame_idx: &lt;out&gt;}</span>
<span></span>            <span class="s2">"non_cond_frame_outputs"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># dict containing {frame_idx: &lt;out&gt;}</span>
<span></span>        <span class="p">}</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"temp_output_dict_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
<span></span>            <span class="s2">"cond_frame_outputs"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># dict containing {frame_idx: &lt;out&gt;}</span>
<span></span>            <span class="s2">"non_cond_frame_outputs"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># dict containing {frame_idx: &lt;out&gt;}</span>
<span></span>        <span class="p">}</span>
<span></span>        <span class="k">return</span> <span class="n">obj_idx</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
<span></span>            <span class="sa">f</span><span class="s2">"Cannot add new object id </span><span class="si">{</span><span class="n">obj_id</span><span class="si">}</span><span class="s2"> after tracking starts. "</span>
<span></span>            <span class="sa">f</span><span class="s2">"All existing object ids: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s1">'obj_ids'</span><span class="p">]</span><span class="si">}</span><span class="s2">. "</span>
<span></span>            <span class="sa">f</span><span class="s2">"Please call 'reset_state' to restart from scratch."</span>
<span></span>        <span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._run_memory_encoder"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._run_memory_encoder</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_run_memory_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">high_res_masks</span><span class="p">,</span> <span class="n">object_score_logits</span><span class="p">,</span> <span class="n">is_mask_from_pts</span><span class="p">)</span>
</code></pre></div><p>Run the memory encoder on masks.</p><p>This is usually after applying non-overlapping constraints to object scores. Since their scores changed, their memory also needs to be computed again with the memory encoder.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>batch_size</code></td><td><code>int</code></td><td>The batch size for processing the frame.</td><td><em>required</em></td></tr><tr><td><code>high_res_masks</code></td><td><code>torch.Tensor</code></td><td>High-resolution masks for which to compute the memory.</td><td><em>required</em></td></tr><tr><td><code>object_score_logits</code></td><td><code>torch.Tensor</code></td><td>Logits representing the object scores.</td><td><em>required</em></td></tr><tr><td><code>is_mask_from_pts</code></td><td><code>bool</code></td><td>Indicates if the mask is derived from point interactions.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>maskmem_features (torch.Tensor)</code></td><td>The encoded mask features.</td></tr><tr><td><code>maskmem_pos_enc (torch.Tensor)</code></td><td>The positional encoding.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1577-L1607"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_run_memory_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">high_res_masks</span><span class="p">,</span> <span class="n">object_score_logits</span><span class="p">,</span> <span class="n">is_mask_from_pts</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Run the memory encoder on masks.</span>
<span></span>
<span></span><span class="sd">    This is usually after applying non-overlapping constraints to object scores. Since their scores changed, their</span>
<span></span><span class="sd">    memory also needs to be computed again with the memory encoder.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        batch_size (int): The batch size for processing the frame.</span>
<span></span><span class="sd">        high_res_masks (torch.Tensor): High-resolution masks for which to compute the memory.</span>
<span></span><span class="sd">        object_score_logits (torch.Tensor): Logits representing the object scores.</span>
<span></span><span class="sd">        is_mask_from_pts (bool): Indicates if the mask is derived from point interactions.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        maskmem_features (torch.Tensor): The encoded mask features.</span>
<span></span><span class="sd">        maskmem_pos_enc (torch.Tensor): The positional encoding.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="c1"># Retrieve correct image features</span>
<span></span>    <span class="n">current_vision_feats</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">feat_sizes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"im"</span><span class="p">],</span> <span class="n">batch_size</span><span class="p">)</span>
<span></span>    <span class="n">maskmem_features</span><span class="p">,</span> <span class="n">maskmem_pos_enc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_encode_new_memory</span><span class="p">(</span>
<span></span>        <span class="n">current_vision_feats</span><span class="o">=</span><span class="n">current_vision_feats</span><span class="p">,</span>
<span></span>        <span class="n">feat_sizes</span><span class="o">=</span><span class="n">feat_sizes</span><span class="p">,</span>
<span></span>        <span class="n">pred_masks_high_res</span><span class="o">=</span><span class="n">high_res_masks</span><span class="p">,</span>
<span></span>        <span class="n">is_mask_from_pts</span><span class="o">=</span><span class="n">is_mask_from_pts</span><span class="p">,</span>
<span></span>        <span class="n">object_score_logits</span><span class="o">=</span><span class="n">object_score_logits</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># "maskmem_pos_enc" is the same across frames, so we only need to store one copy of it</span>
<span></span>    <span class="n">maskmem_pos_enc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_maskmem_pos_enc</span><span class="p">(</span><span class="n">maskmem_pos_enc</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">maskmem_features</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
<span></span>        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"cuda"</span>
<span></span>    <span class="p">),</span> <span class="n">maskmem_pos_enc</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor._run_single_frame_inference"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor._run_single_frame_inference</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_run_single_frame_inference</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">output_dict</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">,</span>
<span></span>    <span class="n">batch_size</span><span class="p">,</span>
<span></span>    <span class="n">is_init_cond_frame</span><span class="p">,</span>
<span></span>    <span class="n">point_inputs</span><span class="p">,</span>
<span></span>    <span class="n">mask_inputs</span><span class="p">,</span>
<span></span>    <span class="n">reverse</span><span class="p">,</span>
<span></span>    <span class="n">run_mem_encoder</span><span class="p">,</span>
<span></span>    <span class="n">prev_sam_mask_logits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div><p>Run tracking on a single frame based on current inputs and previous memory.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>output_dict</code></td><td><code>dict</code></td><td>The dictionary containing the output states of the tracking process.</td><td><em>required</em></td></tr><tr><td><code>frame_idx</code></td><td><code>int</code></td><td>The index of the current frame.</td><td><em>required</em></td></tr><tr><td><code>batch_size</code></td><td><code>int</code></td><td>The batch size for processing the frame.</td><td><em>required</em></td></tr><tr><td><code>is_init_cond_frame</code></td><td><code>bool</code></td><td>Indicates if the current frame is an initial conditioning frame.</td><td><em>required</em></td></tr><tr><td><code>point_inputs</code></td><td><code>dict | None</code></td><td>Input points and their labels.</td><td><em>required</em></td></tr><tr><td><code>mask_inputs</code></td><td><code>torch.Tensor | None</code></td><td>Input binary masks.</td><td><em>required</em></td></tr><tr><td><code>reverse</code></td><td><code>bool</code></td><td>Indicates if the tracking should be performed in reverse order.</td><td><em>required</em></td></tr><tr><td><code>run_mem_encoder</code></td><td><code>bool</code></td><td>Indicates if the memory encoder should be executed.</td><td><em>required</em></td></tr><tr><td><code>prev_sam_mask_logits</code></td><td><code>torch.Tensor | None</code></td><td>Previous mask logits for the current object.</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>dict</code></td><td>A dictionary containing the output of the tracking step, including updated features and predictions.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>The method assumes that <code>point_inputs</code> and <code>mask_inputs</code> are mutually exclusive.</li><li>The method retrieves image features using the <code>get_im_features</code> method.</li><li>The <code>maskmem_pos_enc</code> is assumed to be constant across frames, hence only one copy is stored.</li><li>The <code>fill_holes_in_mask_scores</code> function is commented out and currently unsupported due to CUDA extension requirements.</li></ul></div><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>AssertionError</code></td><td>If both <code>point_inputs</code> and <code>mask_inputs</code> are provided, or neither is provided.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1330-L1402"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_run_single_frame_inference</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">output_dict</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="p">,</span>
<span></span>    <span class="n">batch_size</span><span class="p">,</span>
<span></span>    <span class="n">is_init_cond_frame</span><span class="p">,</span>
<span></span>    <span class="n">point_inputs</span><span class="p">,</span>
<span></span>    <span class="n">mask_inputs</span><span class="p">,</span>
<span></span>    <span class="n">reverse</span><span class="p">,</span>
<span></span>    <span class="n">run_mem_encoder</span><span class="p">,</span>
<span></span>    <span class="n">prev_sam_mask_logits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Run tracking on a single frame based on current inputs and previous memory.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        output_dict (dict): The dictionary containing the output states of the tracking process.</span>
<span></span><span class="sd">        frame_idx (int): The index of the current frame.</span>
<span></span><span class="sd">        batch_size (int): The batch size for processing the frame.</span>
<span></span><span class="sd">        is_init_cond_frame (bool): Indicates if the current frame is an initial conditioning frame.</span>
<span></span><span class="sd">        point_inputs (dict | None): Input points and their labels.</span>
<span></span><span class="sd">        mask_inputs (torch.Tensor | None): Input binary masks.</span>
<span></span><span class="sd">        reverse (bool): Indicates if the tracking should be performed in reverse order.</span>
<span></span><span class="sd">        run_mem_encoder (bool): Indicates if the memory encoder should be executed.</span>
<span></span><span class="sd">        prev_sam_mask_logits (torch.Tensor | None): Previous mask logits for the current object.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (dict): A dictionary containing the output of the tracking step, including updated features and predictions.</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        AssertionError: If both `point_inputs` and `mask_inputs` are provided, or neither is provided.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - The method assumes that `point_inputs` and `mask_inputs` are mutually exclusive.</span>
<span></span><span class="sd">        - The method retrieves image features using the `get_im_features` method.</span>
<span></span><span class="sd">        - The `maskmem_pos_enc` is assumed to be constant across frames, hence only one copy is stored.</span>
<span></span><span class="sd">        - The `fill_holes_in_mask_scores` function is commented out and currently unsupported due to CUDA extension requirements.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="c1"># Retrieve correct image features</span>
<span></span>    <span class="n">current_vision_feats</span><span class="p">,</span> <span class="n">current_vision_pos_embeds</span><span class="p">,</span> <span class="n">feat_sizes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"im"</span><span class="p">],</span> <span class="n">batch_size</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># point and mask should not appear as input simultaneously on the same frame</span>
<span></span>    <span class="k">assert</span> <span class="n">point_inputs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">mask_inputs</span> <span class="ow">is</span> <span class="kc">None</span>
<span></span>    <span class="n">current_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">track_step</span><span class="p">(</span>
<span></span>        <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span><span class="p">,</span>
<span></span>        <span class="n">is_init_cond_frame</span><span class="o">=</span><span class="n">is_init_cond_frame</span><span class="p">,</span>
<span></span>        <span class="n">current_vision_feats</span><span class="o">=</span><span class="n">current_vision_feats</span><span class="p">,</span>
<span></span>        <span class="n">current_vision_pos_embeds</span><span class="o">=</span><span class="n">current_vision_pos_embeds</span><span class="p">,</span>
<span></span>        <span class="n">feat_sizes</span><span class="o">=</span><span class="n">feat_sizes</span><span class="p">,</span>
<span></span>        <span class="n">point_inputs</span><span class="o">=</span><span class="n">point_inputs</span><span class="p">,</span>
<span></span>        <span class="n">mask_inputs</span><span class="o">=</span><span class="n">mask_inputs</span><span class="p">,</span>
<span></span>        <span class="n">output_dict</span><span class="o">=</span><span class="n">output_dict</span><span class="p">,</span>
<span></span>        <span class="n">num_frames</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"num_frames"</span><span class="p">],</span>
<span></span>        <span class="n">track_in_reverse</span><span class="o">=</span><span class="n">reverse</span><span class="p">,</span>
<span></span>        <span class="n">run_mem_encoder</span><span class="o">=</span><span class="n">run_mem_encoder</span><span class="p">,</span>
<span></span>        <span class="n">prev_sam_mask_logits</span><span class="o">=</span><span class="n">prev_sam_mask_logits</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="n">maskmem_features</span> <span class="o">=</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">]</span>
<span></span>    <span class="k">if</span> <span class="n">maskmem_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">current_out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">]</span> <span class="o">=</span> <span class="n">maskmem_features</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"cuda"</span>
<span></span>        <span class="p">)</span>
<span></span>    <span class="c1"># NOTE: Do not support the `fill_holes_in_mask_scores` function since it needs cuda extensions</span>
<span></span>    <span class="c1"># potentially fill holes in the predicted masks</span>
<span></span>    <span class="c1"># if self.fill_hole_area &gt; 0:</span>
<span></span>    <span class="c1">#     pred_masks = current_out["pred_masks"].to(self.device, non_blocking=self.device.type == "cuda")</span>
<span></span>    <span class="c1">#     pred_masks = fill_holes_in_mask_scores(pred_masks, self.fill_hole_area)</span>
<span></span>
<span></span>    <span class="c1"># "maskmem_pos_enc" is the same across frames, so we only need to store one copy of it</span>
<span></span>    <span class="n">current_out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_maskmem_pos_enc</span><span class="p">(</span><span class="n">current_out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">])</span>
<span></span>    <span class="k">return</span> <span class="n">current_out</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.add_new_prompts"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.add_new_prompts</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">add_new_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">,</span> <span class="n">points</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">frame_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div><p>Add new points or masks to a specific frame for a given object ID.</p><p>This method updates the inference state with new prompts (points or masks) for a specified object and frame index. It ensures that the prompts are either points or masks, but not both, and updates the internal state accordingly. It also handles the generation of new segmentations based on the provided prompts and the existing state.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>obj_id</code></td><td><code>int</code></td><td>The ID of the object to which the prompts are associated.</td><td><em>required</em></td></tr><tr><td><code>points</code></td><td><code>torch.Tensor, optional</code></td><td>The coordinates of the points of interest.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>torch.Tensor, optional</code></td><td>The labels corresponding to the points.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>torch.Tensor, optional</code></td><td>Binary masks for the object.</td><td><code>None</code></td></tr><tr><td><code>frame_idx</code></td><td><code>int, optional</code></td><td>The index of the frame to which the prompts are applied.</td><td><code>0</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>The flattened predicted masks.</td></tr><tr><td><code>pred_scores (torch.Tensor)</code></td><td>A tensor of ones indicating the number of objects.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>Only one type of prompt (either points or masks) can be added per call.</li><li>If the frame is being tracked for the first time, it is treated as an initial conditioning frame.</li><li>The method handles the consolidation of outputs and resizing of masks to the original video resolution.</li></ul></div><p><strong>Raises</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>AssertionError</code></td><td>If both <code>masks</code> and <code>points</code> are provided, or neither is provided.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1033-L1133"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">add_new_prompts</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">obj_id</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">frame_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Add new points or masks to a specific frame for a given object ID.</span>
<span></span>
<span></span><span class="sd">    This method updates the inference state with new prompts (points or masks) for a specified object and frame</span>
<span></span><span class="sd">    index. It ensures that the prompts are either points or masks, but not both, and updates the internal state</span>
<span></span><span class="sd">    accordingly. It also handles the generation of new segmentations based on the provided prompts and the existing</span>
<span></span><span class="sd">    state.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        obj_id (int): The ID of the object to which the prompts are associated.</span>
<span></span><span class="sd">        points (torch.Tensor, optional): The coordinates of the points of interest.</span>
<span></span><span class="sd">        labels (torch.Tensor, optional): The labels corresponding to the points.</span>
<span></span><span class="sd">        masks (torch.Tensor, optional): Binary masks for the object.</span>
<span></span><span class="sd">        frame_idx (int, optional): The index of the frame to which the prompts are applied.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): The flattened predicted masks.</span>
<span></span><span class="sd">        pred_scores (torch.Tensor): A tensor of ones indicating the number of objects.</span>
<span></span>
<span></span><span class="sd">    Raises:</span>
<span></span><span class="sd">        AssertionError: If both `masks` and `points` are provided, or neither is provided.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - Only one type of prompt (either points or masks) can be added per call.</span>
<span></span><span class="sd">        - If the frame is being tracked for the first time, it is treated as an initial conditioning frame.</span>
<span></span><span class="sd">        - The method handles the consolidation of outputs and resizing of masks to the original video resolution.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">assert</span> <span class="p">(</span><span class="n">masks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span><span class="n">points</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span> <span class="s2">"'masks' and 'points' prompts are not compatible with each other."</span>
<span></span>    <span class="n">obj_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_obj_id_to_idx</span><span class="p">(</span><span class="n">obj_id</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">point_inputs</span> <span class="o">=</span> <span class="kc">None</span>
<span></span>    <span class="n">pop_key</span> <span class="o">=</span> <span class="s2">"point_inputs_per_obj"</span>
<span></span>    <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">point_inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"point_coords"</span><span class="p">:</span> <span class="n">points</span><span class="p">,</span> <span class="s2">"point_labels"</span><span class="p">:</span> <span class="n">labels</span><span class="p">}</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"point_inputs_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">point_inputs</span>
<span></span>        <span class="n">pop_key</span> <span class="o">=</span> <span class="s2">"mask_inputs_per_obj"</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"mask_inputs_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">masks</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="n">pop_key</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>    <span class="c1"># If this frame hasn't been tracked before, we treat it as an initial conditioning</span>
<span></span>    <span class="c1"># frame, meaning that the inputs points are to generate segments on this frame without</span>
<span></span>    <span class="c1"># using any memory from other frames, like in SAM. Otherwise (if it has been tracked),</span>
<span></span>    <span class="c1"># the input points will be used to correct the already tracked masks.</span>
<span></span>    <span class="n">is_init_cond_frame</span> <span class="o">=</span> <span class="n">frame_idx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"frames_already_tracked"</span><span class="p">]</span>
<span></span>    <span class="n">obj_output_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span>
<span></span>    <span class="n">obj_temp_output_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"temp_output_dict_per_obj"</span><span class="p">][</span><span class="n">obj_idx</span><span class="p">]</span>
<span></span>    <span class="c1"># Add a frame to conditioning output if it's an initial conditioning frame or</span>
<span></span>    <span class="c1"># if the model sees all frames receiving clicks/mask as conditioning frames.</span>
<span></span>    <span class="n">is_cond</span> <span class="o">=</span> <span class="n">is_init_cond_frame</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add_all_frames_to_correct_as_cond</span>
<span></span>    <span class="n">storage_key</span> <span class="o">=</span> <span class="s2">"cond_frame_outputs"</span> <span class="k">if</span> <span class="n">is_cond</span> <span class="k">else</span> <span class="s2">"non_cond_frame_outputs"</span>
<span></span>
<span></span>    <span class="c1"># Get any previously predicted mask logits on this object and feed it along with</span>
<span></span>    <span class="c1"># the new clicks into the SAM mask decoder.</span>
<span></span>    <span class="n">prev_sam_mask_logits</span> <span class="o">=</span> <span class="kc">None</span>
<span></span>    <span class="c1"># lookup temporary output dict first, which contains the most recent output</span>
<span></span>    <span class="c1"># (if not found, then lookup conditioning and non-conditioning frame output)</span>
<span></span>    <span class="k">if</span> <span class="n">point_inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">prev_out</span> <span class="o">=</span> <span class="p">(</span>
<span></span>            <span class="n">obj_temp_output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>            <span class="ow">or</span> <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>            <span class="ow">or</span> <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>        <span class="k">if</span> <span class="n">prev_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">prev_out</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"pred_masks"</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">prev_sam_mask_logits</span> <span class="o">=</span> <span class="n">prev_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
<span></span>                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"cuda"</span>
<span></span>            <span class="p">)</span>
<span></span>            <span class="c1"># Clamp the scale of prev_sam_mask_logits to avoid rare numerical issues.</span>
<span></span>            <span class="n">prev_sam_mask_logits</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="o">-</span><span class="mf">32.0</span><span class="p">,</span> <span class="mf">32.0</span><span class="p">)</span>
<span></span>    <span class="n">current_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_single_frame_inference</span><span class="p">(</span>
<span></span>        <span class="n">output_dict</span><span class="o">=</span><span class="n">obj_output_dict</span><span class="p">,</span>  <span class="c1"># run on the slice of a single object</span>
<span></span>        <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame_idx</span><span class="p">,</span>
<span></span>        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># run on the slice of a single object</span>
<span></span>        <span class="n">is_init_cond_frame</span><span class="o">=</span><span class="n">is_init_cond_frame</span><span class="p">,</span>
<span></span>        <span class="n">point_inputs</span><span class="o">=</span><span class="n">point_inputs</span><span class="p">,</span>
<span></span>        <span class="n">mask_inputs</span><span class="o">=</span><span class="n">masks</span><span class="p">,</span>
<span></span>        <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="c1"># Skip the memory encoder when adding clicks or mask. We execute the memory encoder</span>
<span></span>        <span class="c1"># at the beginning of `propagate_in_video` (after user finalize their clicks). This</span>
<span></span>        <span class="c1"># allows us to enforce non-overlapping constraints on all objects before encoding</span>
<span></span>        <span class="c1"># them into memory.</span>
<span></span>        <span class="n">run_mem_encoder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="n">prev_sam_mask_logits</span><span class="o">=</span><span class="n">prev_sam_mask_logits</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="c1"># Add the output to the output dict (to be used as future memory)</span>
<span></span>    <span class="n">obj_temp_output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_out</span>
<span></span>
<span></span>    <span class="c1"># Resize the output mask to the original video resolution</span>
<span></span>    <span class="n">consolidated_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_consolidate_temp_output_across_obj</span><span class="p">(</span>
<span></span>        <span class="n">frame_idx</span><span class="p">,</span>
<span></span>        <span class="n">is_cond</span><span class="o">=</span><span class="n">is_cond</span><span class="p">,</span>
<span></span>        <span class="n">run_mem_encoder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.get_im_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.get_im_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>Extract and process image features using SAM2's image encoder for subsequent segmentation tasks.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor</code></td><td>The input image tensor.</td><td><em>required</em></td></tr><tr><td><code>batch</code></td><td><code>int, optional</code></td><td>The batch size for expanding features if there are multiple prompts.</td><td><code>1</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>vis_feats (torch.Tensor)</code></td><td>The visual features extracted from the image.</td></tr><tr><td><code>vis_pos_embed (torch.Tensor)</code></td><td>The positional embeddings for the visual features.</td></tr><tr><td><code>feat_sizes (list[tuple])</code></td><td>A list containing the sizes of the extracted features.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><ul><li>If <code>batch</code> is greater than 1, the features are expanded to fit the batch size.</li><li>The method leverages the model's <code>_prepare_backbone_features</code> method to prepare the backbone features.</li></ul></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1252-L1277"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Extract and process image features using SAM2's image encoder for subsequent segmentation tasks.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (torch.Tensor): The input image tensor.</span>
<span></span><span class="sd">        batch (int, optional): The batch size for expanding features if there are multiple prompts.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        vis_feats (torch.Tensor): The visual features extracted from the image.</span>
<span></span><span class="sd">        vis_pos_embed (torch.Tensor): The positional embeddings for the visual features.</span>
<span></span><span class="sd">        feat_sizes (list[tuple]): A list containing the sizes of the extracted features.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        - If `batch` is greater than 1, the features are expanded to fit the batch size.</span>
<span></span><span class="sd">        - The method leverages the model's `_prepare_backbone_features` method to prepare the backbone features.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">set_imgsz</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">)</span>
<span></span>    <span class="n">backbone_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward_image</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">batch</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># expand features if there's more than one prompt</span>
<span></span>        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">feat</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">backbone_out</span><span class="p">[</span><span class="s2">"backbone_fpn"</span><span class="p">]):</span>
<span></span>            <span class="n">backbone_out</span><span class="p">[</span><span class="s2">"backbone_fpn"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">feat</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span></span>        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">backbone_out</span><span class="p">[</span><span class="s2">"vision_pos_enc"</span><span class="p">]):</span>
<span></span>            <span class="n">pos</span> <span class="o">=</span> <span class="n">pos</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span></span>            <span class="n">backbone_out</span><span class="p">[</span><span class="s2">"vision_pos_enc"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">pos</span>
<span></span>    <span class="n">_</span><span class="p">,</span> <span class="n">vis_feats</span><span class="p">,</span> <span class="n">vis_pos_embed</span><span class="p">,</span> <span class="n">feat_sizes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_prepare_backbone_features</span><span class="p">(</span><span class="n">backbone_out</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">vis_feats</span><span class="p">,</span> <span class="n">vis_pos_embed</span><span class="p">,</span> <span class="n">feat_sizes</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.get_model"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.get_model</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div><p>Retrieve and configure the model with binarization enabled.</p><div class="admonition note"><p class="admonition-title">Notes</p><p>This method overrides the base class implementation to set the binarize flag to True.</p></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L924-L932"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Retrieve and configure the model with binarization enabled.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        This method overrides the base class implementation to set the binarize flag to True.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">model</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_model</span><span class="p">()</span>
<span></span>    <span class="n">model</span><span class="o">.</span><span class="n">set_binarize</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.inference"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.inference</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">points</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Perform image segmentation inference based on the given input cues, using the currently loaded image. This</p><p>method leverages SAM's (Segment Anything Model) architecture consisting of image encoder, prompt encoder, and mask decoder for real-time and promptable segmentation tasks.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor</code></td><td>The preprocessed input image in tensor format, with shape (N, C, H, W).</td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td><code>np.ndarray | list, optional</code></td><td>Bounding boxes with shape (N, 4), in XYXY format.</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>np.ndarray | list, optional</code></td><td>Points indicating object locations with shape (N, 2), in pixels.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>np.ndarray | list, optional</code></td><td>Labels for point prompts, shape (N, ). 1 = foreground, 0 = background.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>np.ndarray, optional</code></td><td>Low-resolution masks from previous predictions shape (N,H,W). For SAM H=W=256.</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pred_masks (torch.Tensor)</code></td><td>The output masks in shape CxHxW, where C is the number of generated masks.</td></tr><tr><td><code>pred_scores (torch.Tensor)</code></td><td>An array of length C containing predicted quality scores for each mask.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L934-L1004"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Perform image segmentation inference based on the given input cues, using the currently loaded image. This</span>
<span></span><span class="sd">    method leverages SAM's (Segment Anything Model) architecture consisting of image encoder, prompt</span>
<span></span><span class="sd">    encoder, and mask decoder for real-time and promptable segmentation tasks.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (torch.Tensor): The preprocessed input image in tensor format, with shape (N, C, H, W).</span>
<span></span><span class="sd">        bboxes (np.ndarray | list, optional): Bounding boxes with shape (N, 4), in XYXY format.</span>
<span></span><span class="sd">        points (np.ndarray | list, optional): Points indicating object locations with shape (N, 2), in pixels.</span>
<span></span><span class="sd">        labels (np.ndarray | list, optional): Labels for point prompts, shape (N, ). 1 = foreground, 0 = background.</span>
<span></span><span class="sd">        masks (np.ndarray, optional): Low-resolution masks from previous predictions shape (N,H,W). For SAM H=W=256.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pred_masks (torch.Tensor): The output masks in shape CxHxW, where C is the number of generated masks.</span>
<span></span><span class="sd">        pred_scores (torch.Tensor): An array of length C containing predicted quality scores for each mask.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="c1"># Override prompts if any stored in self.prompts</span>
<span></span>    <span class="n">bboxes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"bboxes"</span><span class="p">,</span> <span class="n">bboxes</span><span class="p">)</span>
<span></span>    <span class="n">points</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"points"</span><span class="p">,</span> <span class="n">points</span><span class="p">)</span>
<span></span>    <span class="n">masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"masks"</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">frame</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">frame</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"im"</span><span class="p">]</span> <span class="o">=</span> <span class="n">im</span>
<span></span>    <span class="n">output_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict"</span><span class="p">]</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># initialize prompts</span>
<span></span>        <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_prompts</span><span class="p">(</span>
<span></span>            <span class="n">im</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)):</span>
<span></span>                <span class="bp">self</span><span class="o">.</span><span class="n">add_new_prompts</span><span class="p">(</span><span class="n">obj_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="n">points</span><span class="p">[[</span><span class="n">i</span><span class="p">]],</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">[[</span><span class="n">i</span><span class="p">]],</span> <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame</span><span class="p">)</span>
<span></span>        <span class="k">elif</span> <span class="n">masks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">masks</span><span class="p">)):</span>
<span></span>                <span class="bp">self</span><span class="o">.</span><span class="n">add_new_prompts</span><span class="p">(</span><span class="n">obj_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="n">masks</span><span class="p">[[</span><span class="n">i</span><span class="p">]],</span> <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">propagate_in_video_preflight</span><span class="p">()</span>
<span></span>
<span></span>    <span class="n">consolidated_frame_inds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"consolidated_frame_inds"</span><span class="p">]</span>
<span></span>    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_idx_to_id"</span><span class="p">])</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">"No points are provided; please add points first"</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]:</span>
<span></span>        <span class="n">storage_key</span> <span class="o">=</span> <span class="s2">"cond_frame_outputs"</span>
<span></span>        <span class="n">current_out</span> <span class="o">=</span> <span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame</span><span class="p">]</span>
<span></span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem_around_input</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem_for_multi_obj</span> <span class="ow">or</span> <span class="n">batch_size</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">):</span>
<span></span>            <span class="c1"># clear non-conditioning memory of the surrounding frames</span>
<span></span>            <span class="bp">self</span><span class="o">.</span><span class="n">_clear_non_cond_mem_around_input</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
<span></span>    <span class="k">elif</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]:</span>
<span></span>        <span class="n">storage_key</span> <span class="o">=</span> <span class="s2">"non_cond_frame_outputs"</span>
<span></span>        <span class="n">current_out</span> <span class="o">=</span> <span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame</span><span class="p">]</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="n">storage_key</span> <span class="o">=</span> <span class="s2">"non_cond_frame_outputs"</span>
<span></span>        <span class="n">current_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_single_frame_inference</span><span class="p">(</span>
<span></span>            <span class="n">output_dict</span><span class="o">=</span><span class="n">output_dict</span><span class="p">,</span>
<span></span>            <span class="n">frame_idx</span><span class="o">=</span><span class="n">frame</span><span class="p">,</span>
<span></span>            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span></span>            <span class="n">is_init_cond_frame</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>            <span class="n">point_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>            <span class="n">mask_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span></span>            <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>            <span class="n">run_mem_encoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_out</span>
<span></span>    <span class="c1"># Create slices of per-object outputs for subsequent interaction with each</span>
<span></span>    <span class="c1"># individual object after tracking.</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">_add_output_per_object</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">current_out</span><span class="p">,</span> <span class="n">storage_key</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"frames_already_tracked"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
<span></span>    <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="n">pred_masks</span> <span class="o">=</span> <span class="n">pred_masks</span><span class="p">[(</span><span class="n">pred_masks</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">mask_threshold</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># filter blank masks</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pred_masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.init_state"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.init_state</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">init_state</span><span class="p">(</span><span class="n">predictor</span><span class="p">)</span>
</code></pre></div><p>Initialize an inference state for the predictor.</p><p>This function sets up the initial state required for performing inference on video data. It includes initializing various dictionaries and ordered dictionaries that will store inputs, outputs, and other metadata relevant to the tracking process.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>predictor</code></td><td><code>SAM2VideoPredictor</code></td><td>The predictor object for which to initialize the state.</td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1206-L1250"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@staticmethod</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">init_state</span><span class="p">(</span><span class="n">predictor</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Initialize an inference state for the predictor.</span>
<span></span>
<span></span><span class="sd">    This function sets up the initial state required for performing inference on video data. It includes</span>
<span></span><span class="sd">    initializing various dictionaries and ordered dictionaries that will store inputs, outputs, and other metadata</span>
<span></span><span class="sd">    relevant to the tracking process.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        predictor (SAM2VideoPredictor): The predictor object for which to initialize the state.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">predictor</span><span class="o">.</span><span class="n">inference_state</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># means initialized</span>
<span></span>        <span class="k">return</span>
<span></span>    <span class="k">assert</span> <span class="n">predictor</span><span class="o">.</span><span class="n">dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span></span>    <span class="k">assert</span> <span class="n">predictor</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">"video"</span>
<span></span>
<span></span>    <span class="n">inference_state</span> <span class="o">=</span> <span class="p">{</span>
<span></span>        <span class="s2">"num_frames"</span><span class="p">:</span> <span class="n">predictor</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">frames</span><span class="p">,</span>
<span></span>        <span class="s2">"point_inputs_per_obj"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># inputs points on each frame</span>
<span></span>        <span class="s2">"mask_inputs_per_obj"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># inputs mask on each frame</span>
<span></span>        <span class="s2">"constants"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># values that don't change across frames (so we only need to hold one copy of them)</span>
<span></span>        <span class="c1"># mapping between client-side object id and model-side object index</span>
<span></span>        <span class="s2">"obj_id_to_idx"</span><span class="p">:</span> <span class="n">OrderedDict</span><span class="p">(),</span>
<span></span>        <span class="s2">"obj_idx_to_id"</span><span class="p">:</span> <span class="n">OrderedDict</span><span class="p">(),</span>
<span></span>        <span class="s2">"obj_ids"</span><span class="p">:</span> <span class="p">[],</span>
<span></span>        <span class="c1"># A storage to hold the model's tracking results and states on each frame</span>
<span></span>        <span class="s2">"output_dict"</span><span class="p">:</span> <span class="p">{</span>
<span></span>            <span class="s2">"cond_frame_outputs"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># dict containing {frame_idx: &lt;out&gt;}</span>
<span></span>            <span class="s2">"non_cond_frame_outputs"</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># dict containing {frame_idx: &lt;out&gt;}</span>
<span></span>        <span class="p">},</span>
<span></span>        <span class="c1"># Slice (view) of each object tracking results, sharing the same memory with "output_dict"</span>
<span></span>        <span class="s2">"output_dict_per_obj"</span><span class="p">:</span> <span class="p">{},</span>
<span></span>        <span class="c1"># A temporary storage to hold new outputs when user interact with a frame</span>
<span></span>        <span class="c1"># to add clicks or mask (it's merged into "output_dict" before propagation starts)</span>
<span></span>        <span class="s2">"temp_output_dict_per_obj"</span><span class="p">:</span> <span class="p">{},</span>
<span></span>        <span class="c1"># Frames that already holds consolidated outputs from click or mask inputs</span>
<span></span>        <span class="c1"># (we directly use their consolidated outputs during tracking)</span>
<span></span>        <span class="s2">"consolidated_frame_inds"</span><span class="p">:</span> <span class="p">{</span>
<span></span>            <span class="s2">"cond_frame_outputs"</span><span class="p">:</span> <span class="nb">set</span><span class="p">(),</span>  <span class="c1"># set containing frame indices</span>
<span></span>            <span class="s2">"non_cond_frame_outputs"</span><span class="p">:</span> <span class="nb">set</span><span class="p">(),</span>  <span class="c1"># set containing frame indices</span>
<span></span>        <span class="p">},</span>
<span></span>        <span class="c1"># metadata for each tracking frame (e.g. which direction it's tracked)</span>
<span></span>        <span class="s2">"tracking_has_started"</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="s2">"frames_already_tracked"</span><span class="p">:</span> <span class="p">[],</span>
<span></span>    <span class="p">}</span>
<span></span>    <span class="n">predictor</span><span class="o">.</span><span class="n">inference_state</span> <span class="o">=</span> <span class="n">inference_state</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.postprocess"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.postprocess</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">)</span>
</code></pre></div><p>Post-process the predictions to apply non-overlapping constraints if required.</p><p>This method extends the post-processing functionality by applying non-overlapping constraints to the predicted masks if the <code>non_overlap_masks</code> flag is set to True. This ensures that the masks do not overlap, which can be useful for certain applications.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>preds</code></td><td><code>tuple[torch.Tensor, torch.Tensor]</code></td><td>The predicted masks and scores from the model.</td><td><em>required</em></td></tr><tr><td><code>img</code></td><td><code>torch.Tensor</code></td><td>The processed image tensor.</td><td><em>required</em></td></tr><tr><td><code>orig_imgs</code></td><td><code>list[np.ndarray]</code></td><td>The original images before processing.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>list</code></td><td>The post-processed predictions.</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">Notes</p><p>If <code>non_overlap_masks</code> is True, the method applies constraints to ensure non-overlapping masks.</p></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1006-L1030"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Post-process the predictions to apply non-overlapping constraints if required.</span>
<span></span>
<span></span><span class="sd">    This method extends the post-processing functionality by applying non-overlapping constraints to the predicted</span>
<span></span><span class="sd">    masks if the `non_overlap_masks` flag is set to True. This ensures that the masks do not overlap, which can be</span>
<span></span><span class="sd">    useful for certain applications.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        preds (tuple[torch.Tensor, torch.Tensor]): The predicted masks and scores from the model.</span>
<span></span><span class="sd">        img (torch.Tensor): The processed image tensor.</span>
<span></span><span class="sd">        orig_imgs (list[np.ndarray]): The original images before processing.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (list): The post-processed predictions.</span>
<span></span>
<span></span><span class="sd">    Notes:</span>
<span></span><span class="sd">        If `non_overlap_masks` is True, the method applies constraints to ensure non-overlapping masks.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">results</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">postprocess</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">orig_imgs</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_overlap_masks</span><span class="p">:</span>
<span></span>        <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
<span></span>            <span class="k">if</span> <span class="n">result</span><span class="o">.</span><span class="n">masks</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">masks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>                <span class="k">continue</span>
<span></span>            <span class="n">result</span><span class="o">.</span><span class="n">masks</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_apply_non_overlapping_constraints</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">masks</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
<span></span>    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2VideoPredictor.propagate_in_video_preflight"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2VideoPredictor.propagate_in_video_preflight</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">propagate_in_video_preflight</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div><p>Prepare inference_state and consolidate temporary outputs before tracking.</p><p>This method marks the start of tracking, disallowing the addition of new objects until the session is reset. It consolidates temporary outputs from <code>temp_output_dict_per_obj</code> and merges them into <code>output_dict</code>. Additionally, it clears non-conditioning memory around input frames and ensures that the state is consistent with the provided inputs.</p><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1136-L1203"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">propagate_in_video_preflight</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Prepare inference_state and consolidate temporary outputs before tracking.</span>
<span></span>
<span></span><span class="sd">    This method marks the start of tracking, disallowing the addition of new objects until the session is reset. It</span>
<span></span><span class="sd">    consolidates temporary outputs from `temp_output_dict_per_obj` and merges them into `output_dict`. Additionally,</span>
<span></span><span class="sd">    it clears non-conditioning memory around input frames and ensures that the state is consistent with the provided</span>
<span></span><span class="sd">    inputs.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="c1"># Tracking has started and we don't allow adding new objects until session is reset.</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"tracking_has_started"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"obj_idx_to_id"</span><span class="p">])</span>
<span></span>
<span></span>    <span class="c1"># Consolidate per-object temporary outputs in "temp_output_dict_per_obj" and</span>
<span></span>    <span class="c1"># add them into "output_dict".</span>
<span></span>    <span class="n">temp_output_dict_per_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"temp_output_dict_per_obj"</span><span class="p">]</span>
<span></span>    <span class="n">output_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict"</span><span class="p">]</span>
<span></span>    <span class="c1"># "consolidated_frame_inds" contains indices of those frames where consolidated</span>
<span></span>    <span class="c1"># temporary outputs have been added (either in this call or any previous calls</span>
<span></span>    <span class="c1"># to `propagate_in_video_preflight`).</span>
<span></span>    <span class="n">consolidated_frame_inds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"consolidated_frame_inds"</span><span class="p">]</span>
<span></span>    <span class="k">for</span> <span class="n">is_cond</span> <span class="ow">in</span> <span class="p">{</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">}:</span>
<span></span>        <span class="c1"># Separately consolidate conditioning and non-conditioning temp outputs</span>
<span></span>        <span class="n">storage_key</span> <span class="o">=</span> <span class="s2">"cond_frame_outputs"</span> <span class="k">if</span> <span class="n">is_cond</span> <span class="k">else</span> <span class="s2">"non_cond_frame_outputs"</span>
<span></span>        <span class="c1"># Find all the frames that contain temporary outputs for any objects</span>
<span></span>        <span class="c1"># (these should be the frames that have just received clicks for mask inputs</span>
<span></span>        <span class="c1"># via `add_new_points` or `add_new_mask`)</span>
<span></span>        <span class="n">temp_frame_inds</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span></span>        <span class="k">for</span> <span class="n">obj_temp_output_dict</span> <span class="ow">in</span> <span class="n">temp_output_dict_per_obj</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>            <span class="n">temp_frame_inds</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">obj_temp_output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span></span>        <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="n">storage_key</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">temp_frame_inds</span><span class="p">)</span>
<span></span>        <span class="c1"># consolidate the temporary output across all objects on this frame</span>
<span></span>        <span class="k">for</span> <span class="n">frame_idx</span> <span class="ow">in</span> <span class="n">temp_frame_inds</span><span class="p">:</span>
<span></span>            <span class="n">consolidated_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_consolidate_temp_output_across_obj</span><span class="p">(</span>
<span></span>                <span class="n">frame_idx</span><span class="p">,</span> <span class="n">is_cond</span><span class="o">=</span><span class="n">is_cond</span><span class="p">,</span> <span class="n">run_mem_encoder</span><span class="o">=</span><span class="kc">True</span>
<span></span>            <span class="p">)</span>
<span></span>            <span class="c1"># merge them into "output_dict" and also create per-object slices</span>
<span></span>            <span class="n">output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">][</span><span class="n">frame_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">consolidated_out</span>
<span></span>            <span class="bp">self</span><span class="o">.</span><span class="n">_add_output_per_object</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="n">consolidated_out</span><span class="p">,</span> <span class="n">storage_key</span><span class="p">)</span>
<span></span>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem_around_input</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clear_non_cond_mem_for_multi_obj</span> <span class="ow">or</span> <span class="n">batch_size</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">):</span>
<span></span>                <span class="c1"># clear non-conditioning memory of the surrounding frames</span>
<span></span>                <span class="bp">self</span><span class="o">.</span><span class="n">_clear_non_cond_mem_around_input</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>
<span></span>        <span class="c1"># clear temporary outputs in `temp_output_dict_per_obj`</span>
<span></span>        <span class="k">for</span> <span class="n">obj_temp_output_dict</span> <span class="ow">in</span> <span class="n">temp_output_dict_per_obj</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>            <span class="n">obj_temp_output_dict</span><span class="p">[</span><span class="n">storage_key</span><span class="p">]</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span></span>
<span></span>    <span class="c1"># edge case: if an output is added to "cond_frame_outputs", we remove any prior</span>
<span></span>    <span class="c1"># output on the same frame in "non_cond_frame_outputs"</span>
<span></span>    <span class="k">for</span> <span class="n">frame_idx</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]:</span>
<span></span>        <span class="n">output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>    <span class="k">for</span> <span class="n">obj_output_dict</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"output_dict_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>        <span class="k">for</span> <span class="n">frame_idx</span> <span class="ow">in</span> <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]:</span>
<span></span>            <span class="n">obj_output_dict</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span></span>    <span class="k">for</span> <span class="n">frame_idx</span> <span class="ow">in</span> <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]:</span>
<span></span>        <span class="k">assert</span> <span class="n">frame_idx</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span>
<span></span>        <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">frame_idx</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Make sure that the frame indices in "consolidated_frame_inds" are exactly those frames</span>
<span></span>    <span class="c1"># with either points or mask inputs (which should be true under a correct workflow).</span>
<span></span>    <span class="n">all_consolidated_frame_inds</span> <span class="o">=</span> <span class="p">(</span>
<span></span>        <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"cond_frame_outputs"</span><span class="p">]</span> <span class="o">|</span> <span class="n">consolidated_frame_inds</span><span class="p">[</span><span class="s2">"non_cond_frame_outputs"</span><span class="p">]</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="n">input_frames_inds</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span></span>    <span class="k">for</span> <span class="n">point_inputs_per_frame</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"point_inputs_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>        <span class="n">input_frames_inds</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">point_inputs_per_frame</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span></span>    <span class="k">for</span> <span class="n">mask_inputs_per_frame</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_state</span><span class="p">[</span><span class="s2">"mask_inputs_per_obj"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<span></span>        <span class="n">input_frames_inds</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">mask_inputs_per_frame</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span></span>    <span class="k">assert</span> <span class="n">all_consolidated_frame_inds</span> <span class="o">==</span> <span class="n">input_frames_inds</span>
</code></pre></div></details><p><br/><br/><hr/><br/></p><h2 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor"><span class="doc-kind doc-kind-class">class</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor</code></h2><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">cfg</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">DEFAULT_CFG</span><span class="p">,</span>
<span></span>    <span class="n">overrides</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">max_obj_num</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
<span></span>    <span class="n">_callbacks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div><p><strong>Bases:</strong> <code>SAM2Predictor</code></p><p>SAM2DynamicInteractivePredictor extends SAM2Predictor to support dynamic interactions with video frames or a</p><p>sequence of images.</p><p>This constructor initializes the SAM2DynamicInteractivePredictor with a given configuration, applies any specified overrides</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>cfg</code></td><td><code>dict[str, Any]</code></td><td>Configuration dictionary containing default settings.</td><td><code>DEFAULT_CFG</code></td></tr><tr><td><code>overrides</code></td><td><code>dict[str, Any] | None</code></td><td>Dictionary of values to override default configuration.</td><td><code>None</code></td></tr><tr><td><code>max_obj_num</code></td><td><code>int</code></td><td>Maximum number of objects to track. Default is 3. this is set to keep fix feature size for the model.</td><td><code>3</code></td></tr><tr><td><code>_callbacks</code></td><td><code>dict[str, Any] | None</code></td><td>Dictionary of callback functions to customize behavior.</td><td><code>None</code></td></tr></tbody></table><p><strong>Attributes</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>memory_bank</code></td><td><code>list</code></td><td>OrderedDict: Stores the states of each image with prompts.</td></tr><tr><td><code>obj_idx_set</code></td><td><code>set</code></td><td>A set to keep track of the object indices that have been added.</td></tr><tr><td><code>obj_id_to_idx</code></td><td><code>OrderedDict</code></td><td>Maps object IDs to their corresponding indices.</td></tr><tr><td><code>obj_idx_to_id</code></td><td><code>OrderedDict</code></td><td>Maps object indices to their corresponding IDs.</td></tr></tbody></table><p><strong>Methods</strong></p><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._obj_id_to_idx"><code>_obj_id_to_idx</code></a></td><td>Map client-side object id to model-side object index.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._prepare_memory_conditioned_features"><code>_prepare_memory_conditioned_features</code></a></td><td>Prepare the memory-conditioned features for the current image state. If obj_idx is provided, it supposes to</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_im_features"><code>get_im_features</code></a></td><td>Initialize the image state by processing the input image and extracting features.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_maskmem_enc"><code>get_maskmem_enc</code></a></td><td>Get memory and positional encoding from memory, which is used to condition the current image features.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.inference"><code>inference</code></a></td><td>Perform inference on a single image with optional bounding boxes, masks, points and object IDs. It has two</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.track_step"><code>track_step</code></a></td><td>Tracking step for the current image state to predict masks.</td></tr><tr><td><a href="#ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.update_memory"><code>update_memory</code></a></td><td>Append the imgState to the memory_bank and update the memory for the model.</td></tr></tbody></table><p><strong>Examples</strong></p><div class="highlight"><pre><span></span><code><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">SAM2DynamicInteractivePredictor</span><span class="p">(</span><span class="n">cfg</span><span class="o">=</span><span class="n">DEFAULT_CFG</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">support_img1</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes1</span><span class="p">,</span> <span class="n">obj_ids</span><span class="o">=</span><span class="n">labels1</span><span class="p">,</span> <span class="n">update_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">results1</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">query_img1</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">predictor</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">support_img2</span><span class="p">,</span> <span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes2</span><span class="p">,</span> <span class="n">obj_ids</span><span class="o">=</span><span class="n">labels2</span><span class="p">,</span> <span class="n">update_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">results2</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">query_img2</span><span class="p">)</span>
</code></pre></div><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1659-L1988"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SAM2DynamicInteractivePredictor</span><span class="p">(</span><span class="n">SAM2Predictor</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""SAM2DynamicInteractivePredictor extends SAM2Predictor to support dynamic interactions with video frames or a</span>
<span></span><span class="sd">    sequence of images.</span>
<span></span>
<span></span><span class="sd">    Attributes:</span>
<span></span><span class="sd">        memory_bank (list): OrderedDict: Stores the states of each image with prompts.</span>
<span></span><span class="sd">        obj_idx_set (set): A set to keep track of the object indices that have been added.</span>
<span></span><span class="sd">        obj_id_to_idx (OrderedDict): Maps object IDs to their corresponding indices.</span>
<span></span><span class="sd">        obj_idx_to_id (OrderedDict): Maps object indices to their corresponding IDs.</span>
<span></span>
<span></span><span class="sd">    Methods:</span>
<span></span><span class="sd">        get_model: Retrieves and configures the model with binarization enabled.</span>
<span></span><span class="sd">        inference: Performs inference on a single image with optional prompts and object IDs.</span>
<span></span><span class="sd">        postprocess: Post-processes the predictions to apply non-overlapping constraints if required.</span>
<span></span><span class="sd">        update_memory: Append the imgState to the memory_bank and update the memory for the model.</span>
<span></span><span class="sd">        track_step: Tracking step for the current image state to predict masks.</span>
<span></span><span class="sd">        get_maskmem_enc: Get memory and positional encoding from the memory bank.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">            &gt;&gt;&gt; predictor = SAM2DynamicInteractivePredictor(cfg=DEFAULT_CFG)</span>
<span></span><span class="sd">            &gt;&gt;&gt; predictor(source=support_img1, bboxes=bboxes1, obj_ids=labels1, update_memory=True)</span>
<span></span><span class="sd">            &gt;&gt;&gt; results1 = predictor(source=query_img1)</span>
<span></span><span class="sd">            &gt;&gt;&gt; predictor(source=support_img2, bboxes=bboxes2, obj_ids=labels2, update_memory=True)</span>
<span></span><span class="sd">            &gt;&gt;&gt; results2 = predictor(source=query_img2)</span>
<span></span><span class="sd">    """</span>
<span></span>
<span></span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<span></span>        <span class="bp">self</span><span class="p">,</span>
<span></span>        <span class="n">cfg</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">DEFAULT_CFG</span><span class="p">,</span>
<span></span>        <span class="n">overrides</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="n">max_obj_num</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
<span></span>        <span class="n">_callbacks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span></span><span class="w">        </span><span class="sd">"""Initialize the predictor with configuration and optional overrides.</span>
<span></span>
<span></span><span class="sd">        This constructor initializes the SAM2DynamicInteractivePredictor with a given configuration, applies any</span>
<span></span><span class="sd">        specified overrides</span>
<span></span>
<span></span><span class="sd">        Args:</span>
<span></span><span class="sd">            cfg (dict[str, Any]): Configuration dictionary containing default settings.</span>
<span></span><span class="sd">            overrides (dict[str, Any] | None): Dictionary of values to override default configuration.</span>
<span></span><span class="sd">            max_obj_num (int): Maximum number of objects to track. Default is 3. this is set to keep fix feature size</span>
<span></span><span class="sd">                for the model.</span>
<span></span><span class="sd">            _callbacks (dict[str, Any] | None): Dictionary of callback functions to customize behavior.</span>
<span></span><span class="sd">        """</span>
<span></span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">overrides</span><span class="p">,</span> <span class="n">_callbacks</span><span class="p">)</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">non_overlap_masks</span> <span class="o">=</span> <span class="kc">True</span>
<span></span>
<span></span>        <span class="c1"># Initialize the memory bank to store image states</span>
<span></span>        <span class="c1"># NOTE: probably need to use dict for better query</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">memory_bank</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>
<span></span>        <span class="c1"># Initialize the object index set and mappings</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">obj_idx_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">obj_id_to_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">obj_idx_to_id</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">max_obj_num</span><span class="p">)))</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">_max_obj_num</span> <span class="o">=</span> <span class="n">max_obj_num</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._obj_id_to_idx"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._obj_id_to_idx</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_obj_id_to_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span>
</code></pre></div><p>Map client-side object id to model-side object index.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>obj_id</code></td><td><code>int</code></td><td>The client-side object ID.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>int</code></td><td>The model-side object index, or None if not found.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1928-L1937"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_obj_id_to_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span></span><span class="w">    </span><span class="sd">"""Map client-side object id to model-side object index.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        obj_id (int): The client-side object ID.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (int): The model-side object index, or None if not found.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">obj_id_to_idx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">obj_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._prepare_memory_conditioned_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor._prepare_memory_conditioned_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prepare_memory_conditioned_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
</code></pre></div><p>Prepare the memory-conditioned features for the current image state. If obj_idx is provided, it supposes to</p><p>prepare features for a specific prompted object in the image. If obj_idx is None, it prepares features for all objects in the image. If there is no memory, it will directly add a no-memory embedding to the current vision features. If there is memory, it will use the memory features from previous frames to condition the current vision features using a transformer attention mechanism.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>obj_idx</code></td><td><code>int | None</code></td><td>The index of the object for which to prepare the features.</td><td><em>required</em></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>pix_feat_with_mem (torch.Tensor)</code></td><td>The memory-conditioned pixel features.</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1881-L1913"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_prepare_memory_conditioned_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span></span><span class="w">    </span><span class="sd">"""Prepare the memory-conditioned features for the current image state. If obj_idx is provided, it supposes to</span>
<span></span><span class="sd">    prepare features for a specific prompted object in the image. If obj_idx is None, it prepares features</span>
<span></span><span class="sd">    for all objects in the image. If there is no memory, it will directly add a no-memory embedding to the</span>
<span></span><span class="sd">    current vision features. If there is memory, it will use the memory features from previous frames to</span>
<span></span><span class="sd">    condition the current vision features using a transformer attention mechanism.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        obj_idx (int | None): The index of the object for which to prepare the features.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        pix_feat_with_mem (torch.Tensor): The memory-conditioned pixel features.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory_bank</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj_idx</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
<span></span>        <span class="c1"># for initial conditioning frames with, encode them without using any previous memory</span>
<span></span>        <span class="c1"># directly add no-mem embedding (instead of using the transformer encoder)</span>
<span></span>        <span class="n">pix_feat_with_mem</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_feats</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">no_mem_embed</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="c1"># for inference frames, use the memory features from previous frames</span>
<span></span>        <span class="n">memory</span><span class="p">,</span> <span class="n">memory_pos_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_maskmem_enc</span><span class="p">()</span>
<span></span>        <span class="n">pix_feat_with_mem</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">memory_attention</span><span class="p">(</span>
<span></span>            <span class="n">curr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vision_feats</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:],</span>
<span></span>            <span class="n">curr_pos</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vision_pos_embeds</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:],</span>
<span></span>            <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">,</span>
<span></span>            <span class="n">memory_pos</span><span class="o">=</span><span class="n">memory_pos_embed</span><span class="p">,</span>
<span></span>            <span class="n">num_obj_ptr_tokens</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># num_obj_ptr_tokens</span>
<span></span>        <span class="p">)</span>
<span></span>    <span class="c1"># reshape the output (HW)BC =&gt; BCHW</span>
<span></span>    <span class="k">return</span> <span class="n">pix_feat_with_mem</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">_max_obj_num</span><span class="p">,</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">memory_attention</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
<span></span>        <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">feat_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
<span></span>    <span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_im_features"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_im_features</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div><p>Initialize the image state by processing the input image and extracting features.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>img</code></td><td><code>torch.Tensor | np.ndarray</code></td><td>The input image tensor or numpy array.</td><td><em>required</em></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1784-L1798"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span></span><span class="w">    </span><span class="sd">"""Initialize the image state by processing the input image and extracting features.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        img (torch.Tensor | np.ndarray): The input image tensor or numpy array.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">vis_feats</span><span class="p">,</span> <span class="n">vis_pos_embed</span><span class="p">,</span> <span class="n">feat_sizes</span> <span class="o">=</span> <span class="n">SAM2VideoPredictor</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_obj_num</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">high_res_features</span> <span class="o">=</span> <span class="p">[</span>
<span></span>        <span class="n">feat</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">feat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="o">*</span><span class="n">feat_size</span><span class="p">)</span>
<span></span>        <span class="k">for</span> <span class="n">feat</span><span class="p">,</span> <span class="n">feat_size</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">vis_feats</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">feat_sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span></span>    <span class="p">]</span>
<span></span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">vision_feats</span> <span class="o">=</span> <span class="n">vis_feats</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">vision_pos_embeds</span> <span class="o">=</span> <span class="n">vis_pos_embed</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">feat_sizes</span> <span class="o">=</span> <span class="n">feat_sizes</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_maskmem_enc"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.get_maskmem_enc</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_maskmem_enc</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
</code></pre></div><p>Get memory and positional encoding from memory, which is used to condition the current image features.</p><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1915-L1926"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_maskmem_enc</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span></span><span class="w">    </span><span class="sd">"""Get memory and positional encoding from memory, which is used to condition the current image features."""</span>
<span></span>    <span class="n">to_cat_memory</span><span class="p">,</span> <span class="n">to_cat_memory_pos_embed</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span></span>    <span class="k">for</span> <span class="n">consolidated_out</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_bank</span><span class="p">:</span>
<span></span>        <span class="n">to_cat_memory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># (H*W, B, C)</span>
<span></span>        <span class="n">maskmem_enc</span> <span class="o">=</span> <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span></span>        <span class="n">maskmem_enc</span> <span class="o">=</span> <span class="n">maskmem_enc</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">maskmem_tpos_enc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_maskmem</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
<span></span>        <span class="n">to_cat_memory_pos_embed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">maskmem_enc</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">to_cat_memory</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span></span>    <span class="n">memory_pos_embed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">to_cat_memory_pos_embed</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">memory</span><span class="p">,</span> <span class="n">memory_pos_embed</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.inference"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.inference</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">im</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">obj_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">update_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
</code></pre></div><p>Perform inference on a single image with optional bounding boxes, masks, points and object IDs. It has two</p><p>modes: one is to run inference on a single image without updating the memory, and the other is to update the memory with the provided prompts and object IDs. When update_memory is True, it will update the memory with the provided prompts and obj_ids. When update_memory is False, it will only run inference on the provided image without updating the memory.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>im</code></td><td><code>torch.Tensor | np.ndarray</code></td><td>The input image tensor or numpy array.</td><td><em>required</em></td></tr><tr><td><code>bboxes</code></td><td><code>list[list[float]] | None</code></td><td>Optional list of bounding boxes to update the memory.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>list[torch.Tensor | np.ndarray] | None</code></td><td>Optional masks to update the memory.</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>list[list[float]] | None</code></td><td>Optional list of points to update the memory, each point is [x, y].</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>list[int] | None</code></td><td>Optional list of object IDs corresponding to the points (&gt;0 for positive, 0 for negative).</td><td><code>None</code></td></tr><tr><td><code>obj_ids</code></td><td><code>list[int] | None</code></td><td>Optional list of object IDs corresponding to the prompts.</td><td><code>None</code></td></tr><tr><td><code>update_memory</code></td><td><code>bool</code></td><td>Flag to indicate whether to update the memory with new objects.</td><td><code>False</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>res_masks (torch.Tensor)</code></td><td>The output masks in shape (C, H, W)</td></tr><tr><td><code>object_score_logits (torch.Tensor)</code></td><td>Quality scores for each mask</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1717-L1782"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">im</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span></span>    <span class="n">bboxes</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">obj_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">update_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span></span><span class="w">    </span><span class="sd">"""Perform inference on a single image with optional bounding boxes, masks, points and object IDs. It has two</span>
<span></span><span class="sd">    modes: one is to run inference on a single image without updating the memory, and the other is to update</span>
<span></span><span class="sd">    the memory with the provided prompts and object IDs. When update_memory is True, it will update the</span>
<span></span><span class="sd">    memory with the provided prompts and obj_ids. When update_memory is False, it will only run inference on</span>
<span></span><span class="sd">    the provided image without updating the memory.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        im (torch.Tensor | np.ndarray): The input image tensor or numpy array.</span>
<span></span><span class="sd">        bboxes (list[list[float]] | None): Optional list of bounding boxes to update the memory.</span>
<span></span><span class="sd">        masks (list[torch.Tensor | np.ndarray] | None): Optional masks to update the memory.</span>
<span></span><span class="sd">        points (list[list[float]] | None): Optional list of points to update the memory, each point is [x, y].</span>
<span></span><span class="sd">        labels (list[int] | None): Optional list of object IDs corresponding to the points (&gt;0 for positive, 0 for</span>
<span></span><span class="sd">            negative).</span>
<span></span><span class="sd">        obj_ids (list[int] | None): Optional list of object IDs corresponding to the prompts.</span>
<span></span><span class="sd">        update_memory (bool): Flag to indicate whether to update the memory with new objects.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        res_masks (torch.Tensor): The output masks in shape (C, H, W)</span>
<span></span><span class="sd">        object_score_logits (torch.Tensor): Quality scores for each mask</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">get_im_features</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
<span></span>    <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_prompts</span><span class="p">(</span>
<span></span>        <span class="n">dst_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">,</span>
<span></span>        <span class="n">src_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span>
<span></span>        <span class="n">points</span><span class="o">=</span><span class="n">points</span><span class="p">,</span>
<span></span>        <span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes</span><span class="p">,</span>
<span></span>        <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
<span></span>        <span class="n">masks</span><span class="o">=</span><span class="n">masks</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="n">update_memory</span><span class="p">:</span>
<span></span>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj_ids</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
<span></span>            <span class="n">obj_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">obj_ids</span><span class="p">]</span>
<span></span>        <span class="k">assert</span> <span class="n">obj_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">"obj_ids must be provided when update_memory is True"</span>
<span></span>        <span class="k">assert</span> <span class="n">masks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
<span></span>            <span class="s2">"bboxes, masks, or points must be provided when update_memory is True"</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># placeholder</span>
<span></span>            <span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">obj_ids</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>            <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">obj_ids</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>        <span class="k">if</span> <span class="n">masks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">obj_ids</span><span class="p">),</span> <span class="s2">"masks and obj_ids must have the same length."</span>
<span></span>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">obj_ids</span><span class="p">),</span> <span class="s2">"points and obj_ids must have the same length."</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">update_memory</span><span class="p">(</span><span class="n">obj_ids</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">current_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">track_step</span><span class="p">()</span>
<span></span>    <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">],</span> <span class="n">current_out</span><span class="p">[</span><span class="s2">"object_score_logits"</span><span class="p">]</span>
<span></span>    <span class="c1"># filter the masks and logits based on the object indices</span>
<span></span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">obj_idx_set</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">"No objects have been added to the state. Please add objects before inference."</span><span class="p">)</span>
<span></span>    <span class="n">idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">obj_idx_set</span><span class="p">)</span>  <span class="c1"># cls id</span>
<span></span>    <span class="n">pred_masks</span><span class="p">,</span> <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">pred_masks</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">pred_scores</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span></span>    <span class="c1"># the original score are in [-32,32], and a object score larger than 0 means the object is present, we map it to [-1,1] range,</span>
<span></span>    <span class="c1"># and use a activate function to make sure the object score logits are non-negative, so that we can use it as a mask</span>
<span></span>    <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="n">pred_scores</span> <span class="o">/</span> <span class="mi">32</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="n">pred_masks</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">pred_scores</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.track_step"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.track_step</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">track_step</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">obj_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">point</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">label</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>
</code></pre></div><p>Tracking step for the current image state to predict masks.</p><p>This method processes the image features and runs the SAM heads to predict masks. If obj_idx is provided, it processes the features for a specific prompted object in the image. If obj_idx is None, it processes the features for all objects in the image. The method supports both mask-based output without SAM and full SAM processing with memory-conditioned features.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>obj_idx</code></td><td><code>int | None</code></td><td>The index of the object for which to predict masks. If None, it processes all objects.</td><td><code>None</code></td></tr><tr><td><code>point</code></td><td><code>torch.Tensor | None</code></td><td>The coordinates of the points of interest with shape (N, 2).</td><td><code>None</code></td></tr><tr><td><code>label</code></td><td><code>torch.Tensor | None</code></td><td>The labels corresponding to the points where 1 means positive clicks, 0 means negative clicks.</td><td><code>None</code></td></tr><tr><td><code>mask</code></td><td><code>torch.Tensor | None</code></td><td>The mask input for the object with shape (H, W).</td><td><code>None</code></td></tr></tbody></table><p><strong>Returns</strong></p><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>current_out (dict[str, Any])</code></td><td>A dictionary containing the current output with mask predictions and object</td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1939-L1988"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">track_step</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">obj_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">point</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">label</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span></span><span class="w">    </span><span class="sd">"""Tracking step for the current image state to predict masks.</span>
<span></span>
<span></span><span class="sd">    This method processes the image features and runs the SAM heads to predict masks. If obj_idx is provided, it</span>
<span></span><span class="sd">    processes the features for a specific prompted object in the image. If obj_idx is None, it processes the</span>
<span></span><span class="sd">    features for all objects in the image. The method supports both mask-based output without SAM and full SAM</span>
<span></span><span class="sd">    processing with memory-conditioned features.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        obj_idx (int | None): The index of the object for which to predict masks. If None, it processes all objects.</span>
<span></span><span class="sd">        point (torch.Tensor | None): The coordinates of the points of interest with shape (N, 2).</span>
<span></span><span class="sd">        label (torch.Tensor | None): The labels corresponding to the points where 1 means positive clicks, 0 means</span>
<span></span><span class="sd">            negative clicks.</span>
<span></span><span class="sd">        mask (torch.Tensor | None): The mask input for the object with shape (H, W).</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        current_out (dict[str, Any]): A dictionary containing the current output with mask predictions and object</span>
<span></span><span class="sd">            pointers. Keys include 'point_inputs', 'mask_inputs', 'pred_masks', 'pred_masks_high_res',</span>
<span></span><span class="sd">            'obj_ptr', 'object_score_logits'.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">use_mask_input_as_output_without_sam</span><span class="p">:</span>
<span></span>        <span class="c1"># When use_mask_input_as_output_without_sam=True, we directly output the mask input</span>
<span></span>        <span class="c1"># (see it as a GT mask) without using a SAM prompt encoder + mask decoder.</span>
<span></span>        <span class="n">pix_feat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_feats</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span></span>        <span class="n">pix_feat</span> <span class="o">=</span> <span class="n">pix_feat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">memory_attention</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">feat_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span></span>        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">low_res_masks</span><span class="p">,</span> <span class="n">high_res_masks</span><span class="p">,</span> <span class="n">obj_ptr</span><span class="p">,</span> <span class="n">object_score_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_use_mask_as_output</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="c1"># fused the visual feature with previous memory features in the memory bank</span>
<span></span>        <span class="n">pix_feat_with_mem</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_memory_conditioned_features</span><span class="p">(</span><span class="n">obj_idx</span><span class="p">)</span>
<span></span>        <span class="c1"># calculate the first feature if adding obj_idx exists(means adding prompts)</span>
<span></span>        <span class="n">pix_feat_with_mem</span> <span class="o">=</span> <span class="n">pix_feat_with_mem</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">obj_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">pix_feat_with_mem</span>
<span></span>        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">low_res_masks</span><span class="p">,</span> <span class="n">high_res_masks</span><span class="p">,</span> <span class="n">obj_ptr</span><span class="p">,</span> <span class="n">object_score_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_forward_sam_heads</span><span class="p">(</span>
<span></span>            <span class="n">backbone_features</span><span class="o">=</span><span class="n">pix_feat_with_mem</span><span class="p">,</span>
<span></span>            <span class="n">point_inputs</span><span class="o">=</span><span class="p">{</span><span class="s2">"point_coords"</span><span class="p">:</span> <span class="n">point</span><span class="p">,</span> <span class="s2">"point_labels"</span><span class="p">:</span> <span class="n">label</span><span class="p">}</span> <span class="k">if</span> <span class="n">obj_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
<span></span>            <span class="n">mask_inputs</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span>
<span></span>            <span class="n">multimask_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>            <span class="n">high_res_features</span><span class="o">=</span><span class="p">[</span><span class="n">feat</span><span class="p">[:</span> <span class="n">pix_feat_with_mem</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">high_res_features</span><span class="p">],</span>
<span></span>        <span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="p">{</span>
<span></span>        <span class="s2">"pred_masks"</span><span class="p">:</span> <span class="n">low_res_masks</span><span class="p">,</span>
<span></span>        <span class="s2">"pred_masks_high_res"</span><span class="p">:</span> <span class="n">high_res_masks</span><span class="p">,</span>
<span></span>        <span class="s2">"obj_ptr"</span><span class="p">:</span> <span class="n">obj_ptr</span><span class="p">,</span>
<span></span>        <span class="s2">"object_score_logits"</span><span class="p">:</span> <span class="n">object_score_logits</span><span class="p">,</span>
<span></span>    <span class="p">}</span>
</code></pre></div></details><p><br/></p><h3 id="ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.update_memory"><span class="doc-kind doc-kind-method">method</span> <code>ultralytics.models.sam.predict.SAM2DynamicInteractivePredictor.update_memory</code></h3><div class="highlight"><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">update_memory</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">obj_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div><p>Append the imgState to the memory_bank and update the memory for the model.</p><p><strong>Args</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>obj_ids</code></td><td><code>list[int]</code></td><td>List of object IDs corresponding to the prompts.</td><td><code>None</code></td></tr><tr><td><code>points</code></td><td><code>torch.Tensor | None</code></td><td>Tensor of shape (B, N, 2) representing the input points for N objects.</td><td><code>None</code></td></tr><tr><td><code>labels</code></td><td><code>torch.Tensor | None</code></td><td>Tensor of shape (B, N) representing the labels for the input points.</td><td><code>None</code></td></tr><tr><td><code>masks</code></td><td><code>torch.Tensor | None</code></td><td>Optional tensor of shape (N, H, W) representing the input masks for N objects.</td><td><code>None</code></td></tr></tbody></table><details><summary>Source code in <code>ultralytics/models/sam/predict.py</code></summary><a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py#L1801-L1879"><i aria-hidden="true" class="fa-brands fa-github" style="margin-right:6px;"></i>View on GitHub</a><div class="highlight"><pre><span></span><code><span></span><span class="nd">@smart_inference_mode</span><span class="p">()</span>
<span></span><span class="k">def</span><span class="w"> </span><span class="nf">update_memory</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">obj_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">labels</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span></span><span class="w">    </span><span class="sd">"""Append the imgState to the memory_bank and update the memory for the model.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        obj_ids (list[int]): List of object IDs corresponding to the prompts.</span>
<span></span><span class="sd">        points (torch.Tensor | None): Tensor of shape (B, N, 2) representing the input points for N objects.</span>
<span></span><span class="sd">        labels (torch.Tensor | None): Tensor of shape (B, N) representing the labels for the input points.</span>
<span></span><span class="sd">        masks (torch.Tensor | None): Optional tensor of shape (N, H, W) representing the input masks for N objects.</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">consolidated_out</span> <span class="o">=</span> <span class="p">{</span>
<span></span>        <span class="s2">"maskmem_features"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="s2">"maskmem_pos_enc"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span></span>        <span class="s2">"pred_masks"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
<span></span>            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_obj_num</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">4</span><span class="p">),</span>
<span></span>            <span class="n">fill_value</span><span class="o">=-</span><span class="mf">1024.0</span><span class="p">,</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
<span></span>            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>        <span class="p">),</span>
<span></span>        <span class="s2">"obj_ptr"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
<span></span>            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_obj_num</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">),</span>
<span></span>            <span class="n">fill_value</span><span class="o">=-</span><span class="mf">1024.0</span><span class="p">,</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
<span></span>            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>        <span class="p">),</span>
<span></span>        <span class="s2">"object_score_logits"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
<span></span>            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_obj_num</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span></span>            <span class="c1"># default to 10.0 for object_score_logits, i.e. assuming the object is</span>
<span></span>            <span class="c1"># present as sigmoid(10)=1, same as in `predict_masks` of `MaskDecoder`</span>
<span></span>            <span class="n">fill_value</span><span class="o">=-</span><span class="mi">32</span><span class="p">,</span>  <span class="c1"># 10.0,</span>
<span></span>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
<span></span>            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>        <span class="p">),</span>
<span></span>    <span class="p">}</span>
<span></span>
<span></span>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">obj_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">obj_ids</span><span class="p">):</span>
<span></span>        <span class="k">assert</span> <span class="n">obj_id</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_obj_num</span>
<span></span>        <span class="n">obj_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_obj_id_to_idx</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">obj_id</span><span class="p">))</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">obj_idx_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">obj_idx</span><span class="p">)</span>
<span></span>        <span class="n">point</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">points</span><span class="p">[[</span><span class="n">i</span><span class="p">]],</span> <span class="n">labels</span><span class="p">[[</span><span class="n">i</span><span class="p">]]</span>
<span></span>        <span class="n">mask</span> <span class="o">=</span> <span class="n">masks</span><span class="p">[[</span><span class="n">i</span><span class="p">]][</span><span class="kc">None</span><span class="p">]</span> <span class="k">if</span> <span class="n">masks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
<span></span>        <span class="c1"># Currently, only bbox prompt or mask prompt is supported, so we assert that bbox is not None.</span>
<span></span>        <span class="k">assert</span> <span class="n">point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">"Either bbox, points or mask is required"</span>
<span></span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">track_step</span><span class="p">(</span><span class="n">obj_idx</span><span class="p">,</span> <span class="n">point</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span></span>        <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">obj_mask</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span>
<span></span>            <span class="k">assert</span> <span class="n">obj_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span> <span class="o">==</span> <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span> <span class="p">(</span>
<span></span>                <span class="sa">f</span><span class="s2">"Expected mask shape </span><span class="si">{</span><span class="n">consolidated_out</span><span class="p">[</span><span class="s1">'pred_masks'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="n">obj_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span><span class="si">}</span><span class="s2"> for object </span><span class="si">{</span><span class="n">obj_idx</span><span class="si">}</span><span class="s2">."</span>
<span></span>            <span class="p">)</span>
<span></span>            <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">][</span><span class="n">obj_idx</span> <span class="p">:</span> <span class="n">obj_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj_mask</span>
<span></span>            <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">][</span><span class="n">obj_idx</span> <span class="p">:</span> <span class="n">obj_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">"obj_ptr"</span><span class="p">]</span>
<span></span>
<span></span>            <span class="k">if</span> <span class="s2">"object_score_logits"</span> <span class="ow">in</span> <span class="n">out</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
<span></span>                <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"object_score_logits"</span><span class="p">][</span><span class="n">obj_idx</span> <span class="p">:</span> <span class="n">obj_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">"object_score_logits"</span><span class="p">]</span>
<span></span>
<span></span>    <span class="n">high_res_masks</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
<span></span>        <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"pred_masks"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"cuda"</span><span class="p">),</span>
<span></span>        <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">imgsz</span><span class="p">,</span>
<span></span>        <span class="n">mode</span><span class="o">=</span><span class="s2">"bilinear"</span><span class="p">,</span>
<span></span>        <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">non_overlap_masks_for_mem_enc</span><span class="p">:</span>
<span></span>        <span class="n">high_res_masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_apply_non_overlapping_constraints</span><span class="p">(</span><span class="n">high_res_masks</span><span class="p">)</span>
<span></span>    <span class="n">maskmem_features</span><span class="p">,</span> <span class="n">maskmem_pos_enc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_encode_new_memory</span><span class="p">(</span>
<span></span>        <span class="n">current_vision_feats</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vision_feats</span><span class="p">,</span>
<span></span>        <span class="n">feat_sizes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">feat_sizes</span><span class="p">,</span>
<span></span>        <span class="n">pred_masks_high_res</span><span class="o">=</span><span class="n">high_res_masks</span><span class="p">,</span>
<span></span>        <span class="n">object_score_logits</span><span class="o">=</span><span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"object_score_logits"</span><span class="p">],</span>
<span></span>        <span class="n">is_mask_from_pts</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"maskmem_features"</span><span class="p">]</span> <span class="o">=</span> <span class="n">maskmem_features</span>
<span></span>    <span class="n">consolidated_out</span><span class="p">[</span><span class="s2">"maskmem_pos_enc"</span><span class="p">]</span> <span class="o">=</span> <span class="n">maskmem_pos_enc</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">memory_bank</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">consolidated_out</span><span class="p">)</span>
</code></pre></div></details><p><br/><br/></p><br/><br/><div class="git-info"><div class="dates-container"><span class="date-item" title="This page was first created on November 12, 2023"><span class="hover-item">ğŸ“…</span> Created 2 years ago </span><span class="date-item" title="This page was last updated on November 23, 2025"><span class="hover-item">âœï¸</span> Updated 0 days ago </span></div><div class="authors-container"><a class="author-link" href="https://github.com/glenn-jocher" title="glenn-jocher (7 changes)"><img alt="glenn-jocher" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/26833433?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/Laughing-q" title="Laughing-q (2 changes)"><img alt="Laughing-q" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/61612323?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/ShuaiLYU" title="ShuaiLYU (1 change)"><img alt="ShuaiLYU" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/31230805?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/jk4e" title="jk4e (1 change)"><img alt="jk4e" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/116908874?v=4&amp;s=96"/></a><a class="author-link" href="https://github.com/Burhan-Q" title="Burhan-Q (1 change)"><img alt="Burhan-Q" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/62214284?v=4&amp;s=96"/></a></div></div><div class="share-buttons"><button class="share-button hover-item" onclick="window.open('https://twitter.com/intent/tweet?url=https%3A%2F%2Fdocs.ultralytics.com%2Freference%2Fmodels%2Fsam%2Fpredict%2F', 'TwitterShare', 'width=550,height=680,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-x-twitter"></i> Tweet </button><button class="share-button hover-item linkedin" onclick="window.open('https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fdocs.ultralytics.com%2Freference%2Fmodels%2Fsam%2Fpredict%2F', 'LinkedinShare', 'width=550,height=730,menubar=no,toolbar=no'); return false;"><i class="fa-brands fa-linkedin-in"></i> Share </button></div><br/></article></div>
<script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div><button class="md-top md-icon" data-md-component="top" hidden="" type="button"><svg class="lucide lucide-circle-arrow-up" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><path d="m16 12-4-4-4 4M12 16V8"></path></svg> Back to top
</button></main><footer class="md-footer"><nav aria-label="Footer" class="md-footer__inner md-grid"><a aria-label="Previous: utils" class="md-footer__link md-footer__link--prev" href="../modules/utils/"><div class="md-footer__button md-icon"><svg class="lucide lucide-arrow-left" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m12 19-7-7 7-7M19 12H5"></path></svg></div><div class="md-footer__title"><span class="md-footer__direction"> Previous </span><div class="md-ellipsis"> utils </div></div></a><a aria-label="Next: loss" class="md-footer__link md-footer__link--next" href="../../utils/loss/"><div class="md-footer__title"><span class="md-footer__direction"> Next </span><div class="md-ellipsis"> loss </div></div><div class="md-footer__button md-icon"><svg class="lucide lucide-arrow-right" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M5 12h14M12 5l7 7-7 7"></path></svg></div></a></nav><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class="md-copyright"><div class="md-copyright__highlight"><a href="https://www.ultralytics.com/" target="_blank">Â© 2025 Ultralytics Inc.</a> All rights reserved. </div> Made with <a href="https://zensical.org/" rel="noopener" target="_blank"> Zensical </a></div><div class="md-social"><a class="md-social__link" href="https://github.com/ultralytics" rel="noopener" target="_blank" title="github.com"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://www.linkedin.com/company/ultralytics/" rel="noopener" target="_blank" title="www.linkedin.com"><svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://x.com/ultralytics" rel="noopener" target="_blank" title="x.com"><svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://youtube.com/ultralytics?sub_confirmation=1" rel="noopener" target="_blank" title="youtube.com"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M549.7 124.1c-6.2-23.7-24.8-42.3-48.3-48.6C458.9 64 288.1 64 288.1 64S117.3 64 74.7 75.5c-23.5 6.3-42 24.9-48.3 48.6C15 167 15 256.4 15 256.4s0 89.4 11.4 132.3c6.3 23.6 24.8 41.5 48.3 47.8C117.3 448 288.1 448 288.1 448s170.8 0 213.4-11.5c23.5-6.3 42-24.2 48.3-47.8 11.4-42.9 11.4-132.3 11.4-132.3s0-89.4-11.4-132.3zM232.2 337.6V175.2l142.7 81.2z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://hub.docker.com/r/ultralytics/ultralytics/" rel="noopener" target="_blank" title="hub.docker.com"><svg viewbox="0 0 640 512" xmlns="http://www.w3.org/2000/svg"><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://pypi.org/project/ultralytics/" rel="noopener" target="_blank" title="pypi.org"><svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 444.7a20.4 20.4 0 1 1 0-40.7 20.4 20.4 0 1 1 0 40.7M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.6-183.4a20.4 20.4 0 1 1 0 40.8 20.4 20.4 0 1 1 0-40.8" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://discord.com/invite/ultralytics" rel="noopener" target="_blank" title="discord.com"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M492.5 69.8c-.2-.3-.4-.6-.8-.7-38.1-17.5-78.4-30-119.7-37.1-.4-.1-.8 0-1.1.1s-.6.4-.8.8c-5.5 9.9-10.5 20.2-14.9 30.6-44.6-6.8-89.9-6.8-134.4 0-4.5-10.5-9.5-20.7-15.1-30.6-.2-.3-.5-.6-.8-.8s-.7-.2-1.1-.2C162.5 39 122.2 51.5 84.1 69c-.3.1-.6.4-.8.7C7.1 183.5-13.8 294.6-3.6 404.2c0 .3.1.5.2.8s.3.4.5.6c44.4 32.9 94 58 146.8 74.2.4.1.8.1 1.1 0s.7-.4.9-.7c11.3-15.4 21.4-31.8 30-48.8.1-.2.2-.5.2-.8s0-.5-.1-.8-.2-.5-.4-.6-.4-.3-.7-.4c-15.8-6.1-31.2-13.4-45.9-21.9-.3-.2-.5-.4-.7-.6s-.3-.6-.3-.9 0-.6.2-.9.3-.5.6-.7c3.1-2.3 6.2-4.7 9.1-7.1.3-.2.6-.4.9-.4s.7 0 1 .1c96.2 43.9 200.4 43.9 295.5 0 .3-.1.7-.2 1-.2s.7.2.9.4c2.9 2.4 6 4.9 9.1 7.2.2.2.4.4.6.7s.2.6.2.9-.1.6-.3.9-.4.5-.6.6c-14.7 8.6-30 15.9-45.9 21.8-.2.1-.5.2-.7.4s-.3.4-.4.7-.1.5-.1.8.1.5.2.8c8.8 17 18.8 33.3 30 48.8.2.3.6.6.9.7s.8.1 1.1 0c52.9-16.2 102.6-41.3 147.1-74.2.2-.2.4-.4.5-.6s.2-.5.2-.8c12.3-126.8-20.5-236.9-86.9-334.5zm-302 267.7c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.4 59.2-52.8 59.2m195.4 0c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.2 59.2-52.8 59.2" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://reddit.com/r/ultralytics" rel="noopener" target="_blank" title="reddit.com"><svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M0 256C0 114.6 114.6 0 256 0s256 114.6 256 256-114.6 256-256 256H37.1c-13.7 0-20.5-16.5-10.9-26.2L75 437C28.7 390.7 0 326.7 0 256m349.6-102.4c23.6 0 42.7-19.1 42.7-42.7s-19.1-42.7-42.7-42.7c-20.6 0-37.8 14.6-41.8 34-34.5 3.7-61.4 33-61.4 68.4v.2c-37.5 1.6-71.8 12.3-99 29.1-10.1-7.8-22.8-12.5-36.5-12.5-33 0-59.8 26.8-59.8 59.8 0 24 14.1 44.6 34.4 54.1 2 69.4 77.6 125.2 170.6 125.2s168.7-55.9 170.6-125.3c20.2-9.6 34.1-30.2 34.1-54 0-33-26.8-59.8-59.8-59.8-13.7 0-26.3 4.6-36.4 12.4-27.4-17-62.1-27.7-100-29.1v-.2c0-25.4 18.9-46.5 43.4-49.9 4.4 18.8 21.3 32.8 41.5 32.8zm-172.5 93.3c16.7 0 29.5 17.6 28.5 39.3s-13.5 29.6-30.3 29.6-31.4-8.8-30.4-30.5S160.3 247 177 247zm190.1 38.3c1 21.7-13.7 30.5-30.4 30.5s-29.3-7.9-30.3-29.6 11.8-39.3 28.5-39.3 31.2 16.6 32.1 38.3zm-48.1 56.7c-10.3 24.6-34.6 41.9-63 41.9s-52.7-17.3-63-41.9c-1.2-2.9.8-6.2 3.9-6.5 18.4-1.9 38.3-2.9 59.1-2.9s40.7 1 59.1 2.9c3.1.3 5.1 3.6 3.9 6.5" fill="currentColor"></path></svg></a><a class="md-social__link" href="https://weixin.qq.com/r/mp/LxckPDfEgWr_rXNf90I9" rel="noopener" target="_blank" title="weixin.qq.com"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M385.2 167.6c6.4 0 12.6.3 18.8 1.1C387.4 90.3 303.3 32 207.7 32 100.5 32 13 104.8 13 197.4c0 53.4 29.3 97.5 77.9 131.6l-19.3 58.6 68-34.1c24.4 4.8 43.8 9.7 68.2 9.7 6.2 0 12.1-.3 18.3-.8-4-12.9-6.2-26.6-6.2-40.8-.1-84.9 72.9-154 165.3-154m-104.5-52.9c14.5 0 24.2 9.7 24.2 24.4 0 14.5-9.7 24.2-24.2 24.2-14.8 0-29.3-9.7-29.3-24.2.1-14.7 14.6-24.4 29.3-24.4m-136.4 48.6c-14.5 0-29.3-9.7-29.3-24.2 0-14.8 14.8-24.4 29.3-24.4 14.8 0 24.4 9.7 24.4 24.4 0 14.6-9.6 24.2-24.4 24.2M563 319.4c0-77.9-77.9-141.3-165.4-141.3-92.7 0-165.4 63.4-165.4 141.3S305 460.7 397.6 460.7c19.3 0 38.9-5.1 58.6-9.9l53.4 29.3-14.8-48.6C534 402.1 563 363.2 563 319.4m-219.1-24.5c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.8 0 24.4 9.7 24.4 19.3 0 10-9.7 19.6-24.4 19.6m107.1 0c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.5 0 24.4 9.7 24.4 19.3.1 10-9.9 19.6-24.4 19.6" fill="currentColor"></path></svg></a></div></div></div></footer></div><div class="md-dialog" data-md-component="dialog"><div class="md-dialog__inner md-typeset"></div></div><div class="md-progress" data-md-component="progress" role="progressbar"></div>
<script id="__config" type="application/json">{"annotate":null,"base":"../../../..","features":["content.action.edit","content.code.annotate","content.code.copy","content.tooltips","toc.follow","navigation.top","navigation.tabs","navigation.tabs.sticky","navigation.prune","navigation.footer","navigation.tracking","navigation.instant","navigation.instant.progress","navigation.indexes","navigation.sections","content.tabs.link"],"search":"../../../../assets/javascripts/workers/search.5e1f2129.min.js","tags":null,"translations":{"clipboard.copied":"Copied to clipboard","clipboard.copy":"Copy to clipboard","search.result.more.one":"1 more on this page","search.result.more.other":"# more on this page","search.result.none":"No matching documents","search.result.one":"1 matching document","search.result.other":"# matching documents","search.result.placeholder":"Type to start searching","search.result.term.missing":"Missing","select.version":"Select version"},"version":null}</script>
<script src="../../../../assets/javascripts/bundle.21aa498e.min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/ultralytics/llm@v0.0.8/js/chat.min.js"></script>
<script src="https://unpkg.com/tablesort@5.6.0/dist/tablesort.min.js"></script>
<script src="../../../../javascript/extra.js"></script>
<script src="../../../../javascript/giscus.js"></script>
<script src="../../../../javascript/tablesort.js"></script>
</body></html>