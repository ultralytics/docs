<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="Explore detailed documentation of various SAM encoder modules such as ImageEncoderViT, PromptEncoder, and more, available in Ultralytics' repository." name="description"/>
<meta content="Ultralytics" name="author"/>
<link href="https://docs.ultralytics.com/reference/models/sam/modules/encoders/" rel="canonical"/>
<link href="../decoders/" rel="prev"/>
<link href="../memory_attention/" rel="next"/>
<link href="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/logo/favicon-yolo.png" rel="icon"/>
<meta content="mkdocs-1.6.1, mkdocs-material-9.6.17" name="generator"/>
<title>Reference for ultralytics/models/sam/modules/encoders.py</title>
<link href="../../../../../assets/stylesheets/main.7e37652d.min.css" rel="stylesheet"/>
<link href="../../../../../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<link href="../../../../../assets/_mkdocstrings.css" rel="stylesheet"/>
<link href="../../../../../stylesheets/style.css" rel="stylesheet"/>
<script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-2M5EHKC0BH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-2M5EHKC0BH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-2M5EHKC0BH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
<meta content="encoders" name="title"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet"/><meta content="Ultralytics, SAM encoder, ImageEncoderViT, PromptEncoder, PositionEmbeddingRandom, Block, Attention, PatchEmbed" name="keywords"/><meta content="website" property="og:type"/><meta content="https://docs.ultralytics.com/reference/models/sam/modules/encoders" property="og:url"/><meta content="encoders" property="og:title"/><meta content="Explore detailed documentation of various SAM encoder modules such as ImageEncoderViT, PromptEncoder, and more, available in Ultralytics' repository." property="og:description"/><meta content="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" property="og:image"/><meta content="summary_large_image" property="twitter:card"/><meta content="https://docs.ultralytics.com/reference/models/sam/modules/encoders" property="twitter:url"/><meta content="encoders" property="twitter:title"/><meta content="Explore detailed documentation of various SAM encoder modules such as ImageEncoderViT, PromptEncoder, and more, available in Ultralytics' repository." property="twitter:description"/><meta content="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" property="twitter:image"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": "Article", "headline": "encoders", "image": ["https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png"], "datePublished": "2023-11-12 02:49:37 +0100", "dateModified": "2024-09-11 18:30:13 +0200", "author": [{"@type": "Organization", "name": "Ultralytics", "url": "https://ultralytics.com/"}], "abstract": "Explore detailed documentation of various SAM encoder modules such as ImageEncoderViT, PromptEncoder, and more, available in Ultralytics' repository."}</script></head>
<body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="default" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#reference-for-ultralyticsmodelssammodulesencoderspy">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
<aside class="md-banner">
<div class="md-banner__inner md-grid md-typeset">
<a class="banner-wrapper" href="https://www.ultralytics.com/events/yolovision" target="_blank">
<div class="banner-content-wrapper">
<p>Register now for</p>
<img alt="Ultralytics YOLO Vision" height="40" loading="lazy" src="https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/6895ca9b3ece7244d9981ab9_yv25-logo.avif"/>
</div>
</a>
</div>
</aside>
</div>
<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Ultralytics YOLO Docs" class="md-header__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs">
<img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Ultralytics YOLO Docs
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
              encoders
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_0" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg>
</label>
<input aria-label="Switch to system preference" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="black" data-md-color-scheme="slate" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to system preference">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
</label>
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_2" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_0" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
</label>
</form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<div class="md-header__option">
<div class="md-select">
<button aria-label="Select language" class="md-header__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"></path></svg>
</button>
<div class="md-select__inner">
<ul class="md-select__list">
<li class="md-select__item">
<a class="md-select__link" href="/" hreflang="en">
              🇬🇧 English
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/zh/" hreflang="zh">
              🇨🇳 简体中文
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/ko/" hreflang="ko">
              🇰🇷 한국어
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/ja/" hreflang="ja">
              🇯🇵 日本語
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/ru/" hreflang="ru">
              🇷🇺 Русский
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/de/" hreflang="de">
              🇩🇪 Deutsch
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/fr/" hreflang="fr">
              🇫🇷 Français
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/es/" hreflang="es">
              🇪🇸 Español
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/pt/" hreflang="pt">
              🇵🇹 Português
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/it/" hreflang="it">
              🇮🇹 Italiano
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/tr/" hreflang="tr">
              🇹🇷 Türkçe
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/vi/" hreflang="vi">
              🇻🇳 Tiếng Việt
            </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="/ar/" hreflang="ar">
              🇸🇦 العربية
            </a>
</li>
</ul>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg>
</div>
<div class="md-source__repository">
    ultralytics/ultralytics
  </div>
</a>
</div>
</nav>
<nav aria-label="Tabs" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../../..">
  Home
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../../../quickstart/">
  Quickstart
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../../../modes/">
  Modes
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../../../tasks/">
  Tasks
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../../../models/">
  Models
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../../../datasets/">
  Datasets
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../../../solutions/">
  Solutions 🚀
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../../../guides/">
  Guides
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../../../integrations/">
  Integrations
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../../../hub/">
  HUB
        </a>
</li>
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="../../../../cfg/__init__/">
  Reference
        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../../../help/">
  Help
        </a>
</li>
</ul>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Ultralytics YOLO Docs" class="md-nav__button md-logo" data-md-component="logo" href="https://www.ultralytics.com/" title="Ultralytics YOLO Docs">
<img alt="logo" src="https://raw.githubusercontent.com/ultralytics/assets/main/logo/Ultralytics_Logotype_Reverse.svg"/>
</a>
    Ultralytics YOLO Docs
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/ultralytics/ultralytics" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg>
</div>
<div class="md-source__repository">
    ultralytics/ultralytics
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../..">
<span class="md-ellipsis">
    Home
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../../quickstart/">
<span class="md-ellipsis">
    Quickstart
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../../modes/">
<span class="md-ellipsis">
    Modes
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../../tasks/">
<span class="md-ellipsis">
    Tasks
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../../models/">
<span class="md-ellipsis">
    Models
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../../datasets/">
<span class="md-ellipsis">
    Datasets
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../../solutions/">
<span class="md-ellipsis">
    Solutions 🚀
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../../guides/">
<span class="md-ellipsis">
    Guides
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../../integrations/">
<span class="md-ellipsis">
    Integrations
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../../hub/">
<span class="md-ellipsis">
    HUB
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_11" type="checkbox"/>
<label class="md-nav__link" for="__nav_11" id="__nav_11_label" tabindex="">
<span class="md-ellipsis">
    Reference
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_11_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_11">
<span class="md-nav__icon md-icon"></span>
            Reference
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_11_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_11_1" id="__nav_11_1_label" tabindex="">
<span class="md-ellipsis">
    cfg
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_11_1_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_11_1">
<span class="md-nav__icon md-icon"></span>
            cfg
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../cfg/__init__/">
<span class="md-ellipsis">
    __init__
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_11_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_11_2" id="__nav_11_2_label" tabindex="">
<span class="md-ellipsis">
    data
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_11_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_11_2">
<span class="md-nav__icon md-icon"></span>
            data
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../data/annotator/">
<span class="md-ellipsis">
    annotator
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../data/augment/">
<span class="md-ellipsis">
    augment
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../data/base/">
<span class="md-ellipsis">
    base
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../data/build/">
<span class="md-ellipsis">
    build
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../data/converter/">
<span class="md-ellipsis">
    converter
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../data/dataset/">
<span class="md-ellipsis">
    dataset
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../data/loaders/">
<span class="md-ellipsis">
    loaders
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../data/split/">
<span class="md-ellipsis">
    split
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../data/split_dota/">
<span class="md-ellipsis">
    split_dota
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../data/utils/">
<span class="md-ellipsis">
    utils
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_11_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_11_3" id="__nav_11_3_label" tabindex="">
<span class="md-ellipsis">
    engine
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_11_3_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_11_3">
<span class="md-nav__icon md-icon"></span>
            engine
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../engine/exporter/">
<span class="md-ellipsis">
    exporter
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../engine/model/">
<span class="md-ellipsis">
    model
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../engine/predictor/">
<span class="md-ellipsis">
    predictor
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../engine/results/">
<span class="md-ellipsis">
    results
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../engine/trainer/">
<span class="md-ellipsis">
    trainer
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../engine/tuner/">
<span class="md-ellipsis">
    tuner
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../engine/validator/">
<span class="md-ellipsis">
    validator
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_11_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_11_4" id="__nav_11_4_label" tabindex="">
<span class="md-ellipsis">
    hub
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_11_4_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_11_4">
<span class="md-nav__icon md-icon"></span>
            hub
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../hub/__init__/">
<span class="md-ellipsis">
    __init__
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../hub/auth/">
<span class="md-ellipsis">
    auth
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../hub/google/__init__/">
<span class="md-ellipsis">
    google
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../hub/session/">
<span class="md-ellipsis">
    session
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../hub/utils/">
<span class="md-ellipsis">
    utils
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_11_5" type="checkbox"/>
<label class="md-nav__link" for="__nav_11_5" id="__nav_11_5_label" tabindex="">
<span class="md-ellipsis">
    models
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_11_5_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_11_5">
<span class="md-nav__icon md-icon"></span>
            models
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../fastsam/model/">
<span class="md-ellipsis">
    fastsam
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../nas/model/">
<span class="md-ellipsis">
    nas
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../rtdetr/model/">
<span class="md-ellipsis">
    rtdetr
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_11_5_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_11_5_4" id="__nav_11_5_4_label" tabindex="0">
<span class="md-ellipsis">
    sam
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_11_5_4_label" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_11_5_4">
<span class="md-nav__icon md-icon"></span>
            sam
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../amg/">
<span class="md-ellipsis">
    amg
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../build/">
<span class="md-ellipsis">
    build
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../model/">
<span class="md-ellipsis">
    model
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_11_5_4_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_11_5_4_4" id="__nav_11_5_4_4_label" tabindex="0">
<span class="md-ellipsis">
    modules
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_11_5_4_4_label" class="md-nav" data-md-level="4">
<label class="md-nav__title" for="__nav_11_5_4_4">
<span class="md-nav__icon md-icon"></span>
            modules
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../blocks/">
<span class="md-ellipsis">
    blocks
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../decoders/">
<span class="md-ellipsis">
    decoders
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    encoders
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
<span class="md-ellipsis">
    encoders
  </span>
</a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.ImageEncoderViT">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> ImageEncoderViT
    </span>
</a>
<nav aria-label=" ImageEncoderViT" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.ImageEncoderViT.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.PromptEncoder">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> PromptEncoder
    </span>
</a>
<nav aria-label=" PromptEncoder" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.PromptEncoder.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.PromptEncoder.get_dense_pe">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> get_dense_pe
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.MemoryEncoder">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> MemoryEncoder
    </span>
</a>
<nav aria-label=" MemoryEncoder" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.MemoryEncoder.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.ImageEncoder">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> ImageEncoder
    </span>
</a>
<nav aria-label=" ImageEncoder" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.ImageEncoder.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.FpnNeck">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> FpnNeck
    </span>
</a>
<nav aria-label=" FpnNeck" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.FpnNeck.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.Hiera">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> Hiera
    </span>
</a>
<nav aria-label=" Hiera" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.Hiera.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../memory_attention/">
<span class="md-ellipsis">
    memory_attention
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../sam/">
<span class="md-ellipsis">
    sam
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../tiny_encoder/">
<span class="md-ellipsis">
    tiny_encoder
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../transformer/">
<span class="md-ellipsis">
    transformer
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../utils/">
<span class="md-ellipsis">
    utils
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../predict/">
<span class="md-ellipsis">
    predict
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../utils/loss/">
<span class="md-ellipsis">
    utils
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../yolo/classify/predict/">
<span class="md-ellipsis">
    yolo
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_11_6" type="checkbox"/>
<label class="md-nav__link" for="__nav_11_6" id="__nav_11_6_label" tabindex="">
<span class="md-ellipsis">
    nn
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_11_6_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_11_6">
<span class="md-nav__icon md-icon"></span>
            nn
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../nn/autobackend/">
<span class="md-ellipsis">
    autobackend
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../nn/modules/activation/">
<span class="md-ellipsis">
    modules
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../nn/tasks/">
<span class="md-ellipsis">
    tasks
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../nn/text_model/">
<span class="md-ellipsis">
    text_model
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_11_7" type="checkbox"/>
<label class="md-nav__link" for="__nav_11_7" id="__nav_11_7_label" tabindex="">
<span class="md-ellipsis">
    solutions
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_11_7_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_11_7">
<span class="md-nav__icon md-icon"></span>
            solutions
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/ai_gym/">
<span class="md-ellipsis">
    ai_gym
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/analytics/">
<span class="md-ellipsis">
    analytics
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/config/">
<span class="md-ellipsis">
    config
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/distance_calculation/">
<span class="md-ellipsis">
    distance_calculation
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/heatmap/">
<span class="md-ellipsis">
    heatmap
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/instance_segmentation/">
<span class="md-ellipsis">
    instance_segmentation
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/object_blurrer/">
<span class="md-ellipsis">
    object_blurrer
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/object_counter/">
<span class="md-ellipsis">
    object_counter
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/object_cropper/">
<span class="md-ellipsis">
    object_cropper
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/parking_management/">
<span class="md-ellipsis">
    parking_management
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/queue_management/">
<span class="md-ellipsis">
    queue_management
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/region_counter/">
<span class="md-ellipsis">
    region_counter
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/security_alarm/">
<span class="md-ellipsis">
    security_alarm
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/similarity_search/">
<span class="md-ellipsis">
    similarity_search
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/solutions/">
<span class="md-ellipsis">
    solutions
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/speed_estimation/">
<span class="md-ellipsis">
    speed_estimation
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/streamlit_inference/">
<span class="md-ellipsis">
    streamlit_inference
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/trackzone/">
<span class="md-ellipsis">
    trackzone
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../solutions/vision_eye/">
<span class="md-ellipsis">
    vision_eye
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_11_8" type="checkbox"/>
<label class="md-nav__link" for="__nav_11_8" id="__nav_11_8_label" tabindex="">
<span class="md-ellipsis">
    trackers
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_11_8_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_11_8">
<span class="md-nav__icon md-icon"></span>
            trackers
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../trackers/basetrack/">
<span class="md-ellipsis">
    basetrack
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../trackers/bot_sort/">
<span class="md-ellipsis">
    bot_sort
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../trackers/byte_tracker/">
<span class="md-ellipsis">
    byte_tracker
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../trackers/track/">
<span class="md-ellipsis">
    track
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../trackers/utils/gmc/">
<span class="md-ellipsis">
    utils
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_11_9" type="checkbox"/>
<label class="md-nav__link" for="__nav_11_9" id="__nav_11_9_label" tabindex="">
<span class="md-ellipsis">
    utils
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_11_9_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_11_9">
<span class="md-nav__icon md-icon"></span>
            utils
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/__init__/">
<span class="md-ellipsis">
    __init__
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/autobatch/">
<span class="md-ellipsis">
    autobatch
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/autodevice/">
<span class="md-ellipsis">
    autodevice
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/benchmarks/">
<span class="md-ellipsis">
    benchmarks
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../utils/callbacks/base/">
<span class="md-ellipsis">
    callbacks
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/checks/">
<span class="md-ellipsis">
    checks
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/dist/">
<span class="md-ellipsis">
    dist
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/downloads/">
<span class="md-ellipsis">
    downloads
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/errors/">
<span class="md-ellipsis">
    errors
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/export/">
<span class="md-ellipsis">
    export
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/files/">
<span class="md-ellipsis">
    files
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/instance/">
<span class="md-ellipsis">
    instance
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/logger/">
<span class="md-ellipsis">
    logger
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/loss/">
<span class="md-ellipsis">
    loss
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/metrics/">
<span class="md-ellipsis">
    metrics
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/ops/">
<span class="md-ellipsis">
    ops
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/patches/">
<span class="md-ellipsis">
    patches
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/plotting/">
<span class="md-ellipsis">
    plotting
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/tal/">
<span class="md-ellipsis">
    tal
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/torch_utils/">
<span class="md-ellipsis">
    torch_utils
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/triton/">
<span class="md-ellipsis">
    triton
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../../utils/tuner/">
<span class="md-ellipsis">
    tuner
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../../../help/">
<span class="md-ellipsis">
    Help
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.ImageEncoderViT">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> ImageEncoderViT
    </span>
</a>
<nav aria-label=" ImageEncoderViT" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.ImageEncoderViT.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.PromptEncoder">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> PromptEncoder
    </span>
</a>
<nav aria-label=" PromptEncoder" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.PromptEncoder.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.PromptEncoder.get_dense_pe">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> get_dense_pe
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.MemoryEncoder">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> MemoryEncoder
    </span>
</a>
<nav aria-label=" MemoryEncoder" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.MemoryEncoder.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.ImageEncoder">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> ImageEncoder
    </span>
</a>
<nav aria-label=" ImageEncoder" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.ImageEncoder.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.FpnNeck">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> FpnNeck
    </span>
</a>
<nav aria-label=" FpnNeck" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.FpnNeck.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.Hiera">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> Hiera
    </span>
</a>
<nav aria-label=" Hiera" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#ultralytics.models.sam.modules.encoders.Hiera.forward">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-method"></code> forward
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<a class="md-content__button md-icon" href="https://github.com/ultralytics/ultralytics/tree/main/docs/en/reference/models/sam/modules/encoders.md" rel="edit" title="Edit this page">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
</a><a class="md-content__button md-icon" href="javascript:void(0)" onclick="copyMarkdownForLLM(this); return false;" title="Copy page in Markdown format"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg></a>
<h1 id="reference-for-ultralyticsmodelssammodulesencoderspy">Reference for <code>ultralytics/models/sam/modules/encoders.py</code></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This file is available at <a href="https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/modules/encoders.py">https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/modules/encoders.py</a>. If you spot a problem please help fix it by <a href="https://docs.ultralytics.com/help/contributing/">contributing</a> a <a href="https://github.com/ultralytics/ultralytics/edit/main/ultralytics/models/sam/modules/encoders.py">Pull Request</a> 🛠️. Thank you 🙏!</p>
</div>
<p><br/></p>
<div class="doc doc-object doc-class">
<h2 class="doc doc-heading" id="ultralytics.models.sam.modules.encoders.ImageEncoderViT">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <span class="doc doc-object-name doc-class-name">ultralytics.models.sam.modules.encoders.ImageEncoderViT</span>
</h2>
<div class="doc-signature highlight"><pre><span></span><code><span></span><span class="nf">ImageEncoderViT</span><span class="p">(</span>
<span></span>    <span class="n">img_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
<span></span>    <span class="n">patch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
<span></span>    <span class="n">in_chans</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
<span></span>    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">768</span><span class="p">,</span>
<span></span>    <span class="n">depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
<span></span>    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
<span></span>    <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span>
<span></span>    <span class="n">out_chans</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
<span></span>    <span class="n">qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span></span>    <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span>
<span></span>    <span class="n">act_layer</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span>
<span></span>    <span class="n">use_abs_pos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span></span>    <span class="n">use_rel_pos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="n">rel_pos_zero_init</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span></span>    <span class="n">window_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span></span>    <span class="n">global_attn_indexes</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
<span></span><span class="p">)</span>
</code></pre></div>
<div class="doc doc-contents first">
<p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>
<p>An image encoder using Vision Transformer (ViT) architecture for encoding images into a compact latent space.</p>
<p>This class processes images by splitting them into patches, applying transformer blocks, and generating a final
encoded representation through a neck module.</p>
<p><span class="doc-section-title">Attributes:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.ImageEncoderViT.img_size">img_size</span></code></td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dimension of input images, assumed to be square.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.ImageEncoderViT.patch_embed">patch_embed</span></code></td>
<td>
<code><a class="autorefs autorefs-internal" href="../blocks/#ultralytics.models.sam.modules.blocks.PatchEmbed" title='&lt;code class="doc-symbol doc-symbol-heading doc-symbol-class"&gt;&lt;/code&gt;            &lt;span class="doc doc-object-name doc-class-name"&gt;ultralytics.models.sam.modules.blocks.PatchEmbed&lt;/span&gt;'>PatchEmbed</a></code>
</td>
<td>
<div class="doc-md-description">
<p>Module for patch embedding.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.ImageEncoderViT.pos_embed">pos_embed</span></code></td>
<td>
<code><span title="torch.nn.Parameter">Parameter</span> | None</code>
</td>
<td>
<div class="doc-md-description">
<p>Absolute positional embedding for patches.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.ImageEncoderViT.blocks">blocks</span></code></td>
<td>
<code><span title="torch.nn.ModuleList">ModuleList</span></code>
</td>
<td>
<div class="doc-md-description">
<p>List of transformer blocks for processing patch embeddings.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.ImageEncoderViT.neck">neck</span></code></td>
<td>
<code><span title="torch.nn.Sequential">Sequential</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Neck module to further process the output.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Methods:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td><code><a class="autorefs autorefs-internal" href="#ultralytics.models.sam.modules.encoders.ImageEncoderViT.forward" title='&lt;code class="doc-symbol doc-symbol-heading doc-symbol-method"&gt;&lt;/code&gt;            &lt;span class="doc doc-object-name doc-function-name"&gt;forward&lt;/span&gt; (&lt;code&gt;ultralytics.models.sam.modules.encoders.ImageEncoderViT.forward&lt;/code&gt;)'>forward</a></code></td>
<td>
<div class="doc-md-description">
<p>Process input through patch embedding, positional embedding, blocks, and neck.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">encoder</span> <span class="o">=</span> <span class="n">ImageEncoderViT</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_image</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>img_size</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Input image size, assumed to be square.</p>
</div>
</td>
<td>
<code>1024</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>patch_size</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Size of image patches.</p>
</div>
</td>
<td>
<code>16</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>in_chans</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of input image channels.</p>
</div>
</td>
<td>
<code>3</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>embed_dim</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dimension of patch embeddings.</p>
</div>
</td>
<td>
<code>768</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>depth</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of transformer blocks.</p>
</div>
</td>
<td>
<code>12</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>num_heads</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of attention heads in each block.</p>
</div>
</td>
<td>
<code>12</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>mlp_ratio</code>
</td>
<td>
<code><span title="float">float</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Ratio of MLP hidden dimension to embedding dimension.</p>
</div>
</td>
<td>
<code>4.0</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>out_chans</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of output channels from the neck module.</p>
</div>
</td>
<td>
<code>256</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>qkv_bias</code>
</td>
<td>
<code><span title="bool">bool</span></code>
</td>
<td>
<div class="doc-md-description">
<p>If True, adds learnable bias to query, key, value projections.</p>
</div>
</td>
<td>
<code>True</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>norm_layer</code>
</td>
<td>
<code><span title="typing.Type">Type</span>[<span title="torch.nn.Module">Module</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Type of normalization layer to use.</p>
</div>
</td>
<td>
<code><span title="torch.nn.LayerNorm">LayerNorm</span></code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>act_layer</code>
</td>
<td>
<code><span title="typing.Type">Type</span>[<span title="torch.nn.Module">Module</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Type of activation layer to use.</p>
</div>
</td>
<td>
<code><span title="torch.nn.GELU">GELU</span></code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>use_abs_pos</code>
</td>
<td>
<code><span title="bool">bool</span></code>
</td>
<td>
<div class="doc-md-description">
<p>If True, uses absolute positional embeddings.</p>
</div>
</td>
<td>
<code>True</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>use_rel_pos</code>
</td>
<td>
<code><span title="bool">bool</span></code>
</td>
<td>
<div class="doc-md-description">
<p>If True, adds relative positional embeddings to attention maps.</p>
</div>
</td>
<td>
<code>False</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>rel_pos_zero_init</code>
</td>
<td>
<code><span title="bool">bool</span></code>
</td>
<td>
<div class="doc-md-description">
<p>If True, initializes relative positional parameters to zero.</p>
</div>
</td>
<td>
<code>True</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>window_size</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Size of attention window for windowed attention blocks.</p>
</div>
</td>
<td>
<code>0</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>global_attn_indexes</code>
</td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="int">int</span>, ...]</code>
</td>
<td>
<div class="doc-md-description">
<p>Indices of blocks that use global attention.</p>
</div>
</td>
<td>
<code>()</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">encoder</span> <span class="o">=</span> <span class="n">ImageEncoderViT</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_image</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>
<details class="quote">
<summary>Source code in <code>ultralytics/models/sam/modules/encoders.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span> 48</span>
<span> 49</span>
<span> 50</span>
<span> 51</span>
<span> 52</span>
<span> 53</span>
<span> 54</span>
<span> 55</span>
<span> 56</span>
<span> 57</span>
<span> 58</span>
<span> 59</span>
<span> 60</span>
<span> 61</span>
<span> 62</span>
<span> 63</span>
<span> 64</span>
<span> 65</span>
<span> 66</span>
<span> 67</span>
<span> 68</span>
<span> 69</span>
<span> 70</span>
<span> 71</span>
<span> 72</span>
<span> 73</span>
<span> 74</span>
<span> 75</span>
<span> 76</span>
<span> 77</span>
<span> 78</span>
<span> 79</span>
<span> 80</span>
<span> 81</span>
<span> 82</span>
<span> 83</span>
<span> 84</span>
<span> 85</span>
<span> 86</span>
<span> 87</span>
<span> 88</span>
<span> 89</span>
<span> 90</span>
<span> 91</span>
<span> 92</span>
<span> 93</span>
<span> 94</span>
<span> 95</span>
<span> 96</span>
<span> 97</span>
<span> 98</span>
<span> 99</span>
<span>100</span>
<span>101</span>
<span>102</span>
<span>103</span>
<span>104</span>
<span>105</span>
<span>106</span>
<span>107</span>
<span>108</span>
<span>109</span>
<span>110</span>
<span>111</span>
<span>112</span>
<span>113</span>
<span>114</span>
<span>115</span>
<span>116</span>
<span>117</span>
<span>118</span>
<span>119</span>
<span>120</span>
<span>121</span>
<span>122</span>
<span>123</span>
<span>124</span>
<span>125</span>
<span>126</span>
<span>127</span>
<span>128</span>
<span>129</span>
<span>130</span>
<span>131</span>
<span>132</span>
<span>133</span>
<span>134</span>
<span>135</span>
<span>136</span>
<span>137</span>
<span>138</span>
<span>139</span>
<span>140</span>
<span>141</span></pre></div></td><td class="code"><div><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">img_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
<span></span>    <span class="n">patch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
<span></span>    <span class="n">in_chans</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
<span></span>    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">768</span><span class="p">,</span>
<span></span>    <span class="n">depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
<span></span>    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
<span></span>    <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span>
<span></span>    <span class="n">out_chans</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
<span></span>    <span class="n">qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span></span>    <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span>
<span></span>    <span class="n">act_layer</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span>
<span></span>    <span class="n">use_abs_pos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span></span>    <span class="n">use_rel_pos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span></span>    <span class="n">rel_pos_zero_init</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span></span>    <span class="n">window_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span></span>    <span class="n">global_attn_indexes</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span></span><span class="w">    </span><span class="sd">"""</span>
<span></span><span class="sd">    Initialize an ImageEncoderViT instance for encoding images using Vision Transformer architecture.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        img_size (int): Input image size, assumed to be square.</span>
<span></span><span class="sd">        patch_size (int): Size of image patches.</span>
<span></span><span class="sd">        in_chans (int): Number of input image channels.</span>
<span></span><span class="sd">        embed_dim (int): Dimension of patch embeddings.</span>
<span></span><span class="sd">        depth (int): Number of transformer blocks.</span>
<span></span><span class="sd">        num_heads (int): Number of attention heads in each block.</span>
<span></span><span class="sd">        mlp_ratio (float): Ratio of MLP hidden dimension to embedding dimension.</span>
<span></span><span class="sd">        out_chans (int): Number of output channels from the neck module.</span>
<span></span><span class="sd">        qkv_bias (bool): If True, adds learnable bias to query, key, value projections.</span>
<span></span><span class="sd">        norm_layer (Type[nn.Module]): Type of normalization layer to use.</span>
<span></span><span class="sd">        act_layer (Type[nn.Module]): Type of activation layer to use.</span>
<span></span><span class="sd">        use_abs_pos (bool): If True, uses absolute positional embeddings.</span>
<span></span><span class="sd">        use_rel_pos (bool): If True, adds relative positional embeddings to attention maps.</span>
<span></span><span class="sd">        rel_pos_zero_init (bool): If True, initializes relative positional parameters to zero.</span>
<span></span><span class="sd">        window_size (int): Size of attention window for windowed attention blocks.</span>
<span></span><span class="sd">        global_attn_indexes (Tuple[int, ...]): Indices of blocks that use global attention.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; encoder = ImageEncoderViT(img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12)</span>
<span></span><span class="sd">        &gt;&gt;&gt; input_image = torch.randn(1, 3, 224, 224)</span>
<span></span><span class="sd">        &gt;&gt;&gt; output = encoder(input_image)</span>
<span></span><span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">img_size</span> <span class="o">=</span> <span class="n">img_size</span>
<span></span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span> <span class="o">=</span> <span class="n">PatchEmbed</span><span class="p">(</span>
<span></span>        <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">),</span>
<span></span>        <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">),</span>
<span></span>        <span class="n">in_chans</span><span class="o">=</span><span class="n">in_chans</span><span class="p">,</span>
<span></span>        <span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span></span>    <span class="k">if</span> <span class="n">use_abs_pos</span><span class="p">:</span>
<span></span>        <span class="c1"># Initialize absolute positional embedding with pretrain image size</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">img_size</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">img_size</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>
<span></span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
<span></span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">):</span>
<span></span>        <span class="n">block</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span>
<span></span>            <span class="n">dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
<span></span>            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
<span></span>            <span class="n">mlp_ratio</span><span class="o">=</span><span class="n">mlp_ratio</span><span class="p">,</span>
<span></span>            <span class="n">qkv_bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">,</span>
<span></span>            <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span>
<span></span>            <span class="n">act_layer</span><span class="o">=</span><span class="n">act_layer</span><span class="p">,</span>
<span></span>            <span class="n">use_rel_pos</span><span class="o">=</span><span class="n">use_rel_pos</span><span class="p">,</span>
<span></span>            <span class="n">rel_pos_zero_init</span><span class="o">=</span><span class="n">rel_pos_zero_init</span><span class="p">,</span>
<span></span>            <span class="n">window_size</span><span class="o">=</span><span class="n">window_size</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">global_attn_indexes</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
<span></span>            <span class="n">input_size</span><span class="o">=</span><span class="p">(</span><span class="n">img_size</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">img_size</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">),</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
<span></span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">neck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
<span></span>        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
<span></span>            <span class="n">embed_dim</span><span class="p">,</span>
<span></span>            <span class="n">out_chans</span><span class="p">,</span>
<span></span>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span></span>            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="p">),</span>
<span></span>        <span class="n">LayerNorm2d</span><span class="p">(</span><span class="n">out_chans</span><span class="p">),</span>
<span></span>        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
<span></span>            <span class="n">out_chans</span><span class="p">,</span>
<span></span>            <span class="n">out_chans</span><span class="p">,</span>
<span></span>            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span></span>            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span></span>            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>        <span class="p">),</span>
<span></span>        <span class="n">LayerNorm2d</span><span class="p">(</span><span class="n">out_chans</span><span class="p">),</span>
<span></span>    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="ultralytics.models.sam.modules.encoders.ImageEncoderViT.forward">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">forward</span>
</h3>
<div class="doc-signature highlight"><pre><span></span><code><span></span><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
</code></pre></div>
<div class="doc doc-contents">
<p>Process input through patch embedding, positional embedding, transformer blocks, and neck module.</p>
<details class="quote">
<summary>Source code in <code>ultralytics/models/sam/modules/encoders.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span>143</span>
<span>144</span>
<span>145</span>
<span>146</span>
<span>147</span>
<span>148</span>
<span>149</span>
<span>150</span>
<span>151</span>
<span>152</span>
<span>153</span>
<span>154</span>
<span>155</span></pre></div></td><td class="code"><div><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span></span><span class="w">    </span><span class="sd">"""Process input through patch embedding, positional embedding, transformer blocks, and neck module."""</span>
<span></span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">pos_embed</span> <span class="o">=</span> <span class="p">(</span>
<span></span>            <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">scale_factor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">img_size</span> <span class="o">/</span> <span class="mi">1024</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span></span>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_size</span> <span class="o">!=</span> <span class="mi">1024</span>
<span></span>            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span>
<span></span>        <span class="p">)</span>
<span></span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">pos_embed</span>
<span></span>    <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
<span></span>        <span class="n">x</span> <span class="o">=</span> <span class="n">blk</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span></span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">neck</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div><p><br/><br/><hr/><br/></p>
<div class="doc doc-object doc-class">
<h2 class="doc doc-heading" id="ultralytics.models.sam.modules.encoders.PromptEncoder">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <span class="doc doc-object-name doc-class-name">ultralytics.models.sam.modules.encoders.PromptEncoder</span>
</h2>
<div class="doc-signature highlight"><pre><span></span><code><span></span><span class="nf">PromptEncoder</span><span class="p">(</span>
<span></span>    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">image_embedding_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
<span></span>    <span class="n">input_image_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
<span></span>    <span class="n">mask_in_chans</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">activation</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div>
<div class="doc doc-contents first">
<p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>
<p>Encode different types of prompts for input to SAM's mask decoder, producing sparse and dense embeddings.</p>
<p><span class="doc-section-title">Attributes:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.PromptEncoder.embed_dim">embed_dim</span></code></td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dimension of the embeddings.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.PromptEncoder.input_image_size">input_image_size</span></code></td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="int">int</span>, <span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Size of the input image as (H, W).</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.PromptEncoder.image_embedding_size">image_embedding_size</span></code></td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="int">int</span>, <span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Spatial size of the image embedding as (H, W).</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.PromptEncoder.pe_layer">pe_layer</span></code></td>
<td>
<code><a class="autorefs autorefs-internal" href="../blocks/#ultralytics.models.sam.modules.blocks.PositionEmbeddingRandom" title='&lt;code class="doc-symbol doc-symbol-heading doc-symbol-class"&gt;&lt;/code&gt;            &lt;span class="doc doc-object-name doc-class-name"&gt;ultralytics.models.sam.modules.blocks.PositionEmbeddingRandom&lt;/span&gt;'>PositionEmbeddingRandom</a></code>
</td>
<td>
<div class="doc-md-description">
<p>Module for random position embedding.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.PromptEncoder.num_point_embeddings">num_point_embeddings</span></code></td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of point embeddings for different types of points.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.PromptEncoder.point_embeddings">point_embeddings</span></code></td>
<td>
<code><span title="torch.nn.ModuleList">ModuleList</span></code>
</td>
<td>
<div class="doc-md-description">
<p>List of point embeddings.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.PromptEncoder.not_a_point_embed">not_a_point_embed</span></code></td>
<td>
<code><span title="torch.nn.Embedding">Embedding</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Embedding for points that are not part of any label.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.PromptEncoder.mask_input_size">mask_input_size</span></code></td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="int">int</span>, <span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Size of the input mask.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.PromptEncoder.mask_downscaling">mask_downscaling</span></code></td>
<td>
<code><span title="torch.nn.Sequential">Sequential</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Neural network for downscaling the mask.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.PromptEncoder.no_mask_embed">no_mask_embed</span></code></td>
<td>
<code><span title="torch.nn.Embedding">Embedding</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Embedding for cases where no mask is provided.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Methods:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td><code><a class="autorefs autorefs-internal" href="#ultralytics.models.sam.modules.encoders.PromptEncoder.get_dense_pe" title='&lt;code class="doc-symbol doc-symbol-heading doc-symbol-method"&gt;&lt;/code&gt;            &lt;span class="doc doc-object-name doc-function-name"&gt;get_dense_pe&lt;/span&gt; (&lt;code&gt;ultralytics.models.sam.modules.encoders.PromptEncoder.get_dense_pe&lt;/code&gt;)'>get_dense_pe</a></code></td>
<td>
<div class="doc-md-description">
<p>Return the positional encoding used to encode point prompts.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><a class="autorefs autorefs-internal" href="#ultralytics.models.sam.modules.encoders.PromptEncoder.forward" title='&lt;code class="doc-symbol doc-symbol-heading doc-symbol-method"&gt;&lt;/code&gt;            &lt;span class="doc doc-object-name doc-function-name"&gt;forward&lt;/span&gt; (&lt;code&gt;ultralytics.models.sam.modules.encoders.PromptEncoder.forward&lt;/code&gt;)'>forward</a></code></td>
<td>
<div class="doc-md-description">
<p>Embed different types of prompts, returning both sparse and dense embeddings.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">prompt_encoder</span> <span class="o">=</span> <span class="n">PromptEncoder</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="mi">16</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">points</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">boxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sparse_embeddings</span><span class="p">,</span> <span class="n">dense_embeddings</span> <span class="o">=</span> <span class="n">prompt_encoder</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">boxes</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">sparse_embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dense_embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span></span><span class="go">torch.Size([1, 7, 256]) torch.Size([1, 256, 64, 64])</span>
</code></pre></div>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>embed_dim</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>The dimension of the embeddings.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>image_embedding_size</code>
</td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="int">int</span>, <span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>The spatial size of the image embedding as (H, W).</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>input_image_size</code>
</td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="int">int</span>, <span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>The padded size of the input image as (H, W).</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>mask_in_chans</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>The number of hidden channels used for encoding input masks.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>activation</code>
</td>
<td>
<code><span title="typing.Type">Type</span>[<span title="torch.nn.Module">Module</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>The activation function to use when encoding input masks.</p>
</div>
</td>
<td>
<code><span title="torch.nn.GELU">GELU</span></code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">prompt_encoder</span> <span class="o">=</span> <span class="n">PromptEncoder</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="mi">16</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">points</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">boxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sparse_embeddings</span><span class="p">,</span> <span class="n">dense_embeddings</span> <span class="o">=</span> <span class="n">prompt_encoder</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">boxes</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">sparse_embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dense_embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span></span><span class="go">torch.Size([1, 7, 256]) torch.Size([1, 256, 64, 64])</span>
</code></pre></div>
<details class="quote">
<summary>Source code in <code>ultralytics/models/sam/modules/encoders.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span>188</span>
<span>189</span>
<span>190</span>
<span>191</span>
<span>192</span>
<span>193</span>
<span>194</span>
<span>195</span>
<span>196</span>
<span>197</span>
<span>198</span>
<span>199</span>
<span>200</span>
<span>201</span>
<span>202</span>
<span>203</span>
<span>204</span>
<span>205</span>
<span>206</span>
<span>207</span>
<span>208</span>
<span>209</span>
<span>210</span>
<span>211</span>
<span>212</span>
<span>213</span>
<span>214</span>
<span>215</span>
<span>216</span>
<span>217</span>
<span>218</span>
<span>219</span>
<span>220</span>
<span>221</span>
<span>222</span>
<span>223</span>
<span>224</span>
<span>225</span>
<span>226</span>
<span>227</span>
<span>228</span>
<span>229</span>
<span>230</span>
<span>231</span>
<span>232</span>
<span>233</span>
<span>234</span>
<span>235</span>
<span>236</span></pre></div></td><td class="code"><div><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">image_embedding_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
<span></span>    <span class="n">input_image_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
<span></span>    <span class="n">mask_in_chans</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">activation</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span></span><span class="w">    </span><span class="sd">"""</span>
<span></span><span class="sd">    Initialize the PromptEncoder module for encoding various types of prompts.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        embed_dim (int): The dimension of the embeddings.</span>
<span></span><span class="sd">        image_embedding_size (Tuple[int, int]): The spatial size of the image embedding as (H, W).</span>
<span></span><span class="sd">        input_image_size (Tuple[int, int]): The padded size of the input image as (H, W).</span>
<span></span><span class="sd">        mask_in_chans (int): The number of hidden channels used for encoding input masks.</span>
<span></span><span class="sd">        activation (Type[nn.Module]): The activation function to use when encoding input masks.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; prompt_encoder = PromptEncoder(256, (64, 64), (1024, 1024), 16)</span>
<span></span><span class="sd">        &gt;&gt;&gt; points = (torch.rand(1, 5, 2), torch.randint(0, 4, (1, 5)))</span>
<span></span><span class="sd">        &gt;&gt;&gt; boxes = torch.rand(1, 2, 2)</span>
<span></span><span class="sd">        &gt;&gt;&gt; masks = torch.rand(1, 1, 256, 256)</span>
<span></span><span class="sd">        &gt;&gt;&gt; sparse_embeddings, dense_embeddings = prompt_encoder(points, boxes, masks)</span>
<span></span><span class="sd">        &gt;&gt;&gt; print(sparse_embeddings.shape, dense_embeddings.shape)</span>
<span></span><span class="sd">        torch.Size([1, 7, 256]) torch.Size([1, 256, 64, 64])</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">input_image_size</span> <span class="o">=</span> <span class="n">input_image_size</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">image_embedding_size</span> <span class="o">=</span> <span class="n">image_embedding_size</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">pe_layer</span> <span class="o">=</span> <span class="n">PositionEmbeddingRandom</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
<span></span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">num_point_embeddings</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># pos/neg point + 2 box corners</span>
<span></span>    <span class="n">point_embeddings</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_point_embeddings</span><span class="p">)]</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">point_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">point_embeddings</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">not_a_point_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
<span></span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">mask_input_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">image_embedding_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">image_embedding_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">mask_downscaling</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
<span></span>        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mask_in_chans</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span></span>        <span class="n">LayerNorm2d</span><span class="p">(</span><span class="n">mask_in_chans</span> <span class="o">//</span> <span class="mi">4</span><span class="p">),</span>
<span></span>        <span class="n">activation</span><span class="p">(),</span>
<span></span>        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">mask_in_chans</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="n">mask_in_chans</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span></span>        <span class="n">LayerNorm2d</span><span class="p">(</span><span class="n">mask_in_chans</span><span class="p">),</span>
<span></span>        <span class="n">activation</span><span class="p">(),</span>
<span></span>        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">mask_in_chans</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">no_mask_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="ultralytics.models.sam.modules.encoders.PromptEncoder.forward">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">forward</span>
</h3>
<div class="doc-signature highlight"><pre><span></span><code><span></span><span class="nf">forward</span><span class="p">(</span>
<span></span>    <span class="n">points</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]],</span>
<span></span>    <span class="n">boxes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<span></span>    <span class="n">masks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
</code></pre></div>
<div class="doc doc-contents">
<p>Embed different types of prompts, returning both sparse and dense embeddings.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>points</code>
</td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>, <span title="torch.Tensor">Tensor</span>] | None</code>
</td>
<td>
<div class="doc-md-description">
<p>Point coordinates and labels to embed. The first
tensor contains coordinates with shape (B, N, 2), and the second tensor contains labels with
shape (B, N).</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>boxes</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span> | None</code>
</td>
<td>
<div class="doc-md-description">
<p>Boxes to embed with shape (B, M, 2, 2), where M is the number of boxes.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>masks</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span> | None</code>
</td>
<td>
<div class="doc-md-description">
<p>Masks to embed with shape (B, 1, H, W).</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Name</th> <th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td><code>sparse_embeddings</code></td> <td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Sparse embeddings for points and boxes with shape (B, N, embed_dim).</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code>dense_embeddings</code></td> <td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dense embeddings for masks of shape (B, embed_dim, embed_H, embed_W).</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">encoder</span> <span class="o">=</span> <span class="n">PromptEncoder</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="mi">16</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">points</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">boxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sparse_emb</span><span class="p">,</span> <span class="n">dense_emb</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">boxes</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">sparse_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dense_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span></span><span class="go">torch.Size([1, 7, 256]) torch.Size([1, 256, 64, 64])</span>
</code></pre></div>
<details class="quote">
<summary>Source code in <code>ultralytics/models/sam/modules/encoders.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span>303</span>
<span>304</span>
<span>305</span>
<span>306</span>
<span>307</span>
<span>308</span>
<span>309</span>
<span>310</span>
<span>311</span>
<span>312</span>
<span>313</span>
<span>314</span>
<span>315</span>
<span>316</span>
<span>317</span>
<span>318</span>
<span>319</span>
<span>320</span>
<span>321</span>
<span>322</span>
<span>323</span>
<span>324</span>
<span>325</span>
<span>326</span>
<span>327</span>
<span>328</span>
<span>329</span>
<span>330</span>
<span>331</span>
<span>332</span>
<span>333</span>
<span>334</span>
<span>335</span>
<span>336</span>
<span>337</span>
<span>338</span>
<span>339</span>
<span>340</span>
<span>341</span>
<span>342</span>
<span>343</span>
<span>344</span>
<span>345</span>
<span>346</span>
<span>347</span>
<span>348</span>
<span>349</span>
<span>350</span>
<span>351</span>
<span>352</span>
<span>353</span></pre></div></td><td class="code"><div><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">points</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
<span></span>    <span class="n">boxes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<span></span>    <span class="n">masks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span></span><span class="w">    </span><span class="sd">"""</span>
<span></span><span class="sd">    Embed different types of prompts, returning both sparse and dense embeddings.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        points (Tuple[torch.Tensor, torch.Tensor] | None): Point coordinates and labels to embed. The first</span>
<span></span><span class="sd">            tensor contains coordinates with shape (B, N, 2), and the second tensor contains labels with</span>
<span></span><span class="sd">            shape (B, N).</span>
<span></span><span class="sd">        boxes (torch.Tensor | None): Boxes to embed with shape (B, M, 2, 2), where M is the number of boxes.</span>
<span></span><span class="sd">        masks (torch.Tensor | None): Masks to embed with shape (B, 1, H, W).</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        sparse_embeddings (torch.Tensor): Sparse embeddings for points and boxes with shape (B, N, embed_dim).</span>
<span></span><span class="sd">        dense_embeddings (torch.Tensor): Dense embeddings for masks of shape (B, embed_dim, embed_H, embed_W).</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; encoder = PromptEncoder(256, (64, 64), (1024, 1024), 16)</span>
<span></span><span class="sd">        &gt;&gt;&gt; points = (torch.rand(1, 5, 2), torch.randint(0, 4, (1, 5)))</span>
<span></span><span class="sd">        &gt;&gt;&gt; boxes = torch.rand(1, 2, 2, 2)</span>
<span></span><span class="sd">        &gt;&gt;&gt; masks = torch.rand(1, 1, 256, 256)</span>
<span></span><span class="sd">        &gt;&gt;&gt; sparse_emb, dense_emb = encoder(points, boxes, masks)</span>
<span></span><span class="sd">        &gt;&gt;&gt; print(sparse_emb.shape, dense_emb.shape)</span>
<span></span><span class="sd">        torch.Size([1, 7, 256]) torch.Size([1, 256, 64, 64])</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_batch_size</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">boxes</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span>    <span class="n">sparse_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
<span></span>        <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">),</span>
<span></span>        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">point_embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span></span>        <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">point_embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">points</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">coords</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">points</span>
<span></span>        <span class="n">point_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_points</span><span class="p">(</span><span class="n">coords</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="n">boxes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">))</span>
<span></span>        <span class="n">sparse_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">sparse_embeddings</span><span class="p">,</span> <span class="n">point_embeddings</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="k">if</span> <span class="n">boxes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">box_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_boxes</span><span class="p">(</span><span class="n">boxes</span><span class="p">)</span>
<span></span>        <span class="n">sparse_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">sparse_embeddings</span><span class="p">,</span> <span class="n">box_embeddings</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">if</span> <span class="n">masks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="n">dense_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_masks</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span>
<span></span>    <span class="k">else</span><span class="p">:</span>
<span></span>        <span class="n">dense_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_mask_embed</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
<span></span>            <span class="n">bs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_embedding_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_embedding_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">sparse_embeddings</span><span class="p">,</span> <span class="n">dense_embeddings</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="ultralytics.models.sam.modules.encoders.PromptEncoder.get_dense_pe">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">get_dense_pe</span>
</h3>
<div class="doc-signature highlight"><pre><span></span><code><span></span><span class="nf">get_dense_pe</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
</code></pre></div>
<div class="doc doc-contents">
<p>Return the dense positional encoding used for encoding point prompts.</p>
<p>Generate a positional encoding for a dense set of points matching the shape of the image
encoding. The encoding is used to provide spatial information to the model when processing point prompts.</p>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Positional encoding tensor with shape (1, embed_dim, H, W), where H and W are the
height and width of the image embedding size, respectively.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">prompt_encoder</span> <span class="o">=</span> <span class="n">PromptEncoder</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="mi">16</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dense_pe</span> <span class="o">=</span> <span class="n">prompt_encoder</span><span class="o">.</span><span class="n">get_dense_pe</span><span class="p">()</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">dense_pe</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span></span><span class="go">torch.Size([1, 256, 64, 64])</span>
</code></pre></div>
<details class="quote">
<summary>Source code in <code>ultralytics/models/sam/modules/encoders.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span>238</span>
<span>239</span>
<span>240</span>
<span>241</span>
<span>242</span>
<span>243</span>
<span>244</span>
<span>245</span>
<span>246</span>
<span>247</span>
<span>248</span>
<span>249</span>
<span>250</span>
<span>251</span>
<span>252</span>
<span>253</span>
<span>254</span>
<span>255</span></pre></div></td><td class="code"><div><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_dense_pe</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span></span><span class="w">    </span><span class="sd">"""</span>
<span></span><span class="sd">    Return the dense positional encoding used for encoding point prompts.</span>
<span></span>
<span></span><span class="sd">    Generate a positional encoding for a dense set of points matching the shape of the image</span>
<span></span><span class="sd">    encoding. The encoding is used to provide spatial information to the model when processing point prompts.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (torch.Tensor): Positional encoding tensor with shape (1, embed_dim, H, W), where H and W are the</span>
<span></span><span class="sd">            height and width of the image embedding size, respectively.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; prompt_encoder = PromptEncoder(256, (64, 64), (1024, 1024), 16)</span>
<span></span><span class="sd">        &gt;&gt;&gt; dense_pe = prompt_encoder.get_dense_pe()</span>
<span></span><span class="sd">        &gt;&gt;&gt; print(dense_pe.shape)</span>
<span></span><span class="sd">        torch.Size([1, 256, 64, 64])</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">image_embedding_size</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div><p><br/><br/><hr/><br/></p>
<div class="doc doc-object doc-class">
<h2 class="doc doc-heading" id="ultralytics.models.sam.modules.encoders.MemoryEncoder">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <span class="doc doc-object-name doc-class-name">ultralytics.models.sam.modules.encoders.MemoryEncoder</span>
</h2>
<div class="doc-signature highlight"><pre><span></span><code><span></span><span class="nf">MemoryEncoder</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
</code></pre></div>
<div class="doc doc-contents first">
<p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>
<p>Encode pixel features and masks into a memory representation for efficient image segmentation.</p>
<p>This class processes pixel-level features and masks, fusing them to generate encoded memory representations
suitable for downstream tasks in image segmentation models like SAM (Segment Anything Model).</p>
<p><span class="doc-section-title">Attributes:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.MemoryEncoder.mask_downsampler">mask_downsampler</span></code></td>
<td>
<code><a class="autorefs autorefs-internal" href="../blocks/#ultralytics.models.sam.modules.blocks.MaskDownSampler" title='&lt;code class="doc-symbol doc-symbol-heading doc-symbol-class"&gt;&lt;/code&gt;            &lt;span class="doc doc-object-name doc-class-name"&gt;ultralytics.models.sam.modules.blocks.MaskDownSampler&lt;/span&gt;'>MaskDownSampler</a></code>
</td>
<td>
<div class="doc-md-description">
<p>Module for downsampling input masks.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.MemoryEncoder.pix_feat_proj">pix_feat_proj</span></code></td>
<td>
<code><span title="torch.nn.Conv2d">Conv2d</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Convolutional layer for projecting pixel features.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.MemoryEncoder.fuser">fuser</span></code></td>
<td>
<code><a class="autorefs autorefs-internal" href="../blocks/#ultralytics.models.sam.modules.blocks.Fuser" title='&lt;code class="doc-symbol doc-symbol-heading doc-symbol-class"&gt;&lt;/code&gt;            &lt;span class="doc doc-object-name doc-class-name"&gt;ultralytics.models.sam.modules.blocks.Fuser&lt;/span&gt;'>Fuser</a></code>
</td>
<td>
<div class="doc-md-description">
<p>Module for fusing pixel features and masks.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.MemoryEncoder.position_encoding">position_encoding</span></code></td>
<td>
<code><a class="autorefs autorefs-internal" href="../blocks/#ultralytics.models.sam.modules.blocks.PositionEmbeddingSine" title='&lt;code class="doc-symbol doc-symbol-heading doc-symbol-class"&gt;&lt;/code&gt;            &lt;span class="doc doc-object-name doc-class-name"&gt;ultralytics.models.sam.modules.blocks.PositionEmbeddingSine&lt;/span&gt;'>PositionEmbeddingSine</a></code>
</td>
<td>
<div class="doc-md-description">
<p>Module for adding positional encoding to features.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.MemoryEncoder.out_proj">out_proj</span></code></td>
<td>
<code><span title="torch.nn.Module">Module</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Output projection layer, either nn.Identity or nn.Conv2d.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Methods:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td><code><a class="autorefs autorefs-internal" href="#ultralytics.models.sam.modules.encoders.MemoryEncoder.forward" title='&lt;code class="doc-symbol doc-symbol-heading doc-symbol-method"&gt;&lt;/code&gt;            &lt;span class="doc doc-object-name doc-function-name"&gt;forward&lt;/span&gt; (&lt;code&gt;ultralytics.models.sam.modules.encoders.MemoryEncoder.forward&lt;/code&gt;)'>forward</a></code></td>
<td>
<div class="doc-md-description">
<p>Process input pixel features and masks to generate encoded memory representations.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">encoder</span> <span class="o">=</span> <span class="n">MemoryEncoder</span><span class="p">(</span><span class="n">out_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pix_feat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">encoded_feat</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">pix_feat</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">encoded_feat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">pos</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span></span><span class="go">torch.Size([1, 256, 64, 64]) torch.Size([1, 128, 64, 64])</span>
</code></pre></div>
<p>This encoder processes pixel-level features and masks, fusing them to generate encoded memory representations
suitable for downstream tasks in image segmentation models like SAM (Segment Anything Model).</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>out_dim</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Output dimension of the encoded features.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>in_dim</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Input dimension of the pixel features.</p>
</div>
</td>
<td>
<code>256</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">encoder</span> <span class="o">=</span> <span class="n">MemoryEncoder</span><span class="p">(</span><span class="n">out_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pix_feat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">encoded_feat</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">pix_feat</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">encoded_feat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">pos</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span></span><span class="go">torch.Size([1, 256, 64, 64]) torch.Size([1, 128, 64, 64])</span>
</code></pre></div>
<details class="quote">
<summary>Source code in <code>ultralytics/models/sam/modules/encoders.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span>383</span>
<span>384</span>
<span>385</span>
<span>386</span>
<span>387</span>
<span>388</span>
<span>389</span>
<span>390</span>
<span>391</span>
<span>392</span>
<span>393</span>
<span>394</span>
<span>395</span>
<span>396</span>
<span>397</span>
<span>398</span>
<span>399</span>
<span>400</span>
<span>401</span>
<span>402</span>
<span>403</span>
<span>404</span>
<span>405</span>
<span>406</span>
<span>407</span>
<span>408</span>
<span>409</span>
<span>410</span>
<span>411</span>
<span>412</span>
<span>413</span>
<span>414</span>
<span>415</span></pre></div></td><td class="code"><div><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">out_dim</span><span class="p">,</span>
<span></span>    <span class="n">in_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>  <span class="c1"># in_dim of pix_feats</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""</span>
<span></span><span class="sd">    Initialize the MemoryEncoder for encoding pixel features and masks into memory representations.</span>
<span></span>
<span></span><span class="sd">    This encoder processes pixel-level features and masks, fusing them to generate encoded memory representations</span>
<span></span><span class="sd">    suitable for downstream tasks in image segmentation models like SAM (Segment Anything Model).</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        out_dim (int): Output dimension of the encoded features.</span>
<span></span><span class="sd">        in_dim (int): Input dimension of the pixel features.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; encoder = MemoryEncoder(out_dim=256, in_dim=256)</span>
<span></span><span class="sd">        &gt;&gt;&gt; pix_feat = torch.randn(1, 256, 64, 64)</span>
<span></span><span class="sd">        &gt;&gt;&gt; masks = torch.randn(1, 1, 64, 64)</span>
<span></span><span class="sd">        &gt;&gt;&gt; encoded_feat, pos = encoder(pix_feat, masks)</span>
<span></span><span class="sd">        &gt;&gt;&gt; print(encoded_feat.shape, pos.shape)</span>
<span></span><span class="sd">        torch.Size([1, 256, 64, 64]) torch.Size([1, 128, 64, 64])</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span></span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">mask_downsampler</span> <span class="o">=</span> <span class="n">MaskDownSampler</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span></span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">pix_feat_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">fuser</span> <span class="o">=</span> <span class="n">Fuser</span><span class="p">(</span><span class="n">CXBlock</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">256</span><span class="p">),</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">position_encoding</span> <span class="o">=</span> <span class="n">PositionEmbeddingSine</span><span class="p">(</span><span class="n">num_pos_feats</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
<span></span>    <span class="k">if</span> <span class="n">out_dim</span> <span class="o">!=</span> <span class="n">in_dim</span><span class="p">:</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="ultralytics.models.sam.modules.encoders.MemoryEncoder.forward">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">forward</span>
</h3>
<div class="doc-signature highlight"><pre><span></span><code><span></span><span class="nf">forward</span><span class="p">(</span>
<span></span>    <span class="n">pix_feat</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">masks</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">skip_mask_sigmoid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span>
</code></pre></div>
<div class="doc doc-contents">
<p>Process pixel features and masks to generate encoded memory representations for segmentation.</p>
<details class="quote">
<summary>Source code in <code>ultralytics/models/sam/modules/encoders.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span>417</span>
<span>418</span>
<span>419</span>
<span>420</span>
<span>421</span>
<span>422</span>
<span>423</span>
<span>424</span>
<span>425</span>
<span>426</span>
<span>427</span>
<span>428</span>
<span>429</span>
<span>430</span>
<span>431</span>
<span>432</span>
<span>433</span>
<span>434</span>
<span>435</span>
<span>436</span>
<span>437</span>
<span>438</span></pre></div></td><td class="code"><div><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">pix_feat</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span></span>    <span class="n">skip_mask_sigmoid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span></span><span class="w">    </span><span class="sd">"""Process pixel features and masks to generate encoded memory representations for segmentation."""</span>
<span></span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">skip_mask_sigmoid</span><span class="p">:</span>
<span></span>        <span class="n">masks</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span>
<span></span>    <span class="n">masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_downsampler</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span>
<span></span>
<span></span>    <span class="c1"># Fuse pix_feats and downsampled masks, in case the visual features are on CPU, cast them to CUDA</span>
<span></span>    <span class="n">pix_feat</span> <span class="o">=</span> <span class="n">pix_feat</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">masks</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pix_feat_proj</span><span class="p">(</span><span class="n">pix_feat</span><span class="p">)</span>
<span></span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">masks</span>
<span></span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fuser</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span></span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span></span>
<span></span>    <span class="n">pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_encoding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="p">{</span><span class="s2">"vision_features"</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s2">"vision_pos_enc"</span><span class="p">:</span> <span class="p">[</span><span class="n">pos</span><span class="p">]}</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div><p><br/><br/><hr/><br/></p>
<div class="doc doc-object doc-class">
<h2 class="doc doc-heading" id="ultralytics.models.sam.modules.encoders.ImageEncoder">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <span class="doc doc-object-name doc-class-name">ultralytics.models.sam.modules.encoders.ImageEncoder</span>
</h2>
<div class="doc-signature highlight"><pre><span></span><code><span></span><span class="nf">ImageEncoder</span><span class="p">(</span><span class="n">trunk</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">neck</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">scalp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<div class="doc doc-contents first">
<p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>
<p>Encode images using a trunk-neck architecture, producing multiscale features and positional encodings.</p>
<p>This class combines a trunk network for feature extraction with a neck network for feature refinement
and positional encoding generation. It can optionally discard the lowest resolution features.</p>
<p><span class="doc-section-title">Attributes:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.ImageEncoder.trunk">trunk</span></code></td>
<td>
<code><span title="torch.nn.Module">Module</span></code>
</td>
<td>
<div class="doc-md-description">
<p>The trunk network for initial feature extraction.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.ImageEncoder.neck">neck</span></code></td>
<td>
<code><span title="torch.nn.Module">Module</span></code>
</td>
<td>
<div class="doc-md-description">
<p>The neck network for feature refinement and positional encoding generation.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.ImageEncoder.scalp">scalp</span></code></td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of lowest resolution feature levels to discard.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Methods:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td><code><a class="autorefs autorefs-internal" href="#ultralytics.models.sam.modules.encoders.ImageEncoder.forward" title='&lt;code class="doc-symbol doc-symbol-heading doc-symbol-method"&gt;&lt;/code&gt;            &lt;span class="doc doc-object-name doc-function-name"&gt;forward&lt;/span&gt; (&lt;code&gt;ultralytics.models.sam.modules.encoders.ImageEncoder.forward&lt;/code&gt;)'>forward</a></code></td>
<td>
<div class="doc-md-description">
<p>Process the input image through the trunk and neck networks.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trunk</span> <span class="o">=</span> <span class="n">SomeTrunkNetwork</span><span class="p">()</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">neck</span> <span class="o">=</span> <span class="n">SomeNeckNetwork</span><span class="p">()</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">encoder</span> <span class="o">=</span> <span class="n">ImageEncoder</span><span class="p">(</span><span class="n">trunk</span><span class="p">,</span> <span class="n">neck</span><span class="p">,</span> <span class="n">scalp</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span></span><span class="go">dict_keys(['vision_features', 'vision_pos_enc', 'backbone_fpn'])</span>
</code></pre></div>
<p>This encoder combines a trunk network for feature extraction with a neck network for feature refinement
and positional encoding generation. It can optionally discard the lowest resolution features.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>trunk</code>
</td>
<td>
<code><span title="torch.nn.Module">Module</span></code>
</td>
<td>
<div class="doc-md-description">
<p>The trunk network for initial feature extraction.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>neck</code>
</td>
<td>
<code><span title="torch.nn.Module">Module</span></code>
</td>
<td>
<div class="doc-md-description">
<p>The neck network for feature refinement and positional encoding generation.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>scalp</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of lowest resolution feature levels to discard.</p>
</div>
</td>
<td>
<code>0</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trunk</span> <span class="o">=</span> <span class="n">SomeTrunkNetwork</span><span class="p">()</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">neck</span> <span class="o">=</span> <span class="n">SomeNeckNetwork</span><span class="p">()</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">encoder</span> <span class="o">=</span> <span class="n">ImageEncoder</span><span class="p">(</span><span class="n">trunk</span><span class="p">,</span> <span class="n">neck</span><span class="p">,</span> <span class="n">scalp</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span></span><span class="go">dict_keys(['vision_features', 'vision_pos_enc', 'backbone_fpn'])</span>
</code></pre></div>
<details class="quote">
<summary>Source code in <code>ultralytics/models/sam/modules/encoders.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span>466</span>
<span>467</span>
<span>468</span>
<span>469</span>
<span>470</span>
<span>471</span>
<span>472</span>
<span>473</span>
<span>474</span>
<span>475</span>
<span>476</span>
<span>477</span>
<span>478</span>
<span>479</span>
<span>480</span>
<span>481</span>
<span>482</span>
<span>483</span>
<span>484</span>
<span>485</span>
<span>486</span>
<span>487</span>
<span>488</span>
<span>489</span>
<span>490</span>
<span>491</span>
<span>492</span>
<span>493</span>
<span>494</span>
<span>495</span>
<span>496</span>
<span>497</span>
<span>498</span></pre></div></td><td class="code"><div><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">trunk</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
<span></span>    <span class="n">neck</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
<span></span>    <span class="n">scalp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""</span>
<span></span><span class="sd">    Initialize the ImageEncoder with trunk and neck networks for feature extraction and refinement.</span>
<span></span>
<span></span><span class="sd">    This encoder combines a trunk network for feature extraction with a neck network for feature refinement</span>
<span></span><span class="sd">    and positional encoding generation. It can optionally discard the lowest resolution features.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        trunk (nn.Module): The trunk network for initial feature extraction.</span>
<span></span><span class="sd">        neck (nn.Module): The neck network for feature refinement and positional encoding generation.</span>
<span></span><span class="sd">        scalp (int): Number of lowest resolution feature levels to discard.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; trunk = SomeTrunkNetwork()</span>
<span></span><span class="sd">        &gt;&gt;&gt; neck = SomeNeckNetwork()</span>
<span></span><span class="sd">        &gt;&gt;&gt; encoder = ImageEncoder(trunk, neck, scalp=1)</span>
<span></span><span class="sd">        &gt;&gt;&gt; image = torch.randn(1, 3, 224, 224)</span>
<span></span><span class="sd">        &gt;&gt;&gt; output = encoder(image)</span>
<span></span><span class="sd">        &gt;&gt;&gt; print(output.keys())</span>
<span></span><span class="sd">        dict_keys(['vision_features', 'vision_pos_enc', 'backbone_fpn'])</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">trunk</span> <span class="o">=</span> <span class="n">trunk</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">neck</span> <span class="o">=</span> <span class="n">neck</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">scalp</span> <span class="o">=</span> <span class="n">scalp</span>
<span></span>    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">trunk</span><span class="o">.</span><span class="n">channel_list</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">neck</span><span class="o">.</span><span class="n">backbone_channel_list</span><span class="p">,</span> <span class="p">(</span>
<span></span>        <span class="sa">f</span><span class="s2">"Channel dims of trunk </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk</span><span class="o">.</span><span class="n">channel_list</span><span class="si">}</span><span class="s2"> and neck </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">neck</span><span class="o">.</span><span class="n">backbone_channel_list</span><span class="si">}</span><span class="s2"> do not match."</span>
<span></span>    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="ultralytics.models.sam.modules.encoders.ImageEncoder.forward">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">forward</span>
</h3>
<div class="doc-signature highlight"><pre><span></span><code><span></span><span class="nf">forward</span><span class="p">(</span><span class="n">sample</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span>
</code></pre></div>
<div class="doc doc-contents">
<p>Encode input through trunk and neck networks, returning multiscale features and positional encodings.</p>
<details class="quote">
<summary>Source code in <code>ultralytics/models/sam/modules/encoders.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span>500</span>
<span>501</span>
<span>502</span>
<span>503</span>
<span>504</span>
<span>505</span>
<span>506</span>
<span>507</span>
<span>508</span>
<span>509</span>
<span>510</span>
<span>511</span>
<span>512</span></pre></div></td><td class="code"><div><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""Encode input through trunk and neck networks, returning multiscale features and positional encodings."""</span>
<span></span>    <span class="n">features</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neck</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trunk</span><span class="p">(</span><span class="n">sample</span><span class="p">))</span>
<span></span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scalp</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span></span>        <span class="c1"># Discard the lowest resolution features</span>
<span></span>        <span class="n">features</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">scalp</span><span class="p">],</span> <span class="n">pos</span><span class="p">[:</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">scalp</span><span class="p">]</span>
<span></span>
<span></span>    <span class="n">src</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span></span>    <span class="k">return</span> <span class="p">{</span>
<span></span>        <span class="s2">"vision_features"</span><span class="p">:</span> <span class="n">src</span><span class="p">,</span>
<span></span>        <span class="s2">"vision_pos_enc"</span><span class="p">:</span> <span class="n">pos</span><span class="p">,</span>
<span></span>        <span class="s2">"backbone_fpn"</span><span class="p">:</span> <span class="n">features</span><span class="p">,</span>
<span></span>    <span class="p">}</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div><p><br/><br/><hr/><br/></p>
<div class="doc doc-object doc-class">
<h2 class="doc doc-heading" id="ultralytics.models.sam.modules.encoders.FpnNeck">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <span class="doc doc-object-name doc-class-name">ultralytics.models.sam.modules.encoders.FpnNeck</span>
</h2>
<div class="doc-signature highlight"><pre><span></span><code><span></span><span class="nf">FpnNeck</span><span class="p">(</span>
<span></span>    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">backbone_channel_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
<span></span>    <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span></span>    <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span></span>    <span class="n">padding</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span></span>    <span class="n">fpn_interp_model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"bilinear"</span><span class="p">,</span>
<span></span>    <span class="n">fuse_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"sum"</span><span class="p">,</span>
<span></span>    <span class="n">fpn_top_down_levels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div>
<div class="doc doc-contents first">
<p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>
<p>A Feature Pyramid Network (FPN) neck variant for multiscale feature fusion in object detection models.</p>
<p>This FPN variant removes the output convolution and uses bicubic interpolation for feature resizing,
similar to ViT positional embedding interpolation.</p>
<p><span class="doc-section-title">Attributes:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.FpnNeck.position_encoding">position_encoding</span></code></td>
<td>
<code><a class="autorefs autorefs-internal" href="../blocks/#ultralytics.models.sam.modules.blocks.PositionEmbeddingSine" title='&lt;code class="doc-symbol doc-symbol-heading doc-symbol-class"&gt;&lt;/code&gt;            &lt;span class="doc doc-object-name doc-class-name"&gt;ultralytics.models.sam.modules.blocks.PositionEmbeddingSine&lt;/span&gt;'>PositionEmbeddingSine</a></code>
</td>
<td>
<div class="doc-md-description">
<p>Sinusoidal positional encoding module.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.FpnNeck.convs">convs</span></code></td>
<td>
<code><span title="torch.nn.ModuleList">ModuleList</span></code>
</td>
<td>
<div class="doc-md-description">
<p>List of convolutional layers for each backbone level.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.FpnNeck.backbone_channel_list">backbone_channel_list</span></code></td>
<td>
<code><span title="typing.List">List</span>[<span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>List of channel dimensions from the backbone.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.FpnNeck.fpn_interp_model">fpn_interp_model</span></code></td>
<td>
<code><span title="str">str</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Interpolation mode for FPN feature resizing.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.FpnNeck.fuse_type">fuse_type</span></code></td>
<td>
<code><span title="str">str</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Type of feature fusion, either 'sum' or 'avg'.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.FpnNeck.fpn_top_down_levels">fpn_top_down_levels</span></code></td>
<td>
<code><span title="typing.List">List</span>[<span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Levels to have top-down features in outputs.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Methods:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td><code><a class="autorefs autorefs-internal" href="#ultralytics.models.sam.modules.encoders.FpnNeck.forward" title='&lt;code class="doc-symbol doc-symbol-heading doc-symbol-method"&gt;&lt;/code&gt;            &lt;span class="doc doc-object-name doc-function-name"&gt;forward&lt;/span&gt; (&lt;code&gt;ultralytics.models.sam.modules.encoders.FpnNeck.forward&lt;/code&gt;)'>forward</a></code></td>
<td>
<div class="doc-md-description">
<p>Perform forward pass through the FPN neck.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">backbone_channels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fpn_neck</span> <span class="o">=</span> <span class="n">FpnNeck</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">backbone_channels</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">backbone_channels</span><span class="p">]</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span><span class="p">,</span> <span class="n">positions</span> <span class="o">=</span> <span class="n">fpn_neck</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">positions</span><span class="p">))</span>
<span></span><span class="go">4 4</span>
</code></pre></div>
<p>This FPN variant removes the output convolution and uses bicubic interpolation for feature resizing,
similar to ViT positional embedding interpolation.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>d_model</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dimension of the model.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>backbone_channel_list</code>
</td>
<td>
<code><span title="typing.List">List</span>[<span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>List of channel dimensions from the backbone.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>kernel_size</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Kernel size for the convolutional layers.</p>
</div>
</td>
<td>
<code>1</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>stride</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Stride for the convolutional layers.</p>
</div>
</td>
<td>
<code>1</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>padding</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Padding for the convolutional layers.</p>
</div>
</td>
<td>
<code>0</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>fpn_interp_model</code>
</td>
<td>
<code><span title="str">str</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Interpolation mode for FPN feature resizing.</p>
</div>
</td>
<td>
<code>'bilinear'</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>fuse_type</code>
</td>
<td>
<code><span title="str">str</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Type of feature fusion, either 'sum' or 'avg'.</p>
</div>
</td>
<td>
<code>'sum'</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>fpn_top_down_levels</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<span title="int">int</span>]]</code>
</td>
<td>
<div class="doc-md-description">
<p>Levels to have top-down features in outputs.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">backbone_channels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fpn_neck</span> <span class="o">=</span> <span class="n">FpnNeck</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">backbone_channels</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">fpn_neck</span><span class="p">)</span>
</code></pre></div>
<details class="quote">
<summary>Source code in <code>ultralytics/models/sam/modules/encoders.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span>542</span>
<span>543</span>
<span>544</span>
<span>545</span>
<span>546</span>
<span>547</span>
<span>548</span>
<span>549</span>
<span>550</span>
<span>551</span>
<span>552</span>
<span>553</span>
<span>554</span>
<span>555</span>
<span>556</span>
<span>557</span>
<span>558</span>
<span>559</span>
<span>560</span>
<span>561</span>
<span>562</span>
<span>563</span>
<span>564</span>
<span>565</span>
<span>566</span>
<span>567</span>
<span>568</span>
<span>569</span>
<span>570</span>
<span>571</span>
<span>572</span>
<span>573</span>
<span>574</span>
<span>575</span>
<span>576</span>
<span>577</span>
<span>578</span>
<span>579</span>
<span>580</span>
<span>581</span>
<span>582</span>
<span>583</span>
<span>584</span>
<span>585</span>
<span>586</span>
<span>587</span>
<span>588</span>
<span>589</span>
<span>590</span>
<span>591</span>
<span>592</span>
<span>593</span>
<span>594</span>
<span>595</span>
<span>596</span>
<span>597</span>
<span>598</span>
<span>599</span>
<span>600</span>
<span>601</span>
<span>602</span>
<span>603</span></pre></div></td><td class="code"><div><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span></span>    <span class="n">backbone_channel_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
<span></span>    <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span></span>    <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span></span>    <span class="n">padding</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span></span>    <span class="n">fpn_interp_model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"bilinear"</span><span class="p">,</span>
<span></span>    <span class="n">fuse_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"sum"</span><span class="p">,</span>
<span></span>    <span class="n">fpn_top_down_levels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""</span>
<span></span><span class="sd">    Initialize a modified Feature Pyramid Network (FPN) neck.</span>
<span></span>
<span></span><span class="sd">    This FPN variant removes the output convolution and uses bicubic interpolation for feature resizing,</span>
<span></span><span class="sd">    similar to ViT positional embedding interpolation.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        d_model (int): Dimension of the model.</span>
<span></span><span class="sd">        backbone_channel_list (List[int]): List of channel dimensions from the backbone.</span>
<span></span><span class="sd">        kernel_size (int): Kernel size for the convolutional layers.</span>
<span></span><span class="sd">        stride (int): Stride for the convolutional layers.</span>
<span></span><span class="sd">        padding (int): Padding for the convolutional layers.</span>
<span></span><span class="sd">        fpn_interp_model (str): Interpolation mode for FPN feature resizing.</span>
<span></span><span class="sd">        fuse_type (str): Type of feature fusion, either 'sum' or 'avg'.</span>
<span></span><span class="sd">        fpn_top_down_levels (Optional[List[int]]): Levels to have top-down features in outputs.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; backbone_channels = [64, 128, 256, 512]</span>
<span></span><span class="sd">        &gt;&gt;&gt; fpn_neck = FpnNeck(256, backbone_channels)</span>
<span></span><span class="sd">        &gt;&gt;&gt; print(fpn_neck)</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">position_encoding</span> <span class="o">=</span> <span class="n">PositionEmbeddingSine</span><span class="p">(</span><span class="n">num_pos_feats</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">convs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">backbone_channel_list</span> <span class="o">=</span> <span class="n">backbone_channel_list</span>
<span></span>    <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">backbone_channel_list</span><span class="p">:</span>
<span></span>        <span class="n">current</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span></span>        <span class="n">current</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
<span></span>            <span class="s2">"conv"</span><span class="p">,</span>
<span></span>            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
<span></span>                <span class="n">in_channels</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
<span></span>                <span class="n">out_channels</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
<span></span>                <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
<span></span>                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
<span></span>                <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
<span></span>            <span class="p">),</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">fpn_interp_model</span> <span class="o">=</span> <span class="n">fpn_interp_model</span>
<span></span>    <span class="k">assert</span> <span class="n">fuse_type</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">"sum"</span><span class="p">,</span> <span class="s2">"avg"</span><span class="p">}</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">fuse_type</span> <span class="o">=</span> <span class="n">fuse_type</span>
<span></span>
<span></span>    <span class="c1"># Levels to have top-down features in its outputs</span>
<span></span>    <span class="c1"># e.g. if fpn_top_down_levels is [2, 3], then only outputs of level 2 and 3</span>
<span></span>    <span class="c1"># have top-down propagation, while outputs of level 0 and level 1 have only</span>
<span></span>    <span class="c1"># lateral features from the same backbone level</span>
<span></span>    <span class="k">if</span> <span class="n">fpn_top_down_levels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span></span>        <span class="c1"># Default is to have top-down features on all levels</span>
<span></span>        <span class="n">fpn_top_down_levels</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="p">))</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">fpn_top_down_levels</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">fpn_top_down_levels</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="ultralytics.models.sam.modules.encoders.FpnNeck.forward">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">forward</span>
</h3>
<div class="doc-signature highlight"><pre><span></span><code><span></span><span class="nf">forward</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span>
</code></pre></div>
<div class="doc doc-contents">
<p>Perform forward pass through the Feature Pyramid Network (FPN) neck.</p>
<p>This method processes a list of input tensors from the backbone through the FPN, applying lateral connections
and top-down feature fusion. It generates output feature maps and corresponding positional encodings.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>xs</code>
</td>
<td>
<code><span title="typing.List">List</span>[<span title="torch.Tensor">Tensor</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>List of input tensors from the backbone, each with shape (B, C, H, W).</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Name</th> <th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td><code>out</code></td> <td>
<code><span title="typing.List">List</span>[<span title="torch.Tensor">Tensor</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>List of output feature maps after FPN processing, each with shape
(B, d_model, H, W).</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code>pos</code></td> <td>
<code><span title="typing.List">List</span>[<span title="torch.Tensor">Tensor</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>List of positional encodings corresponding to each output feature map.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fpn_neck</span> <span class="o">=</span> <span class="n">FpnNeck</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">backbone_channel_list</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">]]</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span><span class="p">,</span> <span class="n">positions</span> <span class="o">=</span> <span class="n">fpn_neck</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">positions</span><span class="p">))</span>
<span></span><span class="go">4 4</span>
</code></pre></div>
<details class="quote">
<summary>Source code in <code>ultralytics/models/sam/modules/encoders.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span>605</span>
<span>606</span>
<span>607</span>
<span>608</span>
<span>609</span>
<span>610</span>
<span>611</span>
<span>612</span>
<span>613</span>
<span>614</span>
<span>615</span>
<span>616</span>
<span>617</span>
<span>618</span>
<span>619</span>
<span>620</span>
<span>621</span>
<span>622</span>
<span>623</span>
<span>624</span>
<span>625</span>
<span>626</span>
<span>627</span>
<span>628</span>
<span>629</span>
<span>630</span>
<span>631</span>
<span>632</span>
<span>633</span>
<span>634</span>
<span>635</span>
<span>636</span>
<span>637</span>
<span>638</span>
<span>639</span>
<span>640</span>
<span>641</span>
<span>642</span>
<span>643</span>
<span>644</span>
<span>645</span>
<span>646</span>
<span>647</span>
<span>648</span>
<span>649</span>
<span>650</span>
<span>651</span>
<span>652</span>
<span>653</span>
<span>654</span>
<span>655</span></pre></div></td><td class="code"><div><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]):</span>
<span></span><span class="w">    </span><span class="sd">"""</span>
<span></span><span class="sd">    Perform forward pass through the Feature Pyramid Network (FPN) neck.</span>
<span></span>
<span></span><span class="sd">    This method processes a list of input tensors from the backbone through the FPN, applying lateral connections</span>
<span></span><span class="sd">    and top-down feature fusion. It generates output feature maps and corresponding positional encodings.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        xs (List[torch.Tensor]): List of input tensors from the backbone, each with shape (B, C, H, W).</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        out (List[torch.Tensor]): List of output feature maps after FPN processing, each with shape</span>
<span></span><span class="sd">            (B, d_model, H, W).</span>
<span></span><span class="sd">        pos (List[torch.Tensor]): List of positional encodings corresponding to each output feature map.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; fpn_neck = FpnNeck(d_model=256, backbone_channel_list=[64, 128, 256, 512])</span>
<span></span><span class="sd">        &gt;&gt;&gt; inputs = [torch.rand(1, c, 32, 32) for c in [64, 128, 256, 512]]</span>
<span></span><span class="sd">        &gt;&gt;&gt; outputs, positions = fpn_neck(inputs)</span>
<span></span><span class="sd">        &gt;&gt;&gt; print(len(outputs), len(positions))</span>
<span></span><span class="sd">        4 4</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">out</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="p">)</span>
<span></span>    <span class="n">pos</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="p">)</span>
<span></span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="p">)</span>
<span></span>    <span class="c1"># FPN forward pass</span>
<span></span>    <span class="c1"># see https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/fpn.py</span>
<span></span>    <span class="n">prev_features</span> <span class="o">=</span> <span class="kc">None</span>
<span></span>    <span class="c1"># Forward in top-down order (from low to high resolution)</span>
<span></span>    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
<span></span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
<span></span>        <span class="n">x</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span></span>        <span class="n">lateral_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="p">[</span><span class="n">n</span> <span class="o">-</span> <span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
<span></span>        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fpn_top_down_levels</span> <span class="ow">and</span> <span class="n">prev_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">top_down_features</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
<span></span>                <span class="n">prev_features</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
<span></span>                <span class="n">scale_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
<span></span>                <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fpn_interp_model</span><span class="p">,</span>
<span></span>                <span class="n">align_corners</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fpn_interp_model</span> <span class="o">==</span> <span class="s2">"nearest"</span> <span class="k">else</span> <span class="kc">False</span><span class="p">),</span>
<span></span>                <span class="n">antialias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span></span>            <span class="p">)</span>
<span></span>            <span class="n">prev_features</span> <span class="o">=</span> <span class="n">lateral_features</span> <span class="o">+</span> <span class="n">top_down_features</span>
<span></span>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fuse_type</span> <span class="o">==</span> <span class="s2">"avg"</span><span class="p">:</span>
<span></span>                <span class="n">prev_features</span> <span class="o">/=</span> <span class="mi">2</span>
<span></span>        <span class="k">else</span><span class="p">:</span>
<span></span>            <span class="n">prev_features</span> <span class="o">=</span> <span class="n">lateral_features</span>
<span></span>        <span class="n">x_out</span> <span class="o">=</span> <span class="n">prev_features</span>
<span></span>        <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_out</span>
<span></span>        <span class="n">pos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_encoding</span><span class="p">(</span><span class="n">x_out</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x_out</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">pos</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div><p><br/><br/><hr/><br/></p>
<div class="doc doc-object doc-class">
<h2 class="doc doc-heading" id="ultralytics.models.sam.modules.encoders.Hiera">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <span class="doc doc-object-name doc-class-name">ultralytics.models.sam.modules.encoders.Hiera</span>
</h2>
<div class="doc-signature highlight"><pre><span></span><code><span></span><span class="nf">Hiera</span><span class="p">(</span>
<span></span>    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">96</span><span class="p">,</span>
<span></span>    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span></span>    <span class="n">drop_path_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
<span></span>    <span class="n">q_pool</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
<span></span>    <span class="n">q_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
<span></span>    <span class="n">stages</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
<span></span>    <span class="n">dim_mul</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>
<span></span>    <span class="n">head_mul</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>
<span></span>    <span class="n">window_pos_embed_bkg_spatial_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">),</span>
<span></span>    <span class="n">window_spec</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span>
<span></span>    <span class="n">global_att_blocks</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
<span></span>    <span class="n">return_interm_layers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span></span><span class="p">)</span>
</code></pre></div>
<div class="doc doc-contents first">
<p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>
<p>Hierarchical vision transformer for efficient multiscale feature extraction in image processing tasks.</p>
<p>This class implements a Hiera model, which is a hierarchical vision transformer architecture designed for
efficient multiscale feature extraction. It uses a series of transformer blocks organized into stages,
with optional pooling and global attention mechanisms.</p>
<p><span class="doc-section-title">Attributes:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.Hiera.window_spec">window_spec</span></code></td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="int">int</span>, ...]</code>
</td>
<td>
<div class="doc-md-description">
<p>Window sizes for each stage.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.Hiera.q_stride">q_stride</span></code></td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="int">int</span>, <span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Downsampling stride between stages.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.Hiera.stage_ends">stage_ends</span></code></td>
<td>
<code><span title="typing.List">List</span>[<span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Indices of the last block in each stage.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.Hiera.q_pool_blocks">q_pool_blocks</span></code></td>
<td>
<code><span title="typing.List">List</span>[<span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Indices of blocks where pooling is applied.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.Hiera.return_interm_layers">return_interm_layers</span></code></td>
<td>
<code><span title="bool">bool</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Whether to return intermediate layer outputs.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.Hiera.patch_embed">patch_embed</span></code></td>
<td>
<code><a class="autorefs autorefs-internal" href="../blocks/#ultralytics.models.sam.modules.blocks.PatchEmbed" title='&lt;code class="doc-symbol doc-symbol-heading doc-symbol-class"&gt;&lt;/code&gt;            &lt;span class="doc doc-object-name doc-class-name"&gt;ultralytics.models.sam.modules.blocks.PatchEmbed&lt;/span&gt;'>PatchEmbed</a></code>
</td>
<td>
<div class="doc-md-description">
<p>Module for patch embedding.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.Hiera.global_att_blocks">global_att_blocks</span></code></td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="int">int</span>, ...]</code>
</td>
<td>
<div class="doc-md-description">
<p>Indices of blocks with global attention.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.Hiera.window_pos_embed_bkg_spatial_size">window_pos_embed_bkg_spatial_size</span></code></td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="int">int</span>, <span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Spatial size for window positional embedding background.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.Hiera.pos_embed">pos_embed</span></code></td>
<td>
<code><span title="torch.nn.Parameter">Parameter</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Positional embedding for the background.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.Hiera.pos_embed_window">pos_embed_window</span></code></td>
<td>
<code><span title="torch.nn.Parameter">Parameter</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Positional embedding for the window.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.Hiera.blocks">blocks</span></code></td>
<td>
<code><span title="torch.nn.ModuleList">ModuleList</span></code>
</td>
<td>
<div class="doc-md-description">
<p>List of MultiScaleBlock modules.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.Hiera.channel_list">channel_list</span></code></td>
<td>
<code><span title="typing.List">List</span>[<span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>List of output channel dimensions for each stage.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Methods:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td><code><span title="ultralytics.models.sam.modules.encoders.Hiera._get_pos_embed">_get_pos_embed</span></code></td>
<td>
<div class="doc-md-description">
<p>Generate positional embeddings by interpolating and combining window and background embeddings.</p>
</div>
</td>
</tr>
<tr class="doc-section-item">
<td><code><a class="autorefs autorefs-internal" href="#ultralytics.models.sam.modules.encoders.Hiera.forward" title='&lt;code class="doc-symbol doc-symbol-heading doc-symbol-method"&gt;&lt;/code&gt;            &lt;span class="doc doc-object-name doc-function-name"&gt;forward&lt;/span&gt; (&lt;code&gt;ultralytics.models.sam.modules.encoders.Hiera.forward&lt;/code&gt;)'>forward</a></code></td>
<td>
<div class="doc-md-description">
<p>Perform the forward pass through the Hiera model.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Hiera</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">96</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stages</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">output_features</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">output_features</span><span class="p">:</span>
<span></span><span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">feat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>
<p>Hiera is a hierarchical vision transformer architecture designed for efficient multiscale feature extraction
in image processing tasks. It uses a series of transformer blocks organized into stages, with optional
pooling and global attention mechanisms.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>embed_dim</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Initial embedding dimension for the model.</p>
</div>
</td>
<td>
<code>96</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>num_heads</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Initial number of attention heads.</p>
</div>
</td>
<td>
<code>1</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>drop_path_rate</code>
</td>
<td>
<code><span title="float">float</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Stochastic depth rate.</p>
</div>
</td>
<td>
<code>0.0</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>q_pool</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of query pooling stages.</p>
</div>
</td>
<td>
<code>3</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>q_stride</code>
</td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="int">int</span>, <span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Downsampling stride between stages.</p>
</div>
</td>
<td>
<code>(2, 2)</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>stages</code>
</td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="int">int</span>, ...]</code>
</td>
<td>
<div class="doc-md-description">
<p>Number of blocks per stage.</p>
</div>
</td>
<td>
<code>(2, 3, 16, 3)</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>dim_mul</code>
</td>
<td>
<code><span title="float">float</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dimension multiplier factor at stage transitions.</p>
</div>
</td>
<td>
<code>2.0</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>head_mul</code>
</td>
<td>
<code><span title="float">float</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Head multiplier factor at stage transitions.</p>
</div>
</td>
<td>
<code>2.0</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>window_pos_embed_bkg_spatial_size</code>
</td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="int">int</span>, <span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Spatial size for window positional embedding background.</p>
</div>
</td>
<td>
<code>(14, 14)</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>window_spec</code>
</td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="int">int</span>, ...]</code>
</td>
<td>
<div class="doc-md-description">
<p>Window sizes for each stage when not using global attention.</p>
</div>
</td>
<td>
<code>(8, 4, 14, 7)</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>global_att_blocks</code>
</td>
<td>
<code><span title="typing.Tuple">Tuple</span>[<span title="int">int</span>, ...]</code>
</td>
<td>
<div class="doc-md-description">
<p>Indices of blocks that use global attention.</p>
</div>
</td>
<td>
<code>(12, 16, 20)</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>return_interm_layers</code>
</td>
<td>
<code><span title="bool">bool</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Whether to return intermediate layer outputs.</p>
</div>
</td>
<td>
<code>True</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Hiera</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">96</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stages</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">output_features</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">output_features</span><span class="p">:</span>
<span></span><span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">feat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>
<details class="quote">
<summary>Source code in <code>ultralytics/models/sam/modules/encoders.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span>692</span>
<span>693</span>
<span>694</span>
<span>695</span>
<span>696</span>
<span>697</span>
<span>698</span>
<span>699</span>
<span>700</span>
<span>701</span>
<span>702</span>
<span>703</span>
<span>704</span>
<span>705</span>
<span>706</span>
<span>707</span>
<span>708</span>
<span>709</span>
<span>710</span>
<span>711</span>
<span>712</span>
<span>713</span>
<span>714</span>
<span>715</span>
<span>716</span>
<span>717</span>
<span>718</span>
<span>719</span>
<span>720</span>
<span>721</span>
<span>722</span>
<span>723</span>
<span>724</span>
<span>725</span>
<span>726</span>
<span>727</span>
<span>728</span>
<span>729</span>
<span>730</span>
<span>731</span>
<span>732</span>
<span>733</span>
<span>734</span>
<span>735</span>
<span>736</span>
<span>737</span>
<span>738</span>
<span>739</span>
<span>740</span>
<span>741</span>
<span>742</span>
<span>743</span>
<span>744</span>
<span>745</span>
<span>746</span>
<span>747</span>
<span>748</span>
<span>749</span>
<span>750</span>
<span>751</span>
<span>752</span>
<span>753</span>
<span>754</span>
<span>755</span>
<span>756</span>
<span>757</span>
<span>758</span>
<span>759</span>
<span>760</span>
<span>761</span>
<span>762</span>
<span>763</span>
<span>764</span>
<span>765</span>
<span>766</span>
<span>767</span>
<span>768</span>
<span>769</span>
<span>770</span>
<span>771</span>
<span>772</span>
<span>773</span>
<span>774</span>
<span>775</span>
<span>776</span>
<span>777</span>
<span>778</span>
<span>779</span>
<span>780</span>
<span>781</span>
<span>782</span>
<span>783</span>
<span>784</span>
<span>785</span>
<span>786</span>
<span>787</span>
<span>788</span>
<span>789</span>
<span>790</span>
<span>791</span>
<span>792</span>
<span>793</span>
<span>794</span>
<span>795</span>
<span>796</span>
<span>797</span>
<span>798</span>
<span>799</span>
<span>800</span>
<span>801</span>
<span>802</span>
<span>803</span>
<span>804</span>
<span>805</span>
<span>806</span>
<span>807</span></pre></div></td><td class="code"><div><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<span></span>    <span class="bp">self</span><span class="p">,</span>
<span></span>    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">96</span><span class="p">,</span>  <span class="c1"># initial embed dim</span>
<span></span>    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># initial number of heads</span>
<span></span>    <span class="n">drop_path_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>  <span class="c1"># stochastic depth</span>
<span></span>    <span class="n">q_pool</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>  <span class="c1"># number of q_pool stages</span>
<span></span>    <span class="n">q_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>  <span class="c1"># downsample stride bet. stages</span>
<span></span>    <span class="n">stages</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>  <span class="c1"># blocks per stage</span>
<span></span>    <span class="n">dim_mul</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>  <span class="c1"># dim_mul factor at stage shift</span>
<span></span>    <span class="n">head_mul</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>  <span class="c1"># head_mul factor at stage shift</span>
<span></span>    <span class="n">window_pos_embed_bkg_spatial_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">),</span>
<span></span>    <span class="c1"># window size per stage, when not using global att.</span>
<span></span>    <span class="n">window_spec</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
<span></span>        <span class="mi">8</span><span class="p">,</span>
<span></span>        <span class="mi">4</span><span class="p">,</span>
<span></span>        <span class="mi">14</span><span class="p">,</span>
<span></span>        <span class="mi">7</span><span class="p">,</span>
<span></span>    <span class="p">),</span>
<span></span>    <span class="c1"># global attn in these blocks</span>
<span></span>    <span class="n">global_att_blocks</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
<span></span>        <span class="mi">12</span><span class="p">,</span>
<span></span>        <span class="mi">16</span><span class="p">,</span>
<span></span>        <span class="mi">20</span><span class="p">,</span>
<span></span>    <span class="p">),</span>
<span></span>    <span class="n">return_interm_layers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># return feats from every stage</span>
<span></span><span class="p">):</span>
<span></span><span class="w">    </span><span class="sd">"""</span>
<span></span><span class="sd">    Initialize a Hiera model, a hierarchical vision transformer for efficient multiscale feature extraction.</span>
<span></span>
<span></span><span class="sd">    Hiera is a hierarchical vision transformer architecture designed for efficient multiscale feature extraction</span>
<span></span><span class="sd">    in image processing tasks. It uses a series of transformer blocks organized into stages, with optional</span>
<span></span><span class="sd">    pooling and global attention mechanisms.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        embed_dim (int): Initial embedding dimension for the model.</span>
<span></span><span class="sd">        num_heads (int): Initial number of attention heads.</span>
<span></span><span class="sd">        drop_path_rate (float): Stochastic depth rate.</span>
<span></span><span class="sd">        q_pool (int): Number of query pooling stages.</span>
<span></span><span class="sd">        q_stride (Tuple[int, int]): Downsampling stride between stages.</span>
<span></span><span class="sd">        stages (Tuple[int, ...]): Number of blocks per stage.</span>
<span></span><span class="sd">        dim_mul (float): Dimension multiplier factor at stage transitions.</span>
<span></span><span class="sd">        head_mul (float): Head multiplier factor at stage transitions.</span>
<span></span><span class="sd">        window_pos_embed_bkg_spatial_size (Tuple[int, int]): Spatial size for window positional embedding background.</span>
<span></span><span class="sd">        window_spec (Tuple[int, ...]): Window sizes for each stage when not using global attention.</span>
<span></span><span class="sd">        global_att_blocks (Tuple[int, ...]): Indices of blocks that use global attention.</span>
<span></span><span class="sd">        return_interm_layers (bool): Whether to return intermediate layer outputs.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; model = Hiera(embed_dim=96, num_heads=1, stages=(2, 3, 16, 3))</span>
<span></span><span class="sd">        &gt;&gt;&gt; input_tensor = torch.randn(1, 3, 224, 224)</span>
<span></span><span class="sd">        &gt;&gt;&gt; output_features = model(input_tensor)</span>
<span></span><span class="sd">        &gt;&gt;&gt; for feat in output_features:</span>
<span></span><span class="sd">        ...     print(feat.shape)</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span></span>
<span></span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">stages</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">window_spec</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">window_spec</span> <span class="o">=</span> <span class="n">window_spec</span>
<span></span>
<span></span>    <span class="n">depth</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">stages</span><span class="p">)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">q_stride</span> <span class="o">=</span> <span class="n">q_stride</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">stage_ends</span> <span class="o">=</span> <span class="p">[</span><span class="nb">sum</span><span class="p">(</span><span class="n">stages</span><span class="p">[:</span><span class="n">i</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">stages</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
<span></span>    <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">q_pool</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_ends</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">q_pool_blocks</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_ends</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]][:</span><span class="n">q_pool</span><span class="p">]</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">return_interm_layers</span> <span class="o">=</span> <span class="n">return_interm_layers</span>
<span></span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span> <span class="o">=</span> <span class="n">PatchEmbed</span><span class="p">(</span>
<span></span>        <span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
<span></span>        <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span>
<span></span>        <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
<span></span>        <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
<span></span>    <span class="p">)</span>
<span></span>    <span class="c1"># Which blocks have global attention?</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">global_att_blocks</span> <span class="o">=</span> <span class="n">global_att_blocks</span>
<span></span>
<span></span>    <span class="c1"># Windowed positional embedding (https://arxiv.org/abs/2311.05613)</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">window_pos_embed_bkg_spatial_size</span> <span class="o">=</span> <span class="n">window_pos_embed_bkg_spatial_size</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">window_pos_embed_bkg_spatial_size</span><span class="p">))</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed_window</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span></span>
<span></span>    <span class="n">dpr</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="p">,</span> <span class="n">depth</span><span class="p">)]</span>  <span class="c1"># stochastic depth decay rule</span>
<span></span>
<span></span>    <span class="n">cur_stage</span> <span class="o">=</span> <span class="mi">1</span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
<span></span>
<span></span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">):</span>
<span></span>        <span class="n">dim_out</span> <span class="o">=</span> <span class="n">embed_dim</span>
<span></span>        <span class="c1"># Lags by a block, so first block of next stage uses an initial window size</span>
<span></span>        <span class="c1"># of previous stage and final window size of current stage</span>
<span></span>        <span class="n">window_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_spec</span><span class="p">[</span><span class="n">cur_stage</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
<span></span>
<span></span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_att_blocks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span></span>            <span class="n">window_size</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_att_blocks</span> <span class="k">else</span> <span class="n">window_size</span>
<span></span>
<span></span>        <span class="k">if</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_ends</span><span class="p">:</span>
<span></span>            <span class="n">dim_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">*</span> <span class="n">dim_mul</span><span class="p">)</span>
<span></span>            <span class="n">num_heads</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_mul</span><span class="p">)</span>
<span></span>            <span class="n">cur_stage</span> <span class="o">+=</span> <span class="mi">1</span>
<span></span>
<span></span>        <span class="n">block</span> <span class="o">=</span> <span class="n">MultiScaleBlock</span><span class="p">(</span>
<span></span>            <span class="n">dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
<span></span>            <span class="n">dim_out</span><span class="o">=</span><span class="n">dim_out</span><span class="p">,</span>
<span></span>            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
<span></span>            <span class="n">drop_path</span><span class="o">=</span><span class="n">dpr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
<span></span>            <span class="n">q_stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_stride</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_pool_blocks</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
<span></span>            <span class="n">window_size</span><span class="o">=</span><span class="n">window_size</span><span class="p">,</span>
<span></span>        <span class="p">)</span>
<span></span>
<span></span>        <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">dim_out</span>
<span></span>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
<span></span>
<span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">channel_list</span> <span class="o">=</span> <span class="p">(</span>
<span></span>        <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">dim_out</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_ends</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span></span>        <span class="k">if</span> <span class="n">return_interm_layers</span>
<span></span>        <span class="k">else</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">dim_out</span><span class="p">]</span>
<span></span>    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="ultralytics.models.sam.modules.encoders.Hiera.forward">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">forward</span>
</h3>
<div class="doc-signature highlight"><pre><span></span><code><span></span><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
</code></pre></div>
<div class="doc doc-contents">
<p>Perform forward pass through Hiera model, extracting multiscale features from input images.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>x</code>
</td>
<td>
<code><span title="torch.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Input tensor with shape (B, C, H, W) representing a batch of images.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="typing.List">List</span>[<span title="torch.Tensor">Tensor</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>List of feature maps at different scales, each with shape (B, C_i, H_i, W_i), where
C_i is the channel dimension and H_i, W_i are the spatial dimensions at scale i. The list is ordered
from highest resolution (fine features) to lowest resolution (coarse features) if return_interm_layers
is True, otherwise contains only the final output.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Examples:</span></p>
<div class="highlight"><pre><span></span><code><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Hiera</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">96</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stages</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">output_features</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">output_features</span><span class="p">:</span>
<span></span><span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">feat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>
<details class="quote">
<summary>Source code in <code>ultralytics/models/sam/modules/encoders.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span>818</span>
<span>819</span>
<span>820</span>
<span>821</span>
<span>822</span>
<span>823</span>
<span>824</span>
<span>825</span>
<span>826</span>
<span>827</span>
<span>828</span>
<span>829</span>
<span>830</span>
<span>831</span>
<span>832</span>
<span>833</span>
<span>834</span>
<span>835</span>
<span>836</span>
<span>837</span>
<span>838</span>
<span>839</span>
<span>840</span>
<span>841</span>
<span>842</span>
<span>843</span>
<span>844</span>
<span>845</span>
<span>846</span>
<span>847</span>
<span>848</span>
<span>849</span>
<span>850</span>
<span>851</span></pre></div></td><td class="code"><div><pre><span></span><code><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span></span><span class="w">    </span><span class="sd">"""</span>
<span></span><span class="sd">    Perform forward pass through Hiera model, extracting multiscale features from input images.</span>
<span></span>
<span></span><span class="sd">    Args:</span>
<span></span><span class="sd">        x (torch.Tensor): Input tensor with shape (B, C, H, W) representing a batch of images.</span>
<span></span>
<span></span><span class="sd">    Returns:</span>
<span></span><span class="sd">        (List[torch.Tensor]): List of feature maps at different scales, each with shape (B, C_i, H_i, W_i), where</span>
<span></span><span class="sd">            C_i is the channel dimension and H_i, W_i are the spatial dimensions at scale i. The list is ordered</span>
<span></span><span class="sd">            from highest resolution (fine features) to lowest resolution (coarse features) if return_interm_layers</span>
<span></span><span class="sd">            is True, otherwise contains only the final output.</span>
<span></span>
<span></span><span class="sd">    Examples:</span>
<span></span><span class="sd">        &gt;&gt;&gt; model = Hiera(embed_dim=96, num_heads=1, stages=(2, 3, 16, 3))</span>
<span></span><span class="sd">        &gt;&gt;&gt; input_tensor = torch.randn(1, 3, 224, 224)</span>
<span></span><span class="sd">        &gt;&gt;&gt; output_features = model(input_tensor)</span>
<span></span><span class="sd">        &gt;&gt;&gt; for feat in output_features:</span>
<span></span><span class="sd">        ...     print(feat.shape)</span>
<span></span><span class="sd">    """</span>
<span></span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span></span>    <span class="c1"># x: (B, H, W, C)</span>
<span></span>
<span></span>    <span class="c1"># Add positional embedding</span>
<span></span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_pos_embed</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">])</span>
<span></span>
<span></span>    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
<span></span>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">blk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">):</span>
<span></span>        <span class="n">x</span> <span class="o">=</span> <span class="n">blk</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span></span>        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_ends</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="ow">or</span> <span class="p">(</span><span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_ends</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_interm_layers</span><span class="p">):</span>
<span></span>            <span class="n">feats</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span></span>            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feats</span><span class="p">)</span>
<span></span>
<span></span>    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div><p><br/><br/></p>
<br/><br/>
<div class="git-info">
<div class="dates-container">
<span class="date-item" title="This page was first created on November 12, 2023">
<span class="hover-item">📅</span> Created 1 year ago
    </span>
<span class="date-item" title="This page was last updated on September 11, 2024">
<span class="hover-item">✏️</span> Updated 11 months ago
    </span>
</div>
<div class="authors-container">
<a class="author-link" href="https://github.com/glenn-jocher" title="glenn-jocher (6 changes)">
<img alt="glenn-jocher" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/26833433?v=4&amp;s=96"/>
</a>
<a class="author-link" href="https://github.com/jk4e" title="jk4e (1 change)">
<img alt="jk4e" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/116908874?v=4&amp;s=96"/>
</a>
<a class="author-link" href="https://github.com/Laughing-q" title="Laughing-q (1 change)">
<img alt="Laughing-q" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/61612323?v=4&amp;s=96"/>
</a>
<a class="author-link" href="https://github.com/Burhan-Q" title="Burhan-Q (1 change)">
<img alt="Burhan-Q" class="hover-item" loading="lazy" src="https://avatars.githubusercontent.com/u/62214284?v=4&amp;s=96"/>
</a>
</div></div><div class="share-buttons">
<button class="share-button hover-item" onclick="window.open('https://twitter.com/intent/tweet?url=https://docs.ultralytics.com/reference/models/sam/modules/encoders', 'TwitterShare', 'width=550,height=680,menubar=no,toolbar=no'); return false;">
<i class="fa-brands fa-x-twitter"></i> Tweet
    </button>
<button class="share-button hover-item linkedin" onclick="window.open('https://www.linkedin.com/shareArticle?url=https://docs.ultralytics.com/reference/models/sam/modules/encoders', 'LinkedinShare', 'width=550,height=730,menubar=no,toolbar=no'); return false;">
<i class="fa-brands fa-linkedin-in"></i> Share
    </button>
</div>
<br/>
</article>
</div>
<script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<button class="md-top md-icon" data-md-component="top" hidden="" type="button">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
</main>
<footer class="md-footer">
<nav aria-label="Footer" class="md-footer__inner md-grid">
<a aria-label="Previous: decoders" class="md-footer__link md-footer__link--prev" href="../decoders/">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
</div>
<div class="md-footer__title">
<span class="md-footer__direction">
                Previous
              </span>
<div class="md-ellipsis">
                decoders
              </div>
</div>
</a>
<a aria-label="Next: memory_attention" class="md-footer__link md-footer__link--next" href="../memory_attention/">
<div class="md-footer__title">
<span class="md-footer__direction">
                Next
              </span>
<div class="md-ellipsis">
                memory_attention
              </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
<div class="md-copyright__highlight">
<a href="https://www.ultralytics.com/" target="_blank">© 2025 Ultralytics Inc.</a> All rights reserved.
    </div>
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
<div class="md-social">
<a class="md-social__link" href="https://github.com/ultralytics" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="https://www.linkedin.com/company/ultralytics/" rel="noopener" target="_blank" title="www.linkedin.com">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="https://x.com/ultralytics" rel="noopener" target="_blank" title="x.com">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="https://youtube.com/ultralytics?sub_confirmation=1" rel="noopener" target="_blank" title="youtube.com">
<svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M549.7 124.1c-6.2-23.7-24.8-42.3-48.3-48.6C458.9 64 288.1 64 288.1 64S117.3 64 74.7 75.5c-23.5 6.3-42 24.9-48.3 48.6C15 167 15 256.4 15 256.4s0 89.4 11.4 132.3c6.3 23.6 24.8 41.5 48.3 47.8C117.3 448 288.1 448 288.1 448s170.8 0 213.4-11.5c23.5-6.3 42-24.2 48.3-47.8 11.4-42.9 11.4-132.3 11.4-132.3s0-89.4-11.4-132.3zM232.2 337.6V175.2l142.7 81.2z" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="https://hub.docker.com/r/ultralytics/ultralytics/" rel="noopener" target="_blank" title="hub.docker.com">
<svg viewbox="0 0 640 512" xmlns="http://www.w3.org/2000/svg"><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="https://pypi.org/project/ultralytics/" rel="noopener" target="_blank" title="pypi.org">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 444.7a20.4 20.4 0 1 1 0-40.7 20.4 20.4 0 1 1 0 40.7M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.6-183.4a20.4 20.4 0 1 1 0 40.8 20.4 20.4 0 1 1 0-40.8" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="https://discord.com/invite/ultralytics" rel="noopener" target="_blank" title="discord.com">
<svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M492.5 69.8c-.2-.3-.4-.6-.8-.7-38.1-17.5-78.4-30-119.7-37.1-.4-.1-.8 0-1.1.1s-.6.4-.8.8c-5.5 9.9-10.5 20.2-14.9 30.6-44.6-6.8-89.9-6.8-134.4 0-4.5-10.5-9.5-20.7-15.1-30.6-.2-.3-.5-.6-.8-.8s-.7-.2-1.1-.2C162.5 39 122.2 51.5 84.1 69c-.3.1-.6.4-.8.7C7.1 183.5-13.8 294.6-3.6 404.2c0 .3.1.5.2.8s.3.4.5.6c44.4 32.9 94 58 146.8 74.2.4.1.8.1 1.1 0s.7-.4.9-.7c11.3-15.4 21.4-31.8 30-48.8.1-.2.2-.5.2-.8s0-.5-.1-.8-.2-.5-.4-.6-.4-.3-.7-.4c-15.8-6.1-31.2-13.4-45.9-21.9-.3-.2-.5-.4-.7-.6s-.3-.6-.3-.9 0-.6.2-.9.3-.5.6-.7c3.1-2.3 6.2-4.7 9.1-7.1.3-.2.6-.4.9-.4s.7 0 1 .1c96.2 43.9 200.4 43.9 295.5 0 .3-.1.7-.2 1-.2s.7.2.9.4c2.9 2.4 6 4.9 9.1 7.2.2.2.4.4.6.7s.2.6.2.9-.1.6-.3.9-.4.5-.6.6c-14.7 8.6-30 15.9-45.9 21.8-.2.1-.5.2-.7.4s-.3.4-.4.7-.1.5-.1.8.1.5.2.8c8.8 17 18.8 33.3 30 48.8.2.3.6.6.9.7s.8.1 1.1 0c52.9-16.2 102.6-41.3 147.1-74.2.2-.2.4-.4.5-.6s.2-.5.2-.8c12.3-126.8-20.5-236.9-86.9-334.5zm-302 267.7c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.4 59.2-52.8 59.2m195.4 0c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.2 59.2-52.8 59.2" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="https://reddit.com/r/ultralytics" rel="noopener" target="_blank" title="reddit.com">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M0 256C0 114.6 114.6 0 256 0s256 114.6 256 256-114.6 256-256 256H37.1c-13.7 0-20.5-16.5-10.9-26.2L75 437C28.7 390.7 0 326.7 0 256m349.6-102.4c23.6 0 42.7-19.1 42.7-42.7s-19.1-42.7-42.7-42.7c-20.6 0-37.8 14.6-41.8 34-34.5 3.7-61.4 33-61.4 68.4v.2c-37.5 1.6-71.8 12.3-99 29.1-10.1-7.8-22.8-12.5-36.5-12.5-33 0-59.8 26.8-59.8 59.8 0 24 14.1 44.6 34.4 54.1 2 69.4 77.6 125.2 170.6 125.2s168.7-55.9 170.6-125.3c20.2-9.6 34.1-30.2 34.1-54 0-33-26.8-59.8-59.8-59.8-13.7 0-26.3 4.6-36.4 12.4-27.4-17-62.1-27.7-100-29.1v-.2c0-25.4 18.9-46.5 43.4-49.9 4.4 18.8 21.3 32.8 41.5 32.8zm-172.5 93.3c16.7 0 29.5 17.6 28.5 39.3s-13.5 29.6-30.3 29.6-31.4-8.8-30.4-30.5S160.3 247 177 247zm190.1 38.3c1 21.7-13.7 30.5-30.4 30.5s-29.3-7.9-30.3-29.6 11.8-39.3 28.5-39.3 31.2 16.6 32.1 38.3zm-48.1 56.7c-10.3 24.6-34.6 41.9-63 41.9s-52.7-17.3-63-41.9c-1.2-2.9.8-6.2 3.9-6.5 18.4-1.9 38.3-2.9 59.1-2.9s40.7 1 59.1 2.9c3.1.3 5.1 3.6 3.9 6.5" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="https://weixin.qq.com/r/mp/LxckPDfEgWr_rXNf90I9" rel="noopener" target="_blank" title="weixin.qq.com">
<svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><path d="M385.2 167.6c6.4 0 12.6.3 18.8 1.1C387.4 90.3 303.3 32 207.7 32 100.5 32 13 104.8 13 197.4c0 53.4 29.3 97.5 77.9 131.6l-19.3 58.6 68-34.1c24.4 4.8 43.8 9.7 68.2 9.7 6.2 0 12.1-.3 18.3-.8-4-12.9-6.2-26.6-6.2-40.8-.1-84.9 72.9-154 165.3-154m-104.5-52.9c14.5 0 24.2 9.7 24.2 24.4 0 14.5-9.7 24.2-24.2 24.2-14.8 0-29.3-9.7-29.3-24.2.1-14.7 14.6-24.4 29.3-24.4m-136.4 48.6c-14.5 0-29.3-9.7-29.3-24.2 0-14.8 14.8-24.4 29.3-24.4 14.8 0 24.4 9.7 24.4 24.4 0 14.6-9.6 24.2-24.4 24.2M563 319.4c0-77.9-77.9-141.3-165.4-141.3-92.7 0-165.4 63.4-165.4 141.3S305 460.7 397.6 460.7c19.3 0 38.9-5.1 58.6-9.9l53.4 29.3-14.8-48.6C534 402.1 563 363.2 563 319.4m-219.1-24.5c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.8 0 24.4 9.7 24.4 19.3 0 10-9.7 19.6-24.4 19.6m107.1 0c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.5 0 24.4 9.7 24.4 19.3.1 10-9.9 19.6-24.4 19.6" fill="currentColor"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<div class="md-progress" data-md-component="progress" role="progressbar"></div>
<script id="__config" type="application/json">{"base": "../../../../..", "features": ["content.action.edit", "content.code.annotate", "content.code.copy", "content.tooltips", "toc.follow", "navigation.top", "navigation.tabs", "navigation.tabs.sticky", "navigation.prune", "navigation.footer", "navigation.tracking", "navigation.instant", "navigation.instant.progress", "navigation.indexes", "navigation.sections", "content.tabs.link"], "search": "../../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
<script src="../../../../../assets/javascripts/bundle.92b07e13.min.js"></script>
<script src="../../../../../javascript/extra.js"></script>
<script src="../../../../../javascript/giscus.js"></script>
<script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
<script src="../../../../../javascript/tablesort.js"></script>
<script>
                    async function copyMarkdownForLLM(button) {
                        const editBtn = document.querySelector('a[title="Edit this page"]');
                        if (!editBtn) return;
                        const originalHTML = button.innerHTML;
                        const checkIcon = '<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 16.17L4.83 12l-1.42 1.41L9 19L21 7l-1.41-1.41L9 16.17z"></path></svg>';
                        // Handle both /blob/ and /tree/ in GitHub URLs
                        let rawUrl = editBtn.href.replace('github.com', 'raw.githubusercontent.com');
                        // Remove /blob/ or /tree/ from the URL
                        rawUrl = rawUrl.replace('/blob/', '/').replace('/tree/', '/');
                        try {
                            const response = await fetch(rawUrl);
                            let markdown = await response.text();
                            // Remove YAML front matter if present
                            if (markdown.startsWith('---')) {
                                const frontMatterEnd = markdown.indexOf('\n---\n', 3);
                                if (frontMatterEnd !== -1) {
                                    markdown = markdown.substring(frontMatterEnd + 5).trim();
                                }
                            }
                            const title = document.querySelector('h1')?.textContent || document.title;
                            const content = `# ${title}\n\nSource: ${window.location.href}\n\n---\n\n${markdown}`;
                            await navigator.clipboard.writeText(content);
                            button.innerHTML = checkIcon + ' Copied!';
                            setTimeout(() => { button.innerHTML = originalHTML; }, 2000);
                        } catch (err) {
                            button.innerHTML = '❌ Failed';
                            setTimeout(() => { button.innerHTML = originalHTML; }, 2000);
                        }
                    }
                    </script></body>
</html>