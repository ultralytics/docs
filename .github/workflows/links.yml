# Ultralytics üöÄ AGPL-3.0 License - https://ultralytics.com/license

# Continuous Integration (CI) GitHub Actions tests broken link checker using https://github.com/lycheeverse/lychee
# Ignores the following status codes to reduce false positives:
#   - 401(Vimeo, 'unauthorized')
#   - 403(OpenVINO, 'forbidden')
#   - 429(Instagram, 'too many requests')
#   - 500(Zenodo, 'cached')
#   - 502(Zenodo, 'bad gateway')
#   - 999(LinkedIn, 'unknown status code')

name: Website links and spellcheck

permissions:
  contents: read

on:
  workflow_dispatch:
    inputs:
      check_links:
        description: 'Check broken links'
        type: boolean
        default: true
      check_spelling:
        description: 'Check spelling'
        type: boolean
        default: true
      check_images:
        description: 'Check large images'
        type: boolean
        default: true
  schedule:
    - cron: "0 0 * * *" # runs at 00:00 UTC every day

jobs:
  Checks:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false # This ensures that if one job fails, the others will still run
      matrix:
        website:
          [www.ultralytics.com, docs.ultralytics.com, handbook.ultralytics.com]
    steps:
      - uses: actions/checkout@v5
        with:
          fetch-depth: 1

      - uses: actions/setup-python@v6
        with:
          python-version: '3.x'

      - uses: astral-sh/setup-uv@v7

      - name: Install lychee and dependencies
        run: |
          curl -sSfL "https://github.com/lycheeverse/lychee/releases/latest/download/lychee-x86_64-unknown-linux-gnu.tar.gz" | sudo tar xz -C /usr/local/bin
          uv pip install --system -r requirements.txt

      - name: Get Website URLs
        run: |
          # Function to parse sitemap URLs
          parse_sitemap() {
            cat - | tr '\n' ' ' | sed 's/<loc>/\n<loc>/g' | grep -oP '(?<=<loc>).*?(?=</loc>)' || true
          }

          # Download initial sitemap and process
          echo "Downloading sitemap..."
          SITEMAP=$(wget -qO- "https://${{ matrix.website }}/sitemap.xml") || { echo "Failed to download sitemap"; exit 1; }
          echo "$SITEMAP" | parse_sitemap > urls.txt

          # Process any subsitemaps if they exist
          if grep -q 'sitemap' urls.txt; then
            echo "Found subsitemaps, processing..."
            grep 'sitemap' urls.txt > subsitemaps.txt
            grep -v 'sitemap' urls.txt > urls.tmp || true
            while read -r submap; do
              echo "Processing submap: $submap"
              SUBMAP_CONTENT=$(wget -qO- "$submap") || { echo "Failed to download submap: $submap"; continue; }
              echo "$SUBMAP_CONTENT" | parse_sitemap >> urls.tmp
            done < subsitemaps.txt
            mv urls.tmp urls.txt || true
          fi

          # Count URLs
          total_urls=$(wc -l < urls.txt)
          echo "Total URLs to be downloaded: $total_urls"

      - name: Download Website
        run: |
          # Set higher wait seconds for discourse community to avoid 429 rate limit errors
          if [ "${{ matrix.website }}" = "community.ultralytics.com" ]; then
            WAIT=1
          else
            WAIT=0.001
          fi

          # Download all URLs
          wget \
          --adjust-extension \
          --reject "*.jpg*,*.jpeg*,*.png*,*.gif*,*.webp*,*.svg*,*.txt" \
          --input-file=urls.txt \
          --no-clobber \
          --no-parent \
          --wait=$WAIT \
          --random-wait \
          --tries=3 \
          --no-verbose \
          --force-directories

      - name: Check image sizes
        if: github.event_name != 'workflow_dispatch' || inputs.check_images
        id: image_sizes
        continue-on-error: true
        run: python utils/check_image_sizes.py "${{ matrix.website }}" "${{ matrix.website }}"

      - name: Check spelling
        if: github.event_name != 'workflow_dispatch' || inputs.check_spelling
        id: codespell
        continue-on-error: true
        run: |
          # Install codespell
          pip install codespell

          # Run codespell on HTML files
          CODESPELL_OUTPUT=$(find ./${{ matrix.website }} -type f -name "*.html" -print0 | xargs -0 codespell \
            --uri-ignore-words-list "*" \
            --ignore-words-list "Hart,wither,Bund,DED,AKS,VAs,RepResNet,iDenfy,WIT,Smoot,EHR,ROUGE,ALS,iTerm,Carmel,FPR,Hach,Calle,ore,COO,MOT,crate,nd,ned,strack,dota,ane,segway,fo,gool,winn,commend,bloc,nam,afterall,skelton,goin,tread" \
            --skip "*.pt,*.pth,*.torchscript,*.onnx,*.tflite,*.pb,*.bin,*.param,*.mlmodel,*.engine,*.npy,*.data*,*.csv,*pnnx*,*venv*,*translat*,*lock*,__pycache__*,*.ico,*.jpg,*.png,*.mp4,*.mov,/runs,/.git,./docs/mkdocs_??.yml" \
            2>&1 || true)

          # Process CODESPELL_OUTPUT 
          MODIFIED_OUTPUT=$(echo "$CODESPELL_OUTPUT" | sed 's#\(.*\):[0-9]*: \(.*\)#\1  \2#; s/\.html//; s#\(.*\)/index  #\1/  #; s#\./##')

          # Prepare the output for Slack with explicit newlines
          ESCAPED_OUTPUT=$(echo "$MODIFIED_OUTPUT" | awk '{printf "%s\\n", $0}')

          echo "CODESPELL_ERRORS<<EOF" >> $GITHUB_ENV
          echo "$ESCAPED_OUTPUT" >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV

          echo "$MODIFIED_OUTPUT"

          if [[ "$CODESPELL_OUTPUT" == *"==>"* ]]; then
            echo "Spelling errors found ‚ö†Ô∏è"
            echo "## üìù Spelling Errors" >> $GITHUB_STEP_SUMMARY
            echo "$MODIFIED_OUTPUT" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi
          echo "No spelling errors found ‚úÖ"

      - name: Check broken links
        if: github.event_name != 'workflow_dispatch' || inputs.check_links
        id: lychee
        uses: ultralytics/actions/retry@main
        with:
          timeout_minutes: 60
          retry_delay_seconds: 900
          retries: 3
          run: |
            # Count successfully downloaded files
            downloaded_files=$(find ${{ matrix.website }} -type f | wc -l)
            echo "Scanning $downloaded_files downloaded pages for broken links..."

            # Create summary.txt with the total page count
            echo "*Results for $downloaded_files pages*" > summary.txt
            echo "" >> summary.txt

            rm -rf .lycheecache
            lychee \
            --scheme 'https' \
            --timeout 60 \
            --insecure \
            --accept 100..=103,200..=299,401,403,429,500,502,504,999 \
            --exclude-all-private \
            --exclude 'https?://(www\.)?(linkedin\.com|twitter\.com|x\.com|instagram\.com|kaggle\.com|tiktok\.com|fonts\.gstatic\.com|fonts\.googleapis\.com|url\.com|tesla\.com|wellfound\.com|fda\.gov|.*\.run\.app|.*\.cloudfunctions\.net|0\.0\.0\.0:5543/predict/from_files)' \
            --exclude-path './**/ci.yml' \
            --github-token ${{ secrets.GITHUB_TOKEN }} \
            --header "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.6478.183 Safari/537.36" \
            --header "Accept: */*" \
            --header "Accept-Language: *" \
            --header "Accept-Encoding: *" \
            --root-dir "$(pwd)/${{ matrix.website }}" \
            './${{ matrix.website }}/**/*.html' | tee -a summary.txt

            # Remove raw append; we'll print the cleaned version instead
            # cat summary.txt >> $GITHUB_STEP_SUMMARY   # CHANGED: removed

            # Simple two-pass approach to remove timeout entries completely
            # First pass: identify page names followed by TIMEOUT and create a pattern file
            grep -B1 "^\[TIMEOUT\]" summary.txt | grep "\.html\]:$" > /tmp/timeout_pages.txt 2>/dev/null || true

            # Second pass: remove TIMEOUT lines, pages that had timeouts, and squeeze blank lines
            ESCAPED_SUMMARY=$(grep -v "^\[TIMEOUT\]" summary.txt | \
              grep -v "Issues found in .* inputs" | \
              grep -F -v -f /tmp/timeout_pages.txt | tr -d '\r' | \
              sed -E 's/[[:space:]]*\|[[:space:]]*Rejected status code[^:]*:[[:space:]]*/ | /g' | \
              cat -s | \
              sed 's/\[//g; s/\]//g; s/\.html//g; s/"/\\"/g' | \
              awk '{printf "%s\\n", $0}')

            # Show the same cleaned output in the Actions summary (matches Slack)
            printf "%b" "$ESCAPED_SUMMARY" >> "$GITHUB_STEP_SUMMARY"

            # Cleanup
            rm -f /tmp/timeout_pages.txt

            echo "SUMMARY<<EOF" >> $GITHUB_ENV
            echo "$ESCAPED_SUMMARY" >> $GITHUB_ENV
            echo "EOF" >> $GITHUB_ENV

            # Raise error if broken links found
            if ! grep -q "0 Errors" summary.txt; then
              exit 1
            fi

      - name: Notify Slack for broken links
        if: always() && steps.lychee.outcome == 'failure' && github.event_name == 'schedule' && github.run_attempt == '1'
        uses: slackapi/slack-github-action@v2.1.1
        with:
          webhook-type: incoming-webhook
          webhook: ${{ matrix.website == 'www.ultralytics.com' && secrets.SLACK_WEBHOOK_URL_WEBSITE || secrets.SLACK_WEBHOOK_URL_YOLO }}
          payload: |
            text: "${{ matrix.website == 'www.ultralytics.com' && '<!subteam^S07ABRC4MLN>' || '<!subteam^S082BPCRAJ3>' }} GitHub Actions: Broken links found for *${{ matrix.website }}* ‚ùå\n\n*Action:* https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n*Author:* ${{ github.actor }}\n\n${{ env.SUMMARY }}\n"

      - name: Notify Slack for spelling errors
        if: always() && steps.codespell.outcome == 'failure' && github.event_name == 'schedule' && github.run_attempt == '1'
        uses: slackapi/slack-github-action@v2.1.1
        with:
          webhook-type: incoming-webhook
          webhook: ${{ matrix.website == 'www.ultralytics.com' && secrets.SLACK_WEBHOOK_URL_WEBSITE || secrets.SLACK_WEBHOOK_URL_YOLO }}
          payload: |
            text: "${{ matrix.website == 'www.ultralytics.com' && '<!subteam^S07ABRC4MLN>' || '<!subteam^S082BPCRAJ3>' }} GitHub Actions: Spelling errors found for *${{ matrix.website }}* ‚ùå\n\n*Action:* https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n*Author:* ${{ github.actor }}\n\n*üìù Spelling Errors:*\n${{ env.CODESPELL_ERRORS }}\n"

      - name: Notify Slack for large images
        if: always() && steps.image_sizes.outcome == 'failure' && github.event_name == 'schedule' && github.run_attempt == '1'
        uses: slackapi/slack-github-action@v2.1.1
        with:
          webhook-type: incoming-webhook
          webhook: ${{ matrix.website == 'www.ultralytics.com' && secrets.SLACK_WEBHOOK_URL_WEBSITE || secrets.SLACK_WEBHOOK_URL_YOLO }}
          payload: |
            text: "${{ matrix.website == 'www.ultralytics.com' && '<!subteam^S07ABRC4MLN>' || '<!subteam^S082BPCRAJ3>' }} GitHub Actions: Large images found for *${{ matrix.website }}* üì¶\n\n*Action:* https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n*Author:* ${{ github.actor }}\n\n${{ env.IMAGE_RESULTS }}\n"
